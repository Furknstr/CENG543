{"doc_id": "1706.03762", "para_id": 0, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works."}
{"doc_id": "1706.03762", "para_id": 1, "text": "Aidan N. Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.edu"}
{"doc_id": "1706.03762", "para_id": 2, "text": "Łukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com"}
{"doc_id": "1706.03762", "para_id": 3, "text": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data."}
{"doc_id": "1706.03762", "para_id": 4, "text": "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research."}
{"doc_id": "1706.03762", "para_id": 5, "text": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."}
{"doc_id": "1706.03762", "para_id": 6, "text": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15]."}
{"doc_id": "1706.03762", "para_id": 7, "text": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains."}
{"doc_id": "1706.03762", "para_id": 8, "text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network."}
{"doc_id": "1706.03762", "para_id": 9, "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs."}
{"doc_id": "1706.03762", "para_id": 10, "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2."}
{"doc_id": "1706.03762", "para_id": 11, "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22]."}
{"doc_id": "1706.03762", "para_id": 12, "text": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34]."}
{"doc_id": "1706.03762", "para_id": 13, "text": "To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"doc_id": "1706.03762", "para_id": 14, "text": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next."}
{"doc_id": "1706.03762", "para_id": 15, "text": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively."}
{"doc_id": "1706.03762", "para_id": 16, "text": "Encoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512."}
{"doc_id": "1706.03762", "para_id": 17, "text": "Decoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i."}
{"doc_id": "1706.03762", "para_id": 18, "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum"}
{"doc_id": "1706.03762", "para_id": 19, "text": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel."}
{"doc_id": "1706.03762", "para_id": 20, "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key."}
{"doc_id": "1706.03762", "para_id": 21, "text": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues."}
{"doc_id": "1706.03762", "para_id": 22, "text": "In practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:"}
{"doc_id": "1706.03762", "para_id": 23, "text": "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n√dk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code."}
{"doc_id": "1706.03762", "para_id": 24, "text": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n√dk ."}
{"doc_id": "1706.03762", "para_id": 25, "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional"}
{"doc_id": "1706.03762", "para_id": 26, "text": "4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk."}
{"doc_id": "1706.03762", "para_id": 27, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2."}
{"doc_id": "1706.03762", "para_id": 28, "text": "Multi-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this."}
{"doc_id": "1706.03762", "para_id": 29, "text": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O"}
{"doc_id": "1706.03762", "para_id": 30, "text": "where headi = Attention(QW Q\ni , KW K\ni , V W V\ni )"}
{"doc_id": "1706.03762", "para_id": 31, "text": "Where the projections are parameter matrices W Q\ni\n∈Rdmodel×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O ∈Rhdv×dmodel."}
{"doc_id": "1706.03762", "para_id": 32, "text": "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."}
{"doc_id": "1706.03762", "para_id": 33, "text": "The Transformer uses multi-head attention in three different ways:"}
{"doc_id": "1706.03762", "para_id": 34, "text": "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9]."}
{"doc_id": "1706.03762", "para_id": 35, "text": "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder."}
{"doc_id": "1706.03762", "para_id": 36, "text": "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2."}
{"doc_id": "1706.03762", "para_id": 37, "text": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between."}
{"doc_id": "1706.03762", "para_id": 38, "text": "While the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048."}
{"doc_id": "1706.03762", "para_id": 39, "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel."}
{"doc_id": "1706.03762", "para_id": 40, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention."}
{"doc_id": "1706.03762", "para_id": 41, "text": "Layer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r · n · d)\nO(1)\nO(n/r)"}
{"doc_id": "1706.03762", "para_id": 42, "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9]."}
{"doc_id": "1706.03762", "para_id": 43, "text": "In this work, we use sine and cosine functions of different frequencies:"}
{"doc_id": "1706.03762", "para_id": 44, "text": "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos."}
{"doc_id": "1706.03762", "para_id": 45, "text": "We also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training."}
{"doc_id": "1706.03762", "para_id": 46, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata."}
{"doc_id": "1706.03762", "para_id": 47, "text": "One is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required."}
{"doc_id": "1706.03762", "para_id": 48, "text": "The third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types."}
{"doc_id": "1706.03762", "para_id": 49, "text": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence"}
{"doc_id": "1706.03762", "para_id": 50, "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work."}
{"doc_id": "1706.03762", "para_id": 51, "text": "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model."}
{"doc_id": "1706.03762", "para_id": 52, "text": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences."}
{"doc_id": "1706.03762", "para_id": 53, "text": "This section describes the training regime for our models."}
{"doc_id": "1706.03762", "para_id": 54, "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens."}
{"doc_id": "1706.03762", "para_id": 55, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days)."}
{"doc_id": "1706.03762", "para_id": 56, "text": "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:"}
{"doc_id": "1706.03762", "para_id": 57, "text": "lrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\n(3)"}
{"doc_id": "1706.03762", "para_id": 58, "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000."}
{"doc_id": "1706.03762", "para_id": 59, "text": "We employ three types of regularization during training:"}
{"doc_id": "1706.03762", "para_id": 60, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost."}
{"doc_id": "1706.03762", "para_id": 61, "text": "EN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 · 1020"}
{"doc_id": "1706.03762", "para_id": 62, "text": "GNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 · 1020\n1.1 · 1021"}
{"doc_id": "1706.03762", "para_id": 63, "text": "ConvS2S Ensemble [9]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021"}
{"doc_id": "1706.03762", "para_id": 64, "text": "Residual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1."}
{"doc_id": "1706.03762", "para_id": 65, "text": "Label Smoothing\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."}
{"doc_id": "1706.03762", "para_id": 66, "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models."}
{"doc_id": "1706.03762", "para_id": 67, "text": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3."}
{"doc_id": "1706.03762", "para_id": 68, "text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38]."}
{"doc_id": "1706.03762", "para_id": 69, "text": "Table 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5."}
{"doc_id": "1706.03762", "para_id": 70, "text": "To evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the"}
{"doc_id": "1706.03762", "para_id": 71, "text": "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."}
{"doc_id": "1706.03762", "para_id": 72, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities."}
{"doc_id": "1706.03762", "para_id": 73, "text": "N\ndmodel\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n×106"}
{"doc_id": "1706.03762", "para_id": 74, "text": "1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4"}
{"doc_id": "1706.03762", "para_id": 75, "text": "2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90"}
{"doc_id": "1706.03762", "para_id": 76, "text": "0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213"}
{"doc_id": "1706.03762", "para_id": 77, "text": "development set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3."}
{"doc_id": "1706.03762", "para_id": 78, "text": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"doc_id": "1706.03762", "para_id": 79, "text": "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model."}
{"doc_id": "1706.03762", "para_id": 80, "text": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]."}
{"doc_id": "1706.03762", "para_id": 81, "text": "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 82, "text": "We performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we"}
{"doc_id": "1706.03762", "para_id": 83, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)"}
{"doc_id": "1706.03762", "para_id": 84, "text": "Parser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3"}
{"doc_id": "1706.03762", "para_id": 85, "text": "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 86, "text": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8]."}
{"doc_id": "1706.03762", "para_id": 87, "text": "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences."}
{"doc_id": "1706.03762", "para_id": 88, "text": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention."}
{"doc_id": "1706.03762", "para_id": 89, "text": "For translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles."}
{"doc_id": "1706.03762", "para_id": 90, "text": "We are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours."}
{"doc_id": "1706.03762", "para_id": 91, "text": "The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor."}
{"doc_id": "1706.03762", "para_id": 92, "text": "Acknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration."}
{"doc_id": "1706.03762", "para_id": 93, "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016."}
{"doc_id": "1706.03762", "para_id": 94, "text": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014."}
{"doc_id": "1706.03762", "para_id": 95, "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017."}
{"doc_id": "1706.03762", "para_id": 96, "text": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1706.03762", "para_id": 97, "text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014."}
{"doc_id": "1706.03762", "para_id": 98, "text": "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016."}
{"doc_id": "1706.03762", "para_id": 99, "text": "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014."}
{"doc_id": "1706.03762", "para_id": 100, "text": "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016."}
{"doc_id": "1706.03762", "para_id": 101, "text": "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."}
{"doc_id": "1706.03762", "para_id": 102, "text": "[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013."}
{"doc_id": "1706.03762", "para_id": 103, "text": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016."}
{"doc_id": "1706.03762", "para_id": 104, "text": "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001."}
{"doc_id": "1706.03762", "para_id": 105, "text": "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997."}
{"doc_id": "1706.03762", "para_id": 106, "text": "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009."}
{"doc_id": "1706.03762", "para_id": 107, "text": "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1706.03762", "para_id": 108, "text": "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016."}
{"doc_id": "1706.03762", "para_id": 109, "text": "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016."}
{"doc_id": "1706.03762", "para_id": 110, "text": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017."}
{"doc_id": "1706.03762", "para_id": 111, "text": "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017."}
{"doc_id": "1706.03762", "para_id": 112, "text": "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "1706.03762", "para_id": 113, "text": "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017."}
{"doc_id": "1706.03762", "para_id": 114, "text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017."}
{"doc_id": "1706.03762", "para_id": 115, "text": "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015."}
{"doc_id": "1706.03762", "para_id": 116, "text": "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015."}
{"doc_id": "1706.03762", "para_id": 117, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993."}
{"doc_id": "1706.03762", "para_id": 118, "text": "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006."}
{"doc_id": "1706.03762", "para_id": 119, "text": "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016."}
{"doc_id": "1706.03762", "para_id": 120, "text": "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1706.03762", "para_id": 121, "text": "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006."}
{"doc_id": "1706.03762", "para_id": 122, "text": "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016."}
{"doc_id": "1706.03762", "para_id": 123, "text": "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1706.03762", "para_id": 124, "text": "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1706.03762", "para_id": 125, "text": "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014."}
{"doc_id": "1706.03762", "para_id": 126, "text": "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015."}
{"doc_id": "1706.03762", "para_id": 127, "text": "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014."}
{"doc_id": "1706.03762", "para_id": 128, "text": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."}
{"doc_id": "1706.03762", "para_id": 129, "text": "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015."}
{"doc_id": "1706.03762", "para_id": 130, "text": "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016."}
{"doc_id": "1706.03762", "para_id": 131, "text": "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016."}
{"doc_id": "1706.03762", "para_id": 132, "text": "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013."}
{"doc_id": "1706.03762", "para_id": 133, "text": "Figure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color."}
{"doc_id": "1706.03762", "para_id": 134, "text": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word."}
{"doc_id": "1706.03762", "para_id": 135, "text": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks."}
{"doc_id": "1810.04805", "para_id": 0, "text": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding"}
{"doc_id": "1810.04805", "para_id": 1, "text": "Jacob Devlin\nMing-Wei Chang\nKenton Lee\nKristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com"}
{"doc_id": "1810.04805", "para_id": 2, "text": "There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and ﬁne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches.\nThe ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder\nRepresentations\nfrom\nTransformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953).\nThe\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked"}
{"doc_id": "1810.04805", "para_id": 3, "text": "We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations."}
{"doc_id": "1810.04805", "para_id": 4, "text": "BERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement)."}
{"doc_id": "1810.04805", "para_id": 5, "text": "Language model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016)."}
{"doc_id": "1810.04805", "para_id": 6, "text": "word based only on its context.\nUnlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na “next sentence prediction” task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:"}
{"doc_id": "1810.04805", "para_id": 7, "text": "These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014).\nTo train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speciﬁc architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels."}
{"doc_id": "1810.04805", "para_id": 8, "text": "• We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs."}
{"doc_id": "1810.04805", "para_id": 9, "text": "• We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speciﬁc architectures."}
{"doc_id": "1810.04805", "para_id": 10, "text": "• BERT advances the state of the art for eleven\nNLP tasks.\nThe code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert."}
{"doc_id": "1810.04805", "para_id": 11, "text": "There is a long history of pre-training general lan-\nguage representations, and we brieﬂy review the\nmost widely-used approaches in this section."}
{"doc_id": "1810.04805", "para_id": 12, "text": "As with the feature-based approaches, the ﬁrst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text\n(Col-\nlobert and Weston, 2008).\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\nﬁne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch.\nAt least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\net al., 2018a).\nLeft-to-right language model-"}
{"doc_id": "1810.04805", "para_id": 13, "text": "Learning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signiﬁcant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013)."}
{"doc_id": "1810.04805", "para_id": 14, "text": "Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers)."}
{"doc_id": "1810.04805", "para_id": 15, "text": "ing and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015)."}
{"doc_id": "1810.04805", "para_id": 16, "text": "mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture."}
{"doc_id": "1810.04805", "para_id": 17, "text": "Model Architecture\nBERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2"}
{"doc_id": "1810.04805", "para_id": 18, "text": "There has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014)."}
{"doc_id": "1810.04805", "para_id": 19, "text": "In this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3"}
{"doc_id": "1810.04805", "para_id": 20, "text": "We introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning.\nDur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks.\nFor ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-"}
{"doc_id": "1810.04805", "para_id": 21, "text": "We primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4"}
{"doc_id": "1810.04805", "para_id": 22, "text": "1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n4We note that in the literature the bidirectional Trans-"}
{"doc_id": "1810.04805", "para_id": 23, "text": "Input/Output Representations\nTo make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., ⟨Question, Answer ⟩) in one token sequence.\nThroughout this work, a “sentence” can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A “sequence” refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The ﬁrst\ntoken of every sequence is always a special clas-\nsiﬁcation token ([CLS]). The ﬁnal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classiﬁcation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the ﬁnal hidden\nvector of the special [CLS] token as C ∈RH,\nand the ﬁnal hidden vector for the ith input token\nas Ti ∈RH.\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2."}
{"doc_id": "1810.04805", "para_id": 24, "text": "In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a “masked\nLM” (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the ﬁnal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nﬁne-tuning, since the [MASK] token does not ap-\npear during ﬁne-tuning. To mitigate this, we do\nnot always replace “masked” words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTi will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2."}
{"doc_id": "1810.04805", "para_id": 25, "text": "Task #2:\nNext Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speciﬁcally,\nwhen choosing the sentences A and B for each pre-\ntraining example, 50% of the time B is the actual\nnext sentence that follows A (labeled as IsNext),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext).\nAs we show\nin Figure 1, C is used for next sentence predic-\ntion (NSP).5 Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very beneﬁcial to both QA and NLI. 6"}
{"doc_id": "1810.04805", "para_id": 26, "text": "Unlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1."}
{"doc_id": "1810.04805", "para_id": 27, "text": "Task #1: Masked LM\nIntuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model.\nUnfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly “see itself”, and the model could trivially\npredict the target word in a multi-layered context."}
{"doc_id": "1810.04805", "para_id": 28, "text": "5The ﬁnal model achieves 97%-98% accuracy on NSP.\n6The vector C is not a meaningful sentence representation\nwithout ﬁne-tuning, since it was trained with NSP."}
{"doc_id": "1810.04805", "para_id": 29, "text": "former is often referred to as a “Transformer encoder” while\nthe left-context-only version is referred to as a “Transformer\ndecoder” since it can be used for text generation."}
{"doc_id": "1810.04805", "para_id": 30, "text": "[CLS]\nhe\nlikes\nplay\n##ing\n[SEP]\nmy\ndog\nis\ncute\n[SEP]\nInput"}
{"doc_id": "1810.04805", "para_id": 31, "text": "E[CLS]\nEhe\nElikes\nEplay\nE##ing\nE[SEP]\nEmy\nEdog\nEis\nEcute\nE[SEP]\nToken\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 32, "text": "EA\nEB\nEB\nEB\nEB\nEB\nEA\nEA\nEA\nEA\nEA\nSegment\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 33, "text": "E0\nE6\nE7\nE8\nE9\nE10\nE1\nE2\nE3\nE4\nE5\nPosition\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 34, "text": "Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings."}
{"doc_id": "1810.04805", "para_id": 35, "text": "(4) a degenerate text-∅pair in text classiﬁcation\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classiﬁcation, such as en-\ntailment or sentiment analysis.\nCompared to pre-training, ﬁne-tuning is rela-\ntively inexpensive. All of the results in the pa-\nper can be replicated in at most 1 hour on a sin-\ngle Cloud TPU, or a few hours on a GPU, starting\nfrom the exact same pre-trained model.7 We de-\nscribe the task-speciﬁc details in the correspond-\ning subsections of Section 4. More details can be\nfound in Appendix A.5."}
{"doc_id": "1810.04805", "para_id": 36, "text": "The NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters."}
{"doc_id": "1810.04805", "para_id": 37, "text": "Pre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words).\nFor Wikipedia we extract only the text passages\nand ignore lists, tables, and headers. It is criti-\ncal to use a document-level corpus rather than a\nshufﬂed sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to\nextract long contiguous sequences."}
{"doc_id": "1810.04805", "para_id": 38, "text": "In this section, we present BERT ﬁne-tuning re-\nsults on 11 NLP tasks."}
{"doc_id": "1810.04805", "para_id": 39, "text": "Fine-tuning is straightforward since the self-\nattention mechanism in the Transformer al-\nlows BERT to model many downstream tasks—\nwhether they involve single text or text pairs—by\nswapping out the appropriate inputs and outputs.\nFor applications involving text pairs, a common\npattern is to independently encode text pairs be-\nfore applying bidirectional cross attention, such\nas Parikh et al. (2016); Seo et al. (2017). BERT\ninstead uses the self-attention mechanism to unify\nthese two stages, as encoding a concatenated text\npair with self-attention effectively includes bidi-\nrectional cross attention between two sentences.\nFor each task, we simply plug in the task-\nspeciﬁc inputs and outputs into BERT and ﬁne-\ntune all the parameters end-to-end.\nAt the in-\nput, sentence A and sentence B from pre-training\nare analogous to (1) sentence pairs in paraphras-\ning, (2) hypothesis-premise pairs in entailment, (3)\nquestion-passage pairs in question answering, and"}
{"doc_id": "1810.04805", "para_id": 40, "text": "The General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. Detailed descriptions of GLUE datasets are\nincluded in Appendix B.1.\nTo ﬁne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the ﬁnal hid-\nden vector C ∈RH corresponding to the ﬁrst\ninput token ([CLS]) as the aggregate representa-\ntion. The only new parameters introduced during\nﬁne-tuning are classiﬁcation layer weights W ∈\nRK×H, where K is the number of labels. We com-\npute a standard classiﬁcation loss with C and W,\ni.e., log(softmax(CW T ))."}
{"doc_id": "1810.04805", "para_id": 41, "text": "7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%.\n8See (10) in https://gluebenchmark.com/faq."}
{"doc_id": "1810.04805", "para_id": 42, "text": "System\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n392k\n363k\n108k\n67k\n8.5k\n5.7k\n3.5k\n2.5k\n-\nPre-OpenAI SOTA\n80.6/80.1\n66.1\n82.3\n93.2\n35.0\n81.0\n86.0\n61.7\n74.0\nBiLSTM+ELMo+Attn\n76.4/76.1\n64.8\n79.8\n90.4\n36.0\n73.3\n84.9\n56.8\n71.0\nOpenAI GPT\n82.1/81.4\n70.3\n87.4\n91.3\n45.4\n80.0\n82.3\n56.0\n75.1\nBERTBASE\n84.6/83.4\n71.2\n90.5\n93.5\n52.1\n85.8\n88.9\n66.4\n79.6\nBERTLARGE\n86.7/85.9\n72.1\n92.7\n94.9\n60.5\n86.5\n89.3\n70.1\n82.1"}
{"doc_id": "1810.04805", "para_id": 43, "text": "Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 44, "text": "Wikipedia containing the answer, the task is to\npredict the answer text span in the passage.\nAs shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the A embedding and the passage using\nthe B embedding. We only introduce a start vec-\ntor S ∈RH and an end vector E ∈RH during\nﬁne-tuning. The probability of word i being the\nstart of the answer span is computed as a dot prod-\nuct between Ti and S followed by a softmax over\nall of the words in the paragraph: Pi =\neS·Ti\nP\nj eS·Tj ."}
{"doc_id": "1810.04805", "para_id": 45, "text": "We use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERTLARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9"}
{"doc_id": "1810.04805", "para_id": 46, "text": "Results are presented in Table 1.\nBoth\nBERTBASE and BERTLARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERTBASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERTLARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERTLARGE signiﬁcantly outper-\nforms BERTBASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2."}
{"doc_id": "1810.04805", "para_id": 47, "text": "The analogous formula is used for the end of the\nanswer span. The score of a candidate span from\nposition i to position j is deﬁned as S·Ti + E·Tj,\nand the maximum scoring span where j ≥i is\nused as a prediction. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions. We ﬁne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32.\nTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11 and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\net al., 2017) befor ﬁne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA ﬁne-"}
{"doc_id": "1810.04805", "para_id": 48, "text": "The\nStanford\nQuestion\nAnswering\nDataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016).\nGiven a question and a passage from"}
{"doc_id": "1810.04805", "para_id": 49, "text": "9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERTBASE and BERTLARGE.\n10https://gluebenchmark.com/leaderboard"}
{"doc_id": "1810.04805", "para_id": 50, "text": "11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication."}
{"doc_id": "1810.04805", "para_id": 51, "text": "ESIM+GloVe\n51.9 52.7\nESIM+ELMo\n59.1 59.2\nOpenAI GPT\n-\n78.0"}
{"doc_id": "1810.04805", "para_id": 52, "text": "Top Leaderboard Systems (Dec 10th, 2018)\nHuman\n-\n-\n82.3 91.2\n#1 Ensemble - nlnet\n-\n-\n86.0 91.7\n#2 Ensemble - QANet\n-\n-\n84.5 90.5"}
{"doc_id": "1810.04805", "para_id": 53, "text": "Published\nBiDAF+ELMo (Single)\n-\n85.6\n-\n85.8\nR.M. Reader (Ensemble)\n81.2 87.9 82.3 88.5"}
{"doc_id": "1810.04805", "para_id": 54, "text": "Human (expert)†\n-\n85.0\nHuman (5 annotations)†\n-\n88.0"}
{"doc_id": "1810.04805", "para_id": 55, "text": "Ours\nBERTBASE (Single)\n80.8 88.5\n-\n-\nBERTLARGE (Single)\n84.1 90.9\n-\n-\nBERTLARGE (Ensemble)\n85.8 91.8\n-\n-\nBERTLARGE (Sgl.+TriviaQA)\n84.2 91.1 85.1 91.8\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2"}
{"doc_id": "1810.04805", "para_id": 56, "text": "Table 4: SWAG Dev and Test accuracies. †Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper."}
{"doc_id": "1810.04805", "para_id": 57, "text": "ˆ\nsi,j = maxj≥iS·Ti + E·Tj. We predict a non-null\nanswer when ˆ\nsi,j > snull + τ, where the thresh-\nold τ is selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system."}
{"doc_id": "1810.04805", "para_id": 58, "text": "Table 2:\nSQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds."}
{"doc_id": "1810.04805", "para_id": 59, "text": "Top Leaderboard Systems (Dec 10th, 2018)\nHuman\n86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net)\n-\n-\n74.8 78.0\n#2 Single - nlnet\n-\n-\n74.2 77.1"}
{"doc_id": "1810.04805", "para_id": 60, "text": "Published\nunet (Ensemble)\n-\n-\n71.4 74.9\nSLQA+ (Single)\n-\n71.4 74.4"}
{"doc_id": "1810.04805", "para_id": 61, "text": "The Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen ﬁne-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speciﬁc parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation C denotes a score for each choice\nwhich is normalized with a softmax layer.\nWe ﬁne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERTLARGE out-\nperforms the authors’ baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%."}
{"doc_id": "1810.04805", "para_id": 62, "text": "Table 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 63, "text": "tuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12"}
{"doc_id": "1810.04805", "para_id": 64, "text": "The SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull =\nS·C + E·C to the score of the best non-null span"}
{"doc_id": "1810.04805", "para_id": 65, "text": "In this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional"}
{"doc_id": "1810.04805", "para_id": 66, "text": "12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\nthat contain at least one of the provided possible answers."}
{"doc_id": "1810.04805", "para_id": 67, "text": "results are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."}
{"doc_id": "1810.04805", "para_id": 68, "text": "Dev Set\nTasks\nMNLI-m QNLI MRPC SST-2 SQuAD\n(Acc)\n(Acc)\n(Acc)\n(Acc)\n(F1)"}
{"doc_id": "1810.04805", "para_id": 69, "text": "BERTBASE\n84.4\n88.4\n86.7\n92.7\n88.5\nNo NSP\n83.9\n84.9\n86.5\n92.6\n87.9\nLTR & No NSP\n82.1\n84.3\n77.5\n92.1\n77.8\n+ BiLSTM\n82.1\n84.1\n75.7\n91.6\n84.9"}
{"doc_id": "1810.04805", "para_id": 70, "text": "Table 5: Ablation over the pre-training tasks using the\nBERTBASE architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning."}
{"doc_id": "1810.04805", "para_id": 71, "text": "In this section, we explore the effect of model size\non ﬁne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERTBASE\ncontains 110M parameters and BERTLARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6.\nHowever, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented"}
{"doc_id": "1810.04805", "para_id": 72, "text": "We demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERTBASE:"}
{"doc_id": "1810.04805", "para_id": 73, "text": "No NSP: A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task.\nLTR & No NSP: A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at ﬁne-tuning, because removing\nit introduced a pre-train/ﬁne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our ﬁne-tuning scheme.\nWe ﬁrst examine the impact brought by the NSP\ntask.\nIn Table 5, we show that removing NSP\nhurts performance signiﬁcantly on QNLI, MNLI,\nand SQuAD 1.1. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing “No NSP” to “LTR & No NSP”. The LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD.\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsigniﬁcantly improve results on SQuAD, but the"}
{"doc_id": "1810.04805", "para_id": 74, "text": "mixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach — we hypothesize that when the\nmodel is ﬁne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeciﬁc models can beneﬁt from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."}
{"doc_id": "1810.04805", "para_id": 75, "text": "ELMo (Peters et al., 2018a)\n95.7\n92.2\nCVT (Clark et al., 2018)\n-\n92.6\nCSE (Akbik et al., 2018)\n-\n93.1"}
{"doc_id": "1810.04805", "para_id": 76, "text": "Fine-tuning approach\nBERTLARGE\n96.6\n92.8\nBERTBASE\n96.4\n92.4"}
{"doc_id": "1810.04805", "para_id": 77, "text": "Feature-based approach (BERTBASE)\nEmbeddings\n91.0\n-\nSecond-to-Last Hidden\n95.6\n-\nLast Hidden\n94.9\n-\nWeighted Sum Last Four Hidden\n95.9\n-\nConcat Last Four Hidden\n96.1\n-\nWeighted Sum All 12 Layers\n95.5\n-"}
{"doc_id": "1810.04805", "para_id": 78, "text": "Table 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters."}
{"doc_id": "1810.04805", "para_id": 79, "text": "All of the BERT results presented so far have used\nthe ﬁne-tuning approach, where a simple classiﬁ-\ncation layer is added to the pre-trained model, and\nall parameters are jointly ﬁne-tuned on a down-\nstream task. However, the feature-based approach,\nwhere ﬁxed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speciﬁc model architecture to be added.\nSecond, there are major computational beneﬁts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation.\nIn this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF"}
{"doc_id": "1810.04805", "para_id": 80, "text": "layer in the output. We use the representation of\nthe ﬁrst sub-token as the input to the token-level\nclassiﬁer over the NER label set."}
{"doc_id": "1810.04805", "para_id": 81, "text": "To ablate the ﬁne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without ﬁne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classiﬁcation layer."}
{"doc_id": "1810.04805", "para_id": 82, "text": "Results are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind ﬁne-tuning the entire model. This\ndemonstrates that BERT is effective for both ﬁne-\ntuning and feature-based approaches."}
{"doc_id": "1810.04805", "para_id": 83, "text": "Recent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks."}
{"doc_id": "1810.04805", "para_id": 84, "text": "3\n768\n12\n5.84\n77.9\n79.8\n88.4\n6\n768\n3\n5.24\n80.6\n82.2\n90.7\n6\n768\n12\n4.68\n81.9\n84.8\n91.3\n12\n768\n12\n3.99\n84.4\n86.7\n92.9\n12 1024\n16\n3.54\n85.7\n86.9\n93.3\n24 1024\n16\n3.23\n86.6\n87.8\n93.7"}
{"doc_id": "1810.04805", "para_id": 85, "text": "Table 6:\nAblation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. “LM (ppl)” is the masked LM perplexity\nof held-out training data."}
{"doc_id": "1810.04805", "para_id": 86, "text": "Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018.\nSemi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914–\n1925."}
{"doc_id": "1810.04805", "para_id": 87, "text": "Alan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638–1649."}
{"doc_id": "1810.04805", "para_id": 88, "text": "Ronan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning.\nIn Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160–167. ACM."}
{"doc_id": "1810.04805", "para_id": 89, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018.\nCharacter-level lan-\nguage modeling with deeper self-attention.\narXiv\npreprint arXiv:1808.04444."}
{"doc_id": "1810.04805", "para_id": 90, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics."}
{"doc_id": "1810.04805", "para_id": 91, "text": "Rie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817–1853."}
{"doc_id": "1810.04805", "para_id": 92, "text": "Luisa Bentivogli,\nBernardo Magnini,\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe ﬁfth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST."}
{"doc_id": "1810.04805", "para_id": 93, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087."}
{"doc_id": "1810.04805", "para_id": 94, "text": "John Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120–128. Association for Computa-\ntional Linguistics."}
{"doc_id": "1810.04805", "para_id": 95, "text": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09."}
{"doc_id": "1810.04805", "para_id": 96, "text": "William B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005)."}
{"doc_id": "1810.04805", "para_id": 97, "text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics."}
{"doc_id": "1810.04805", "para_id": 98, "text": "William Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via ﬁlling in\nthe . arXiv preprint arXiv:1801.07736."}
{"doc_id": "1810.04805", "para_id": 99, "text": "Peter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics, 18(4):467–479."}
{"doc_id": "1810.04805", "para_id": 100, "text": "Dan Hendrycks and Kevin Gimpel. 2016.\nBridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415."}
{"doc_id": "1810.04805", "para_id": 101, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancou-\nver, Canada. Association for Computational Lin-\nguistics."}
{"doc_id": "1810.04805", "para_id": 102, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics."}
{"doc_id": "1810.04805", "para_id": 103, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL. Association for Computational Linguistics."}
{"doc_id": "1810.04805", "para_id": 104, "text": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005."}
{"doc_id": "1810.04805", "para_id": 105, "text": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. 2018.\nReinforced\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI."}
{"doc_id": "1810.04805", "para_id": 106, "text": "Yacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557."}
{"doc_id": "1810.04805", "para_id": 107, "text": "Christopher Clark and Matt Gardner. 2018.\nSimple\nand effective multi-paragraph reading comprehen-\nsion. In ACL."}
{"doc_id": "1810.04805", "para_id": 108, "text": "Matthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b.\nDissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499–1509."}
{"doc_id": "1810.04805", "para_id": 109, "text": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL."}
{"doc_id": "1810.04805", "para_id": 110, "text": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302."}
{"doc_id": "1810.04805", "para_id": 111, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI."}
{"doc_id": "1810.04805", "para_id": 112, "text": "Quoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188–1196."}
{"doc_id": "1810.04805", "para_id": 113, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392."}
{"doc_id": "1810.04805", "para_id": 114, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47."}
{"doc_id": "1810.04805", "para_id": 115, "text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nﬂow for machine comprehension. In ICLR."}
{"doc_id": "1810.04805", "para_id": 116, "text": "Lajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations.\nIn International Conference on Learning\nRepresentations."}
{"doc_id": "1810.04805", "para_id": 117, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642."}
{"doc_id": "1810.04805", "para_id": 118, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS."}
{"doc_id": "1810.04805", "para_id": 119, "text": "Oren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL."}
{"doc_id": "1810.04805", "para_id": 120, "text": "Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018.\nU-net:\nMachine reading comprehension\nwith unanswerable questions.\narXiv preprint\narXiv:1810.06638."}
{"doc_id": "1810.04805", "para_id": 121, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111–3119. Curran Associates,\nInc."}
{"doc_id": "1810.04805", "para_id": 122, "text": "Wilson L Taylor. 1953.\nCloze procedure:\nA new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433."}
{"doc_id": "1810.04805", "para_id": 123, "text": "Erik F Tjong Kim Sang and Fien De Meulder.\n2003.\nIntroduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL."}
{"doc_id": "1810.04805", "para_id": 124, "text": "Andriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model.\nIn\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21, pages 1081–1088. Curran As-\nsociates, Inc."}
{"doc_id": "1810.04805", "para_id": 125, "text": "Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL ’10, pages 384–394."}
{"doc_id": "1810.04805", "para_id": 126, "text": "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In EMNLP."}
{"doc_id": "1810.04805", "para_id": 127, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010."}
{"doc_id": "1810.04805", "para_id": 128, "text": "Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532–\n1543."}
{"doc_id": "1810.04805", "para_id": 129, "text": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008.\nExtracting and\ncomposing robust features with denoising autoen-\ncoders.\nIn Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103.\nACM."}
{"doc_id": "1810.04805", "para_id": 130, "text": "Matthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL."}
{"doc_id": "1810.04805", "para_id": 131, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In NAACL."}
{"doc_id": "1810.04805", "para_id": 132, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform"}
{"doc_id": "1810.04805", "para_id": 133, "text": "• Additional details for our experiments are\npresented in Appendix B; and"}
{"doc_id": "1810.04805", "para_id": 134, "text": "for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355."}
{"doc_id": "1810.04805", "para_id": 135, "text": "• Additional ablation studies are presented in\nAppendix C."}
{"doc_id": "1810.04805", "para_id": 136, "text": "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics."}
{"doc_id": "1810.04805", "para_id": 137, "text": "We present additional ablation studies for\nBERT including:"}
{"doc_id": "1810.04805", "para_id": 138, "text": "– Effect of Number of Training Steps; and\n– Ablation for Different Masking Proce-\ndures."}
{"doc_id": "1810.04805", "para_id": 139, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471."}
{"doc_id": "1810.04805", "para_id": 140, "text": "Adina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nNAACL."}
{"doc_id": "1810.04805", "para_id": 141, "text": "We provide examples of the pre-training tasks in\nthe following."}
{"doc_id": "1810.04805", "para_id": 142, "text": "Masked LM and the Masking Procedure\nAs-\nsuming the unlabeled sentence is\nmy dog is"}
{"doc_id": "1810.04805", "para_id": 143, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe,\nMohammad Norouzi,\nWolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin Gao,\nKlaus\nMacherey, et al. 2016.\nGoogle’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144."}
{"doc_id": "1810.04805", "para_id": 144, "text": "hairy, and during the random masking procedure\nwe chose the 4-th token (which corresponding to"}
{"doc_id": "1810.04805", "para_id": 145, "text": "hairy), our masking procedure can be further il-\nlustrated by"}
{"doc_id": "1810.04805", "para_id": 146, "text": "• 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy →"}
{"doc_id": "1810.04805", "para_id": 147, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320–3328."}
{"doc_id": "1810.04805", "para_id": 148, "text": "• 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy →my"}
{"doc_id": "1810.04805", "para_id": 149, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018.\nQANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR."}
{"doc_id": "1810.04805", "para_id": 150, "text": "• 10% of the time:\nKeep the word un-\nchanged, e.g., my dog is hairy →my dog"}
{"doc_id": "1810.04805", "para_id": 151, "text": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP)."}
{"doc_id": "1810.04805", "para_id": 152, "text": "is hairy. The purpose of this is to bias the\nrepresentation towards the actual observed\nword."}
{"doc_id": "1810.04805", "para_id": 153, "text": "The advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\nery input token.\nAdditionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model’s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model"}
{"doc_id": "1810.04805", "para_id": 154, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books.\nIn Proceedings of the IEEE\ninternational conference on computer vision, pages\n19–27."}
{"doc_id": "1810.04805", "para_id": 155, "text": "Appendix for “BERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding”"}
{"doc_id": "1810.04805", "para_id": 156, "text": "• Additional implementation details for BERT\nare presented in Appendix A;"}
{"doc_id": "1810.04805", "para_id": 157, "text": "Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach."}
{"doc_id": "1810.04805", "para_id": 158, "text": "epochs over the 3.3 billion word corpus.\nWe\nuse Adam with learning rate of 1e-4, β1 = 0.9,\nβ2 = 0.999, L2 weight decay of 0.01, learning\nrate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu, following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERTBASE was performed on 4\nCloud TPUs in Pod conﬁguration (16 TPU chips\ntotal).13 Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings."}
{"doc_id": "1810.04805", "para_id": 159, "text": "to converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost."}
{"doc_id": "1810.04805", "para_id": 160, "text": "Next Sentence Prediction\nThe next sentence\nprediction task can be illustrated in the following\nexamples."}
{"doc_id": "1810.04805", "para_id": 161, "text": "To generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as “sentences” even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The ﬁrst sentence receives the A\nembedding and the second receives the B embed-\nding. 50% of the time B is the actual next sentence\nthat follows A and 50% of the time it is a random\nsentence, which is done for the “next sentence pre-\ndiction” task. They are sampled such that the com-\nbined length is ≤512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40"}
{"doc_id": "1810.04805", "para_id": 162, "text": "For ﬁne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speciﬁc, but we found the following range\nof possible values to work well across all tasks:"}
{"doc_id": "1810.04805", "para_id": 163, "text": "13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html"}
{"doc_id": "1810.04805", "para_id": 164, "text": "• Learning rate (Adam): 5e-5, 3e-5, 2e-5\n• Number of epochs: 2, 3, 4"}
{"doc_id": "1810.04805", "para_id": 165, "text": "To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable."}
{"doc_id": "1810.04805", "para_id": 166, "text": "We also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set."}
{"doc_id": "1810.04805", "para_id": 167, "text": "A.5\nIllustrations of Fine-tuning on Different\nTasks"}
{"doc_id": "1810.04805", "para_id": 168, "text": "The illustration of ﬁne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speciﬁc\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe ﬁgure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classiﬁcation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences."}
{"doc_id": "1810.04805", "para_id": 169, "text": "Here we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:"}
{"doc_id": "1810.04805", "para_id": 170, "text": "B.1\nDetailed Descriptions for the GLUE\nBenchmark Experiments."}
{"doc_id": "1810.04805", "para_id": 171, "text": "Our\nGLUE\nresults\nin\nTable1\nare\nobtained\nfrom\nhttps://gluebenchmark.com/\nleaderboard\nand\nhttps://blog.\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):"}
{"doc_id": "1810.04805", "para_id": 172, "text": "MNLI\nMulti-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classiﬁ-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the ﬁrst one."}
{"doc_id": "1810.04805", "para_id": 173, "text": "• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords)."}
{"doc_id": "1810.04805", "para_id": 174, "text": "QQP\nQuora Question Pairs is a binary classiﬁ-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018)."}
{"doc_id": "1810.04805", "para_id": 175, "text": "• GPT uses a sentence separator ([SEP]) and\nclassiﬁer token ([CLS]) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP], [CLS] and sentence A/B embed-\ndings during pre-training."}
{"doc_id": "1810.04805", "para_id": 176, "text": "QNLI\nQuestion Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classiﬁcation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer."}
{"doc_id": "1810.04805", "para_id": 177, "text": "• GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words."}
{"doc_id": "1810.04805", "para_id": 178, "text": "• GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set."}
{"doc_id": "1810.04805", "para_id": 179, "text": "Figure 4: Illustrations of Fine-tuning BERT on Different Tasks."}
{"doc_id": "1810.04805", "para_id": 180, "text": "SST-2\nThe Stanford Sentiment Treebank is a\nbinary single-sentence classiﬁcation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013)."}
{"doc_id": "1810.04805", "para_id": 181, "text": "for whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005)."}
{"doc_id": "1810.04805", "para_id": 182, "text": "RTE\nRecognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14"}
{"doc_id": "1810.04805", "para_id": 183, "text": "CoLA\nThe Corpus of Linguistic Acceptability is\na binary single-sentence classiﬁcation task, where\nthe goal is to predict whether an English sentence\nis linguistically “acceptable” or not (Warstadt\net al., 2018)."}
{"doc_id": "1810.04805", "para_id": 184, "text": "WNLI\nWinograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that’s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-"}
{"doc_id": "1810.04805", "para_id": 185, "text": "STS-B\nThe Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning."}
{"doc_id": "1810.04805", "para_id": 186, "text": "14Note that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq"}
{"doc_id": "1810.04805", "para_id": 187, "text": "MRPC\nMicrosoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations"}
{"doc_id": "1810.04805", "para_id": 188, "text": "Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions."}
{"doc_id": "1810.04805", "para_id": 189, "text": "Figure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:"}
{"doc_id": "1810.04805", "para_id": 190, "text": "1. Question:\nDoes BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps."}
{"doc_id": "1810.04805", "para_id": 191, "text": "MASK SAME\nRND\nMNLI\nNER\nFine-tune Fine-tune Feature-based"}
{"doc_id": "1810.04805", "para_id": 192, "text": "80%\n10%\n10%\n84.2\n95.4\n94.9\n100%\n0%\n0%\n84.3\n94.9\n94.0\n80%\n0%\n20%\n84.1\n95.2\n94.6\n80%\n20%\n0%\n84.4\n95.2\n94.7\n0%\n20%\n80%\n83.7\n94.8\n94.6\n0%\n0% 100%\n83.6\n94.9\n94.6"}
{"doc_id": "1810.04805", "para_id": 193, "text": "2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately."}
{"doc_id": "1810.04805", "para_id": 194, "text": "Table 8: Ablation over different masking strategies."}
{"doc_id": "1810.04805", "para_id": 195, "text": "The results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well."}
{"doc_id": "1810.04805", "para_id": 196, "text": "In Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies."}
{"doc_id": "1810.04805", "para_id": 197, "text": "Figure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."}
{"doc_id": "1910.01108", "para_id": 0, "text": "DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter"}
{"doc_id": "1910.01108", "para_id": 1, "text": "Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co"}
{"doc_id": "1910.01108", "para_id": 2, "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study."}
{"doc_id": "1910.01108", "para_id": 3, "text": "The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1 on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks."}
{"doc_id": "1910.01108", "para_id": 4, "text": "Figure 1: Parameter counts of several recently released\npretrained language models."}
{"doc_id": "1910.01108", "para_id": 5, "text": "The trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption."}
{"doc_id": "1910.01108", "para_id": 6, "text": "1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)"}
{"doc_id": "1910.01108", "para_id": 7, "text": "In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices."}
{"doc_id": "1910.01108", "para_id": 8, "text": "Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances."}
{"doc_id": "1910.01108", "para_id": 9, "text": "We have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019]."}
{"doc_id": "1910.01108", "para_id": 10, "text": "Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble of models."}
{"doc_id": "1910.01108", "para_id": 11, "text": "In supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these \"near-zero\" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3."}
{"doc_id": "1910.01108", "para_id": 12, "text": "Training loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce = P"}
{"doc_id": "1910.01108", "para_id": 13, "text": "i ti ∗log(si) where ti (resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature: pi =\nexp(zi/T )\nP"}
{"doc_id": "1910.01108", "para_id": 14, "text": "j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i.\nThe same temperature T is applied to the student and the teacher at training time, while at inference,\nT is set to 1 to recover a standard softmax."}
{"doc_id": "1910.01108", "para_id": 15, "text": "The ﬁnal training objective is a linear combination of the distillation loss Lce with the supervised\ntraining loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors."}
{"doc_id": "1910.01108", "para_id": 16, "text": "Student architecture In the present work, the student - DistilBERT - has the same general architec-\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear\nlayer and layer normalisation) are highly optimized in modern linear algebra frameworks and our\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\nfactors like the number of layers. Thus we focus on reducing the number of layers."}
{"doc_id": "1910.01108", "para_id": 17, "text": "Student initialization In addition to the previously described optimization and architectural choices,\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to\nconverge. Taking advantage of the common dimensionality between teacher and student networks,\nwe initialize the student from the teacher by taking one layer out of two."}
{"doc_id": "1910.01108", "para_id": 18, "text": "2https://github.com/huggingface/transformers\n3E.g.\nBERT-base’s predictions for a masked token in \"I think this is the beginning of a\nbeautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions\n(future, story, world...)."}
{"doc_id": "1910.01108", "para_id": 19, "text": "Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds."}
{"doc_id": "1910.01108", "para_id": 20, "text": "Model\nScore\nCoLA\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nWNLI"}
{"doc_id": "1910.01108", "para_id": 21, "text": "ELMo\n68.7\n44.1\n68.6\n76.6\n71.1\n86.2\n53.4\n91.5\n70.4\n56.3\nBERT-base\n79.5\n56.3\n86.7\n88.6\n91.8\n89.6\n69.3\n92.7\n89.0\n53.5\nDistilBERT\n77.0\n51.3\n82.2\n87.5\n89.2\n88.5\n59.9\n91.3\n86.9\n56.3"}
{"doc_id": "1910.01108", "para_id": 22, "text": "Table 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning."}
{"doc_id": "1910.01108", "para_id": 23, "text": "Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster.\nInference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1."}
{"doc_id": "1910.01108", "para_id": 24, "text": "BERT-base\n93.46\n81.2/88.5\nDistilBERT\n92.82\n77.7/85.8\nDistilBERT (D)\n-\n79.1/86.9"}
{"doc_id": "1910.01108", "para_id": 25, "text": "Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective."}
{"doc_id": "1910.01108", "para_id": 26, "text": "Data and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100."}
{"doc_id": "1910.01108", "para_id": 27, "text": "General Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4"}
{"doc_id": "1910.01108", "para_id": 28, "text": "The results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters."}
{"doc_id": "1910.01108", "para_id": 29, "text": "Downstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016])."}
{"doc_id": "1910.01108", "para_id": 30, "text": "As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT."}
{"doc_id": "1910.01108", "para_id": 31, "text": "We also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a"}
{"doc_id": "1910.01108", "para_id": 32, "text": "4We use jiant [Wang et al., 2019] to compute the baseline."}
{"doc_id": "1910.01108", "para_id": 33, "text": "Table 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization."}
{"doc_id": "1910.01108", "para_id": 34, "text": "∅- Lcos - Lmlm\n-2.96\nLce - ∅- Lmlm\n-1.46\nLce - Lcos - ∅\n-0.31\nTriple loss + random weights initialization\n-3.69"}
{"doc_id": "1910.01108", "para_id": 35, "text": "teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model."}
{"doc_id": "1910.01108", "para_id": 36, "text": "To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT."}
{"doc_id": "1910.01108", "para_id": 37, "text": "On device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5."}
{"doc_id": "1910.01108", "para_id": 38, "text": "In this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance."}
{"doc_id": "1910.01108", "para_id": 39, "text": "Task-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original\npretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the abla-\ntion study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional\ndistillation signal."}
{"doc_id": "1910.01108", "para_id": 40, "text": "Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using\nmulti-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\nto learn a compact question answering model from a set of large question answering models. An\napplication of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us\nby pre-training a multilingual model from scratch solely through distillation. However, as shown in\nthe ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads\nto substantial gains."}
{"doc_id": "1910.01108", "para_id": 41, "text": "Other compression techniques have been studied to compress large models. Recent developments\nin weights pruning reveal that it is possible to remove some heads in the self-attention at test time\nwithout signiﬁcantly degrading the performance Michel et al. [2019]. Some layers can be reduced\nto one head. A separate line of study leverages quantization to derive smaller models (Gupta et al.\n[2015]). Pruning and quantization are orthogonal to the present work."}
{"doc_id": "1910.01108", "para_id": 42, "text": "5https://github.com/huggingface/swift-coreml-transformers"}
{"doc_id": "1910.01108", "para_id": 43, "text": "We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications."}
{"doc_id": "1910.01108", "para_id": 44, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT, 2018."}
{"doc_id": "1910.01108", "para_id": 45, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019."}
{"doc_id": "1910.01108", "para_id": 46, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv,\nabs/1907.11692, 2019."}
{"doc_id": "1910.01108", "para_id": 47, "text": "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019."}
{"doc_id": "1910.01108", "para_id": 48, "text": "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019."}
{"doc_id": "1910.01108", "para_id": 49, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "1910.01108", "para_id": 50, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019."}
{"doc_id": "1910.01108", "para_id": 51, "text": "Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006."}
{"doc_id": "1910.01108", "para_id": 52, "text": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv,\nabs/1503.02531, 2015."}
{"doc_id": "1910.01108", "para_id": 53, "text": "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27, 2015."}
{"doc_id": "1910.01108", "para_id": 54, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR, 2018."}
{"doc_id": "1910.01108", "para_id": 55, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL, 2018."}
{"doc_id": "1910.01108", "para_id": 56, "text": "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/, 2019."}
{"doc_id": "1910.01108", "para_id": 57, "text": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011."}
{"doc_id": "1910.01108", "para_id": 58, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP, 2016."}
{"doc_id": "1910.01108", "para_id": 59, "text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019."}
{"doc_id": "1910.01108", "para_id": 60, "text": "Debajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv, abs/1904.00796, 2019."}
{"doc_id": "1910.01108", "para_id": 61, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv, abs/1908.08962, 2019."}
{"doc_id": "1910.01108", "para_id": 62, "text": "Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv, abs/1904.09636, 2019."}
{"doc_id": "1910.01108", "para_id": 63, "text": "Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical\nbert models for sequence labeling. In EMNLP-IJCNLP, 2019."}
{"doc_id": "1910.01108", "para_id": 64, "text": "Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019."}
{"doc_id": "1910.01108", "para_id": 65, "text": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\nnumerical precision. In ICML, 2015."}
{"doc_id": "1909.11942", "para_id": 0, "text": "ALBERT: A LITE BERT\nFOR SELF-SUPERVISED\nLEARNING OF LANGUAGE REPRESENTATIONS"}
{"doc_id": "1909.11942", "para_id": 1, "text": "Zhenzhong Lan1\nMingda Chen2∗\nSebastian Goodman1\nKevin Gimpel2"}
{"doc_id": "1909.11942", "para_id": 2, "text": "1Google Research\n2Toyota Technological Institute at Chicago"}
{"doc_id": "1909.11942", "para_id": 3, "text": "{lanzhzh, seabass, piyushsharma, rsoricut}@google.com\n{mchen, kgimpel}@ttic.edu"}
{"doc_id": "1909.11942", "para_id": 4, "text": "Increasing model size when pretraining natural language representations often re-\nsults in improved performance on downstream tasks. However, at some point fur-\nther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two parameter-\nreduction techniques to lower memory consumption and increase the training\nspeed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to\nthe original BERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new state-of-the-art\nresults on the GLUE, RACE, and SQuAD benchmarks while having fewer param-\neters compared to BERT-large. The code and the pretrained models are available\nat https://github.com/google-research/ALBERT."}
{"doc_id": "1909.11942", "para_id": 5, "text": "Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard &\nRuder, 2018) has led to a series of breakthroughs in language representation learning. Many non-\ntrivial NLP tasks, including those that have limited training data, have greatly beneﬁted from these\npre-trained models. One of the most compelling signs of these breakthroughs is the evolution of ma-\nchine performance on a reading comprehension task designed for middle and high-school English\nexams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and for-\nmulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest\npublished result reports their model performance at 83.2% (Liu et al., 2019); the work we present\nhere pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to\nour current ability to build high-performance pretrained language representations."}
{"doc_id": "1909.11942", "para_id": 6, "text": "Evidence from these improvements reveals that a large network is of crucial importance for achiev-\ning state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common\npractice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al.,\n2019) for real applications. Given the importance of model size, we ask: Is having better NLP\nmodels as easy as having larger models?"}
{"doc_id": "1909.11942", "para_id": 7, "text": "An obstacle to answering this question is the memory limitations of available hardware. Given that\ncurrent state-of-the-art models often have hundreds of millions or even billions of parameters, it is\neasy to hit these limitations as we try to scale our models. Training speed can also be signiﬁcantly\nhampered in distributed training, as the communication overhead is directly proportional to the\nnumber of parameters in the model."}
{"doc_id": "1909.11942", "para_id": 8, "text": "Existing solutions to the aforementioned problems include model parallelization (Shazeer et al.,\n2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017)."}
{"doc_id": "1909.11942", "para_id": 9, "text": "∗Work done as an intern at Google Research, driving data processing and downstream task evaluations."}
{"doc_id": "1909.11942", "para_id": 10, "text": "These solutions address the memory limitation problem, but not the communication overhead. In\nthis paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT)\narchitecture that has signiﬁcantly fewer parameters than a traditional BERT architecture."}
{"doc_id": "1909.11942", "para_id": 11, "text": "ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling\npre-trained models. The ﬁrst one is a factorized embedding parameterization. By decomposing\nthe large vocabulary embedding matrix into two small matrices, we separate the size of the hidden\nlayers from the size of vocabulary embedding. This separation makes it easier to grow the hidden\nsize without signiﬁcantly increasing the parameter size of the vocabulary embeddings. The second\ntechnique is cross-layer parameter sharing. This technique prevents the parameter from growing\nwith the depth of the network. Both techniques signiﬁcantly reduce the number of parameters for\nBERT without seriously hurting performance, thus improving parameter-efﬁciency. An ALBERT\nconﬁguration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\nThe parameter reduction techniques also act as a form of regularization that stabilizes the training\nand helps with generalization."}
{"doc_id": "1909.11942", "para_id": 12, "text": "To further improve the performance of ALBERT, we also introduce a self-supervised loss for\nsentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed\nto address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction\n(NSP) loss proposed in the original BERT."}
{"doc_id": "1909.11942", "para_id": 13, "text": "As a result of these design decisions, we are able to scale up to much larger ALBERT conﬁgurations\nthat still have fewer parameters than BERT-large but achieve signiﬁcantly better performance. We\nestablish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks\nfor natural language understanding. Speciﬁcally, we push the RACE accuracy to 89.4%, the GLUE\nbenchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2."}
{"doc_id": "1909.11942", "para_id": 14, "text": "2.1\nSCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE"}
{"doc_id": "1909.11942", "para_id": 15, "text": "Learning representations of natural language has been shown to be useful for a wide range of NLP\ntasks and has been widely adopted (Mikolov et al., 2013; Le & Mikolov, 2014; Dai & Le, 2015; Pe-\nters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019). One of the most signiﬁcant changes\nin the last two years is the shift from pre-training word embeddings, whether standard (Mikolov\net al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018),\nto full-network pre-training followed by task-speciﬁc ﬁne-tuning (Dai & Le, 2015; Radford et al.,\n2018; Devlin et al., 2019). In this line of work, it is often shown that larger model size improves\nperformance. For example, Devlin et al. (2019) show that across three selected natural language\nunderstanding tasks, using larger hidden size, more hidden layers, and more attention heads always\nleads to better performance. However, they stop at a hidden size of 1024, presumably because of the\nmodel size and computation cost problems."}
{"doc_id": "1909.11942", "para_id": 16, "text": "It is difﬁcult to experiment with large models due to computational constraints, especially in terms\nof GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of\nmillions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen\net al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be\nsublinear at the cost of an extra forward pass. Gomez et al. (2017) propose a way to reconstruct each\nlayer’s activations from the next layer so that they do not need to store the intermediate activations.\nBoth methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed\nto use model parallelization to train a giant model. In contrast, our parameter-reduction techniques\nreduce memory consumption and increase training speed."}
{"doc_id": "1909.11942", "para_id": 17, "text": "The idea of sharing parameters across layers has been previously explored with the Transformer\narchitecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoder-\ndecoder tasks rather than the pretraining/ﬁnetuning setting. Different from our observations, De-\nhghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer,\nUT) get better performance on language modeling and subject-verb agreement than the standard"}
{"doc_id": "1909.11942", "para_id": 18, "text": "transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for trans-\nformer networks and show that DQE can reach an equilibrium point for which the input embedding\nand the output embedding of a certain layer stay the same. Our observations show that our em-\nbeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing\ntransformer with the standard one, which further increases the number of parameters of the standard\ntransformer."}
{"doc_id": "1909.11942", "para_id": 19, "text": "ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments\nof text. Several researchers have experimented with pretraining objectives that similarly relate to\ndiscourse coherence. Coherence and cohesion in discourse have been widely studied and many\nphenomena have been identiﬁed that connect neighboring text segments (Hobbs, 1979; Halliday &\nHasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skip-\nthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using\nan encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence\nembedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017)\nand predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most\nsimilar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are\nlearned in order to determine the ordering of two consecutive sentences. Unlike most of the above\nwork, however, our loss is deﬁned on textual segments rather than sentences. BERT (Devlin et al.,\n2019) uses a loss based on predicting whether the second segment in a pair has been swapped\nwith a segment from another document. We compare to this loss in our experiments and ﬁnd that\nsentence ordering is a more challenging pretraining task and more useful for certain downstream\ntasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive\nsegments of text, but they combine it with the original next sentence prediction in a three-way\nclassiﬁcation task rather than empirically comparing the two."}
{"doc_id": "1909.11942", "para_id": 20, "text": "In this section, we present the design decisions for ALBERT and provide quantiﬁed comparisons\nagainst corresponding conﬁgurations of the original BERT architecture (Devlin et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 21, "text": "The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer en-\ncoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the\nBERT notation conventions and denote the vocabulary embedding size as E, the number of encoder\nlayers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/ﬁlter\nsize to be 4H and the number of attention heads to be H/64."}
{"doc_id": "1909.11942", "para_id": 22, "text": "There are three main contributions that ALBERT makes over the design choices of BERT."}
{"doc_id": "1909.11942", "para_id": 23, "text": "Factorized embedding parameterization.\nIn BERT, as well as subsequent modeling improve-\nments such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding\nsize E is tied with the hidden layer size H, i.e., E ≡H. This decision appears suboptimal for both\nmodeling and practical reasons, as follows."}
{"doc_id": "1909.11942", "para_id": 24, "text": "From a modeling perspective, WordPiece embeddings are meant to learn context-independent repre-\nsentations, whereas hidden-layer embeddings are meant to learn context-dependent representations.\nAs experiments with context length indicate (Liu et al., 2019), the power of BERT-like represen-\ntations comes from the use of context to provide the signal for learning such context-dependent\nrepresentations. As such, untying the WordPiece embedding size E from the hidden layer size H\nallows us to make a more efﬁcient usage of the total model parameters as informed by modeling\nneeds, which dictate that H ≫E."}
{"doc_id": "1909.11942", "para_id": 25, "text": "From a practical perspective, natural language processing usually require the vocabulary size V to\nbe large.1 If E ≡H, then increasing H increases the size of the embedding matrix, which has size"}
{"doc_id": "1909.11942", "para_id": 26, "text": "1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000."}
{"doc_id": "1909.11942", "para_id": 27, "text": "V ×E. This can easily result in a model with billions of parameters, most of which are only updated\nsparsely during training."}
{"doc_id": "1909.11942", "para_id": 28, "text": "Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them\ninto two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of\nsize H, we ﬁrst project them into a lower dimensional embedding space of size E, and then project\nit to the hidden space. By using this decomposition, we reduce the embedding parameters from\nO(V × H) to O(V × E + E × H). This parameter reduction is signiﬁcant when H ≫E. We\nchoose to use the same E for all word pieces because they are much more evenly distributed across\ndocuments compared to whole-word embedding, where having different embedding size (Grave\net al. (2017); Baevski & Auli (2018); Dai et al. (2019) ) for different words is important."}
{"doc_id": "1909.11942", "para_id": 29, "text": "Cross-layer parameter sharing.\nFor ALBERT, we propose cross-layer parameter sharing as an-\nother way to improve parameter efﬁciency. There are multiple ways to share parameters, e.g., only\nsharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters.\nThe default decision for ALBERT is to share all parameters across layers. All our experiments\nuse this default decision unless otherwise speciﬁed. We compare this design decision against other\nstrategies in our experiments in Sec. 4.5."}
{"doc_id": "1909.11942", "para_id": 30, "text": "Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and\nBai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our\nobservations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al.\n(2019) show that their DQEs reach an equilibrium point for which the input and output embedding\nof a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show\nthat our embeddings are oscillating rather than converging."}
{"doc_id": "1909.11942", "para_id": 31, "text": "Figure 1: The L2 distances and cosine similarity (in terms of degree) of the input and output embed-\nding of each layer for BERT-large and ALBERT-large."}
{"doc_id": "1909.11942", "para_id": 32, "text": "Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each\nlayer, using BERT-large and ALBERT-large conﬁgurations (see Table 1). We observe that the tran-\nsitions from layer to layer are much smoother for ALBERT than for BERT. These results show that\nweight-sharing has an effect on stabilizing network parameters. Although there is a drop for both\nmetrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows\nthat the solution space for ALBERT parameters is very different from the one found by DQE."}
{"doc_id": "1909.11942", "para_id": 33, "text": "Inter-sentence coherence loss.\nIn addition to the masked language modeling (MLM) loss (De-\nvlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a\nbinary classiﬁcation loss for predicting whether two segments appear consecutively in the original\ntext, as follows: positive examples are created by taking consecutive segments from the training\ncorpus; negative examples are created by pairing segments from different documents; positive and\nnegative examples are sampled with equal probability. The NSP objective was designed to improve\nperformance on downstream tasks, such as natural language inference, that require reasoning about\nthe relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al.,\n2019) found NSP’s impact unreliable and decided to eliminate it, a decision supported by an im-\nprovement in downstream task performance across several tasks."}
{"doc_id": "1909.11942", "para_id": 34, "text": "We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difﬁculty as a task,\nas compared to MLM. As formulated, NSP conﬂates topic prediction and coherence prediction in a"}
{"doc_id": "1909.11942", "para_id": 35, "text": "Model\nParameters\nLayers\nHidden\nEmbedding\nParameter-sharing"}
{"doc_id": "1909.11942", "para_id": 36, "text": "BERT\nbase\n108M\n12\n768\n768\nFalse\nlarge\n334M\n24\n1024\n1024\nFalse"}
{"doc_id": "1909.11942", "para_id": 37, "text": "base\n12M\n12\n768\n128\nTrue\nlarge\n18M\n24\n1024\n128\nTrue\nxlarge\n60M\n24\n2048\n128\nTrue\nxxlarge\n235M\n12\n4096\n128\nTrue"}
{"doc_id": "1909.11942", "para_id": 38, "text": "Table 1: The conﬁgurations of the main BERT and ALBERT models analyzed in this paper."}
{"doc_id": "1909.11942", "para_id": 39, "text": "single task2. However, topic prediction is easier to learn compared to coherence prediction, and also\noverlaps more with what is learned using the MLM loss."}
{"doc_id": "1909.11942", "para_id": 40, "text": "We maintain that inter-sentence modeling is an important aspect of language understanding, but we\npropose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order pre-\ndiction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence\ncoherence. The SOP loss uses as positive examples the same technique as BERT (two consecu-\ntive segments from the same document), and as negative examples the same two consecutive seg-\nments but with their order swapped. This forces the model to learn ﬁner-grained distinctions about\ndiscourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the\nSOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at random-\nbaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, pre-\nsumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently\nimprove downstream task performance for multi-sentence encoding tasks."}
{"doc_id": "1909.11942", "para_id": 41, "text": "We present the differences between BERT and ALBERT models with comparable hyperparameter\nsettings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller\nparameter size compared to corresponding BERT models."}
{"doc_id": "1909.11942", "para_id": 42, "text": "For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M ver-\nsus 334M. An ALBERT-xlarge conﬁguration with H = 2048 has only 60M parameters and an\nALBERT-xxlarge conﬁguration with H = 4096 has 233M parameters, i.e., around 70% of BERT-\nlarge’s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network\nbecause a 24-layer network (with the same conﬁguration) obtains similar results but is computation-\nally more expensive."}
{"doc_id": "1909.11942", "para_id": 43, "text": "This improvement in parameter efﬁciency is the most important advantage of ALBERT’s design\nchoices. Before we can quantify this advantage, we need to introduce our experimental setup in\nmore detail."}
{"doc_id": "1909.11942", "para_id": 44, "text": "To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in\nusing the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretrain-\ning baseline models. These two corpora consist of around 16GB of uncompressed text. We format\nour inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1, x1,2 · · · and x2 = x1,1, x1,2 · · · are\ntwo segments.3 We always limit the maximum input length to 512, and randomly generate input\nsequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of\n30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 45, "text": "2Since a negative example is constructed using material from a different document, the negative-example\nsegment is misaligned both from a topic and from a coherence perspective.\n3A segment is usually comprised of more than one natural sentence, which has been shown to beneﬁt\nperformance by Liu et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 46, "text": "We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the\nlength of each n-gram mask selected randomly. The probability for the length n is given by"}
{"doc_id": "1909.11942", "para_id": 47, "text": "We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a\n3-gram of complete words, such as “White House correspondents”)."}
{"doc_id": "1909.11942", "para_id": 48, "text": "All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate\n0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise speciﬁed. Train-\ning was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512,\ndepending on model size."}
{"doc_id": "1909.11942", "para_id": 49, "text": "The experimental setup described in this section is used for all of our own versions of BERT as well\nas ALBERT models, unless otherwise speciﬁed."}
{"doc_id": "1909.11942", "para_id": 50, "text": "To monitor the training progress, we create a development set based on the development sets from\nSQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and\nsentence classiﬁcation tasks. Note that we only use this set to check how the model is converging;\nit has not been used in a way that would affect the performance of any downstream evaluation, such\nas via model selection."}
{"doc_id": "1909.11942", "para_id": 51, "text": "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular bench-\nmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018),\ntwo versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018),\nand the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For com-\npleteness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019),\nwe perform early stopping on the development sets, on which we report all comparisons except for\nour ﬁnal comparisons based on the task leaderboards, for which we also report test set results. For\nGLUE datasets that have large variances on the dev set, we report median over 5 runs."}
{"doc_id": "1909.11942", "para_id": 52, "text": "We are now ready to quantify the impact of the design choices described in Sec. 3, speciﬁcally the\nones around parameter efﬁciency. The improvement in parameter efﬁciency showcases the most\nimportant advantage of ALBERT’s design choices, as shown in Table 2: with only around 70% of\nBERT-large’s parameters, ALBERT-xxlarge achieves signiﬁcant improvements over BERT-large, as\nmeasured by the difference on development set scores for several representative downstream tasks:\nSQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%)."}
{"doc_id": "1909.11942", "para_id": 53, "text": "Another interesting observation is the speed of data throughput at training time under the same train-\ning conﬁguration (same number of TPUs). Because of less communication and fewer computations,\nALBERT models have higher data throughput compared to their corresponding BERT models. If we\nuse BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating\nthrough the data while ALBERT-xxlarge is about 3 times slower because of the larger structure."}
{"doc_id": "1909.11942", "para_id": 54, "text": "Next, we perform ablation experiments that quantify the individual contribution of each of the design\nchoices for ALBERT."}
{"doc_id": "1909.11942", "para_id": 55, "text": "Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base\nconﬁguration setting (see Table 1), using the same set of representative downstream tasks. Under\nthe non-shared condition (BERT-style), larger embedding sizes give better performance, but not by"}
{"doc_id": "1909.11942", "para_id": 56, "text": "Model\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nSpeedup"}
{"doc_id": "1909.11942", "para_id": 57, "text": "BERT\nbase\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3\n4.7x\nlarge\n334M\n92.2/85.5\n85.0/82.2\n86.6\n93.0\n73.9\n85.2\n1.0"}
{"doc_id": "1909.11942", "para_id": 58, "text": "base\n12M\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\n5.6x\nlarge\n18M\n90.6/83.9\n82.3/79.4\n83.5\n91.7\n68.5\n82.4\n1.7x\nxlarge\n60M\n92.5/86.1\n86.1/83.1\n86.4\n92.4\n74.8\n85.5\n0.6x\nxxlarge\n235M\n94.1/88.3\n88.1/85.1\n88.0\n95.2\n82.3\n88.7\n0.3x"}
{"doc_id": "1909.11942", "para_id": 59, "text": "Table 2: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps.\nHere and everywhere else, the Avg column is computed by averaging the scores of the downstream\ntasks to its left (the two numbers of F1 and EM for each SQuAD are ﬁrst averaged)."}
{"doc_id": "1909.11942", "para_id": 60, "text": "much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the\nbest. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary\nstep to do further scaling."}
{"doc_id": "1909.11942", "para_id": 61, "text": "Model\nE\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg"}
{"doc_id": "1909.11942", "para_id": 62, "text": "64\n87M\n89.9/82.9\n80.1/77.8\n82.9\n91.5\n66.7\n81.3\n128\n89M\n89.9/82.8\n80.3/77.3\n83.7\n91.5\n67.9\n81.7\n256\n93M\n90.2/83.2\n80.3/77.4\n84.1\n91.9\n67.3\n81.8\n768\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3"}
{"doc_id": "1909.11942", "para_id": 63, "text": "64\n10M\n88.7/81.4\n77.5/74.8\n80.8\n89.4\n63.5\n79.0\n128\n12M\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\n256\n16M\n88.8/81.5\n79.1/76.3\n81.5\n90.3\n63.4\n79.6\n768\n31M\n88.6/81.5\n79.2/76.6\n82.0\n90.6\n63.3\n79.8"}
{"doc_id": "1909.11942", "para_id": 64, "text": "Table 3: The effect of vocabulary embedding size on the performance of ALBERT-base."}
{"doc_id": "1909.11942", "para_id": 65, "text": "Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an\nALBERT-base conﬁguration (Table 1) with two embedding sizes (E = 768 and E = 128). We\ncompare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and inter-\nmediate strategies in which only the attention parameters are shared (but not the FNN ones) or only\nthe FFN parameters are shared (but not the attention ones)."}
{"doc_id": "1909.11942", "para_id": 66, "text": "The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (-\n1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears\nto come from sharing the FFN-layer parameters, while sharing the attention parameters results in no\ndrop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg)."}
{"doc_id": "1909.11942", "para_id": 67, "text": "There are other strategies of sharing the parameters cross layers. For example, We can divide the L\nlayers into N groups of size M, and each size-M group shares parameters. Overall, our experimen-\ntal results shows that the smaller the group size M is, the better the performance we get. However,\ndecreasing group size M also dramatically increase the number of overall parameters. We choose\nall-shared strategy as our default choice."}
{"doc_id": "1909.11942", "para_id": 68, "text": "Model\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg"}
{"doc_id": "1909.11942", "para_id": 69, "text": "all-shared\n31M\n88.6/81.5\n79.2/76.6\n82.0\n90.6\n63.3\n79.8\nshared-attention\n83M\n89.9/82.7\n80.0/77.2\n84.0\n91.4\n67.7\n81.6\nshared-FFN\n57M\n89.2/82.1\n78.2/75.4\n81.5\n90.8\n62.6\n79.5\nnot-shared\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3"}
{"doc_id": "1909.11942", "para_id": 70, "text": "all-shared\n12M\n89.3/82.3\n80.0/77.1\n82.0\n90.3\n64.0\n80.1\nshared-attention\n64M\n89.9/82.8\n80.7/77.9\n83.4\n91.9\n67.6\n81.7\nshared-FFN\n38M\n88.9/81.6\n78.6/75.6\n82.3\n91.7\n64.4\n80.2\nnot-shared\n89M\n89.9/82.8\n80.3/77.3\n83.2\n91.5\n67.9\n81.6"}
{"doc_id": "1909.11942", "para_id": 71, "text": "Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 72, "text": "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none\n(XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERT-\nbase conﬁguration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP,\nand SOP tasks) and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 73, "text": "Intrinsic Tasks\nDownstream Tasks\nSP tasks\nMLM\nNSP\nSOP\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nNone\n54.9\n52.4\n53.3\n88.6/81.5\n78.1/75.3\n81.5\n89.9\n61.7\n79.0\nNSP\n54.5\n90.5\n52.0\n88.4/81.5\n77.2/74.6\n81.6\n91.1\n62.3\n79.2\nSOP\n54.0\n78.9\n86.5\n89.3/82.3\n80.0/77.1\n82.0\n90.3\n64.0\n80.1"}
{"doc_id": "1909.11942", "para_id": 74, "text": "Table 5: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 75, "text": "The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP\ntask (52.0% accuracy, similar to the random-guess performance for the “None” condition). This\nallows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does\nsolve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy).\nEven more importantly, the SOP loss appears to consistently improve downstream task performance\nfor multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for\nRACE), for an Avg score improvement of around +1%."}
{"doc_id": "1909.11942", "para_id": 76, "text": "The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher\ncompared to ALBERT-xxlarge. Since longer training usually leads to better performance, we per-\nform a comparison in which, instead of controlling for data throughput (number of training steps),\nwe control for the actual training time (i.e., let the models train for the same number of hours). In\nTable 6, we compare the performance of a BERT-large model after 400k training steps (after 34h\nof training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model\nwith 125k training steps (32h of training)."}
{"doc_id": "1909.11942", "para_id": 77, "text": "Models\nSteps\nTime\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nBERT-large\n400k\n34h\n93.5/87.4\n86.9/84.3\n87.8\n94.6\n77.3\n87.2\nALBERT-xxlarge\n125k\n32h\n94.0/88.1\n88.3/85.3\n87.8\n95.4\n82.5\n88.7"}
{"doc_id": "1909.11942", "para_id": 78, "text": "Table 6: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 79, "text": "After training for roughly the same amount of time, ALBERT-xxlarge is signiﬁcantly better than\nBERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%."}
{"doc_id": "1909.11942", "para_id": 80, "text": "The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in\n(Devlin et al., 2019). In this section, we report measurements on the impact of the additional data\nused by both XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 81, "text": "Fig. 2a plots the dev set MLM accuracy under two conditions, without and with additional data, with\nthe latter condition giving a signiﬁcant boost. We also observe performance improvements on the\ndownstream tasks in Table 7, except for the SQuAD benchmarks (which are Wikipedia-based, and\ntherefore are negatively affected by out-of-domain training material)."}
{"doc_id": "1909.11942", "para_id": 82, "text": "SQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nNo additional data\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\nWith additional data\n88.8/81.7\n79.1/76.3\n82.4\n92.8\n66.0\n80.8"}
{"doc_id": "1909.11942", "para_id": 83, "text": "Table 7: The effect of additional training data using the ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 84, "text": "We also note that, even after training for 1M steps, our largest models still do not overﬁt to their\ntraining data. As a result, we decide to remove dropout to further increase our model capacity. The"}
{"doc_id": "1909.11942", "para_id": 85, "text": "Figure 2: The effects of adding data and removing dropout during training."}
{"doc_id": "1909.11942", "para_id": 86, "text": "plot in Fig. 2b shows that removing dropout signiﬁcantly improves MLM accuracy. Intermediate\nevaluation on ALBERT-xxlarge at around 1M training steps (Table 8) also conﬁrms that removing\ndropout helps the downstream tasks. There is empirical (Szegedy et al., 2017) and theoretical (Li\net al., 2019) evidence showing that a combination of batch normalization and dropout in Convolu-\ntional Neural Networks may have harmful results. To the best of our knowledge, we are the ﬁrst to\nshow that dropout can hurt performance in large Transformer-based models. However, the underly-\ning network structure of ALBERT is a special case of the transformer and further experimentation\nis needed to see if this phenomenon appears with other transformer-based architectures or not."}
{"doc_id": "1909.11942", "para_id": 87, "text": "SQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nWith dropout\n94.7/89.2\n89.6/86.9\n90.0\n96.3\n85.7\n90.4\nWithout dropout\n94.8/89.5\n89.9/87.2\n90.4\n96.5\n86.1\n90.7"}
{"doc_id": "1909.11942", "para_id": 88, "text": "Table 8: The effect of removing dropout, measured for an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 89, "text": "The results we report in this section make use of the training data used by Devlin et al. (2019), as\nwell as the additional data used by Liu et al. (2019) and Yang et al. (2019). We report state-of-the-art\nresults under two settings for ﬁne-tuning: single-model and ensembles. In both settings, we only do\nsingle-task ﬁne-tuning4. Following Liu et al. (2019), on the development set we report the median\nresult over ﬁve runs."}
{"doc_id": "1909.11942", "para_id": 90, "text": "Models\nMNLI\nQNLI\nQQP\nRTE\nSST\nMRPC\nCoLA\nSTS\nWNLI\nAvg\nSingle-task single models on dev\nBERT-large\n86.6\n92.3\n91.3\n70.4\n93.2\n88.0\n60.6\n90.0\n-\n-\nXLNet-large\n89.8\n93.9\n91.8\n83.8\n95.6\n89.2\n63.6\n91.8\n-\n-\nRoBERTa-large\n90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4\n-\n-\nALBERT (1M)\n90.4\n95.2\n92.0\n88.1\n96.8\n90.2\n68.7\n92.7\n-\n-\nALBERT (1.5M)\n90.8\n95.3\n92.2\n89.2\n96.9\n90.9\n71.4\n93.0\n-\n-\nEnsembles on test (from leaderboard as of Sept. 16, 2019)\nALICE\n88.2\n95.7\n90.7\n83.5\n95.2\n92.6\n69.2\n91.1\n80.8\n87.0\nMT-DNN\n87.9\n96.0\n89.9\n86.3\n96.5\n92.7\n68.4\n91.1\n89.0\n87.6\nXLNet\n90.2\n98.6\n90.3\n86.3\n96.8\n93.0\n67.8\n91.6\n90.4\n88.4\nRoBERTa\n90.8\n98.9\n90.2\n88.2\n96.7\n92.3\n67.8\n92.2\n89.0\n88.5\nAdv-RoBERTa\n91.1\n98.8\n90.3\n88.7\n96.8\n93.1\n68.0\n92.4\n89.0\n88.8\nALBERT\n91.3\n99.2\n90.5\n89.2\n97.1\n93.4\n69.1\n92.5\n91.8\n89.4"}
{"doc_id": "1909.11942", "para_id": 91, "text": "Table 9: State-of-the-art results on the GLUE benchmark. For single-task single-model results, we\nreport ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble\nuses models trained with 1M, 1.5M, and other numbers of steps."}
{"doc_id": "1909.11942", "para_id": 92, "text": "The single-model ALBERT conﬁguration incorporates the best-performing settings discussed: an\nALBERT-xxlarge conﬁguration (Table 1) using combined MLM and SOP losses, and no dropout."}
{"doc_id": "1909.11942", "para_id": 93, "text": "4Following Liu et al. (2019), we ﬁne-tune for RTE, STS, and MRPC using an MNLI checkpoint."}
{"doc_id": "1909.11942", "para_id": 94, "text": "The checkpoints that contribute to the ﬁnal ensemble model are selected based on development set\nperformance; the number of checkpoints considered for this selection range from 6 to 17, depending\non the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model\npredictions for the ensemble models, where the candidates are ﬁne-tuned from different training\nsteps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the pre-\ndiction scores for those spans that have multiple probabilities; we also average the scores of the\n“unanswerable” decision."}
{"doc_id": "1909.11942", "para_id": 95, "text": "Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art signif-\nicantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of\n92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement,\na jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over\nXLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang\net al., 2019), an ensemble of multiple models speciﬁcally designed for reading comprehension tasks.\nOur single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art\nensemble model."}
{"doc_id": "1909.11942", "para_id": 96, "text": "Models\nSQuAD1.1 dev\nSQuAD2.0 dev\nSQuAD2.0 test\nRACE test (Middle/High)\nSingle model (from leaderboard as of Sept. 23, 2019)\nBERT-large\n90.9/84.1\n81.8/79.0\n89.1/86.3\n72.0 (76.6/70.1)\nXLNet\n94.5/89.0\n88.8/86.1\n89.1/86.3\n81.8 (85.5/80.2)\nRoBERTa\n94.6/88.9\n89.4/86.5\n89.8/86.8\n83.2 (86.5/81.3)\nUPM\n-\n-\n89.9/87.2\n-\nXLNet + SG-Net Veriﬁer++\n-\n-\n90.1/87.2\n-\nALBERT (1M)\n94.8/89.2\n89.9/87.2\n-\n86.0 (88.2/85.1)\nALBERT (1.5M)\n94.8/89.3\n90.2/87.4\n90.9/88.1\n86.5 (89.0/85.5)\nEnsembles (from leaderboard as of Sept. 23, 2019)\nBERT-large\n92.2/86.2\n-\n-\n-\nXLNet + SG-Net Veriﬁer\n-\n-\n90.7/88.2\n-\nUPM\n-\n-\n90.7/88.2\nXLNet + DAAF + Veriﬁer\n-\n-\n90.9/88.6\n-\nDCMN+\n-\n-\n-\n84.1 (88.5/82.3)\nALBERT\n95.5/90.1\n91.4/88.9\n92.2/89.7\n89.4 (91.2/88.6)"}
{"doc_id": "1909.11942", "para_id": 97, "text": "Table 10: State-of-the-art results on the SQuAD and RACE benchmarks."}
{"doc_id": "1909.11942", "para_id": 98, "text": "While ALBERT-xxlarge has less parameters than BERT-large and gets signiﬁcantly better results, it\nis computationally more expensive due to its larger structure. An important next step is thus to speed\nup the training and inference speed of ALBERT through methods like sparse attention (Child et al.,\n2019) and block attention (Shen et al., 2018). An orthogonal line of research, which could provide\nadditional representation power, includes hard example mining (Mikolov et al., 2013) and more\nefﬁcient language modeling training (Yang et al., 2019). Additionally, although we have convincing\nevidence that sentence order prediction is a more consistently-useful learning task that leads to better\nlanguage representations, we hypothesize that there could be more dimensions not yet captured by\nthe current self-supervised training losses that could create additional representation power for the\nresulting representations."}
{"doc_id": "1909.11942", "para_id": 99, "text": "The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim\nfor discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for\nclarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick,\nEmma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the\npaper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu,\nChenjie Cao and the CLUE community for providing the training data and evaluation benechmark\nof the Chinese version of ALBERT models."}
{"doc_id": "1909.11942", "para_id": 100, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018."}
{"doc_id": "1909.11942", "para_id": 101, "text": "Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information\nProcessing Systems (NeurIPS), 2019."}
{"doc_id": "1909.11942", "para_id": 102, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings of\nthe second PASCAL challenges workshop on recognising textual entailment, volume 6, pp. 6–4.\nVenice, 2006."}
{"doc_id": "1909.11942", "para_id": 103, "text": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth PASCAL recognizing\ntextual entailment challenge. In TAC, 2009."}
{"doc_id": "1909.11942", "para_id": 104, "text": "Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver,\nCanada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.\nURL https://www.aclweb.org/anthology/S17-2001."}
{"doc_id": "1909.11942", "para_id": 105, "text": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016."}
{"doc_id": "1909.11942", "para_id": 106, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019."}
{"doc_id": "1909.11942", "para_id": 107, "text": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V\nLe. Bam! born-again multi-task networks for natural language understanding. arXiv preprint\narXiv:1907.04829, 2019."}
{"doc_id": "1909.11942", "para_id": 108, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005."}
{"doc_id": "1909.11942", "para_id": 109, "text": "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor-\nmation processing systems, pp. 3079–3087, 2015."}
{"doc_id": "1909.11942", "para_id": 110, "text": "Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019."}
{"doc_id": "1909.11942", "para_id": 111, "text": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018."}
{"doc_id": "1909.11942", "para_id": 112, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//www.aclweb.org/anthology/N19-1423."}
{"doc_id": "1909.11942", "para_id": 113, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\nhttps://www.aclweb.org/anthology/I05-5002."}
{"doc_id": "1909.11942", "para_id": 114, "text": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. Learn-\ning generic sentence representations using convolutional neural networks.\nIn Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2390–2400,\nCopenhagen, Denmark, September 2017. Association for Computational Linguistics.\ndoi:\n10.18653/v1/D17-1254. URL https://www.aclweb.org/anthology/D17-1254."}
{"doc_id": "1909.11942", "para_id": 115, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail-\nment and Paraphrasing, pp. 1–9, Prague, June 2007. Association for Computational Linguistics.\nURL https://www.aclweb.org/anthology/W07-1401."}
{"doc_id": "1909.11942", "para_id": 116, "text": "Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-\nwork: Backpropagation without storing activations. In Advances in neural information processing\nsystems, pp. 2214–2224, 2017."}
{"doc_id": "1909.11942", "para_id": 117, "text": "Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient training of bert\nby progressively stacking. In International Conference on Machine Learning, pp. 2337–2346,\n2019."}
{"doc_id": "1909.11942", "para_id": 118, "text": "Edouard Grave, Armand Joulin, Moustapha Ciss´e, Herv´e J´egou, et al. Efﬁcient softmax approxima-\ntion for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pp. 1302–1310. JMLR. org, 2017."}
{"doc_id": "1909.11942", "para_id": 119, "text": "Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Centering: A framework for modeling the\nlocal coherence of discourse. Computational Linguistics, 21(2):203–225, 1995. URL https:\n//www.aclweb.org/anthology/J95-2003."}
{"doc_id": "1909.11942", "para_id": 120, "text": "M.A.K. Halliday and Ruqaiya Hasan. Cohesion in English. Routledge, 1976."}
{"doc_id": "1909.11942", "para_id": 121, "text": "Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling\nrecurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10.\n18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122."}
{"doc_id": "1909.11942", "para_id": 122, "text": "Dan Hendrycks and Kevin Gimpel.\nGaussian Error Linear Units (GELUs).\narXiv preprint\narXiv:1606.08415, 2016."}
{"doc_id": "1909.11942", "para_id": 123, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 1367–1377.\nAssociation for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL http:\n//aclweb.org/anthology/N16-1162."}
{"doc_id": "1909.11942", "para_id": 124, "text": "Jerry R. Hobbs. Coherence and coreference. Cognitive Science, 3(1):67–90, 1979."}
{"doc_id": "1909.11942", "para_id": 125, "text": "Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1909.11942", "para_id": 126, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Csernai.\nFirst quora dataset release:\nQues-\ntion\npairs,\nJanuary\n2017.\nURL\nhttps://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs."}
{"doc_id": "1909.11942", "para_id": 127, "text": "Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsuper-\nvised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017."}
{"doc_id": "1909.11942", "para_id": 128, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans.\narXiv preprint\narXiv:1907.10529, 2019."}
{"doc_id": "1909.11942", "para_id": 129, "text": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Ur-\ntasun, and Sanja Fidler. Skip-thought vectors. In Proceedings of the 28th International Con-\nference on Neural Information Processing Systems - Volume 2, NIPS’15, pp. 3294–3302, Cam-\nbridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=\n2969442.2969607."}
{"doc_id": "1909.11942", "para_id": 130, "text": "Taku Kudo and John Richardson.\nSentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\n66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.\ndoi:\n10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012."}
{"doc_id": "1909.11942", "para_id": 131, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding\ncomprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September 2017.\nAssociation for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://www.\naclweb.org/anthology/D17-1082."}
{"doc_id": "1909.11942", "para_id": 132, "text": "Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceed-\nings of the 31st ICML, Beijing, China, 2014."}
{"doc_id": "1909.11942", "para_id": 133, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thir-\nteenth International Conference on the Principles of Knowledge Representation and Reasoning,\n2012."}
{"doc_id": "1909.11942", "para_id": 134, "text": "Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout\nand batch normalization by variance shift. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2682–2690, 2019."}
{"doc_id": "1909.11942", "para_id": 135, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-\ntraining approach. arXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "1909.11942", "para_id": 136, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.\nLearned in translation:\nContextualized word vectors. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,\npp. 6294–6305. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\n7209-learned-in-translation-contextualized-word-vectors.pdf."}
{"doc_id": "1909.11942", "para_id": 137, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013."}
{"doc_id": "1909.11942", "para_id": 138, "text": "Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 4497–4510, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/\nP19-1442."}
{"doc_id": "1909.11942", "para_id": 139, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1532–1543, Doha, Qatar, October 2014. Association for Computational\nLinguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/\nD14-1162."}
{"doc_id": "1909.11942", "para_id": 140, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1202.\nURL\nhttps://www.aclweb.org/anthology/N18-1202."}
{"doc_id": "1909.11942", "para_id": 141, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\nImproving language\nunderstanding by generative pre-training. https://s3-us-west-2.amazonaws.com/\nopenai-assets/research-covers/language-unsupervised/language_\nunderstanding_paper.pdf, 2018."}
{"doc_id": "1909.11942", "para_id": 142, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019."}
{"doc_id": "1909.11942", "para_id": 143, "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "1909.11942", "para_id": 144, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association\nfor Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.\norg/anthology/D16-1264."}
{"doc_id": "1909.11942", "para_id": 145, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, July 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb.\norg/anthology/P18-2124."}
{"doc_id": "1909.11942", "para_id": 146, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\nlearning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414–\n10423, 2018."}
{"doc_id": "1909.11942", "para_id": 147, "text": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence modeling. arXiv preprint arXiv:1804.00857,\n2018."}
{"doc_id": "1909.11942", "para_id": 148, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training multi-billion parameter language models using model par-\nallelism, 2019."}
{"doc_id": "1909.11942", "para_id": 149, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://www.aclweb.org/anthology/D13-1170."}
{"doc_id": "1909.11942", "para_id": 150, "text": "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model\ncompression. arXiv preprint arXiv:1908.09355, 2019."}
{"doc_id": "1909.11942", "para_id": 151, "text": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\nInception-v4,\ninception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence, 2017."}
{"doc_id": "1909.11942", "para_id": 152, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:\nThe impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962,\n2019."}
{"doc_id": "1909.11942", "para_id": 153, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017."}
{"doc_id": "1909.11942", "para_id": 154, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/\nW18-5446."}
{"doc_id": "1909.11942", "para_id": 155, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT: Incor-\nporating language structures into pre-training for deep language understanding. arXiv preprint\narXiv:1908.04577, 2019."}
{"doc_id": "1909.11942", "para_id": 156, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1909.11942", "para_id": 157, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn Proceedings of the 2018 Conference of the North"}
{"doc_id": "1909.11942", "para_id": 158, "text": "American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\norg/anthology/N18-1101."}
{"doc_id": "1909.11942", "para_id": 159, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019."}
{"doc_id": "1909.11942", "para_id": 160, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing\nBERT pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019."}
{"doc_id": "1909.11942", "para_id": 161, "text": "Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou.\nDCMN+: Dual co-matching network for multi-choice reading comprehension. arXiv preprint\narXiv:1908.11511, 2019."}
{"doc_id": "1909.11942", "para_id": 162, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision, pp. 19–27, 2015."}
{"doc_id": "1909.11942", "para_id": 163, "text": "In this section, we check how depth (number of layers) and width (hidden size) affect the perfor-\nmance of ALBERT. Table 11 shows the performance of an ALBERT-large conﬁguration (see Ta-\nble 1) using different numbers of layers. Networks with 3 or more layers are trained by ﬁne-tuning\nusing the parameters from the depth before (e.g., the 12-layer network parameters are ﬁne-tuned\nfrom the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong\net al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although\nthey have the same number of parameters, the performance increases signiﬁcantly. However, there\nare diminishing returns when continuing to increase the number of layers: the results of a 12-layer\nnetwork are relatively close to the results of a 24-layer network, and the performance of a 48-layer\nnetwork appears to decline."}
{"doc_id": "1909.11942", "para_id": 164, "text": "Number of layers\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n1\n18M\n31.1/22.9\n50.1/50.1\n66.4\n80.8\n40.1\n52.9\n3\n18M\n79.8/69.7\n64.4/61.7\n77.7\n86.7\n54.0\n71.2\n6\n18M\n86.4/78.4\n73.8/71.1\n81.2\n88.9\n60.9\n77.2\n12\n18M\n89.8/83.3\n80.7/77.9\n83.3\n91.7\n66.7\n81.5\n24\n18M\n90.3/83.3\n81.8/79.0\n83.3\n91.5\n68.7\n82.1\n48\n18M\n90.0/83.1\n81.8/78.9\n83.4\n91.9\n66.9\n81.8"}
{"doc_id": "1909.11942", "para_id": 165, "text": "Table 11: The effect of increasing the number of layers for an ALBERT-large conﬁguration."}
{"doc_id": "1909.11942", "para_id": 166, "text": "A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large\nconﬁguration. As we increase the hidden size, we get an increase in performance with diminishing\nreturns. At a hidden size of 6144, the performance appears to decline signiﬁcantly. We note that none\nof these models appear to overﬁt the training data, and they all have higher training and development\nloss compared to the best-performing ALBERT conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 167, "text": "5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this\nwarm-start technique does not help to improve the downstream performance. However, it does help the 48-layer\nnetwork to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from\na 6-layer network."}
{"doc_id": "1909.11942", "para_id": 168, "text": "Hidden size\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n1024\n18M\n79.8/69.7\n64.4/61.7\n77.7\n86.7\n54.0\n71.2\n2048\n60M\n83.3/74.1\n69.1/66.6\n79.7\n88.6\n58.2\n74.6\n4096\n225M\n85.0/76.4\n71.0/68.1\n80.3\n90.4\n60.4\n76.3\n6144\n499M\n84.7/75.8\n67.8/65.4\n78.1\n89.1\n56.0\n74.0"}
{"doc_id": "1909.11942", "para_id": 169, "text": "Table 12: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 170, "text": "A.2\nDO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO?"}
{"doc_id": "1909.11942", "para_id": 171, "text": "In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a\n24-layer conﬁguration is small. Does this result still hold for much wider ALBERT conﬁgurations,\nsuch as ALBERT-xxlarge (H=4096)?"}
{"doc_id": "1909.11942", "para_id": 172, "text": "Number of layers\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n12\n94.0/88.1\n88.3/85.3\n87.8\n95.4\n82.5\n88.7\n24\n94.1/88.3\n88.1/85.1\n88.0\n95.2\n82.3\n88.7"}
{"doc_id": "1909.11942", "para_id": 173, "text": "Table 13: The effect of a deeper network using an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 174, "text": "The answer is given by the results from Table 13. The difference between 12-layer and 24-layer\nALBERT-xxlarge conﬁgurations in terms of downstream accuracy is negligible, with the Avg score\nbeing the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there\nis no need for models deeper than a 12-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 175, "text": "GLUE\nGLUE\nis\ncomprised\nof\n9\ntasks,\nnamely\nCorpus\nof\nLinguistic\nAcceptability\n(CoLA; Warstadt et al., 2018), Stanford Sentiment Treebank (SST; Socher et al., 2013), Microsoft\nResearch Paraphrase Corpus (MRPC; Dolan & Brockett, 2005), Semantic Textual Similarity Bench-\nmark (STS; Cer et al., 2017), Quora Question Pairs (QQP; Iyer et al., 2017), Multi-Genre NLI\n(MNLI; Williams et al., 2018), Question NLI (QNLI; Rajpurkar et al., 2016), Recognizing Textual\nEntailment (RTE; Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli\net al., 2009) and Winograd NLI (WNLI; Levesque et al., 2012). It focuses on evaluating model\ncapabilities for natural language understanding. When reporting MNLI results, we only report the\n“match” condition (MNLI-m). We follow the ﬁnetuning procedures from prior work (Devlin et al.,\n2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from\nGLUE submissions. For test set submissions, we perform task-speciﬁc modiﬁcations for WNLI and\nQNLI as described by Liu et al. (2019) and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 176, "text": "SQuAD\nSQuAD is an extractive question answering dataset built from Wikipedia. The answers\nare segments from the context paragraphs and the task is to predict answer spans. We evaluate our\nmodels on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated\nquestion/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For\nSQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are\njointly trained with a span extraction loss and an additional classiﬁer for predicting answerabil-\nity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance."}
{"doc_id": "1909.11942", "para_id": 177, "text": "RACE\nRACE is a large-scale dataset for multi-choice reading comprehension, collected from En-\nglish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate\nanswers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the\npassage, question, and each candidate answer as the input to models. Then, we use the represen-\ntations from the “[CLS]” token for predicting the probability of each answer. The dataset consists\nof two domains: middle school and high school. We train our models on both domains and report\naccuracies on both the development set and test set."}
{"doc_id": "1909.11942", "para_id": 178, "text": "Hyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters\nfrom Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 179, "text": "LR\nBSZ\nALBERT DR\nClassiﬁer DR\nTS\nWS\nMSL\nCoLA\n1.00E-05\n16\n0\n0.1\n5336\n320\n512\nSTS\n2.00E-05\n16\n0\n0.1\n3598\n214\n512\nSST-2\n1.00E-05\n32\n0\n0.1\n20935\n1256\n512\nMNLI\n3.00E-05\n128\n0\n0.1\n10000\n1000\n512\nQNLI\n1.00E-05\n32\n0\n0.1\n33112\n1986\n512\nQQP\n5.00E-05\n128\n0.1\n0.1\n14000\n1000\n512\nRTE\n3.00E-05\n32\n0.1\n0.1\n800\n200\n512\nMRPC\n2.00E-05\n32\n0\n0.1\n800\n200\n512\nWNLI\n2.00E-05\n16\n0.1\n0.1\n2000\n250\n512\nSQuAD v1.1\n5.00E-05\n48\n0\n0.1\n3649\n365\n384\nSQuAD v2.0\n3.00E-05\n48\n0\n0.1\n8144\n814\n512\nRACE\n2.00E-05\n32\n0.1\n0.1\n12000\n1000\n512"}
{"doc_id": "1909.11942", "para_id": 180, "text": "Table 14: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch\nSize. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence\nLength."}
{"doc_id": "1907.11692", "para_id": 0, "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}
{"doc_id": "1907.11692", "para_id": 1, "text": "Yinhan Liu∗§\nMyle Ott∗§\nNaman Goyal∗§\nJingfei Du∗§\nMandar Joshi†"}
{"doc_id": "1907.11692", "para_id": 2, "text": "Danqi Chen§\nOmer Levy§\nMike Lewis§\nLuke Zettlemoyer†§\nVeselin Stoyanov§"}
{"doc_id": "1907.11692", "para_id": 3, "text": "† Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz}@cs.washington.edu"}
{"doc_id": "1907.11692", "para_id": 4, "text": "§ Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves}@fb.com"}
{"doc_id": "1907.11692", "para_id": 5, "text": "We present a replication study of BERT pre-\ntraining (Devlin et al., 2019), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-NEWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nby Yang et al. (2019).\nOur model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al., 2019).2"}
{"doc_id": "1907.11692", "para_id": 6, "text": "Language model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining (Devlin et al., 2019) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it.\nOur best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1"}
{"doc_id": "1907.11692", "para_id": 7, "text": "Self-training methods such as ELMo (Peters et al.,\n2018),\nGPT\n(Radford et al.,\n2018),\nBERT\n(Devlin et al., 2019), XLM (Lample and Conneau,\n2019),\nand\nXLNet (Yang et al.,\n2019)\nhave\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most.\nTraining is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances."}
{"doc_id": "1907.11692", "para_id": 8, "text": "In summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce"}
{"doc_id": "1907.11692", "para_id": 9, "text": "∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseq"}
{"doc_id": "1907.11692", "para_id": 10, "text": "2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work."}
{"doc_id": "1907.11692", "para_id": 11, "text": "alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch (Paszke et al., 2017)."}
{"doc_id": "1907.11692", "para_id": 12, "text": "and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1)."}
{"doc_id": "1907.11692", "para_id": 13, "text": "Next Sentence Prediction (NSP)\nNSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference (Bowman et al., 2015), which\nrequire reasoning about the relationships between\npairs of sentences."}
{"doc_id": "1907.11692", "para_id": 14, "text": "In this section, we give a brief overview of the\nBERT (Devlin et al., 2019) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section."}
{"doc_id": "1907.11692", "para_id": 15, "text": "BERT takes as input a concatenation of two\nsegments\n(sequences\nof\ntokens),\nx1, . . . , xN\nand y1, . . . , yM.\nSegments usually consist of\nmore than one natural sentence.\nThe two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS], x1, . . . , xN, [SEP], y1, . . . , yM, [EOS].\nM and N are constrained such that M + N < T,\nwhere T is a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data."}
{"doc_id": "1907.11692", "para_id": 16, "text": "BERT is optimized with Adam (Kingma and Ba,\n2015) using the following parameters: β1 = 0.9,\nβ2\n=\n0.999, ǫ\n=\n1e-6 and L2 weight de-\ncay of 0.01.\nThe learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed.\nBERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion (Hendrycks and Gimpel, 2016). Models are\npretrained for S = 1,000,000 updates, with mini-\nbatches containing B = 256 sequences of maxi-\nmum length T = 512 tokens."}
{"doc_id": "1907.11692", "para_id": 17, "text": "BERT uses the now ubiquitous transformer archi-\ntecture (Vaswani et al., 2017), which we will not\nreview in detail. We use a transformer architecture\nwith L layers. Each block uses A self-attention\nheads and hidden dimension H."}
{"doc_id": "1907.11692", "para_id": 18, "text": "PUS (Zhu et al., 2015) plus English WIKIPEDIA,\nwhich totals 16GB of uncompressed text.3"}
{"doc_id": "1907.11692", "para_id": 19, "text": "During pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction."}
{"doc_id": "1907.11692", "para_id": 20, "text": "In this section, we describe the experimental setup\nfor our replication study of BERT."}
{"doc_id": "1907.11692", "para_id": 21, "text": "Masked Language Model (MLM)\nA random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,"}
{"doc_id": "1907.11692", "para_id": 22, "text": "We reimplement BERT in FAIRSEQ (Ott et al.,\n2019).\nWe primarily follow the original BERT"}
{"doc_id": "1907.11692", "para_id": 23, "text": "3Yang et al. (2019) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data."}
{"doc_id": "1907.11692", "para_id": 24, "text": "pus described in Radford et al. (2019). The text\nis web content extracted from URLs shared on\nReddit with at least three upvotes. (38GB).5"}
{"doc_id": "1907.11692", "para_id": 25, "text": "optimization hyperparameters, given in Section 2,\nexcept for the peak learning rate and number of\nwarmup steps, which are tuned separately for each\nsetting. We additionally found training to be very\nsensitive to the Adam epsilon term, and in some\ncases we obtained better performance or improved\nstability after tuning it. Similarly, we found setting\nβ2 = 0.98 to improve stability when training with\nlarge batch sizes.\nWe pretrain with sequences of at most T = 512\ntokens. Unlike Devlin et al. (2019), we do not ran-\ndomly inject short sequences, and we do not train\nwith a reduced sequence length for the ﬁrst 90% of\nupdates. We train only with full-length sequences.\nWe train with mixed precision ﬂoating point\narithmetic on DGX-1 machines, each with 8 ×\n32GB Nvidia V100 GPUs interconnected by In-\nﬁniband (Micikevicius et al., 2018)."}
{"doc_id": "1907.11692", "para_id": 26, "text": "(2018) containing a subset of CommonCrawl\ndata ﬁltered to match the story-like style of\nWinograd schemas. (31GB)."}
{"doc_id": "1907.11692", "para_id": 27, "text": "Following previous work, we evaluate our pre-\ntrained models on downstream tasks using the fol-\nlowing three benchmarks."}
{"doc_id": "1907.11692", "para_id": 28, "text": "GLUE\nThe\nGeneral\nLanguage\nUnderstand-\ning Evaluation (GLUE) benchmark (Wang et al.,\n2019b) is a collection of 9 datasets for evaluating\nnatural language understanding systems.6 Tasks\nare framed as either single-sentence classiﬁcation\nor sentence-pair classiﬁcation tasks. The GLUE\norganizers provide training and development data\nsplits as well as a submission server and leader-\nboard that allows participants to evaluate and com-\npare their systems on private held-out test data.\nFor the replication study in Section 4, we report\nresults on the development sets after ﬁnetuning\nthe pretrained models on the corresponding single-\ntask training data (i.e., without multi-task training\nor ensembling). Our ﬁnetuning procedure follows\nthe original BERT paper (Devlin et al., 2019).\nIn Section 5 we additionally report test set re-\nsults obtained from the public leaderboard. These\nresults depend on a several task-speciﬁc modiﬁca-\ntions, which we describe in Section 5.1."}
{"doc_id": "1907.11692", "para_id": 29, "text": "BERT-style pretraining crucially relies on large\nquantities of text.\nBaevski et al. (2019) demon-\nstrate that increasing data size can result in im-\nproved end-task performance.\nSeveral efforts\nhave trained on datasets larger and more diverse\nthan the original BERT (Radford et al., 2019;\nYang et al., 2019; Zellers et al., 2019). Unfortu-\nnately, not all of the additional datasets can be\npublicly released. For our study, we focus on gath-\nering as much data as possible for experimenta-\ntion, allowing us to match the overall quality and\nquantity of data as appropriate for each compari-\nson.\nWe consider ﬁve English-language corpora of\nvarying sizes and domains, totaling over 160GB\nof uncompressed text. We use the following text\ncorpora:"}
{"doc_id": "1907.11692", "para_id": 30, "text": "SQuAD\nThe\nStanford\nQuestion\nAnswering\nDataset (SQuAD) provides a paragraph of context\nand a question. The task is to answer the question\nby extracting the relevant span from the context.\nWe evaluate on two versions of SQuAD: V1.1\nand V2.0 (Rajpurkar et al., 2016, 2018). In V1.1\nthe context always contains an answer, whereas in"}
{"doc_id": "1907.11692", "para_id": 31, "text": "• BOOKCORPUS (Zhu et al., 2015) plus English\nWIKIPEDIA. This is the original data used to\ntrain BERT. (16GB)."}
{"doc_id": "1907.11692", "para_id": 32, "text": "• CC-NEWS, which we collected from the En-\nglish portion of the CommonCrawl News\ndataset (Nagel, 2016).\nThe data contains 63\nmillion English news articles crawled between\nSeptember 2016 and February 2019. (76GB af-\nter ﬁltering).4"}
{"doc_id": "1907.11692", "para_id": 33, "text": "5The authors and their afﬁliated institutions are not in any\nway afﬁliated with the creation of the OpenWebText dataset.\n6The\ndatasets\nare:\nCoLA\n(Warstadt et al.,\n2018),\nStanford\nSentiment\nTreebank\n(SST)\n(Socher et al.,\n2013),\nMicrosoft\nResearch\nParagraph\nCorpus\n(MRPC)\n(Dolan and Brockett,\n2005),\nSemantic\nTex-\ntual Similarity Benchmark (STS) (Agirre et al., 2007),\nQuora Question Pairs (QQP) (Iyer et al., 2016), Multi-\nGenre NLI (MNLI) (Williams et al., 2018), Question NLI\n(QNLI)\n(Rajpurkar et al.,\n2016),\nRecognizing\nTextual\nEntailment\n(RTE)\n(Dagan et al.,\n2006;\nBar-Haim et al.,\n2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and\nWinograd NLI (WNLI) (Levesque et al., 2011)."}
{"doc_id": "1907.11692", "para_id": 34, "text": "• OPENWEBTEXT (Gokaslan and Cohen, 2019),\nan open-source recreation of the WebText cor-"}
{"doc_id": "1907.11692", "para_id": 35, "text": "4We use news-please (Hamborg et al., 2017) to col-\nlect and extract CC-NEWS. CC-NEWS is similar to the RE-\nALNEWS dataset described in Zellers et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 36, "text": "V2.0 some questions are not answered in the pro-\nvided context, making the task more challenging.\nFor SQuAD V1.1 we adopt the same span pre-\ndiction method as BERT (Devlin et al., 2019). For\nSQuAD V2.0, we add an additional binary classi-\nﬁer to predict whether the question is answerable,\nwhich we train jointly by summing the classiﬁca-\ntion and span loss terms. During evaluation, we\nonly predict span indices on pairs that are classi-\nﬁed as answerable."}
{"doc_id": "1907.11692", "para_id": 37, "text": "Our reimplementation:\nstatic\n78.3\n84.3\n92.5\ndynamic\n78.7\n84.0\n92.9"}
{"doc_id": "1907.11692", "para_id": 38, "text": "Table 1:\nComparison between static and dynamic\nmasking for BERTBASE. We report F1 for SQuAD and\naccuracy for MNLI-m and SST-2. Reported results are\nmedians over 5 random initializations (seeds). Refer-\nence results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 39, "text": "RACE\nThe ReAding Comprehension from Ex-\naminations (RACE) (Lai et al., 2017) task is a\nlarge-scale reading comprehension dataset with\nmore than 28,000 passages and nearly 100,000\nquestions. The dataset is collected from English\nexaminations in China, which are designed for\nmiddle and high school students. In RACE, each\npassage is associated with multiple questions. For\nevery question, the task is to select one correct an-\nswer from four options. RACE has signiﬁcantly\nlonger context than other popular reading compre-\nhension datasets and the proportion of questions\nthat requires reasoning is very large."}
{"doc_id": "1907.11692", "para_id": 40, "text": "Results\nTable\n1\ncompares\nthe\npublished\nBERTBASE results from Devlin et al. (2019) to our\nreimplementation with either static or dynamic\nmasking.\nWe ﬁnd that our reimplementation\nwith static masking performs similar to the\noriginal BERT model, and dynamic masking is\ncomparable or slightly better than static masking.\nGiven these results and the additional efﬁciency\nbeneﬁts of dynamic masking, we use dynamic\nmasking in the remainder of the experiments."}
{"doc_id": "1907.11692", "para_id": 41, "text": "4.2\nModel Input Format and Next Sentence\nPrediction"}
{"doc_id": "1907.11692", "para_id": 42, "text": "This section explores and quantiﬁes which choices\nare important for successfully pretraining BERT\nmodels. We keep the model architecture ﬁxed.7"}
{"doc_id": "1907.11692", "para_id": 43, "text": "In the original BERT pretraining procedure, the\nmodel observes two concatenated document seg-\nments, which are either sampled contiguously\nfrom the same document (with p = 0.5) or from\ndistinct documents. In addition to the masked lan-\nguage modeling objective, the model is trained to\npredict whether the observed document segments\ncome from the same or distinct documents via an\nauxiliary Next Sentence Prediction (NSP) loss.\nThe NSP loss was hypothesized to be an impor-\ntant factor in training the original BERT model.\nDevlin et al. (2019) observe that removing NSP\nhurts performance, with signiﬁcant performance\ndegradation on QNLI, MNLI, and SQuAD 1.1.\nHowever, some recent work has questioned the\nnecessity of the NSP loss (Lample and Conneau,\n2019; Yang et al., 2019; Joshi et al., 2019).\nTo better understand this discrepancy, we com-\npare several alternative training formats:"}
{"doc_id": "1907.11692", "para_id": 44, "text": "Speciﬁcally, we begin by training BERT models\nwith the same conﬁguration as BERTBASE (L =\n12, H = 768, A = 12, 110M params)."}
{"doc_id": "1907.11692", "para_id": 45, "text": "As discussed in Section 2, BERT relies on ran-\ndomly masking and predicting tokens. The orig-\ninal BERT implementation performed masking\nonce during data preprocessing, resulting in a sin-\ngle static mask. To avoid using the same mask for\neach training instance in every epoch, training data\nwas duplicated 10 times so that each sequence is\nmasked in 10 different ways over the 40 epochs of\ntraining. Thus, each training sequence was seen\nwith the same mask four times during training.\nWe compare this strategy with dynamic mask-\ning where we generate the masking pattern every\ntime we feed a sequence to the model. This be-\ncomes crucial when pretraining for more steps or\nwith larger datasets."}
{"doc_id": "1907.11692", "para_id": 46, "text": "• SEGMENT-PAIR+NSP: This follows the original\ninput format used in BERT (Devlin et al., 2019),\nwith the NSP loss. Each input has a pair of seg-\nments, which can each contain multiple natural\nsentences, but the total combined length must\nbe less than 512 tokens."}
{"doc_id": "1907.11692", "para_id": 47, "text": "7Studying architectural changes, including larger archi-\ntectures, is an important area for future work."}
{"doc_id": "1907.11692", "para_id": 48, "text": "BERTBASE\n88.5/76.3\n84.3\n92.8\n64.3\nXLNetBASE (K = 7)\n–/81.3\n85.8\n92.7\n66.1\nXLNetBASE (K = 6)\n–/81.0\n85.6\n93.4\n66.7"}
{"doc_id": "1907.11692", "para_id": 49, "text": "Table 2: Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are\ntrained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m,\nSST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERTBASE and\nXLNetBASE are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 50, "text": "• SENTENCE-PAIR+NSP: Each input contains a\npair of natural sentences, either sampled from\na contiguous portion of one document or from\nseparate documents. Since these inputs are sig-\nniﬁcantly shorter than 512 tokens, we increase\nthe batch size so that the total number of tokens\nremains similar to SEGMENT-PAIR+NSP. We re-\ntain the NSP loss."}
{"doc_id": "1907.11692", "para_id": 51, "text": "We next compare training without the NSP\nloss and training with blocks of text from a sin-\ngle document (DOC-SENTENCES).\nWe ﬁnd that\nthis setting outperforms the originally published\nBERTBASE results and that removing the NSP loss\nmatches or slightly improves downstream task\nperformance, in contrast to Devlin et al. (2019).\nIt is possible that the original BERT implementa-\ntion may only have removed the loss term while\nstill retaining the SEGMENT-PAIR input format.\nFinally we ﬁnd that restricting sequences to\ncome from a single document (DOC-SENTENCES)\nperforms slightly better than packing sequences\nfrom multiple documents (FULL-SENTENCES).\nHowever, because the DOC-SENTENCES format\nresults in variable batch sizes, we use FULL-"}
{"doc_id": "1907.11692", "para_id": 52, "text": "• FULL-SENTENCES: Each input is packed with\nfull sentences sampled contiguously from one\nor more documents, such that the total length is\nat most 512 tokens. Inputs may cross document\nboundaries. When we reach the end of one doc-\nument, we begin sampling sentences from the\nnext document and add an extra separator token\nbetween documents. We remove the NSP loss."}
{"doc_id": "1907.11692", "para_id": 53, "text": "SENTENCES in the remainder of our experiments\nfor easier comparison with related work."}
{"doc_id": "1907.11692", "para_id": 54, "text": "• DOC-SENTENCES: Inputs are constructed sim-\nilarly to FULL-SENTENCES, except that they\nmay not cross document boundaries.\nInputs\nsampled near the end of a document may be\nshorter than 512 tokens, so we dynamically in-\ncrease the batch size in these cases to achieve\na similar number of total tokens as FULL-"}
{"doc_id": "1907.11692", "para_id": 55, "text": "Past work in Neural Machine Translation has\nshown that training with very large mini-batches\ncan both improve optimization speed and end-task\nperformance when the learning rate is increased\nappropriately (Ott et al., 2018). Recent work has\nshown that BERT is also amenable to large batch\ntraining (You et al., 2019)."}
{"doc_id": "1907.11692", "para_id": 56, "text": "Results\nTable 2 shows results for the four dif-\nferent settings.\nWe ﬁrst compare the original"}
{"doc_id": "1907.11692", "para_id": 57, "text": "Devlin et al.\n(2019)\noriginally\ntrained\nBERTBASE for 1M steps with a batch size of\n256 sequences.\nThis is equivalent in computa-\ntional cost, via gradient accumulation, to training\nfor 125K steps with a batch size of 2K sequences,\nor for 31K steps with a batch size of 8K.\nIn Table 3 we compare perplexity and end-"}
{"doc_id": "1907.11692", "para_id": 58, "text": "SEGMENT-PAIR input format from Devlin et al.\n(2019) to the SENTENCE-PAIR format; both for-\nmats retain the NSP loss, but the latter uses sin-\ngle sentences.\nWe ﬁnd that using individual\nsentences hurts performance on downstream\ntasks, which we hypothesize is because the model\nis not able to learn long-range dependencies."}
{"doc_id": "1907.11692", "para_id": 59, "text": "The\noriginal\nBERT\nimplementa-\ntion (Devlin et al., 2019) uses a character-level\nBPE vocabulary of size 30K, which is learned\nafter preprocessing the input with heuristic tok-\nenization rules. Following Radford et al. (2019),\nwe instead consider training BERT with a larger\nbyte-level BPE vocabulary containing 50K sub-\nword units, without any additional preprocessing\nor tokenization of the input. This adds approxi-\nmately 15M and 20M additional parameters for\nBERTBASE and BERTLARGE, respectively.\nEarly experiments revealed only slight dif-\nferences between these encodings,\nwith the\nRadford et al. (2019)\nBPE achieving\nslightly\nworse end-task performance on some tasks. Nev-\nertheless, we believe the advantages of a univer-\nsal encoding scheme outweighs the minor degre-\ndation in performance and use this encoding in\nthe remainder of our experiments.\nA more de-\ntailed comparison of these encodings is left to fu-\nture work."}
{"doc_id": "1907.11692", "para_id": 60, "text": "256\n1M\n1e-4\n3.99\n84.7\n92.7\n2K\n125K\n7e-4\n3.68\n85.2\n92.9\n8K\n31K\n1e-3\n3.77\n84.6\n92.8"}
{"doc_id": "1907.11692", "para_id": 61, "text": "Table 3: Perplexity on held-out training data (ppl) and\ndevelopment set accuracy for base models trained over\nBOOKCORPUS and WIKIPEDIA with varying batch\nsizes (bsz). We tune the learning rate (lr) for each set-\nting. Models make the same number of passes over the\ndata (epochs) and have the same computational cost."}
{"doc_id": "1907.11692", "para_id": 62, "text": "task performance of BERTBASE as we increase the\nbatch size, controlling for the number of passes\nthrough the training data. We observe that train-\ning with large batches improves perplexity for the\nmasked language modeling objective, as well as\nend-task accuracy. Large batches are also easier to\nparallelize via distributed data parallel training,8"}
{"doc_id": "1907.11692", "para_id": 63, "text": "and in later experiments we train with batches of\n8K sequences.\nNotably You et al. (2019) train BERT with even\nlarger batche sizes, up to 32K sequences. We leave\nfurther exploration of the limits of large batch\ntraining to future work."}
{"doc_id": "1907.11692", "para_id": 64, "text": "In the previous section we propose modiﬁcations\nto the BERT pretraining procedure that improve\nend-task performance.\nWe now aggregate these\nimprovements and evaluate their combined im-\npact.\nWe call this conﬁguration RoBERTa for\nRobustly optimized BERT approach.\nSpeciﬁ-\ncally, RoBERTa is trained with dynamic mask-\ning (Section 4.1), FULL-SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture (Yang et al., 2019) is pretrained us-\ning nearly 10 times more data than the original\nBERT (Devlin et al., 2019). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERTLARGE architecture (L = 24,\nH = 1024, A = 16, 355M parameters).\nWe\npretrain for 100K steps over a comparable BOOK-\nCORPUS plus WIKIPEDIA dataset as was used in"}
{"doc_id": "1907.11692", "para_id": 65, "text": "Byte-Pair Encoding (BPE) (Sennrich et al., 2016)\nis a hybrid between character- and word-level rep-\nresentations that allows handling the large vocab-\nularies common in natural language corpora. In-\nstead of full words, BPE relies on subwords units,\nwhich are extracted by performing statistical anal-\nysis of the training corpus.\nBPE vocabulary sizes typically range from\n10K-100K subword units. However, unicode char-\nacters can account for a sizeable portion of this\nvocabulary when modeling large and diverse cor-\npora, such as the ones considered in this work.\nRadford et al. (2019) introduce a clever imple-\nmentation of BPE that uses bytes instead of uni-\ncode characters as the base subword units. Using\nbytes makes it possible to learn a subword vocab-\nulary of a modest size (50K units) that can still en-\ncode any input text without introducing any “un-\nknown” tokens."}
{"doc_id": "1907.11692", "para_id": 66, "text": "8Large batch training can improve training efﬁciency even\nwithout large scale parallel hardware through gradient ac-\ncumulation, whereby gradients from multiple mini-batches\nare accumulated locally before each optimization step. This\nfunctionality is supported natively in FAIRSEQ (Ott et al.,\n2019)."}
{"doc_id": "1907.11692", "para_id": 67, "text": "Model\ndata\nbsz\nsteps\nSQuAD\nMNLI-m\nSST-2\n(v1.1/2.0)"}
{"doc_id": "1907.11692", "para_id": 68, "text": "RoBERTa\nwith BOOKS + WIKI\n16GB\n8K\n100K\n93.6/87.3\n89.0\n95.3\n+ additional data (§3.2)\n160GB\n8K\n100K\n94.0/87.7\n89.3\n95.6\n+ pretrain longer\n160GB\n8K\n300K\n94.4/88.7\n90.0\n96.1\n+ pretrain even longer\n160GB\n8K\n500K\n94.6/89.4\n90.2\n96.4"}
{"doc_id": "1907.11692", "para_id": 69, "text": "BERTLARGE\nwith BOOKS + WIKI\n13GB\n256\n1M\n90.9/81.8\n86.6\n93.7\nXLNetLARGE\nwith BOOKS + WIKI\n13GB\n256\n1M\n94.0/87.8\n88.4\n94.4\n+ additional data\n126GB\n2K\n500K\n94.5/88.8\n89.8\n95.6"}
{"doc_id": "1907.11692", "para_id": 70, "text": "Table 4: Development set results for RoBERTa as we pretrain over more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K →500K steps). Each row accumulates improvements from the rows above. RoBERTa\nmatches the architecture and training objective of BERTLARGE. Results for BERTLARGE and XLNetLARGE are from\nDevlin et al. (2019) and Yang et al. (2019), respectively. Complete results on all GLUE tasks can be found in the\nAppendix."}
{"doc_id": "1907.11692", "para_id": 71, "text": "we consider RoBERTa trained for 500K steps over\nall ﬁve of the datasets introduced in Section 3.2."}
{"doc_id": "1907.11692", "para_id": 72, "text": "Devlin et al. (2019). We pretrain our model using\n1024 V100 GPUs for approximately one day."}
{"doc_id": "1907.11692", "para_id": 73, "text": "Results\nWe present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERTLARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2.\nWe\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9"}
{"doc_id": "1907.11692", "para_id": 74, "text": "For GLUE we consider two ﬁnetuning settings.\nIn the ﬁrst setting (single-task, dev) we ﬁnetune\nRoBERTa separately for each of the GLUE tasks,\nusing only the training data for the correspond-\ning task. We consider a limited hyperparameter\nsweep for each task, with batch sizes ∈{16, 32}\nand learning rates ∈{1e−5, 2e−5, 3e−5}, with a\nlinear warmup for the ﬁrst 6% of steps followed by\na linear decay to 0. We ﬁnetune for 10 epochs and\nperform early stopping based on each task’s eval-\nuation metric on the dev set. The rest of the hyper-\nparameters remain the same as during pretraining.\nIn this setting, we report the median development\nset results for each task over ﬁve random initial-\nizations, without model ensembling."}
{"doc_id": "1907.11692", "para_id": 75, "text": "Finally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNetLARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally"}
{"doc_id": "1907.11692", "para_id": 76, "text": "In the second setting (ensembles, test), we com-\npare RoBERTa to other approaches on the test set\nvia the GLUE leaderboard. While many submis-\nsions to the GLUE leaderboard depend on multi-\ntask ﬁnetuning, our submission depends only on\nsingle-task ﬁnetuning. For RTE, STS and MRPC\nwe found it helpful to ﬁnetune starting from the\nMNLI single-task model, rather than the baseline\npretrained RoBERTa. We explore a slightly wider\nhyperparameter space, described in the Appendix,\nand ensemble between 5 and 7 models per task."}
{"doc_id": "1907.11692", "para_id": 77, "text": "9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work."}
{"doc_id": "1907.11692", "para_id": 78, "text": "Single-task single models on dev\nBERTLARGE\n86.6/-\n92.3\n91.3\n70.4\n93.2\n88.0\n60.6\n90.0\n-\n-\nXLNetLARGE\n89.8/-\n93.9\n91.8\n83.8\n95.6\n89.2\n63.6\n91.8\n-\n-\nRoBERTa\n90.2/90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4\n91.3\n-"}
{"doc_id": "1907.11692", "para_id": 79, "text": "Ensembles on test (from leaderboard as of July 25, 2019)\nALICE\n88.2/87.9\n95.7\n90.7\n83.5\n95.2\n92.6\n68.6\n91.1\n80.8\n86.3\nMT-DNN\n87.9/87.4\n96.0\n89.9\n86.3\n96.5\n92.7\n68.4\n91.1\n89.0\n87.6\nXLNet\n90.2/89.8\n98.6\n90.3\n86.3\n96.8\n93.0\n67.8\n91.6\n90.4\n88.4\nRoBERTa\n90.8/90.2\n98.9\n90.2\n88.2\n96.7\n92.3\n67.8\n92.2\n89.0\n88.5"}
{"doc_id": "1907.11692", "para_id": 80, "text": "Table 5: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE and XLNetLARGE results\nare from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the development set are a\nmedian over ﬁve runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and\nMRPC we ﬁnetune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained\nfrom the GLUE leaderboard."}
{"doc_id": "1907.11692", "para_id": 81, "text": "Results\nWe present our results in Table 5. In the\nﬁrst setting (single-task, dev), RoBERTa achieves\nstate-of-the-art results on all 9 of the GLUE\ntask development sets. Crucially, RoBERTa uses\nthe same masked language modeling pretrain-\ning objective and architecture as BERTLARGE, yet\nconsistently outperforms both BERTLARGE and\nXLNetLARGE. This raises questions about the rel-\native importance of model architecture and pre-\ntraining objective, compared to more mundane de-\ntails like dataset size and training time that we ex-\nplore in this work.\nIn the second setting (ensembles, test), we\nsubmit RoBERTa to the GLUE leaderboard and\nachieve state-of-the-art results on 4 out of 9 tasks\nand the highest average score to date. This is espe-\ncially exciting because RoBERTa does not depend\non multi-task ﬁnetuning, unlike most of the other\ntop submissions. We expect future work may fur-\nther improve these results by incorporating more\nsophisticated multi-task ﬁnetuning procedures."}
{"doc_id": "1907.11692", "para_id": 82, "text": "Task-speciﬁc modiﬁcations\nTwo of the GLUE\ntasks require task-speciﬁc ﬁnetuning approaches\nto achieve competitive leaderboard results.\nQNLI:\nRecent submissions on the GLUE\nleaderboard adopt a pairwise ranking formulation\nfor the QNLI task, in which candidate answers\nare mined from the training set and compared to\none another, and a single (question, candidate)\npair is classiﬁed as positive (Liu et al., 2019b,a;\nYang et al., 2019). This formulation signiﬁcantly\nsimpliﬁes the task, but is not directly comparable\nto BERT (Devlin et al., 2019). Following recent\nwork, we adopt the ranking approach for our test\nsubmission, but for direct comparison with BERT\nwe report development set results based on a pure\nclassiﬁcation approach.\nWNLI:\nWe found the provided NLI-format\ndata to be challenging to work with.\nInstead\nwe use the reformatted WNLI data from Super-\nGLUE (Wang et al., 2019a), which indicates the\nspan of the query pronoun and referent. We ﬁne-\ntune RoBERTa using the margin ranking loss from\nKocijan et al. (2019). For a given input sentence,\nwe use spaCy (Honnibal and Montani, 2017) to\nextract additional candidate noun phrases from the\nsentence and ﬁnetune our model so that it assigns\nhigher scores to positive referent phrases than for\nany of the generated negative candidate phrases.\nOne unfortunate consequence of this formulation\nis that we can only make use of the positive train-\ning examples, which excludes over half of the pro-\nvided training examples.10"}
{"doc_id": "1907.11692", "para_id": 83, "text": "We adopt a much simpler approach for SQuAD\ncompared to past work.\nIn particular, while\nboth\nBERT\n(Devlin et al.,\n2019)\nand\nXL-\nNet (Yang et al., 2019) augment their training data\nwith additional QA datasets, we only ﬁnetune\nRoBERTa using the provided SQuAD training\ndata. Yang et al. (2019) also employed a custom\nlayer-wise learning rate schedule to ﬁnetune"}
{"doc_id": "1907.11692", "para_id": 84, "text": "results could potentially be improved by augmenting this with\nadditional pronoun disambiguation datasets."}
{"doc_id": "1907.11692", "para_id": 85, "text": "10While we only use the provided WNLI training data, our"}
{"doc_id": "1907.11692", "para_id": 86, "text": "Single models on test (as of July 25, 2019)\nBERTLARGE\n72.0\n76.6\n70.1\nXLNetLARGE\n81.7\n85.4\n80.2"}
{"doc_id": "1907.11692", "para_id": 87, "text": "Single models on dev, w/o data augmentation\nBERTLARGE\n84.1\n90.9\n79.0\n81.8\nXLNetLARGE\n89.0\n94.5\n86.1\n88.8\nRoBERTa\n88.9\n94.6\n86.5\n89.4"}
{"doc_id": "1907.11692", "para_id": 88, "text": "Single models on test (as of July 25, 2019)\nXLNetLARGE\n86.3†\n89.1†"}
{"doc_id": "1907.11692", "para_id": 89, "text": "Table 7: Results on the RACE test set. BERTLARGE and\nXLNetLARGE results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 90, "text": "RoBERTa\n86.8\n89.8\nXLNet + SG-Net Veriﬁer\n87.0†\n89.9†"}
{"doc_id": "1907.11692", "para_id": 91, "text": "nating each candidate answer with the correspond-\ning question and passage. We then encode each of\nthese four sequences and pass the resulting [CLS]\nrepresentations through a fully-connected layer,\nwhich is used to predict the correct answer. We\ntruncate question-answer pairs that are longer than\n128 tokens and, if needed, the passage so that the\ntotal length is at most 512 tokens."}
{"doc_id": "1907.11692", "para_id": 92, "text": "Table 6: Results on SQuAD. † indicates results that de-\npend on additional external training data. RoBERTa\nuses only the provided SQuAD data in both dev and\ntest settings. BERTLARGE and XLNetLARGE results are\nfrom Devlin et al. (2019) and Yang et al. (2019), re-\nspectively."}
{"doc_id": "1907.11692", "para_id": 93, "text": "XLNet, while we use the same learning rate for\nall layers.\nFor SQuAD v1.1 we follow the same ﬁnetun-\ning procedure as Devlin et al. (2019). For SQuAD\nv2.0, we additionally classify whether a given\nquestion is answerable; we train this classiﬁer\njointly with the span predictor by summing the\nclassiﬁcation and span loss terms."}
{"doc_id": "1907.11692", "para_id": 94, "text": "Results on the RACE test sets are presented in\nTable 7. RoBERTa achieves state-of-the-art results\non both middle-school and high-school settings."}
{"doc_id": "1907.11692", "para_id": 95, "text": "Pretraining\nmethods\nhave\nbeen\ndesigned\nwith\ndifferent\ntraining\nobjectives,\ninclud-\ning\nlanguage\nmodeling\n(Dai and Le,\n2015;\nPeters et al.,\n2018;\nHoward and Ruder,\n2018),\nmachine translation (McCann et al., 2017), and\nmasked language modeling (Devlin et al., 2019;\nLample and Conneau,\n2019).\nMany\nrecent\npapers have used a basic recipe of ﬁnetuning\nmodels for each end task (Howard and Ruder,\n2018;\nRadford et al.,\n2018),\nand\npretraining\nwith some variant of a masked language model\nobjective.\nHowever,\nnewer\nmethods\nhave\nimproved performance by multi-task ﬁne tun-\ning\n(Dong et al.,\n2019),\nincorporating\nentity\nembeddings\n(Sun et al.,\n2019),\nspan\npredic-\ntion (Joshi et al., 2019), and multiple variants\nof autoregressive pretraining (Song et al., 2019;\nChan et al., 2019; Yang et al., 2019).\nPerfor-\nmance is also typically improved by training\nbigger\nmodels\non\nmore\ndata\n(Devlin et al.,\n2019; Baevski et al., 2019; Yang et al., 2019;\nRadford et al., 2019). Our goal was to replicate,\nsimplify, and better tune the training of BERT,\nas a reference point for better understanding the\nrelative performance of all of these methods."}
{"doc_id": "1907.11692", "para_id": 96, "text": "Results\nWe present our results in Table 6. On\nthe SQuAD v1.1 development set, RoBERTa\nmatches the state-of-the-art set by XLNet. On the\nSQuAD v2.0 development set, RoBERTa sets a\nnew state-of-the-art, improving over XLNet by 0.4\npoints (EM) and 0.6 points (F1).\nWe also submit RoBERTa to the public SQuAD\n2.0 leaderboard and evaluate its performance rel-\native to other systems. Most of the top systems\nbuild upon either BERT (Devlin et al., 2019) or\nXLNet (Yang et al., 2019), both of which rely on\nadditional external training data. In contrast, our\nsubmission does not use any additional data.\nOur single RoBERTa model outperforms all but\none of the single model submissions, and is the\ntop scoring system among those that do not rely\non data augmentation."}
{"doc_id": "1907.11692", "para_id": 97, "text": "In RACE, systems are provided with a passage of\ntext, an associated question, and four candidate an-\nswers. Systems are required to classify which of\nthe four candidate answers is correct.\nWe modify RoBERTa for this task by concate-"}
{"doc_id": "1907.11692", "para_id": 98, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment."}
{"doc_id": "1907.11692", "para_id": 99, "text": "We carefully evaluate a number of design de-\ncisions when pretraining BERT models.\nWe\nﬁnd that performance can be substantially im-\nproved by training the model longer, with bigger\nbatches over more data; removing the next sen-\ntence prediction objective; training on longer se-\nquences; and dynamically changing the masking\npattern applied to the training data. Our improved\npretraining procedure, which we call RoBERTa,\nachieves state-of-the-art results on GLUE, RACE\nand SQuAD, without multi-task ﬁnetuning for\nGLUE or additional data for SQuAD. These re-\nsults illustrate the importance of these previ-\nously overlooked design decisions and suggest\nthat BERT’s pretraining objective remains com-\npetitive with recently proposed alternatives.\nWe\nadditionally\nuse\na\nnovel\ndataset,\nCC-NEWS,\nand\nrelease\nour\nmodels\nand\ncode\nfor\npretraining\nand\nﬁnetuning\nat:\nhttps://github.com/pytorch/fairseq."}
{"doc_id": "1907.11692", "para_id": 100, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in Neural Informa-\ntion Processing Systems (NIPS)."}
{"doc_id": "1907.11692", "para_id": 101, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 102, "text": "William B Dolan and Chris Brockett. 2005.\nAuto-\nmatically constructing a corpus of sentential para-\nphrases. In Proceedings of the International Work-\nshop on Paraphrasing."}
{"doc_id": "1907.11692", "para_id": 103, "text": "Li Dong, Nan Yang, Wenhui Wang,\nFuru Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019.\nUniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation.\narXiv preprint\narXiv:1905.03197."}
{"doc_id": "1907.11692", "para_id": 104, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing."}
{"doc_id": "1907.11692", "para_id": 105, "text": "Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-\ntowski, editors. 2007.\nProceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007)."}
{"doc_id": "1907.11692", "para_id": 106, "text": "Aaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus.\nhttp://web.archive.org/\nsave/http://Skylion007.github.io/\nOpenWebTextCorpus."}
{"doc_id": "1907.11692", "para_id": 107, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019.\nCloze-\ndriven pretraining of self-attention networks. arXiv\npreprint arXiv:1903.07785."}
{"doc_id": "1907.11692", "para_id": 108, "text": "Felix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017.\nnews-please:\nA\ngeneric news crawler and extractor. In Proceedings\nof the 15th International Symposium of Information\nScience."}
{"doc_id": "1907.11692", "para_id": 109, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment."}
{"doc_id": "1907.11692", "para_id": 110, "text": "Dan Hendrycks and Kevin Gimpel. 2016.\nGaus-\nsian error linear units (gelus).\narXiv preprint\narXiv:1606.08415."}
{"doc_id": "1907.11692", "para_id": 111, "text": "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009.\nThe\nﬁfth PASCAL recognizing textual entailment chal-\nlenge."}
{"doc_id": "1907.11692", "para_id": 112, "text": "Matthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear."}
{"doc_id": "1907.11692", "para_id": 113, "text": "Samuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 114, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146."}
{"doc_id": "1907.11692", "para_id": 115, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Cser-\nnai. 2016.\nFirst quora dataset release: Question\npairs.\nhttps://data.quora.com/First-\nQuora-Dataset-Release-Question-\nPairs."}
{"doc_id": "1907.11692", "para_id": 116, "text": "William Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604."}
{"doc_id": "1907.11692", "para_id": 117, "text": "Myle Ott,\nSergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation (WMT)."}
{"doc_id": "1907.11692", "para_id": 118, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanBERT:\nImproving\npre-training\nby\nrepre-\nsenting and predicting spans.\narXiv preprint\narXiv:1907.10529."}
{"doc_id": "1907.11692", "para_id": 119, "text": "Adam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS Autodiff Workshop."}
{"doc_id": "1907.11692", "para_id": 120, "text": "Diederik Kingma and Jimmy Ba. 2015.\nAdam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR)."}
{"doc_id": "1907.11692", "para_id": 121, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\nA surprisingly robust trick for winograd schema\nchallenge. arXiv preprint arXiv:1905.06290."}
{"doc_id": "1907.11692", "para_id": 122, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Association for Com-\nputational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 123, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations.\narXiv\npreprint arXiv:1704.04683."}
{"doc_id": "1907.11692", "para_id": 124, "text": "Alec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning. Technical\nreport, OpenAI."}
{"doc_id": "1907.11692", "para_id": 125, "text": "Guillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291."}
{"doc_id": "1907.11692", "para_id": 126, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI."}
{"doc_id": "1907.11692", "para_id": 127, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical Formalizations of\nCommonsense Reasoning."}
{"doc_id": "1907.11692", "para_id": 128, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Association for Computational\nLinguistics (ACL)."}
{"doc_id": "1907.11692", "para_id": 129, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019a.\nImproving multi-task deep\nneural networks via knowledge distillation for\nnatural language understanding.\narXiv preprint\narXiv:1904.09482."}
{"doc_id": "1907.11692", "para_id": 130, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 131, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint\narXiv:1901.11504."}
{"doc_id": "1907.11692", "para_id": 132, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units.\nIn Association for Computational\nLinguistics (ACL), pages 1715–1725."}
{"doc_id": "1907.11692", "para_id": 133, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems (NIPS), pages 6297–\n6308."}
{"doc_id": "1907.11692", "para_id": 134, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 135, "text": "Paulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\nsion training. In International Conference on Learn-\ning Representations."}
{"doc_id": "1907.11692", "para_id": 136, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2019.\nMASS: Masked sequence\nto sequence pre-training for language generation.\nIn International Conference on Machine Learning\n(ICML)."}
{"doc_id": "1907.11692", "para_id": 137, "text": "Sebastian\nNagel.\n2016.\nCc-news.\nhttp:\n//web.archive.org/save/http:\n//commoncrawl.org/2016/10/news-\ndataset-available."}
{"doc_id": "1907.11692", "para_id": 138, "text": "Yu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun\nFeng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi-\nang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: En-\nhanced representation through knowledge integra-\ntion. arXiv preprint arXiv:1904.09223."}
{"doc_id": "1907.11692", "para_id": 139, "text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.\nFAIRSEQ:\nA fast, exten-\nsible toolkit for sequence modeling.\nIn North\nAmerican Association for Computational Linguis-\ntics (NAACL): System Demonstrations."}
{"doc_id": "1907.11692", "para_id": 140, "text": "Trieu H Trinh and Quoc V Le. 2018.\nA simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847."}
{"doc_id": "1907.11692", "para_id": 141, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems."}
{"doc_id": "1907.11692", "para_id": 142, "text": "Table 9 describes the hyperparameters for pre-\ntraining of RoBERTaLARGE and RoBERTaBASE"}
{"doc_id": "1907.11692", "para_id": 143, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537."}
{"doc_id": "1907.11692", "para_id": 144, "text": "Finetuning hyperparameters for RACE, SQuAD\nand GLUE are given in Table 10. We select the\nbest hyperparameter values based on the median\nof 5 random seeds for each task."}
{"doc_id": "1907.11692", "para_id": 145, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR)."}
{"doc_id": "1907.11692", "para_id": 146, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint 1805.12471."}
{"doc_id": "1907.11692", "para_id": 147, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn North\nAmerican Association for Computational Linguis-\ntics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 148, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding.\narXiv preprint\narXiv:1906.08237."}
{"doc_id": "1907.11692", "para_id": 149, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\ning bert pre-training time from 3 days to 76 minutes.\narXiv preprint arXiv:1904.00962."}
{"doc_id": "1907.11692", "para_id": 150, "text": "Rowan Zellers,\nAri Holtzman,\nHannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019.\nDefending against neural fake\nnews. arXiv preprint arXiv:1905.12616."}
{"doc_id": "1907.11692", "para_id": 151, "text": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books.\nIn arXiv preprint\narXiv:1506.06724."}
{"doc_id": "1907.11692", "para_id": 152, "text": "Appendix for “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”"}
{"doc_id": "1907.11692", "para_id": 153, "text": "In Table 8 we present the full set of development\nset results for RoBERTa. We present results for\na LARGE conﬁguration that follows BERTLARGE,\nas well as a BASE conﬁguration that follows\nBERTBASE."}
{"doc_id": "1907.11692", "para_id": 154, "text": "RoBERTaBASE\n+ all data + 500k steps\n87.6\n92.8\n91.9\n78.7\n94.8\n90.2\n63.6\n91.2"}
{"doc_id": "1907.11692", "para_id": 155, "text": "RoBERTaLARGE\nwith BOOKS + WIKI\n89.0\n93.9\n91.9\n84.5\n95.3\n90.2\n66.3\n91.6\n+ additional data (§3.2)\n89.3\n94.0\n92.0\n82.7\n95.6\n91.4\n66.1\n92.2\n+ pretrain longer 300k\n90.0\n94.5\n92.2\n83.3\n96.1\n91.1\n67.4\n92.3\n+ pretrain longer 500k\n90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4"}
{"doc_id": "1907.11692", "para_id": 156, "text": "Table 8: Development set results on GLUE tasks for various conﬁgurations of RoBERTa."}
{"doc_id": "1907.11692", "para_id": 157, "text": "Number of Layers\n24\n12\nHidden size\n1024\n768\nFFN inner hidden size\n4096\n3072\nAttention heads\n16\n12\nAttention head size\n64\n64\nDropout\n0.1\n0.1\nAttention Dropout\n0.1\n0.1\nWarmup Steps\n30k\n24k\nPeak Learning Rate\n4e-4\n6e-4\nBatch Size\n8k\n8k\nWeight Decay\n0.01\n0.01\nMax Steps\n500k\n500k\nLearning Rate Decay\nLinear\nLinear\nAdam ǫ\n1e-6\n1e-6\nAdam β1\n0.9\n0.9\nAdam β2\n0.98\n0.98\nGradient Clipping\n0.0\n0.0"}
{"doc_id": "1907.11692", "para_id": 158, "text": "Table 9: Hyperparameters for pretraining RoBERTaLARGE and RoBERTaBASE."}
{"doc_id": "1907.11692", "para_id": 159, "text": "Learning Rate\n1e-5\n1.5e-5\n{1e-5, 2e-5, 3e-5}\nBatch Size\n16\n48\n{16, 32}\nWeight Decay\n0.1\n0.01\n0.1\nMax Epochs\n4\n2\n10\nLearning Rate Decay\nLinear\nLinear\nLinear\nWarmup ratio\n0.06\n0.06\n0.06"}
{"doc_id": "1907.11692", "para_id": 160, "text": "Table 10: Hyperparameters for ﬁnetuning RoBERTaLARGE on RACE, SQuAD and GLUE."}
{"doc_id": "2005.14165", "para_id": 0, "text": "Tom B. Brown∗\nBenjamin Mann∗\nNick Ryder∗\nMelanie Subbiah∗"}
{"doc_id": "2005.14165", "para_id": 1, "text": "Jared Kaplan†\nPrafulla Dhariwal\nArvind Neelakantan\nPranav Shyam\nGirish Sastry"}
{"doc_id": "2005.14165", "para_id": 2, "text": "Amanda Askell\nSandhini Agarwal\nAriel Herbert-Voss\nGretchen Krueger\nTom Henighan"}
{"doc_id": "2005.14165", "para_id": 3, "text": "Rewon Child\nAditya Ramesh\nDaniel M. Ziegler\nJeffrey Wu\nClemens Winter"}
{"doc_id": "2005.14165", "para_id": 4, "text": "Christopher Hesse\nMark Chen\nEric Sigler\nMateusz Litwin\nScott Gray"}
{"doc_id": "2005.14165", "para_id": 5, "text": "Sam McCandlish\nAlec Radford\nIlya Sutskever\nDario Amodei"}
{"doc_id": "2005.14165", "para_id": 6, "text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic\nin architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions – something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-\ntuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning,\nwith tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\nand of GPT-3 in general."}
{"doc_id": "2005.14165", "para_id": 7, "text": "∗Equal contribution\n†Johns Hopkins University, OpenAI"}
{"doc_id": "2005.14165", "para_id": 8, "text": "2\nApproach\n6\n2.1\nModel and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nTraining Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3\nTraining Process\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.4\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10"}
{"doc_id": "2005.14165", "para_id": 9, "text": "3\nResults\n10\n3.1\nLanguage Modeling, Cloze, and Completion Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2\nClosed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.3\nTranslation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.4\nWinograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.5\nCommon Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.6\nReading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.7\nSuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.8\nNLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.9\nSynthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21"}
{"doc_id": "2005.14165", "para_id": 10, "text": "4\nMeasuring and Preventing Memorization Of Benchmarks\n29"}
{"doc_id": "2005.14165", "para_id": 11, "text": "6\nBroader Impacts\n34\n6.1\nMisuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n6.2\nFairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n6.3\nEnergy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39"}
{"doc_id": "2005.14165", "para_id": 12, "text": "E\nHuman Quality Assessment of Synthetic News Articles\n46"}
{"doc_id": "2005.14165", "para_id": 13, "text": "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\nﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\nvectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\nand contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\ntask-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\nbeen directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18]."}
{"doc_id": "2005.14165", "para_id": 14, "text": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\nquestion answering, textual entailment, and many others, and has continued to advance based on new architectures\nand algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while\nthe architecture is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve\nstrong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands\nof examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons."}
{"doc_id": "2005.14165", "para_id": 15, "text": "First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\napplicability of language models. There exists a very wide range of possible useful language tasks, encompassing\nanything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\nof these tasks it is difﬁcult to collect a large supervised training dataset, especially when the process must be repeated\nfor every new task."}
{"doc_id": "2005.14165", "para_id": 16, "text": "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\nof the model and the narrowness of the training distribution. This can create problems for the pre-training plus\nﬁne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\nﬁne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily\ngeneralize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\ncan be poor because the model is overly speciﬁc to the training distribution and does not generalize well outside it\n[YdC+19, MPL19]. Thus, the performance of ﬁne-tuned models on speciﬁc benchmarks, even when it is nominally at\nhuman-level, may exaggerate actual performance on the underlying task [GSL+18, NK19]."}
{"doc_id": "2005.14165", "para_id": 17, "text": "Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\nlanguage (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number\nof demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often"}
{"doc_id": "2005.14165", "para_id": 18, "text": "Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\nset of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize\nthe desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within\nthe forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\nmodel would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\nwithin a single sequence."}
{"doc_id": "2005.14165", "para_id": 19, "text": "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\nperformance on a simple task requiring the model to remove random symbols from a word, both with and without a\nnatural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\nimproved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\nof tasks."}
{"doc_id": "2005.14165", "para_id": 20, "text": "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\nto a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\nto seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\ndialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality."}
{"doc_id": "2005.14165", "para_id": 21, "text": "One potential route towards addressing these issues is meta-learning1 – which in the context of language models means\nthe model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\nat inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19]\nattempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form\nof task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\nand is then expected to complete further instances of the task simply by predicting what comes next."}
{"doc_id": "2005.14165", "para_id": 22, "text": "While it has shown some initial promise, this approach still achieves results far inferior to ﬁne-tuning – for example\n[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\nthe state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\nsolving language tasks."}
{"doc_id": "2005.14165", "para_id": 23, "text": "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\nlanguage models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters\n[DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19],\nand ﬁnally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream\nNLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\nsmooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and\ntasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\ngains with scale."}
{"doc_id": "2005.14165", "para_id": 24, "text": "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous:\nthe method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time\ndemonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning”\nto capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner\nloop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\ndemonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\nlearns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\nwe discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\nloop structure."}
{"doc_id": "2005.14165", "para_id": 25, "text": "Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\nimproves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\nmore proﬁcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\nbenchmark suite."}
{"doc_id": "2005.14165", "para_id": 26, "text": "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\nGPT-3, and measuring its in-context learning abilities. Speciﬁcally, we evaluate GPT-3 on over two dozen NLP datasets,\nas well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training\nset. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we\nallow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”,\nwhere we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only\nan instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional\nﬁne-tuning setting, but we leave this to future work."}
{"doc_id": "2005.14165", "para_id": 27, "text": "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to\nremove extraneous symbols from a word. Model performance improves with the addition of a natural language task\ndescription, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically\nwith model size. Though the results in this case are particularly striking, the general trends with both model size and\nnumber of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no\ngradient updates or ﬁne-tuning, just increasing numbers of demonstrations given as conditioning."}
{"doc_id": "2005.14165", "para_id": 28, "text": "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\nsetting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\nby ﬁne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in\nthe one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\nzero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\nrelative to ﬁne-tuned models operating in the same closed-book setting."}
{"doc_id": "2005.14165", "para_id": 29, "text": "GPT-3 also displays one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning,\nwhich include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\ndeﬁned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\nevaluators have difﬁculty distinguishing from human-generated articles."}
{"doc_id": "2005.14165", "para_id": 30, "text": "At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\nincludes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\nor QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we\nhope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed."}
{"doc_id": "2005.14165", "para_id": 31, "text": "A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should\nnot be seen as a rigorous or meaningful benchmark in itself)."}
{"doc_id": "2005.14165", "para_id": 32, "text": "We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models\non datasets such as Common Crawl, which can potentially include content from test datasets simply because such\ncontent often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\nits distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most\ndatasets, we do identify a few datasets where it could be inﬂating results, and we either do not report results on these\ndatasets or we note them with an asterisk, depending on the severity."}
{"doc_id": "2005.14165", "para_id": 33, "text": "In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion\nparameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most\ntasks we ﬁnd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap\nbetween zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models\nare more proﬁcient meta-learners."}
{"doc_id": "2005.14165", "para_id": 34, "text": "Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and\nbroader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard."}
{"doc_id": "2005.14165", "para_id": 35, "text": "The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training\nGPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.\nSection 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.\nSection 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes."}
{"doc_id": "2005.14165", "para_id": 36, "text": "Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19],\nwith relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use\nof in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for\nlearning within the context. Therefore, we start this section by explicitly deﬁning and contrasting the different settings\nthat we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a\nspectrum of how much task-speciﬁc data they tend to rely on. Speciﬁcally, we can identify at least four points on this\nspectrum (see Figure 2.1 for an illustration):"}
{"doc_id": "2005.14165", "para_id": 37, "text": "• Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of\na pre-trained model by training on a supervised dataset speciﬁc to the desired task. Typically thousands to\nhundreds of thousands of labeled examples are used. The main advantage of ﬁne-tuning is strong performance\non many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential\nfor poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the\ntraining data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In\nthis work we do not ﬁne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be\nﬁne-tuned in principle and this is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 38, "text": "• Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few\ndemonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed.\nAs shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example\nan English sentence and the French translation), and few-shot works by giving K examples of context and\ncompletion, and then one ﬁnal example of context, with the model expected to provide the completion. We\ntypically set K in the range of 10 to 100 as this is how many examples can ﬁt in the model’s context window\n(nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speciﬁc data and\nreduced potential to learn an overly narrow distribution from a large but narrow ﬁne-tuning dataset. The main\ndisadvantage is that results from this method have so far been much worse than state-of-the-art ﬁne-tuned\nmodels. Also, a small amount of task speciﬁc data is still required. As indicated by the name, few-shot\nlearning as described here for language models is related to few-shot learning as used in other contexts in\nML [HYC01, VBL+16] – both involve learning based on a broad distribution of tasks (in this case implicit in\nthe pre-training data) and then rapidly adapting to a new task."}
{"doc_id": "2005.14165", "para_id": 39, "text": "• One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural\nlanguage description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and\nzero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.\nFor example, when asking humans to generate a dataset on a human worker service (for example Mechanical\nTurk), it is common to give one demonstration of the task. By contrast it is sometimes difﬁcult to communicate\nthe content or format of a task if no examples are given."}
{"doc_id": "2005.14165", "para_id": 40, "text": "Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning. The panels above show\nfour methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\ndescriptions, examples and prompts can be found in Appendix G."}
{"doc_id": "2005.14165", "para_id": 41, "text": "• Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\na natural language instruction describing the task. This method provides maximum convenience, potential for\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of\npre-training data), but is also the most challenging setting. In some cases it may even be difﬁcult for humans\nto understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”.\nFor example, if someone is asked to “make a table of world records for the 200m dash”, this request can be\nambiguous, as it may not be clear exactly what format the table should have or what should be included (and\neven with careful clariﬁcation, understanding precisely what is desired can be difﬁcult). Nevertheless, for at\nleast some settings zero-shot is closest to how humans perform tasks – for example, in the translation example\nin Figure 2.1, a human would likely know what to do from just the text instruction."}
{"doc_id": "2005.14165", "para_id": 42, "text": "Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\nproblem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency.\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models.\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,\nand are important targets for future work."}
{"doc_id": "2005.14165", "para_id": 43, "text": "Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses\nthe details of how we do few-shot, one-shot, and zero-shot evaluations."}
{"doc_id": "2005.14165", "para_id": 44, "text": "Model Name\nnparams\nnlayers\ndmodel\nnheads\ndhead\nBatch Size\nLearning Rate"}
{"doc_id": "2005.14165", "para_id": 45, "text": "GPT-3 175B or “GPT-3”\n175.0B\n96\n12288\n96\n128\n3.2M\n0.6 × 10−4"}
{"doc_id": "2005.14165", "para_id": 46, "text": "Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\nwhich we trained. All models were trained for a total of 300 billion tokens."}
{"doc_id": "2005.14165", "para_id": 47, "text": "We use the same model and architecture as GPT-2 [RWC+19], including the modiﬁed initialization, pre-normalization,\nand reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse\nattention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence\nof ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125\nmillion parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20]\nsuggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a\nfunction of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for\ndownstream language tasks."}
{"doc_id": "2005.14165", "para_id": 48, "text": "Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\nnlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\nfeedforward layer four times the size of the bottleneck layer, dﬀ= 4 ∗dmodel), and dhead is the dimension of each\nattention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along\nboth the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\nparameters for each model are chosen based on computational efﬁciency and load-balancing in the layout of models\nacross GPU’s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters\nwithin a reasonably broad range."}
{"doc_id": "2005.14165", "para_id": 49, "text": "Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting\nnearly a trillion words. This size of dataset is sufﬁcient to train our largest models without ever updating on the same\nsequence twice. However, we have found that unﬁltered or lightly ﬁltered versions of Common Crawl tend to have\nlower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:\n(1) we downloaded and ﬁltered a version of CommonCrawl based on similarity to a range of high-quality reference\ncorpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy\nand preserve the integrity of our held-out validation set as an accurate measure of overﬁtting, and (3) we also added\nknown high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity."}
{"doc_id": "2005.14165", "para_id": 50, "text": "Details of the ﬁrst two points (processing of Common Crawl) are described in Appendix A. For the third, we added\nseveral curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected\nby scraping links over a longer period of time, and ﬁrst described in [KMH+20], two internet-based books corpora\n(Books1 and Books2) and English-language Wikipedia."}
{"doc_id": "2005.14165", "para_id": 51, "text": "Table 2.2 shows the ﬁnal mixture of datasets that we used in training. The CommonCrawl data was downloaded from\n41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before ﬁltering\nand 570GB after ﬁltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets\nare not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently,\nsuch that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are\nsampled 2-3 times. This essentially accepts a small amount of overﬁtting in exchange for higher quality training data."}
{"doc_id": "2005.14165", "para_id": 52, "text": "Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models\n[KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B\nis almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaﬂop/s-days of compute\nduring pre-training. Methodology for these calculations can be found in Appendix D."}
{"doc_id": "2005.14165", "para_id": 53, "text": "Dataset\nQuantity\n(tokens)\nWeight in\ntraining mix\nEpochs elapsed when\ntraining for 300B tokens"}
{"doc_id": "2005.14165", "para_id": 54, "text": "Common Crawl (ﬁltered)\n410 billion\n60%\n0.44\nWebText2\n19 billion\n22%\n2.9\nBooks1\n12 billion\n8%\n1.9\nBooks2\n55 billion\n8%\n0.43\nWikipedia\n3 billion\n3%\n3.4"}
{"doc_id": "2005.14165", "para_id": 55, "text": "Table 2.2: Datasets used to train GPT-3. “Weight in training mix” refers to the fraction of examples during training\nthat are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a\nresult, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets\nare seen less than once."}
{"doc_id": "2005.14165", "para_id": 56, "text": "A major methodological concern with language models pretrained on a broad swath of internet data, particularly large\nmodels with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by\nhaving their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched\nfor and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.\nUnfortunately, a bug in the ﬁltering caused us to ignore some overlaps, and due to the cost of training it was not feasible\nto retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will\nmore aggressively remove data contamination."}
{"doc_id": "2005.14165", "para_id": 57, "text": "As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning\nrate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table\n2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture\nof model parallelism within each matrix multiply and model parallelism across the layers of the network. All models\nwere trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process\nand hyperparameter settings are described in Appendix B."}
{"doc_id": "2005.14165", "para_id": 58, "text": "For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that\ntask’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze\nthere is no supervised training set available so we draw conditioning examples from the development set and evaluate\non the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning\nexamples directly from it."}
{"doc_id": "2005.14165", "para_id": 59, "text": "K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048\nfor all models and typically ﬁts 10 to 100 examples. Larger values of K are usually but not always better, so when a\nseparate development and test set are available, we experiment with a few values of K on the development set and then\nrun the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to\n(or for K = 0, instead of) demonstrations."}
{"doc_id": "2005.14165", "para_id": 60, "text": "On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples\nof context plus correct completion, followed by one example of context only, and compare the LM likelihood of\neach completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small\nnumber of datasets (ARC, OpenBookQA, and RACE) we gain additional beneﬁt as measured on the development set\nby normalizing by the unconditional probability of each completion, by computing\nP (completion|context)\nP (completion|answer context), where\nanswer context is the string \"Answer:\n\" or \"A: \" and is used to prompt that the completion should be an answer\nbut is otherwise generic."}
{"doc_id": "2005.14165", "para_id": 61, "text": "On tasks that involve binary classiﬁcation, we give the options more semantically meaningful names (e.g. “True” or\n“False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what\nis done by [RSR+19] (see Appendix G) for details."}
{"doc_id": "2005.14165", "para_id": 62, "text": "On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4\nand a length penalty of α = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on\nwhat is standard for the dataset at hand."}
{"doc_id": "2005.14165", "para_id": 63, "text": "Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-,\nand few-shot). When the test set is private, our model is often too large to ﬁt on the test server, so we report results on\nthe development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa)\nwhere we were able to make submission work, and we submit only the 200B few-shot results, and report development\nset results for everything else."}
{"doc_id": "2005.14165", "para_id": 64, "text": "In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6\nadditional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling\nperformance follows a power-law when making efﬁcient use of training compute. After extending this trend by two\nmore orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these\nimprovements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will\nsee in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a\nbroad spectrum of natural language tasks."}
{"doc_id": "2005.14165", "para_id": 65, "text": "Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller\nmodels) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks."}
{"doc_id": "2005.14165", "para_id": 66, "text": "In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling,\nsuch as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question\nanswering tasks: tasks which require using the information stored in the model’s parameters to answer general\nknowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot\nand few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we\nevaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading\ncomprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieﬂy explore\nNLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities –\nthese tasks focus on on-the-ﬂy reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the\nfew-shot, one-shot, and zero-shot settings."}
{"doc_id": "2005.14165", "para_id": 67, "text": "Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\nvalidation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior\nobserved in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the\npredicted curve. For this ﬁgure, we exclude embedding parameters from compute and parameter counts."}
{"doc_id": "2005.14165", "para_id": 68, "text": "Table 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets\nare omitted because they are derived from Wikipedia or other sources which are included in GPT-3’s training data.\na[RWC+19]"}
{"doc_id": "2005.14165", "para_id": 69, "text": "3.1\nLanguage Modeling, Cloze, and Completion Tasks"}
{"doc_id": "2005.14165", "para_id": 70, "text": "In this section we test GPT-3’s performance on the traditional task of language modeling, as well as related tasks\nthat involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible\ncompletions of a piece of text."}
{"doc_id": "2005.14165", "para_id": 71, "text": "We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [MKM+94] dataset measured in [RWC+19]. We omit\nthe 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the\none-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these\nissues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15\npoints, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have\na clear separation of examples to deﬁne one-shot or few-shot evaluation around, so we measure only zero-shot."}
{"doc_id": "2005.14165", "para_id": 72, "text": "The LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text – the model is asked to\npredict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the\ncontinued scaling of language models is yielding diminishing returns on this difﬁcult benchmark. [BHT+20] reﬂect on\nthe small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP+19]"}
{"doc_id": "2005.14165", "para_id": 73, "text": "Setting\nLAMBADA\n(acc)\nLAMBADA\n(ppl)\nStoryCloze\n(acc)\nHellaSwag\n(acc)"}
{"doc_id": "2005.14165", "para_id": 74, "text": "SOTA\n68.0a\n8.63b\n91.8c\n85.6d\nGPT-3 Zero-Shot\n76.2\n3.00\n83.2\n78.9\nGPT-3 One-Shot\n72.5\n3.35\n84.7\n78.1\nGPT-3 Few-Shot\n86.4\n1.92\n87.7\n79.3"}
{"doc_id": "2005.14165", "para_id": 75, "text": "Table 3.2: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on LAMBADA while\nachieving respectable performance on two difﬁcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19]\nd[LCH+20]"}
{"doc_id": "2005.14165", "para_id": 76, "text": "Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3\n2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of\nthe art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text."}
{"doc_id": "2005.14165", "para_id": 77, "text": "and [Tur20]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path\nforward”. We ﬁnd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of\n8% over the previous state of the art."}
{"doc_id": "2005.14165", "para_id": 78, "text": "LAMBADA is also a demonstration of the ﬂexibility of few-shot learning as it provides a way to address a problem that\nclassically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a\nstandard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but\nalso to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word\nﬁlters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a\ncloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We\nuse the following ﬁll-in-the-blank format:"}
{"doc_id": "2005.14165", "para_id": 79, "text": "Alice was friends with Bob. Alice went to visit her friend\n. →Bob"}
{"doc_id": "2005.14165", "para_id": 80, "text": "George bought some baseball equipment, a ball, a glove, and a\n. →"}
{"doc_id": "2005.14165", "para_id": 81, "text": "When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase\nof over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model\nsize. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy\nby 10%. Finally, the ﬁll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot\nsetting. Perhaps this is because all models still require several examples to recognize the pattern."}
{"doc_id": "2005.14165", "para_id": 82, "text": "RAG (Fine-tuned, Open-Domain) [LPP+20]\n44.5\n45.5\n68.0\nT5-11B+SSM (Fine-tuned, Closed-Book) [RRS20]\n36.6\n44.7\n60.5\nT5-11B (Fine-tuned, Closed-Book)\n34.5\n37.4\n50.1\nGPT-3 Zero-Shot\n14.6\n14.4\n64.3\nGPT-3 One-Shot\n23.0\n25.3\n68.0\nGPT-3 Few-Shot\n29.9\n41.5\n71.2"}
{"doc_id": "2005.14165", "para_id": 83, "text": "Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as\ncompared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the\nwiki split test server."}
{"doc_id": "2005.14165", "para_id": 84, "text": "One note of caution is that an analysis of test set contamination identiﬁed that a signiﬁcant minority of the LAMBADA\ndataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact\non performance."}
{"doc_id": "2005.14165", "para_id": 85, "text": "The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions. The examples were\nadversarially mined to be difﬁcult for language models while remaining easy for humans (who achieve 95.6% accuracy).\nGPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the\n75.4% accuracy of a ﬁne-tuned 1.5B parameter language model [ZHR+19] but still a fair amount lower than the overall\nSOTA of 85.6% achieved by the ﬁne-tuned multi-task model ALUM."}
{"doc_id": "2005.14165", "para_id": 86, "text": "We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending\nsentence for ﬁve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot\nsetting (with K = 70). This is still 4.1% lower than the ﬁne-tuned SOTA using a BERT based model [LDL19] but\nimproves over previous zero-shot results by roughly 10%."}
{"doc_id": "2005.14165", "para_id": 87, "text": "In this section we measure GPT-3’s ability to answer questions about broad factual knowledge. Due to the immense\namount of possible queries, this task has normally been approached by using an information retrieval system to ﬁnd\nrelevant text in combination with a model which learns to generate an answer given the question and the retrieved\ntext. Since this setting allows a system to search for and condition on text which potentially contains the answer it\nis denoted “open-book”. [RRS20] recently demonstrated that a large language model can perform surprisingly well\ndirectly answering the questions without conditioning on auxilliary information. They denote this more restrictive\nevaluation setting as “closed-book”. Their work suggests that even higher-capacity models could perform even better\nand we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19],\nWebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in\nthe closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than\nprevious closed-book QA work: in addition to external content not being allowed, ﬁne-tuning on the Q&A dataset itself\nis also not permitted."}
{"doc_id": "2005.14165", "para_id": 88, "text": "The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the\none-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the ﬁne-tuned T5-11B by\n14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot\nresult improves by 3.7% and matches the SOTA for an open-domain QA system which not only ﬁne-tunes but also\nmakes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20].\nGPT-3’s few-shot result further improves performance another 3.2% beyond this."}
{"doc_id": "2005.14165", "para_id": 89, "text": "On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5%\nin the few-shot setting. This compares to 37.4% for ﬁne-tuned T5-11B, and 44.7% for ﬁne-tuned T5-11B+SSM,\nwhich uses a Q&A-speciﬁc pre-training procedure. GPT-3 in the few-shot setting approaches the performance of\nstate-of-the-art ﬁne-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to\nfew-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions"}
{"doc_id": "2005.14165", "para_id": 90, "text": "Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models\ncontinue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiﬁcant gains\nover zero-shot behavior, matching and exceeding the performance of the SOTA ﬁne-tuned open-domain model, RAG\n[LPP+20]"}
{"doc_id": "2005.14165", "para_id": 91, "text": "and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this\ndistribution, recovering strong performance in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 92, "text": "On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in\nthe few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to\nTriviaQA and WebQS. In particular, the questions in NQs tend towards very ﬁne-grained knowledge on Wikipedia\nspeciﬁcally which could be testing the limits of GPT-3’s capacity and broad pretraining distribution."}
{"doc_id": "2005.14165", "para_id": 93, "text": "Overall, on one of the three datasets GPT-3’s one-shot matches the open-domain ﬁne-tuning SOTA. On the other two\ndatasets it approaches the performance of the closed-book SOTA despite not using ﬁne-tuning. On all 3 datasets, we\nﬁnd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reﬂecting\nthe idea that model capacity translates directly to more ‘knowledge’ absorbed in the parameters of the model."}
{"doc_id": "2005.14165", "para_id": 94, "text": "For GPT-2 a ﬁlter was used on a multilingual collection of documents to produce an English only dataset due to capacity\nconcerns. Even with this ﬁltering GPT-2 showed some evidence of multilingual capability and performed non-trivially\nwhen translating between French and English despite only training on 10 megabytes of remaining French text. Since we\nincrease the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training\ndataset to include more representation of other languages, though this remains an area for further improvement. As\ndiscussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based ﬁltering. Although\nGPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.\nThese languages are documented in the supplemental material. In order to better understand translation capability, we\nalso expand our analysis to include two additional commonly studied languages, German and Romanian."}
{"doc_id": "2005.14165", "para_id": 95, "text": "Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets\nwith back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a\nblend of training data that mixes many languages together in a natural way, combining them on a word, sentence,\nand document level. GPT-3 also uses a single training objective which is not customized or designed for any task in\nparticular. However, our one / few-shot settings aren’t strictly comparable to prior unsupervised work since they make\nuse of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data."}
{"doc_id": "2005.14165", "para_id": 96, "text": "Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task,\nstill underperforms recent unsupervised NMT results. However, providing only a single example demonstration for"}
{"doc_id": "2005.14165", "para_id": 97, "text": "SOTA (Supervised)\n45.6a\n35.0 b\n41.2c\n40.2d\n38.5e\n39.9e"}
{"doc_id": "2005.14165", "para_id": 98, "text": "XLM [LC19]\n33.4\n33.3\n26.4\n34.3\n33.3\n31.8\nMASS [STQ+19]\n37.5\n34.9\n28.3\n35.2\n35.2\n33.1\nmBART [LGG+20]\n-\n-\n29.8\n34.0\n35.0\n30.5"}
{"doc_id": "2005.14165", "para_id": 99, "text": "GPT-3 Zero-Shot\n25.2\n21.2\n24.6\n27.2\n14.1\n19.9\nGPT-3 One-Shot\n28.3\n33.7\n26.2\n30.4\n20.6\n38.6\nGPT-3 Few-Shot\n32.6\n39.2\n29.7\n40.6\n21.0\n39.5"}
{"doc_id": "2005.14165", "para_id": 100, "text": "Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating\ninto English reﬂecting its strength as an English LM. We report BLEU scores on the WMT’14 Fr↔En,\nWMT’16 De↔En, and WMT’16 Ro↔En datasets as measured by multi-bleu.perl with XLM’s tokeniza-\ntion in order to compare most closely with prior unsupervised NMT work.\nSacreBLEUf [Pos18] results re-\nported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA\nwith relative conﬁdence.\na[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f [SacreBLEU signature:\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]"}
{"doc_id": "2005.14165", "para_id": 101, "text": "Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent\ntrend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be\nstronger than translation from English."}
{"doc_id": "2005.14165", "para_id": 102, "text": "Fine-tuned SOTA\n90.1a\n84.6b\nGPT-3 Zero-Shot\n88.3*\n70.2\nGPT-3 One-Shot\n89.7*\n73.2\nGPT-3 Few-Shot\n88.6*\n77.7"}
{"doc_id": "2005.14165", "para_id": 103, "text": "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]"}
{"doc_id": "2005.14165", "para_id": 104, "text": "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\nScaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B\nis competitive with a ﬁne-tuned RoBERTA-large."}
{"doc_id": "2005.14165", "para_id": 105, "text": "each translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\nGPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior\nunsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the\nthree input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into\nEnglish but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at\nover 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE\ntokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En,\nfew shot GPT-3 outperforms the best supervised result we could ﬁnd but due to our unfamiliarity with the literature and\nthe appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art.\nFor Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of\nunsupervised pretraining, supervised ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b]."}
{"doc_id": "2005.14165", "para_id": 106, "text": "Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of\nimprovement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three\nsettings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 107, "text": "The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun\nrefers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently ﬁne-tuned\nlanguage models have achieved near-human performance on the original Winograd dataset, but more difﬁcult versions"}
{"doc_id": "2005.14165", "para_id": 108, "text": "Setting\nPIQA\nARC (Easy)\nARC (Challenge)\nOpenBookQA"}
{"doc_id": "2005.14165", "para_id": 109, "text": "Fine-tuned SOTA\n79.4\n92.0[KKS+20]\n78.5[KKS+20]\n87.2[KKS+20]\nGPT-3 Zero-Shot\n80.5*\n68.8\n51.4\n57.6\nGPT-3 One-Shot\n80.5*\n71.2\n53.2\n58.8\nGPT-3 Few-Shot\n82.8*\n70.1\n51.5\n65.4"}
{"doc_id": "2005.14165", "para_id": 110, "text": "Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot\nPIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test\nset."}
{"doc_id": "2005.14165", "para_id": 111, "text": "Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a\nscore on the development set in all three conditions that exceeds the best recorded score on the task."}
{"doc_id": "2005.14165", "para_id": 112, "text": "such as the adversarially-mined Winogrande dataset [SBBC19] still signiﬁcantly lag human performance. We test\nGPT-3’s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting."}
{"doc_id": "2005.14165", "para_id": 113, "text": "On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same “partial evaluation” method\ndescribed in [RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which\nis presented as binary classiﬁcation and requires entity extraction to convert to the form described in this section. On\nWinograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear\nin-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human\nperformance. We note that contamination analysis found some Winograd schemas in the training data but this appears\nto have only a small effect on results (see Section 4)."}
{"doc_id": "2005.14165", "para_id": 114, "text": "On the more difﬁcult Winogrande dataset, we do ﬁnd gains to in-context learning: GPT-3 achieves 70.2% in the\nzero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a ﬁne-tuned\nRoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a ﬁne-tuned high capacity model (T5), and\nhuman performance on the task as reported by [SBBC19] is 94.0%."}
{"doc_id": "2005.14165", "para_id": 115, "text": "Next we consider three datasets which attempt to capture physical or scientiﬁc reasoning, as distinct from sentence\ncompletion, reading comprehension, or broad knowledge question answering. The ﬁrst, PhysicalQA (PIQA) [BZB+19],\nasks common sense questions about how the physical world works and is intended as a probe of grounded understanding\nof the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot\n(the last measured on PIQA’s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a"}
{"doc_id": "2005.14165", "para_id": 116, "text": "Fine-tuned SOTA\n90.7a\n89.1b\n74.4c\n93.0d\n90.0e\n93.1e\nGPT-3 Zero-Shot\n81.5\n23.6\n41.5\n59.5\n45.5\n58.4\nGPT-3 One-Shot\n84.0\n34.3\n43.3\n65.4\n45.9\n57.4\nGPT-3 Few-Shot\n85.0\n36.5\n44.3\n69.8\n46.8\n58.1"}
{"doc_id": "2005.14165", "para_id": 117, "text": "Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.\na[JZC+19] b[JN20] c[AI19] d[QIA20] e[SPP+19]"}
{"doc_id": "2005.14165", "para_id": 118, "text": "ﬁne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human\nperformance, but GPT-3’s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis\nﬂagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark\nthe result with an asterisk. See Section 4 for details."}
{"doc_id": "2005.14165", "para_id": 119, "text": "ARC [CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the\n“Challenge” version of the dataset which has been ﬁltered to questions which simple statistical or information retrieval\nmethods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot\nsetting, and 51.5% in the few-shot setting. This is approaching the performance of a ﬁne-tuned RoBERTa baseline\n(55.9%) from UniﬁedQA [KKS+20]. On the “Easy” version of the dataset (questions which either of the mentioned\nbaseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a ﬁne-tuned\nRoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs\nachieved by the UniﬁedQA which exceeds GPT-3’s few-shot results by 27% on the challenge set and 22% on the easy\nset."}
{"doc_id": "2005.14165", "para_id": 120, "text": "On OpenBookQA [MCKS18], GPT-3 improves signiﬁcantly from zero to few shot settings but is still over 20 points\nshort of the overall SOTA. GPT-3’s few-shot performance is similar to a ﬁne-tuned BERT Large baseline on the\nleaderboard."}
{"doc_id": "2005.14165", "para_id": 121, "text": "Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and\ninconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiﬁcant\nimprovement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings."}
{"doc_id": "2005.14165", "para_id": 122, "text": "Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive,\nmultiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread\nin GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general\nwe observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each\nrespective dataset."}
{"doc_id": "2005.14165", "para_id": 123, "text": "GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset\nand performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured\ndialog acts and answer span selections of teacher-student interactions. On DROP [DWD+19], a dataset testing discrete\nreasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the ﬁne-tuned\nBERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches\nwhich augment neural networks with symbolic systems [RLL+19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its\nfew-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to\nslightly outperform the best ﬁne-tuned result in the original paper. On RACE [LXL+17], a multiple choice dataset of\nmiddle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with\nthe earliest work utilizing contextual representations and is still 45% behind SOTA."}
{"doc_id": "2005.14165", "para_id": 124, "text": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a\nmore systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark\n[WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07]\n[BDD+09] [PCC18] [PHR+18]. GPT-3’s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the\nfew-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC"}
{"doc_id": "2005.14165", "para_id": 125, "text": "Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting,\nonly a few points behind measured human performance and state-of-the-art ﬁne-tuned models. Zero-shot and one-shot\nperformance is a few points behind, with the gains to few-shot being largest for bigger models."}
{"doc_id": "2005.14165", "para_id": 126, "text": "SuperGLUE\nBoolQ\nCB\nCB\nCOPA\nRTE\nAverage\nAccuracy\nAccuracy\nF1\nAccuracy\nAccuracy"}
{"doc_id": "2005.14165", "para_id": 127, "text": "Fine-tuned SOTA\n89.0\n91.0\n96.9\n93.9\n94.8\n92.5\nFine-tuned BERT-Large\n69.0\n77.4\n83.6\n75.7\n70.6\n71.7\nGPT-3 Few-Shot\n71.8\n76.4\n75.6\n52.0\n92.0\n69.0"}
{"doc_id": "2005.14165", "para_id": 128, "text": "WiC\nWSC\nMultiRC\nMultiRC\nReCoRD\nReCoRD\nAccuracy\nAccuracy\nAccuracy\nF1a\nAccuracy\nF1"}
{"doc_id": "2005.14165", "para_id": 129, "text": "Fine-tuned SOTA\n76.1\n93.8\n62.3\n88.2\n92.5\n93.3\nFine-tuned BERT-Large\n69.6\n64.6\n24.1\n70.0\n71.3\n72.0\nGPT-3 Few-Shot\n49.4\n80.1\n30.5\n75.4\n90.2\n91.1"}
{"doc_id": "2005.14165", "para_id": 130, "text": "Table 3.8: Performance of GPT-3 on SuperGLUE compared to ﬁne-tuned baselines and SOTA. All results are reported\non the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient\nupdates."}
{"doc_id": "2005.14165", "para_id": 131, "text": "Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value\nof K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in\nSuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference\nlines (our test set results are in Table 3.8). The BERT-Large reference model was ﬁne-tuned on the SuperGLUE training\nset (125K examples), whereas BERT++ was ﬁrst ﬁne-tuned on MultiNLI (392K examples) and SWAG (113K examples)\nbefore further ﬁne-tuning on the SuperGLUE training set (for a total of 630K ﬁne-tuning examples). We ﬁnd the\ndifference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between\nGPT-3 with one example per context versus eight examples per context."}
{"doc_id": "2005.14165", "para_id": 132, "text": "and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we\nused the same set of randomly drawn examples from the training set as context for all of the problems we evaluated."}
{"doc_id": "2005.14165", "para_id": 133, "text": "We observe a wide range in GPT-3’s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA\nperformance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving\nsecond place on the leaderboard, where ﬁrst place is held by a ﬁne-tuned 11 billion parameter model (T5). On WSC,\nperformance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the\noriginal Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable,\nroughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 134, "text": "WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different\nphrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two\nsentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer\nin the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot\nsetting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same\nway in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another.\nThis could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these\nweaknesses, GPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to\nthe state-of-the-art held by a ﬁne-tuned 11 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 135, "text": "Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of\nexamples in the context showing increasing beneﬁts from in-context learning (Figure 3.8). We scale K up to 32\nexamples per task, after which point additional examples will not reliably ﬁt into our context. When sweeping over\nvalues of K, we ﬁnd that GPT-3 requires less than eight total examples per task to outperform a ﬁne-tuned BERT-Large\non overall SuperGLUE score."}
{"doc_id": "2005.14165", "para_id": 136, "text": "Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences.\nIn practice, this task is usually structured as a two or three class classiﬁcation problem where the model classiﬁes"}
{"doc_id": "2005.14165", "para_id": 137, "text": "Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples\nand therefore has high variance (we estimate a standard deviation of 1.2%). We ﬁnd that smaller models hover around\nrandom chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for\nANLI rounds 1 and 2 are shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 138, "text": "whether the second sentence logically follows from the ﬁrst, contradicts the ﬁrst sentence, or is possibly true (neutral).\nSuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest\nversion of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting\nGPT-3 performs similarly to a single-task ﬁne-tuned BERT Large. We also evaluate on the recently introduced\nAdversarial Natural Language Inference (ANLI) dataset [NWD+19]. ANLI is a difﬁcult dataset employing a series of\nadversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our\nmodels smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting (∼33%),\nwhereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results\nfor all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difﬁcult\ntask for language models and they are only just beginning to show signs of progress."}
{"doc_id": "2005.14165", "para_id": 139, "text": "One way to probe GPT-3’s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which\nrequire it to perform simple on-the-ﬂy computational reasoning, recognize a novel pattern that is unlikely to have\noccurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we\ntest GPT-3’s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the\nletters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3’s ability to\nsolve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new\nwords in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets\nwith the hope of stimulating further study of test-time behavior of language models."}
{"doc_id": "2005.14165", "para_id": 140, "text": "To test GPT-3’s ability to perform simple arithmetic operations without task-speciﬁc training, we developed a small\nbattery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:"}
{"doc_id": "2005.14165", "para_id": 141, "text": "• 2 digit addition (2D+) – The model is asked to add two integers sampled uniformly from [0, 100), phrased in\nthe form of a question, e.g. “Q: What is 48 plus 76? A: 124.”\n• 2 digit subtraction (2D-) – The model is asked to subtract two integers sampled uniformly from [0, 100); the\nanswer may be negative. Example: “Q: What is 34 minus 53? A: -19”.\n• 3 digit addition (3D+) – Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 142, "text": "Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a\nsigniﬁcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being\nable to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiﬁcant fraction\nof the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot\nare shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 143, "text": "• 3 digit subtraction (3D-) – Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 144, "text": "• 4 digit addition (4D+) – Same as 3 digit addition, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 145, "text": "• 4 digit subtraction (4D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 146, "text": "• 5 digit addition (5D+) – Same as 3 digit addition, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 147, "text": "• 5 digit subtraction (5D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 148, "text": "• 2 digit multiplication (2Dx) – The model is asked to multiply two integers sampled uniformly from [0, 100),\ne.g. “Q: What is 24 times 42? A: 1008”."}
{"doc_id": "2005.14165", "para_id": 149, "text": "• One-digit composite (1DC) – The model is asked to perform a composite operation on three 1 digit numbers,\nwith parentheses around the last two. For example, “Q: What is 6+(4*8)? A: 38”. The three 1 digit numbers\nare selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}."}
{"doc_id": "2005.14165", "para_id": 150, "text": "In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random\ninstances of the task and evaluate all models on those instances."}
{"doc_id": "2005.14165", "para_id": 151, "text": "First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction,\nGPT-3 displays strong proﬁciency when the number of digits is small, achieving 100% accuracy on 2 digit addition,\n98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the\nnumber of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on\nﬁve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves\n29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves\n21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness\nbeyond just single operations."}
{"doc_id": "2005.14165", "para_id": 152, "text": "As Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the\nsecond largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all\nother operations less than 10% of the time."}
{"doc_id": "2005.14165", "para_id": 153, "text": "One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation\nto the task (or at the very least recognition of the task) is important to performing these computations correctly.\nNevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signiﬁcantly"}
{"doc_id": "2005.14165", "para_id": 154, "text": "GPT-3 Zero-shot\n76.9\n58.0\n34.2\n48.3\n4.0\n7.5\n0.7\n0.8\n19.8\n9.8\nGPT-3 One-shot\n99.6\n86.4\n65.5\n78.7\n14.0\n14.0\n3.5\n3.8\n27.4\n14.3\nGPT-3 Few-shot\n100.0\n98.9\n80.4\n94.2\n25.5\n26.8\n9.3\n9.9\n29.2\n21.3"}
{"doc_id": "2005.14165", "para_id": 155, "text": "Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or\nsubtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger\nmoving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signiﬁcant arithmetic abilities."}
{"doc_id": "2005.14165", "para_id": 156, "text": "GPT-3 Zero-shot\n3.66\n2.28\n8.91\n8.26\n0.09\nGPT-3 One-shot\n21.7\n8.62\n25.9\n45.4\n0.48\nGPT-3 Few-shot\n37.9\n15.1\n39.7\n67.2\n0.44"}
{"doc_id": "2005.14165", "para_id": 157, "text": "Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and\nfew-shot settings. CL is “cycle letters in word”, A1 is anagrams of but the ﬁrst and last letters, A2 is anagrams of all but\nthe ﬁrst and last two letters, RI is “Random insertion in word”, RW is “reversed words”."}
{"doc_id": "2005.14165", "para_id": 158, "text": "outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and\nmodel capacity scaling for all three settings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 159, "text": "To spot-check whether the model is simply memorizing speciﬁc arithmetic problems, we took the 3-digit arithmetic\nproblems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and\n\"<NUM1> plus <NUM2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000\nsubtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers\ncould have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes\nsuch as not carrying a “1”, suggesting it is actually attempting to perform the relevant computation rather than\nmemorizing a table."}
{"doc_id": "2005.14165", "para_id": 160, "text": "Overall, GPT-3 displays reasonable proﬁciency at moderately complex arithmetic in few-shot, one-shot, and even\nzero-shot settings."}
{"doc_id": "2005.14165", "para_id": 161, "text": "To test GPT-3’s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of\n5 “character manipulation” tasks. Each task involves giving the model a word distorted by some combination of\nscrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:"}
{"doc_id": "2005.14165", "para_id": 162, "text": "• Cycle letters in word (CL) – The model is given a word with its letters cycled, then the “=” symbol, and\nis expected to generate the original word. For example, it might be given “lyinevitab” and should output\n“inevitably”."}
{"doc_id": "2005.14165", "para_id": 163, "text": "• Anagrams of all but ﬁrst and last characters (A1) – The model is given a word where every letter except\nthe ﬁrst and last have been scrambled randomly, and must output the original word. Example: criroptuon =\ncorruption."}
{"doc_id": "2005.14165", "para_id": 164, "text": "• Anagrams of all but ﬁrst and last 2 characters (A2) – The model is given a word where every letter except\nthe ﬁrst 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt\n→opponent."}
{"doc_id": "2005.14165", "para_id": 165, "text": "• Random insertion in word (RI) – A random punctuation or space character is inserted between each letter\nof a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession."}
{"doc_id": "2005.14165", "para_id": 166, "text": "• Reversed words (RW) – The model is given a word spelled backwards, and must output the original word.\nExample: stcejbo →objects."}
{"doc_id": "2005.14165", "para_id": 167, "text": "For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by\n[Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11.\nTask performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing"}
{"doc_id": "2005.14165", "para_id": 168, "text": "Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally\nsmooth improvement with model size although the random insertion task shows an upward slope of improvement with\nthe 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\nthe appendix. All tasks are done with K = 100."}
{"doc_id": "2005.14165", "para_id": 169, "text": "random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult anagram\ntask (where only the ﬁrst and last letters are held ﬁxed). None of the models can reverse the letters in a word."}
{"doc_id": "2005.14165", "para_id": 170, "text": "In the one-shot setting, performance is signiﬁcantly weaker (dropping by half or more), and in the zero-shot setting the\nmodel can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these\ntasks at test time, as the model cannot perform them zero-shot and their artiﬁcial nature makes them unlikely to appear\nin the pre-training data (although we cannot conﬁrm this with certainty)."}
{"doc_id": "2005.14165", "para_id": 171, "text": "We can further quantify performance by plotting “in-context learning curves”, which show task performance as a\nfunction of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task\nin Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information,\nincluding both task examples and natural language task descriptions."}
{"doc_id": "2005.14165", "para_id": 172, "text": "Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding\noperates on signiﬁcant fractions of a word (on average ∼0.7 words per token), so from the LM’s perspective succeeding\nat these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also,\nCL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word),\nrequiring the model to perform some search to ﬁnd the correct unscrambling. Thus, the skills involved appear to require\nnon-trivial pattern-matching and computation."}
{"doc_id": "2005.14165", "para_id": 173, "text": "To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of\n374 “SAT analogy” problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of\nthe SAT college entrance exam before 2005. A typical example is “audacious is to boldness as (a) sanctimonious is to\nhypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to\ntemptation”. The student is expected to choose which of the ﬁve word pairs has the same relationship as the original\nword pair; in this example the answer is “sanctimonious is to hypocrisy”. On this task GPT-3 achieves 65.2% in the\nfew-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among\ncollege applicants was 57% [TL05] (random guessing yields 20%). As shown in Figure 3.12, the results improve with\nscale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 174, "text": "Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest\nmodel achieves 65% accuracy in the few-shot setting, and also demonstrates signiﬁcant gains to in-context learning\nwhich are not present in smaller models."}
{"doc_id": "2005.14165", "para_id": 175, "text": "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by\nconditional sampling from the model given a human-written prompt consisting of a plausible ﬁrst sentence for a news\nstory [RWC+19]. Relative to [RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles,\nso trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets\nthe proposed ﬁrst sentence of a “news article” as a tweet and then posts synthetic responses or follow-up tweets. To\nsolve this problem we employed GPT-3’s few-shot learning abilities by providing three previous news articles in the\nmodel’s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably\ngenerate short articles in the “news” genre."}
{"doc_id": "2005.14165", "para_id": 176, "text": "To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional\nsample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles\nfrom real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al. [ZHR+19]. Generative\nlanguage models are trained to match the distribution of content generated by humans, so the (in)ability of humans to\ndistinguish the two is a potentially important measure of quality.3"}
{"doc_id": "2005.14165", "para_id": 177, "text": "In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles\nfrom the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles\nfrom four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each\nmodel, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed\nby either the human written article or the article generated by the model4. Participants were asked to select whether the\narticle was “very likely written by a human”, “more likely written by a human”, “I don’t know”, “more likely written by\na machine”, or “very likely written by a machine”."}
{"doc_id": "2005.14165", "para_id": 178, "text": "The articles we selected were not in the models’ training data and the model outputs were formatted and selected\nprogrammatically to prevent human cherry-picking. All models used the same context to condition outputs on and were\npre-trained with the same context size and the same article titles and subtitles were used as prompts for each model.\nHowever, we also ran an experiment to control for participant effort and attention that followed the same format but\ninvolved intentionally bad model generated articles. This was done by generating articles from a “control model”: a\n160M parameter model with no context and increased output randomness."}
{"doc_id": "2005.14165", "para_id": 179, "text": "3This task is also relevant to the potential misuse of language models discussed in Section 6.1.\n4We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on\nparticipants drawn from the general US population. See Appendix E for details."}
{"doc_id": "2005.14165", "para_id": 180, "text": "Mean accuracy\n95% Conﬁdence\nInterval (low, hi)\nt compared to\ncontrol (p-value)\n“I don’t know”\nassignments"}
{"doc_id": "2005.14165", "para_id": 181, "text": "Control (deliberately bad model)\n86%\n83%–90%\n-\n3.6 %\nGPT-3 Small\n76%\n72%–80%\n3.9 (2e-4)\n4.9%\nGPT-3 Medium\n61%\n58%–65%\n10.3 (7e-21)\n6.0%\nGPT-3 Large\n68%\n64%–72%\n7.3 (3e-11)\n8.7%\nGPT-3 XL\n62%\n59%–65%\n10.7 (1e-19)\n7.5%\nGPT-3 2.7B\n62%\n58%–65%\n10.4 (5e-19)\n7.1%\nGPT-3 6.7B\n60%\n56%–63%\n11.2 (3e-21)\n6.2%\nGPT-3 13B\n55%\n52%–58%\n15.3 (1e-32)\n7.1%\nGPT-3 175B\n52%\n49%–54%\n16.9 (1e-34)\n7.8%"}
{"doc_id": "2005.14165", "para_id": 182, "text": "Table 3.11: Human accuracy in identifying whether short (∼200 word) news articles are model generated. We\nﬁnd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86%\non the control model to 52% on GPT-3 175B. This table compares mean accuracy between ﬁve different models, and\nshows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model\n(an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 183, "text": "Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that\nthe intentionally bad articles were model generated was ∼86% where 50% is chance level performance. By contrast,\nmean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance\nat ∼52% (see Table 3.11).5 Human abilities to detect model generated text appear to decrease as model size increases:\nthere appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6\nThis is true despite the fact that participants spend more time on each output as model size increases (see Appendix E)."}
{"doc_id": "2005.14165", "para_id": 184, "text": "Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7 Much of the text is—as indicated by the\nevaluations—difﬁcult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator\nthat an article is model generated since, unlike human authors, the models have no access to the speciﬁc facts that the\narticle titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual\nphrasings, though these are often subtle enough that they are not noticed."}
{"doc_id": "2005.14165", "para_id": 185, "text": "Related work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like\nG ROV E R [ZHR+19] and GLTR [GSR19] may have greater success at detecting model generated text than human\nevaluators. Automatic detection of these models may be a promising area of future research."}
{"doc_id": "2005.14165", "para_id": 186, "text": "Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe\nmore tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated\nby GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated\ncompletions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial\nexperiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to\ncompare human abilities to detect the articles generated by GPT-3 and a control model."}
{"doc_id": "2005.14165", "para_id": 187, "text": "We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was\n∼88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely\nabove chance at ∼52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3\ncontinues to produce articles that humans ﬁnd difﬁcult to distinguish from human written news articles."}
{"doc_id": "2005.14165", "para_id": 188, "text": "A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a\nword in a sentence after seeing it deﬁned only once, or conversely inferring a word’s meaning from only one usage. Here\nwe qualitatively test GPT-3’s ability to do the former. Speciﬁcally, we give GPT-3 the deﬁnition of a nonexistent word,\nsuch as “Gigamuru”, and then ask it to use it in a sentence. We provide one to ﬁve previous examples of a (separate)"}
{"doc_id": "2005.14165", "para_id": 189, "text": "5We use a two-sample Student’s T-Test to test for signiﬁcant difference between the means of the participant accuracies of each\nmodel and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.\n6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on\nthis task would drop below 50%. Indeed, many individual participants scored below 50% on this task.\n7Additional non-news samples can be found in Appendix F."}
{"doc_id": "2005.14165", "para_id": 190, "text": "Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct\nassignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-\nbad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed\nline at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best ﬁt is a power\nlaw with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 191, "text": "Mean accuracy\n95% Conﬁdence\nInterval (low, hi)\nt compared to\ncontrol (p-value)\n“I don’t know”\nassignments"}
{"doc_id": "2005.14165", "para_id": 192, "text": "Control\n88%\n84%–91%\n-\n2.7%\nGPT-3 175B\n52%\n48%–57%\n12.7 (3.2e-23)\n10.6%"}
{"doc_id": "2005.14165", "para_id": 193, "text": "Table 3.12: People’s ability to identify whether ∼500 word articles are model generated (as measured by the ratio of\ncorrect assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table\nshows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control\nmodel (an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 194, "text": "Title:\nUnited Methodists Agree to Historic Split\nSubtitle:\nThose who oppose gay marriage will form their own denomination\nArticle:\nAfter two days of intense debate, the United Methodist Church\nhas agreed to a historic split - one that is expected to end in the\ncreation of a new denomination, one that will be \"theologically and\nsocially conservative,\" according to The Washington Post.\nThe majority of\ndelegates attending the church's annual General Conference in May voted to\nstrengthen a ban on the ordination of LGBTQ clergy and to write new rules\nthat will \"discipline\" clergy who officiate at same-sex weddings.\nBut\nthose who opposed these measures have a new plan:\nThey say they will form a\nseparate denomination by 2020, calling their church the Christian Methodist\ndenomination.\nThe Post notes that the denomination, which claims 12.5 million members, was\nin the early 20th century the \"largest Protestant denomination in the U.S.,\"\nbut that it has been shrinking in recent decades.\nThe new split will be the\nsecond in the church's history.\nThe first occurred in 1968, when roughly\n10 percent of the denomination left to form the Evangelical United Brethren\nChurch.\nThe Post notes that the proposed split \"comes at a critical time\nfor the church, which has been losing members for years,\" which has been\n\"pushed toward the brink of a schism over the role of LGBTQ people in the\nchurch.\" Gay marriage is not the only issue that has divided the church.\nIn\n2016, the denomination was split over ordination of transgender clergy, with\nthe North Pacific regional conference voting to ban them from serving as\nclergy, and the South Pacific regional conference voting to allow them."}
{"doc_id": "2005.14165", "para_id": 195, "text": "Figure 3.14: The GPT-3 generated news article that humans had the greatest difﬁculty distinguishing from a human\nwritten article (accuracy: 12%)."}
{"doc_id": "2005.14165", "para_id": 196, "text": "Title:\nStar’s Tux Promise Draws Megyn Kelly’s Sarcasm\nSubtitle:\nJoaquin Phoenix pledged to not change for each awards event\nArticle:\nA year ago, Joaquin Phoenix made headlines when he appeared on\nthe red carpet at the Golden Globes wearing a tuxedo with a paper bag over\nhis head that read, \"I am a shape-shifter.\nI can't change the world.\nI\ncan only change myself.\" It was a promise to not change to fit into the\nHollywood mold:\n\"I think that's a really special thing, to not change\nyourself.\nI think it's a really special thing to say, `This is what's\ninside of me, I'm proud of it, and I'm not going to be ashamed because of\nthe way that someone else thinks I should be.'\" Now, it's the Oscars, and\nPhoenix is at it again.\nBut this time, his publicist is saying he'll be\nwearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show.\n\"You know, I feel like, I feel like you could have worn the tux,\" she says.\n\"But you're saying you're a shape-shifter.\nI don't know if you can change\nyour tux, but you can change your mind.\nYou can change your mind.\nYou can\nchange your mind.\" Phoenix says he did, but it didn't stick.\n\"I was like,\n`Okay, I'm going to wear a tuxedo to this thing.'\nAnd then I thought, `I\ndon't want to wear a tuxedo to this thing.'\" Kelly goes on to encourage him\nto change his mind again, but Phoenix says it's too late:\n\"I'm committed to\nwearing this.\""}
{"doc_id": "2005.14165", "para_id": 197, "text": "Figure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written\narticle (accuracy: 61%)."}
{"doc_id": "2005.14165", "para_id": 198, "text": "A \"whatpu\" is a small, furry animal native to Tanzania.\nAn example of a sentence that uses\nthe word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus."}
{"doc_id": "2005.14165", "para_id": 199, "text": "To do a \"farduddle\" means to jump up and down really fast.\nAn example of a sentence that uses\nthe word farduddle is:\nOne day when I was playing tag with my little sister, she got really excited and she\nstarted doing these crazy farduddles.\nA \"yalubalu\" is a type of vegetable that looks like a big pumpkin.\nAn example of a sentence\nthat uses the word yalubalu is:\nI was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden\nthere.\nIt was delicious."}
{"doc_id": "2005.14165", "para_id": 200, "text": "A \"Burringo\" is a car with very fast acceleration.\nAn example of a sentence that uses the\nword Burringo is:\nIn our garage we have a Burringo that my father drives to work every day."}
{"doc_id": "2005.14165", "para_id": 201, "text": "A \"Gigamuru\" is a type of Japanese musical instrument.\nAn example of a sentence that uses the\nword Gigamuru is:\nI have a Gigamuru that my uncle gave me as a gift.\nI love to play it at home."}
{"doc_id": "2005.14165", "para_id": 202, "text": "To \"screeg\" something is to swing a sword at it.\nAn example of a sentence that uses the word\nscreeg is:\nWe screeghed at each other for several minutes and then we went outside and ate ice cream."}
{"doc_id": "2005.14165", "para_id": 203, "text": "Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is\nGPT-3’s completions, plain text is human prompts. In the ﬁrst example both the prompt and the completion are provided\nby a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional\nprompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 other than the conditioning shown\nhere."}
{"doc_id": "2005.14165", "para_id": 204, "text": "nonexistent word being deﬁned and used in a sentence, so the task is few-shot in terms of previous examples of the\nbroad task and one-shot in terms of the speciﬁc word. Table 3.16 shows the 6 examples we generated; all deﬁnitions\nwere human-generated, and the ﬁrst answer was human-generated as conditioning while the subsequent answers were\ngenerated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try\nany prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the ﬁnal\nsentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of\nthe word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy\nsword ﬁght. Overall, GPT-3 appears to be at least proﬁcient at the task of using novel words in a sentence."}
{"doc_id": "2005.14165", "para_id": 205, "text": "Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-\nshot setting by giving prompts of the form \"Poor English Input:\n<sentence>\\n Good English Output:\n<sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any\nomissions or repeats). Results are shown in Figure 3.17."}
{"doc_id": "2005.14165", "para_id": 206, "text": "4\nMeasuring and Preventing Memorization Of Benchmarks"}
{"doc_id": "2005.14165", "para_id": 207, "text": "Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our\nbenchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research\nwithout established best practices. While it is common practice to train large models without investigating contamination,\ngiven the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to."}
{"doc_id": "2005.14165", "para_id": 208, "text": "This concern is not just hypothetical. One of the ﬁrst papers to train a language model on Common Crawl data [TL18]\ndetected and removed a training document which overlapped with one of their evaluation datasets. Other work such\nas GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, ﬁnding that"}
{"doc_id": "2005.14165", "para_id": 209, "text": "Poor English input:\nI eated the purple berries.\nGood English output:\nI ate the purple berries.\nPoor English input:\nThank you for picking me as your designer.\nI’d appreciate it.\nGood English output:\nThank you for choosing me as your designer.\nI appreciate it.\nPoor English input:\nThe mentioned changes have done.\nor I did the alteration that you\nrequested.\nor I changed things you wanted and did the modifications.\nGood English output:\nThe requested changes have been made.\nor I made the alteration that you\nrequested.\nor I changed things you wanted and made the modifications.\nPoor English input:\nI’d be more than happy to work with you in another project.\nGood English output:\nI'd be more than happy to work with you on another project."}
{"doc_id": "2005.14165", "para_id": 210, "text": "Poor English input:\nPlease provide me with a short brief of the design you’re looking for and\nthat’d be nice if you could share some examples or project you did before.\nGood English output:\nPlease provide me with a brief description of the design you're\nlooking for and that would be nice if you could share some examples or projects you have\ndone before."}
{"doc_id": "2005.14165", "para_id": 211, "text": "Poor English input:\nThe patient was died.\nGood English output:\nThe patient died."}
{"doc_id": "2005.14165", "para_id": 212, "text": "Poor English input:\nWe think that Leslie likes ourselves.\nGood English output:\nWe think that Leslie likes us."}
{"doc_id": "2005.14165", "para_id": 213, "text": "Poor English input:\nJanet broke Bill on the finger.\nGood English output:\nJanet broke Bill's finger."}
{"doc_id": "2005.14165", "para_id": 214, "text": "Poor English input:\nMary arranged for, in St.\nLouis, John to rent a house cheap.\nGood English output:\nMary arranged for John to rent a house in St.\nLouis."}
{"doc_id": "2005.14165", "para_id": 215, "text": "Poor English input:\nToday I have went to the store to to buys some many bottle of water.\nGood English output:\nToday I went to the store to buy some bottles of water."}
{"doc_id": "2005.14165", "para_id": 216, "text": "Poor English input:\nI have tried to hit ball with bat, but my swing is has miss.\nGood English output:\nI tried to hit the ball with the bat, but my swing missed."}
{"doc_id": "2005.14165", "para_id": 217, "text": "Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface\nis GPT-3’s completions, plain text is human prompts. In the ﬁrst few examples example both the prompt and the\ncompletion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives\nsuccessive additional prompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 aside from\nthe ﬁrst few examples as conditioning and the “Poor English input/Good English output” framing. We note that the\ndistinction between ”poor” and ”good” English (and the terms themselves) is complex, contextual, and contested. As\nthe example mentioning the rental of a house shows, assumptions that the model makes about what “good” is can even\nlead it to make errors (here, the model not only adjusts grammar, but also removes the word ”cheap” in a way that alters\nmeaning)."}
{"doc_id": "2005.14165", "para_id": 218, "text": "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\nsplit of our training distribution. Though there is some gap between training and validation performance, the gap grows\nonly minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\nrather than overﬁtting."}
{"doc_id": "2005.14165", "para_id": 219, "text": "although models did perform moderately better on data that overlapped between training and testing, this did not\nsigniﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent)."}
{"doc_id": "2005.14165", "para_id": 220, "text": "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\nmagnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\nfor contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\ndoes not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with which it was\ndeduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as\nlarge as feared."}
{"doc_id": "2005.14165", "para_id": 221, "text": "We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap\nbetween our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a\nbug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t\nfeasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts\nresults."}
{"doc_id": "2005.14165", "para_id": 222, "text": "For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, deﬁned roughly as\nexamples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when\nit is shorter than 13-grams). The goal is to very conservatively ﬂag anything that could potentially be contamination,\nso as to produce a clean subset that is free of contamination with high conﬁdence. The exact procedure is detailed in\nAppendix C."}
{"doc_id": "2005.14165", "para_id": 223, "text": "We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\nsubset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a\nsigniﬁcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be\ninﬂating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a\nquarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence\nthat contamination level and performance difference are correlated. We conclude that either our conservative method\nsubstantially overestimated contamination or that contamination has little effect on performance."}
{"doc_id": "2005.14165", "para_id": 224, "text": "Below, we review in more detail the few speciﬁc cases where either (1) the model performs signiﬁcantly worse on\nthe cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference\ndifﬁcult."}
{"doc_id": "2005.14165", "para_id": 225, "text": "Our analysis ﬂagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension\n(QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English"}
{"doc_id": "2005.14165", "para_id": 226, "text": "Figure 4.2: Benchmark contamination analysis\nWe constructed cleaned versions of each of our benchmarks to\ncheck for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the\ndataset is known with high conﬁdence to be clean, and the y-axis shows the difference in performance when evaluating\nonly on the veriﬁed clean subset. Performance on most benchmarks changed negligibly, but some were ﬂagged for\nfurther review. On inspection we ﬁnd some evidence for contamination of the PIQA and Winograd results, and we mark\nthe corresponding results in Section 3 with an asterisk. We ﬁnd no evidence that other benchmarks are affected."}
{"doc_id": "2005.14165", "para_id": 227, "text": "translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false\npositives. We summarize the results for each group of tasks below:"}
{"doc_id": "2005.14165", "para_id": 228, "text": "• Reading Comprehension: Our initial analysis ﬂagged >90% of task examples from QuAC, SQuAD2, and\nDROP as potentially contaminated, so large that even measuring the differential on a clean subset was difﬁcult.\nUpon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source\ntext was present in our training data but the question/answer pairs were not, meaning the model gains only\nbackground information and cannot memorize the answer to a speciﬁc question.\n• German translation: We found 25% of the examples in the WMT16 German-English test set were marked\nas potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the\nﬂagged examples contain paired sentences resembling NMT training data and collisions were monolingual\nmatches mostly of snippets of events discussed in the news.\n• Reversed Words and Anagrams: Recall that these tasks are of the form “alaok = koala”. Due to the\nshort length of these tasks, we used 2-grams for ﬁltering (ignoring punctuation). After inspecting the ﬂagged\noverlaps, we found that they were not typically instances of real reversals or unscramblings in the training set,\nbut rather palindromes or trivial unscramblings, e.g “kayak = kayak”. The amount of overlap was small,\nbut removing the trivial tasks lead to an increase in difﬁculty and thus a spurious signal. Related to this, the\nsymbol insertion task shows high overlap but no effect on performance – this is because that task involves\nremoving non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to\nmany spurious matches.\n• PIQA: The overlap analysis ﬂagged 29% of examples as contaminated, and observed a 3 percentage point\nabsolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was\nreleased after our training set was created and its labels are hidden, some of the web pages used by the\ncrowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller\nmodel with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias\nrather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot\nrigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential\ncontamination.\n• Winograd: The overlap analysis ﬂagged 45% of examples, and found a 2.6% decrease in performance on the\nclean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in\nfact present in our training set, though presented in a different format than we present the task to the model.\nAlthough the decrease in performance is small, we mark our Winograd results in the main paper with an\nasterisk."}
{"doc_id": "2005.14165", "para_id": 229, "text": "• Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the\nChildren’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably\nextract a clean subset here, we do not report results on these datasets, even though we intended to when starting\nthis work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language\nmodeling benchmark."}
{"doc_id": "2005.14165", "para_id": 230, "text": "We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply\nto verify how much actual contamination existed. These appeared to often contain false positives. They had either\nno actual contamination, or had contamination that did not give away the answer to the task. One notable exception\nwas LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very\nsmall, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our ﬁll-in-the-blank format\nprecludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this\npaper, the potential contamination is noted in the results section."}
{"doc_id": "2005.14165", "para_id": 231, "text": "An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the\nsame distribution as the original dataset. It remains possible that memorization inﬂates results but at the same time\nis precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number\nof shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small\nmodels, which are unlikely to be memorizing."}
{"doc_id": "2005.14165", "para_id": 232, "text": "Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright\nremove problematic results, depending on the severity. Much work remains to be done to address this important and\nsubtle issue for the ﬁeld in general, both when designing benchmarks and when training models. For a more detailed\nexplanation of our analysis, we refer the reader to Appendix C."}
{"doc_id": "2005.14165", "para_id": 233, "text": "GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for\nfuture work."}
{"doc_id": "2005.14165", "para_id": 234, "text": "First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct\npredecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although\nthe overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to\nlose coherence over sufﬁciently long passages, contradict themselves, and occasionally contain non-sequitur sentences\nor paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of\nGPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed\ninformally that GPT-3 seems to have special difﬁculty with “common sense physics”, despite doing well on some\ndatasets (such as PIQA [BZB+19]) that test this domain. Speciﬁcally GPT-3 has difﬁculty with questions of the type\n“If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable\ngaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when\nevaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same\nway in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading\ncomprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks."}
{"doc_id": "2005.14165", "para_id": 235, "text": "GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused\non exploring in-context learning behavior in autoregressive language models because it is straightforward to both\nsample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional\narchitectures or other training objectives such as denoising. This is a noticeable difference from much of the recent\nliterature, which has documented improved ﬁne-tuning performance when using these approaches over standard\nlanguage models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks\nwhich empirically beneﬁt from bidirectionality. This may include ﬁll-in-the-blank tasks, tasks that involve looking back\nand comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then\ngenerating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a\nfew of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves\ncomparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and\nRACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at ﬁne-tuning\nthan GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with\nfew- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”."}
{"doc_id": "2005.14165", "para_id": 236, "text": "A more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether\nautoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the"}
{"doc_id": "2005.14165", "para_id": 237, "text": "pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to\npredict and what is less important. [RRS20] demonstrate beneﬁts of customizing prediction to entities of interest. Also,\nwith self-supervised objectives, task speciﬁcation relies on forcing the desired task into a prediction problem, whereas\nultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed\nactions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains\nof experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world\n[BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a\ndifferent approach is likely to be necessary. Promising future directions in this vein might include learning the objective\nfunction from humans [ZSW+19a], ﬁne-tuning with reinforcement learning, or adding additional modalities such as\nimages to provide grounding and a better model of the world [CLY+19]."}
{"doc_id": "2005.14165", "para_id": 238, "text": "Another limitation broadly shared by language models is poor sample efﬁciency during pre-training. While GPT-3\ntakes a step towards test-time sample efﬁciency closer to that of humans (one-shot or zero-shot), it still sees much more\ntext during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efﬁciency is\nan important direction for future work, and might come from grounding in the physical world to provide additional\ninformation, or from algorithmic improvements."}
{"doc_id": "2005.14165", "para_id": 239, "text": "A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot\nlearning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identiﬁes tasks that it\nhas learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that\nare drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format,\nto adapting to a speciﬁc style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on\nthis spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deﬁning nonsense words\nseem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although\npossibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what\nhumans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training\nand identifying them at test time would be an advance for language models, but nevertheless understanding precisely\nhow few-shot learning works is an important unexplored direction for future research."}
{"doc_id": "2005.14165", "para_id": 240, "text": "A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are\nboth expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of\nmodels of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large\nmodels down to a manageable size for speciﬁc tasks. Large models such as GPT-3 contain a very wide range of skills,\nmost of which are not needed for a speciﬁc task, suggesting that in principle aggressive distillation may be possible.\nDistillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters;\nnew challenges and opportunities may be associated with applying it to models of this size."}
{"doc_id": "2005.14165", "para_id": 241, "text": "Finally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable,\nit is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in\nperformance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This\nlast issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special\nconcern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts\n(Section 6)."}
{"doc_id": "2005.14165", "para_id": 242, "text": "Language models have a wide range of beneﬁcial applications for society, including code and writing auto-completion,\ngrammar assistance, game narrative generation, improving search engine responses, and answering questions. But\nthey also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over\nsmaller models and increases the difﬁculty of distinguishing synthetic text from human-written text. It therefore has the\npotential to advance both the beneﬁcial and harmful applications of language models."}
{"doc_id": "2005.14165", "para_id": 243, "text": "Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily\ngreater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this\nare numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in\nSection 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brieﬂy\ndiscuss issues of energy efﬁciency (Section 6.3)."}
{"doc_id": "2005.14165", "para_id": 244, "text": "Malicious uses of language models can be somewhat difﬁcult to anticipate because they often involve repurposing\nlanguage models in a very different environment or for a different purpose than researchers intended. To help with this,\nwe can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying\nthreats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact\n[Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures."}
{"doc_id": "2005.14165", "para_id": 245, "text": "Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples\ninclude misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing\nand social engineering pretexting. Many of these applications bottleneck on human beings to write sufﬁciently high\nquality text. Language models that produce high quality text generation could lower existing barriers to carrying out\nthese activities and increase their efﬁcacy."}
{"doc_id": "2005.14165", "para_id": 246, "text": "The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to\ngenerate several paragraphs of synthetic content that people ﬁnd difﬁcult to distinguish from human-written text in\n3.9.4 represents a concerning milestone in this regard."}
{"doc_id": "2005.14165", "para_id": 247, "text": "Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors\nwho may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced\n(e.g. state-sponsored) groups with long-term agendas [SBC+19]."}
{"doc_id": "2005.14165", "para_id": 248, "text": "To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat\ngroups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did\nﬁnd signiﬁcant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances\nof experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated\nwith media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is\nnot immediate, but signiﬁcant improvements in reliability could change this."}
{"doc_id": "2005.14165", "para_id": 249, "text": "Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about\npossible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible\ndifference in operations that may see potential gains by using language models. The assessment was that language\nmodels may not be worth investing signiﬁcant resources in because there has been no convincing demonstration that\ncurrent language models are signiﬁcantly better than current methods for generating text, and because methods for\n“targeting” or “controlling” the content of language models are still at a very early stage."}
{"doc_id": "2005.14165", "para_id": 250, "text": "Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their\nagenda. TTPs are inﬂuenced by economic factors like scalability and ease of deployment; phishing is extremely popular\namong all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login\ncredentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment."}
{"doc_id": "2005.14165", "para_id": 251, "text": "Ease of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the adoption of TTPs.\nThe outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k\ntruncation) they are not able to perform consistently without human feedback. If a social media disinformation bot\nproduces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the\namount of human labor required in operating this bot. But a human is still needed to ﬁlter the outputs, which restricts\nhow scalable the operation can be."}
{"doc_id": "2005.14165", "para_id": 252, "text": "Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will\neventually develop language models that are sufﬁciently consistent and steerable that they will be of greater interest to\nmalicious actors. We expect this will introduce challenges for the broader research community, and hope to work on\nthis through a combination of mitigation research, prototyping, and coordinating with other technical developers."}
{"doc_id": "2005.14165", "para_id": 253, "text": "Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning,\nsince model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and\nproducing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in\nthe model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation. 8"}
{"doc_id": "2005.14165", "para_id": 254, "text": "Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and\nbehaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely\npresent and could be studied in follow-up work. This is a preliminary analysis and does not reﬂect all of the model’s\nbiases even within the studied categories."}
{"doc_id": "2005.14165", "para_id": 255, "text": "Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reﬂect stereotypes\npresent in their training data. Below we discuss our preliminary ﬁndings of bias along the dimensions of gender, race,\nand religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how\nthey are different in this dimension."}
{"doc_id": "2005.14165", "para_id": 256, "text": "In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found\nthat occupations in general have a higher probability of being followed by a male gender identiﬁer than a female one\n(in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant).\n83% of the 388 occupations we tested were more likely to be followed by a male identiﬁer by GPT-3. We measured\nthis by feeding the model a context such as \"The detective was a\" and then looking at the probability of the\nmodel following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.).\nIn particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus\nwere heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and\nsheriff. Occupations that were more likely to be followed by female identiﬁers include midwife, nurse, receptionist,\nhousekeeper etc."}
{"doc_id": "2005.14165", "para_id": 257, "text": "We also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation}\nwas a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\"\n(Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent\n{occupation} was a,\" the majority of occupations had an even higher probability of being followed by a\nmale identiﬁer than a female one than was the case with our original neutral prompt, \"The {occupation} was\na\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male\nwith a similar probability than for our original neutral prompt.\nThe average occupation bias - measured as\n1\nnjobs\nP"}
{"doc_id": "2005.14165", "para_id": 258, "text": "P (male|Context)) ) - was −1.11 for the Neutral Variant, −2.14 for the Competent Variant and −1.15\nfor the Incompetent Variant."}
{"doc_id": "2005.14165", "para_id": 259, "text": "We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further\ncorroborated the model’s tendency to associate most occupations with males. One method measured the mod-\nels ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model\na context such as \"The advisor met with the advisee because she wanted to get advice about job\napplications.\n‘She’ refers to the\" and found the option with the lowest probability between the two possi-\nble options (Choices between Occupation Option: advisor; Participant Option: advisee)."}
{"doc_id": "2005.14165", "para_id": 260, "text": "Occupation and participant words often have societal biases associated with them such as the assumption that most\noccupants are by default male. We found that the language models learnt some of these biases such as a tendency to\nassociate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of\nall the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences\nwhere the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All\nother models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns\nwith the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers\nsome preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger\nmodels are more robust than smaller models."}
{"doc_id": "2005.14165", "para_id": 261, "text": "We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-\nselected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature"}
{"doc_id": "2005.14165", "para_id": 262, "text": "8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work.\nSee, for example, [HZJ+19, NBR20, SCNP19]."}
{"doc_id": "2005.14165", "para_id": 263, "text": "Table 6.1: Most Biased Descriptive Words in 175B Model"}
{"doc_id": "2005.14165", "para_id": 264, "text": "Top 10 Most Biased Male Descriptive Words with Raw\nCo-Occurrence Counts\nTop 10 Most Biased Female Descriptive Words with Raw\nCo-Occurrence Counts"}
{"doc_id": "2005.14165", "para_id": 265, "text": "Average Number of Co-Occurrences Across All Words:\n17.5\nAverage Number of Co-Occurrences Across All Words:\n23.9"}
{"doc_id": "2005.14165", "para_id": 266, "text": "Large (16)\nOptimistic (12)\nMostly (15)\nBubbly (12)\nLazy (14)\nNaughty (12)\nFantastic (13)\nEasy-going (12)\nEccentric (13)\nPetite (10)\nProtect (10)\nTight (10)\nJolly (10)\nPregnant (10)\nStable (9)\nGorgeous (28)\nPersonable (22)\nSucked (8)\nSurvive (7)\nBeautiful (158)"}
{"doc_id": "2005.14165", "para_id": 267, "text": "of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She\nwas very\", \"He would be described as\", \"She would be described as\"9. We looked at the adjectives and\nadverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more\noften described using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men who were\nmore often described using adjectives that span a greater spectrum."}
{"doc_id": "2005.14165", "para_id": 268, "text": "Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each\nword co-occurred with a pronoun indicator. “Most Favored” here indicates words which were most skewed towards a\ncategory by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective,\nwe have also included the average for the number of co-occurrences across all qualifying words for each gender."}
{"doc_id": "2005.14165", "para_id": 269, "text": "To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\",\n\"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800\nsamples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White\nor Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that\nlanguage models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored\nhow race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred\ndisproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive\nwords (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid:\n-87.5) and a score of 0 indicating neutral words (eg. sloping, chalet)."}
{"doc_id": "2005.14165", "para_id": 270, "text": "It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that\nfocused on racial features; these results are not from the models talking about race in the wild but talking about race in\nan experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply\nlooking at word co-occurrences, the resulting sentiment can reﬂect socio-historical factors - for instance, text relating to\na discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated\nwith a negative sentiment under this testing methodology."}
{"doc_id": "2005.14165", "para_id": 271, "text": "Across the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the\nother hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences\nnarrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and\nhighlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data."}
{"doc_id": "2005.14165", "para_id": 272, "text": "9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not\nrequire the isolation of instances in which ‘they’ refers to a singular noun from those where it didn’t, but other forms of gender bias\nare likely present and could be studied using different approaches."}
{"doc_id": "2005.14165", "para_id": 273, "text": "Religion\nMost Favored Descriptive Words\nAtheism\n‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’,\n‘Characterized’"}
{"doc_id": "2005.14165", "para_id": 274, "text": "Buddhism\n‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En-\nlightenment’, ‘Non-Violent’"}
{"doc_id": "2005.14165", "para_id": 275, "text": "Christianity\n‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com-\nments’, ‘Ofﬁcially’"}
{"doc_id": "2005.14165", "para_id": 276, "text": "Hinduism\n‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’"}
{"doc_id": "2005.14165", "para_id": 277, "text": "Islam\n‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’,\n‘Prophet’"}
{"doc_id": "2005.14165", "para_id": 278, "text": "Judaism\n‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’"}
{"doc_id": "2005.14165", "para_id": 279, "text": "Table 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model."}
{"doc_id": "2005.14165", "para_id": 280, "text": "We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam,\nand Judaism, by generating 800 model outputs of length ≈50 with a temperature of 1 and a top p of 0.9 for every\nprompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each\nof the six religious categories listed above. We then allowed the model to naturally carry out completions and created a\ncorpus of such completions for studying co-occurrence of words."}
{"doc_id": "2005.14165", "para_id": 281, "text": "The following is an example output from the model:"}
{"doc_id": "2005.14165", "para_id": 282, "text": "\"Buddhists are divided into two main branches - Theravada and Mahayana.\nTheravada\nis the more conservative branch, centering on monastic life and the earliest sutras\nand refusing to recognize the later Mahayana sutras as authentic.\""}
{"doc_id": "2005.14165", "para_id": 283, "text": "Similar to race, we found that the models make associations with religious terms that indicate some propensity to reﬂect\nhow these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such\nas ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such\nas violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in\nthe top 40 most favored words for Islam in GPT-3."}
{"doc_id": "2005.14165", "para_id": 284, "text": "We have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\nand to highlight the inherent difﬁculties in characterizing biases in large-scale generative models; we expect this to be an\narea of continuous research for us and are excited to discuss different methodological approaches with the community.\nWe view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but\nwe recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18]."}
{"doc_id": "2005.14165", "para_id": 285, "text": "Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this\nis also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions speciﬁc to large\nlanguage models. In order to pave the way for effective bias prevention in general purpose models, there is a need for\nbuilding a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for\nthese models. There is room for more research that engages with the literature outside NLP, better articulates normative\nstatements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20].\nThus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been\nshown to have blind spots [GG19, NvNvdG19] but in a holistic manner."}
{"doc_id": "2005.14165", "para_id": 286, "text": "Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3\n175B consumed several thousand petaﬂop/s-days of compute during pre-training, compared to tens of petaﬂop/s-days\nfor a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efﬁciency of such\nmodels, as advocated by [SDSE19]."}
{"doc_id": "2005.14165", "para_id": 287, "text": "The use of large-scale pre-training also gives another lens through which to view the efﬁciency of large models - we\nshould consider not only the resources that go into training them, but how these resources are amortized over the\nlifetime of a model, which will subsequently be used for a variety of purposes and ﬁne-tuned for speciﬁc tasks. Though\nmodels like GPT-3 consume signiﬁcant resources during training, they can be surprisingly efﬁcient once trained: even\nwith the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or\nonly a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down\nthe cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efﬁcient\nversions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efﬁciency\nof such models over time, similar to trends observed in image recognition and neural machine translation [HB20]."}
{"doc_id": "2005.14165", "para_id": 288, "text": "Several lines of work have focused on increasing parameter count and/or computation in language models as a\nmeans to improve generative or task performance. An early work scaled LSTM based language models to over a\nbillion parameters [JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling\nup parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size:\n213 million parameters [VSP+17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters\n[RWC+19], 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and most recently 17 billion parameters\n[Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of\nincreasing models’ capacity to store information without increased computational cost. These approaches rely on the\nconditional computation framework [BLC13] and speciﬁcally, the mixture-of-experts method [SMM+17] has been\nused to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19],\nthough only a small fraction of the parameters are actually used on each forward pass. A third approach increases\ncomputation without increasing parameters; examples of this approach include adaptive computation time [Gra16] and\nthe universal transformer [DGV+18]. Our work focuses on the ﬁrst approach (scaling compute and parameters together,\nby straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ\nthis strategy."}
{"doc_id": "2005.14165", "para_id": 289, "text": "Several efforts have also systematically studied the effect of scale on language model performance. [KMH+20,\nRRBS19, LWS+20, HNA+17], ﬁnd a smooth power-law trend in loss as autoregressive language models are scaled up.\nThis work suggests that this trend largely continues as models continue to scale up (although a slight bending of the\ncurve can perhaps be detected in Figure 3.1), and we also ﬁnd relatively smooth increases in many (though not all)\ndownstream tasks across 3 orders of magnitude of scaling."}
{"doc_id": "2005.14165", "para_id": 290, "text": "Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language\nmodels that are as small as possible. This approach includes ALBERT [LCG+19] as well as general [HVD15] and"}
{"doc_id": "2005.14165", "para_id": 291, "text": "task-speciﬁc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and\ntechniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint\nof giant models."}
{"doc_id": "2005.14165", "para_id": 292, "text": "As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable\neffort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19,\nIBGC+14, CCE+18, MCKS18], reading comprehension [CHI+18, RCM19], and adversarially constructed datasets\ndesigned to be difﬁcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many\nof these datasets."}
{"doc_id": "2005.14165", "para_id": 293, "text": "Many previous efforts have focused speciﬁcally on question-answering, which constitutes a signiﬁcant fraction of the\ntasks we tested on. Recent efforts include [RSR+19, RRS20], which ﬁne-tuned an 11 billion parameter language model,\nand [GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on\nin-context learning but could be combined in the future with those of [GLT+20, LPP+20]."}
{"doc_id": "2005.14165", "para_id": 294, "text": "Metalearning in language models has been utilized in [RWC+19], though with much more limited results and no\nsystematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it\nstructurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including\nmatching networks [VBL+16], RL2 [DSC+16], learning to optimize [RL16, ADG+16, LM17] and MAML [FAL17].\nOur approach of stufﬁng the model’s context with previous examples is most structurally similar to RL2 and also\nresembles [HYC01], in that an inner loop of adaptation takes place through computation in the model’s activations\nacross timesteps, without updating the weights, while an outer loop (in this case just language model pre-training)\nupdates the weights, and implicitly learns the ability to adapt to or at least recognize tasks deﬁned at inference-time.\nFew-shot auto-regressive density estimation was explored in [RCP+17] and [GWC+18] studied low-resource NMT as\na few-shot learning problem."}
{"doc_id": "2005.14165", "para_id": 295, "text": "While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained\nlanguage models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-ﬁeld with\nsimilar goals is semi-supervised learning where approaches such as UDA [XDH+19] also explore methods of ﬁne-tuning\nwhen very little labeled data is available."}
{"doc_id": "2005.14165", "para_id": 296, "text": "Giving multi-task models instructions in natural language was ﬁrst formalized in a supervised setting with [MKXS18]\nand utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting\ntasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for\nmulti-task ﬁne-tuning rather than for in-context learning without weight updates."}
{"doc_id": "2005.14165", "para_id": 297, "text": "Another approach to increasing generality and transfer-learning capability in language models is multi-task learning\n[Car97], which ﬁne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for\neach one. If successful multi-task learning could allow a single model to be used for many tasks without updating the\nweights (similar to our in-context learning approach), or alternatively could improve sample efﬁciency when updating\nthe weights for a new task. Multi-task learning has shown some promising initial results [LGH+15, LSP+18] and\nmulti-stage ﬁne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed\nthe boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and\nset up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of\ntasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate\na broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human\ninteraction [ZSW+19b], or active learning [Mac92]."}
{"doc_id": "2005.14165", "para_id": 298, "text": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based\nbidirectionality [DCLT18], preﬁxLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu-\ntations during training [YDY+19], architectures that improve the efﬁciency of sampling [DYY+19], improvements in\ndata and training procedures [LOG+19], and efﬁciency increases in the embedding parameters [LCG+19]. Many of\nthese techniques provide signiﬁcant gains on downstream tasks. In this work we continue to focus on pure autoregressive\nlanguage models, both in order to focus on in-context learning performance and to reduce the complexity of our large\nmodel implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s\nperformance on downstream tasks, especially in the ﬁne-tuning setting, and combining GPT-3’s scale with these\nalgorithmic techniques is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 299, "text": "We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and\nbenchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of"}
{"doc_id": "2005.14165", "para_id": 300, "text": "state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at\ntasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning.\nWe also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results\nsuggest that very large language models may be an important ingredient in the development of adaptable, general\nlanguage systems."}
{"doc_id": "2005.14165", "para_id": 301, "text": "The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub\nPachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea\nVoss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up\nthis project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura\nBurda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early\ndiscussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments,\nChris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of\npeople who created content that was used in the training of the model, and to those who were involved in indexing or\nupvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure\nand supercomputing teams for making it possible to train models at this scale."}
{"doc_id": "2005.14165", "para_id": 302, "text": "Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu\nimplemented the large-scale models, training infrastructure, and model-parallel strategies."}
{"doc_id": "2005.14165", "para_id": 303, "text": "Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments."}
{"doc_id": "2005.14165", "para_id": 304, "text": "Ben Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on the training data."}
{"doc_id": "2005.14165", "para_id": 305, "text": "Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and\nGirish Sastry implemented the downstream tasks and the software framework for supporting them, including creation\nof synthetic tasks."}
{"doc_id": "2005.14165", "para_id": 306, "text": "Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and\napplied scaling laws to help predict and guide model and data scaling decisions for the research."}
{"doc_id": "2005.14165", "para_id": 307, "text": "Ben Mann implemented sampling without replacement during training."}
{"doc_id": "2005.14165", "para_id": 308, "text": "Alec Radford originally demonstrated few-shot learning occurs in language models."}
{"doc_id": "2005.14165", "para_id": 309, "text": "Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically\nstudied in-context learning curves, task prompting, and evaluation methods."}
{"doc_id": "2005.14165", "para_id": 310, "text": "Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully\nhalf-precision training."}
{"doc_id": "2005.14165", "para_id": 311, "text": "Rewon Child and Mark Chen developed an early version of our model-parallel strategy."}
{"doc_id": "2005.14165", "para_id": 312, "text": "Rewon Child and Scott Gray contributed the sparse transformer."}
{"doc_id": "2005.14165", "para_id": 313, "text": "Aditya Ramesh experimented with loss scaling strategies for pretraining."}
{"doc_id": "2005.14165", "para_id": 314, "text": "Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search."}
{"doc_id": "2005.14165", "para_id": 315, "text": "Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature."}
{"doc_id": "2005.14165", "para_id": 316, "text": "Sandhini Agarwal conducted the fairness and representation analysis."}
{"doc_id": "2005.14165", "para_id": 317, "text": "Girish Sastry and Amanda Askell conducted the human evaluations of the model."}
{"doc_id": "2005.14165", "para_id": 318, "text": "Ariel Herbert-Voss conducted the threat analysis of malicious use."}
{"doc_id": "2005.14165", "para_id": 319, "text": "Gretchen Krueger edited and red-teamed the policy sections of the paper."}
{"doc_id": "2005.14165", "para_id": 320, "text": "Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner\noptimized OpenAI’s clusters to run the largest models efﬁciently."}
{"doc_id": "2005.14165", "para_id": 321, "text": "Scott Gray developed fast GPU kernels used during training."}
{"doc_id": "2005.14165", "para_id": 322, "text": "Jack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and\nbroader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work."}
{"doc_id": "2005.14165", "para_id": 323, "text": "Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal,\nAmanda Askell, Girish Sastry, and Jack Clark wrote the paper."}
{"doc_id": "2005.14165", "para_id": 324, "text": "Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work."}
{"doc_id": "2005.14165", "para_id": 325, "text": "Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated\nthe beneﬁt of weight decay for training."}
{"doc_id": "2005.14165", "para_id": 326, "text": "Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla,\nRewon, Alec, and Aditya on their work."}
{"doc_id": "2005.14165", "para_id": 327, "text": "As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1)\nﬁltering Common Crawl and (2) fuzzy deduplication:"}
{"doc_id": "2005.14165", "para_id": 328, "text": "1. In order to improve the quality of Common Crawl, we developed an automatic ﬁltering method to remove low\nquality documents. Using the original WebText as a proxy for high-quality documents, we trained a classiﬁer\nto distinguish these from raw Common Crawl. We then used this classiﬁer to re-sample Common Crawl by\nprioritizing documents which were predicted by the classiﬁer to be higher quality. The classiﬁer is trained\nusing logistic regression classiﬁer with features from Spark’s standard tokenizer and HashingTF 10. For the\npositive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books\ncorpus as the positive examples, and for the negative examples, we used unﬁltered Common Crawl. We used\nthis classiﬁer to score Common Crawl documents. We kept each document in our dataset iff"}
{"doc_id": "2005.14165", "para_id": 329, "text": "We chose α = 9 in order to take mostly documents the classiﬁer scored highly, but still include some documents\nthat were out of distribution. α was chosen to match the distribution of scores from our classiﬁer on WebText.\nWe found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative\ntext samples."}
{"doc_id": "2005.14165", "para_id": 330, "text": "2. To further improve model quality and prevent overﬁtting (which becomes increasingly important as model\ncapacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with\nother documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the\nsame features as were used for classiﬁcation above. We also fuzzily removed WebText from Common Crawl.\nOverall this decreased dataset size by an average of 10%."}
{"doc_id": "2005.14165", "para_id": 331, "text": "After ﬁltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in\nAppendix C."}
{"doc_id": "2005.14165", "para_id": 332, "text": "To train all versions of GPT-3, we use Adam with β1 = 0.9, β2 = 0.95, and ϵ = 10−8, we clip the global norm of the\ngradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260\nbillion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the ﬁrst 375\nmillion tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over\nthe ﬁrst 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during\ntraining (until an epoch boundary is reached) to minimize overﬁtting. All models use weight decay of 0.1 to provide a\nsmall amount of regularization [LH17]."}
{"doc_id": "2005.14165", "para_id": 333, "text": "During training we always train on sequences of the full nctx = 2048 token context window, packing multiple\ndocuments into a single sequence when documents are shorter than 2048, in order to increase computational efﬁciency.\nSequences with multiple documents are not masked in any special way but instead documents within a sequence\nare delimited with a special end of text token, giving the language model the information necessary to infer that\ncontext separated by the end of text token is unrelated. This allows for efﬁcient training without need for any special\nsequence-speciﬁc masking."}
{"doc_id": "2005.14165", "para_id": 334, "text": "In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on\nmethodology and results."}
{"doc_id": "2005.14165", "para_id": 335, "text": "Initial training set ﬁltering\nWe attempted to remove text occurring in benchmarks from training data by searching\nfor 13−gram overlaps between all test/development sets used in this work and our training data, and we removed\nthe colliding 13−gram as well as a 200 character window around it, splitting the original document into pieces. For\nﬁltering purposes we deﬁne a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than\n200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and"}
{"doc_id": "2005.14165", "para_id": 336, "text": "10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF"}
{"doc_id": "2005.14165", "para_id": 337, "text": "removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long\ndocuments such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in\nwhich the Wikipedia article quotes a single line from a book. We ignored 13−grams that matched more than 10 training\ndocuments, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar\ncontent that we likely do want the model to learn, rather than undesired speciﬁc overlaps with test sets. Examples for\nvarious frequencies can be found in the GPT-3 release repository11."}
{"doc_id": "2005.14165", "para_id": 338, "text": "Overlap methodology\nFor our benchmark overlap analysis in Section 4, we used a variable number of words N to\ncheck for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation,\nwhitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic\ntasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data\nmarked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test\ncontamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps\nbetween test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl\ndocuments per Section 2.2."}
{"doc_id": "2005.14165", "para_id": 339, "text": "We deﬁne a ‘dirty’ example as one with any N-gram overlap with any training document, and a ‘clean’ example as one\nwith no collision."}
{"doc_id": "2005.14165", "para_id": 340, "text": "Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed\nby this analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it\nwas infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling\nbenchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this\npaper. Overlaps are shown in Table C.1"}
{"doc_id": "2005.14165", "para_id": 341, "text": "Overlap results\nTo understand how much having seen some of the data helps the model perform on downstream\ntasks, we ﬁlter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report\nthe relative percent change between the clean score and the original score. If the clean score is more than 1% or 2%\nworse than the overall score, it suggests the model may have overﬁt to the examples it has seen. If the clean score is\nsigniﬁcantly better, our ﬁltering scheme may have preferentially marked easier examples as dirty."}
{"doc_id": "2005.14165", "para_id": 342, "text": "This overlap metric tends to show a high rate of false positives for datasets that contain background information (but\nnot answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words\nlong, which we ignored in our ﬁltering process (except for wordscrambling tasks). One instance where this technique\nseems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The\ninformation required to answer the question is in a passage provided to the model, so having seen the passage during\ntraining but not the questions and answers does not meaningfully constitute cheating. We conﬁrmed that every matching\ntraining document contained only the source passage, and none of the questions and answers in the dataset. The more\nlikely explanation for the decrease in performance is that the 6% of examples that remain after ﬁltering come from a\nslightly different distribution than the dirty examples."}
{"doc_id": "2005.14165", "para_id": 343, "text": "Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but\nthere is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive\nto contamination. See Section 4 for details on the datasets we ﬂagged for further review."}
{"doc_id": "2005.14165", "para_id": 344, "text": "11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md"}
{"doc_id": "2005.14165", "para_id": 345, "text": "Name\nSplit\nMetric\nN\nAcc/F1/BLEU\nTotal\nCount\nDirty\nAcc/F1/BLEU\nDirty\nCount\nClean\nAcc/F1/BLEU\nClean\nCount\nClean\nPercentage"}
{"doc_id": "2005.14165", "para_id": 346, "text": "Quac\ndev\nf1\n13\n44.3\n7353\n44.3\n7315\n54.1\n38\n1%\n20%\nSQuADv2\ndev\nf1\n13\n69.8\n11873\n69.9\n11136\n68.4\n737\n6%\n-2%\nDROP\ndev\nf1\n13\n36.5\n9536\n37.0\n8898\n29.5\n638\n7%\n-21%\nSymbol Insertion\ndev\nacc\n7\n66.9\n10000\n66.8\n8565\n67.1\n1435\n14%\n0%\nCoQa\ndev\nf1\n13\n86.0\n7983\n85.3\n5107\n87.1\n2876\n36%\n1%\nReCoRD\ndev\nacc\n13\n89.5\n10000\n90.3\n6110\n88.2\n3890\n39%\n-1%\nWinograd\ntest\nacc\n9\n88.6\n273\n90.2\n164\n86.2\n109\n40%\n-3%\nBoolQ\ndev\nacc\n13\n76.0\n3270\n75.8\n1955\n76.3\n1315\n40%\n0%\nMultiRC\ndev\nacc\n13\n74.2\n953\n73.4\n558\n75.3\n395\n41%\n1%\nRACE-h\ntest\nacc\n13\n46.8\n3498\n47.0\n1580\n46.7\n1918\n55%\n0%\nLAMBADA\ntest\nacc\n13\n86.4\n5153\n86.9\n2209\n86.0\n2944\n57%\n0%\nLAMBADA (No Blanks)\ntest\nacc\n13\n77.8\n5153\n78.5\n2209\n77.2\n2944\n57%\n-1%\nWSC\ndev\nacc\n13\n76.9\n104\n73.8\n42\n79.0\n62\n60%\n3%\nPIQA\ndev\nacc\n8\n82.3\n1838\n89.9\n526\n79.3\n1312\n71%\n-4%\nRACE-m\ntest\nacc\n13\n58.5\n1436\n53.0\n366\n60.4\n1070\n75%\n3%\nDe→En 16\ntest\nbleu-sb\n12\n43.0\n2999\n47.4\n739\n40.8\n2260\n75%\n-5%\nEn→De 16\ntest\nbleu-sb\n12\n30.9\n2999\n32.6\n739\n29.9\n2260\n75%\n-3%\nEn→Ro 16\ntest\nbleu-sb\n12\n25.8\n1999\n24.9\n423\n26.1\n1576\n79%\n1%\nRo→En 16\ntest\nbleu-sb\n12\n41.3\n1999\n40.4\n423\n41.6\n1576\n79%\n1%\nWebQs\ntest\nacc\n8\n41.5\n2032\n41.6\n428\n41.5\n1604\n79%\n0%\nANLI R1\ntest\nacc\n13\n36.8\n1000\n40.5\n200\n35.9\n800\n80%\n-3%\nANLI R2\ntest\nacc\n13\n34.0\n1000\n29.4\n177\n35.0\n823\n82%\n3%\nTriviaQA\ndev\nacc\n10\n71.2\n7993\n70.8\n1390\n71.3\n6603\n83%\n0%\nANLI R3\ntest\nacc\n13\n40.2\n1200\n38.3\n196\n40.5\n1004\n84%\n1%\nEn→Fr 14\ntest\nbleu-sb\n13\n39.9\n3003\n38.3\n411\n40.3\n2592\n86%\n1%\nFr→En 14\ntest\nbleu-sb\n13\n41.4\n3003\n40.9\n411\n41.4\n2592\n86%\n0%\nWiC\ndev\nacc\n13\n51.4\n638\n53.1\n49\n51.3\n589\n92%\n0%\nRTE\ndev\nacc\n13\n71.5\n277\n71.4\n21\n71.5\n256\n92%\n0%\nCB\ndev\nacc\n13\n80.4\n56\n100.0\n4\n78.8\n52\n93%\n-2%\nAnagrams 2\ndev\nacc\n2\n40.2\n10000\n76.2\n705\n37.4\n9295\n93%\n-7%\nReversed Words\ndev\nacc\n2\n0.4\n10000\n1.5\n660\n0.3\n9340\n93%\n-26%\nOpenBookQA\ntest\nacc\n8\n65.4\n500\n58.1\n31\n65.9\n469\n94%\n1%\nARC (Easy)\ntest\nacc\n11\n70.1\n2268\n77.5\n89\n69.8\n2179\n96%\n0%\nAnagrams 1\ndev\nacc\n2\n15.0\n10000\n49.8\n327\n13.8\n9673\n97%\n-8%\nCOPA\ndev\nacc\n9\n93.0\n100\n100.0\n3\n92.8\n97\n97%\n0%\nARC (Challenge)\ntest\nacc\n12\n51.6\n1144\n45.2\n31\n51.8\n1113\n97%\n0%\nHellaSwag\ndev\nacc\n13\n79.3\n10042\n86.2\n152\n79.2\n9890\n98%\n0%\nNQs\ntest\nacc\n11\n29.9\n3610\n32.7\n52\n29.8\n3558\n99%\n0%\nCycled Letters\ndev\nacc\n2\n38.6\n10000\n20.5\n73\n38.7\n9927\n99%\n0%\nSAT Analogies\ndev\nacc\n9\n65.8\n374\n100.0\n2\n65.6\n372\n99%\n0%\nStoryCloze\ntest\nacc\n13\n87.7\n1871\n100.0\n2\n87.6\n1869\n100%\n0%\nWinogrande\ndev\nacc\n13\n77.7\n1267\n-\n0\n77.7\n1267\n100%\n0%"}
{"doc_id": "2005.14165", "para_id": 347, "text": "Table C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it\nhas a single N-gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the\npercent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows\nthe number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we\nuse the metric speciﬁed in “Metric”. These scores come from evaluations with a different seed for the random examples\nused for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper."}
{"doc_id": "2005.14165", "para_id": 348, "text": "This appendix contains the calculations that were used to derive the approximate compute used to train the language\nmodels in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10%\nof the total compute for the models we are analyzing."}
{"doc_id": "2005.14165", "para_id": 349, "text": "Calculations can be seen in Table D.1 and are explained within the table caption."}
{"doc_id": "2005.14165", "para_id": 350, "text": "Total train\ncompute\n(ﬂops)\nParams\n(M)\nTraining tokens\n(billions)"}
{"doc_id": "2005.14165", "para_id": 351, "text": "T5-Small\n2.08E+00\n1.80E+20\n60\n1,000\n3\n3\n1\n0.5\nT5-Base\n7.64E+00\n6.60E+20\n220\n1,000\n3\n3\n1\n0.5\nT5-Large\n2.67E+01\n2.31E+21\n770\n1,000\n3\n3\n1\n0.5\nT5-3B\n1.04E+02\n9.00E+21\n3,000\n1,000\n3\n3\n1\n0.5\nT5-11B\n3.82E+02\n3.30E+22\n11,000\n1,000\n3\n3\n1\n0.5\nBERT-Base\n1.89E+00\n1.64E+20\n109\n250\n6\n3\n2\n1.0\nBERT-Large\n6.16E+00\n5.33E+20\n355\n250\n6\n3\n2\n1.0\nRoBERTa-Base\n1.74E+01\n1.50E+21\n125\n2,000\n6\n3\n2\n1.0\nRoBERTa-Large\n4.93E+01\n4.26E+21\n355\n2,000\n6\n3\n2\n1.0\nGPT-3 Small\n2.60E+00\n2.25E+20\n125\n300\n6\n3\n2\n1.0\nGPT-3 Medium\n7.42E+00\n6.41E+20\n356\n300\n6\n3\n2\n1.0\nGPT-3 Large\n1.58E+01\n1.37E+21\n760\n300\n6\n3\n2\n1.0\nGPT-3 XL\n2.75E+01\n2.38E+21\n1,320\n300\n6\n3\n2\n1.0\nGPT-3 2.7B\n5.52E+01\n4.77E+21\n2,650\n300\n6\n3\n2\n1.0\nGPT-3 6.7B\n1.39E+02\n1.20E+22\n6,660\n300\n6\n3\n2\n1.0\nGPT-3 13B\n2.68E+02\n2.31E+22\n12,850\n300\n6\n3\n2\n1.0\nGPT-3 175B\n3.64E+03\n3.14E+23\n174,600\n300\n6\n3\n2\n1.0"}
{"doc_id": "2005.14165", "para_id": 352, "text": "Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each\nmodel was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are\nactive for each token during a forward or backwards pass. We then note that each token is involved in a single addition\nand a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of\n3x to account for the backwards pass (as computing both ∂params"}
{"doc_id": "2005.14165", "para_id": 353, "text": "∂loss use a similar amount of compute as the\nforwards pass. Combining the previous two numbers, we get the total ﬂops per parameter per token. We multiply this\nvalue by the total training tokens and the total parameters to yield the number of total ﬂops used during training. We\nreport both ﬂops and petaﬂop/s-day (each of which are 8.64e+19 ﬂops)."}
{"doc_id": "2005.14165", "para_id": 354, "text": "E\nHuman Quality Assessment of Synthetic News Articles"}
{"doc_id": "2005.14165", "para_id": 355, "text": "This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic\nnews articles from real news articles. We ﬁrst describe the experiments on the ∼200 word news articles, and then\ndescribe the preliminary investigation of ∼500 word news articles generated by GPT-3."}
{"doc_id": "2005.14165", "para_id": 356, "text": "Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for\nfailing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean\nparticipant age was ∼38 years old. All participants were recruited through Positly, which maintains a whitelist of\nhigh-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic\nrestrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined\nby pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were\nnot allowed to take part in an experiment more than once."}
{"doc_id": "2005.14165", "para_id": 357, "text": "Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used\nthe article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B\n(GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a\nword count closest to that of the human written article was selected automatically. This was to minimize the effect\nthat completion length might have on participants’ judgments. The same output procedure for each model with the\nexception of the removal of the intentionally bad control model, as described in the main text."}
{"doc_id": "2005.14165", "para_id": 358, "text": "Model\nParticipants\nRecruited\nParticipants\nExcluded\nGenders\n(m:f:other)\nMean\nAge"}
{"doc_id": "2005.14165", "para_id": 359, "text": "Control\n76\n7\n32:37:0\n39\n216:216\nGPT-3 Small\n80\n7\n41:31:1\n40\n216:188\nGPT-3 Medium\n80\n7\n46:28:2\n39\n216:202\nGPT-3 Large\n81\n24\n46:28:2\n37\n216:200\nGPT-3 XL\n79\n14\n32:32:1\n38\n216:199\nGPT-3 2.7B\n80\n11\n36:33:0\n40\n216:202\nGPT-3 6.7B\n76\n5\n46:28:2\n37\n216:195\nGPT-3 13.0B\n81\n13\n46:28:2\n37\n216:209\nGPT-3 175B\n80\n9\n42:29:0\n37\n216:216"}
{"doc_id": "2005.14165", "para_id": 360, "text": "Table E.1: Participant details and article lengths for each experiment to evaluate human detection of ∼200 word model\ngenerated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 361, "text": "Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model\nsize increases. Duration on the control model is indicated with the dashed line. Line of best ﬁt is a linear model on a log\nscale with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 362, "text": "In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz\nB. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the\narticles with human written completions in quiz A had model generated completions in quiz B and vice versa. The\norder of quiz question was shufﬂed for each participant. Participants could leave comments and were asked to indicate\nif they had seen the articles before. Participants were instructed not to look up the articles or their content during the\nquiz and at the end of the quiz were asked if they had looked anything up during the quiz."}
{"doc_id": "2005.14165", "para_id": 363, "text": "Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for\neach model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When\nplotting a regression line in the graph of average participant accuracy vs model size, we ﬁt a power law of the form\nax−b. The 95% conﬁdence intervals were estimated from the t-distribution of the sample mean."}
{"doc_id": "2005.14165", "para_id": 364, "text": "Duration statistics: In the main text, we discussed the ﬁnding that the ability of human participants to distinguish\nmodel and human generated news articles decreases as our models become larger. We have also found that the\naverage time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower"}
{"doc_id": "2005.14165", "para_id": 365, "text": "Model\nParticipants\nRecruited\nParticipants\nExcluded\nGenders\n(m:f:other)\nMean\nAge"}
{"doc_id": "2005.14165", "para_id": 366, "text": "Control\n79\n17\n32:37:0\n39\n569:464\nGPT-3 175B\n81\n19\n32:30:0\n40\n569:498"}
{"doc_id": "2005.14165", "para_id": 367, "text": "Table E.2: Participant details and article lengths for the experiments investigating human detection of ∼500 word\nmodel generated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 368, "text": "accuracy scores despite increased time investment from participants supports the ﬁnding that larger models generate\nharder-to-distinguish news articles."}
{"doc_id": "2005.14165", "para_id": 369, "text": "Preliminary investigation of ∼500 word articles: We recruited 160 unique US-based participants to take part in 2\nexperiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from\nlate 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then\nused the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model\nfrom the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half\nhuman written and half model generated articles. Comprehension questions were added and articles were shown to\nparticipants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task.\nModel generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments."}
{"doc_id": "2005.14165", "para_id": 370, "text": "GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure\nF.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the\nstyle of Wallace Stevens. We ﬁrst experimented with a few prompts, then generated four samples with no additional\nediting or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with P = 0.9). Completions were\ntruncated when the model began to write a new title and author heading, or broke into prose commentary."}
{"doc_id": "2005.14165", "para_id": 371, "text": "Context →\nThe City\nBY C. P. CAVAFY\nTRANSLATED BY EDMUND KEELEY\n[Poem text omitted]"}
{"doc_id": "2005.14165", "para_id": 372, "text": "-------- Generated Poem 1 --------\n-------- Generated Poem 3 --------"}
{"doc_id": "2005.14165", "para_id": 373, "text": "I must have shadows on the way\nThe sun was all we had.\nNow, in the shade\nIf I am to walk I must have\nAll is changed.\nThe mind must dwell on those\nEach step taken slowly and alone\nWhite fields, that to its eyes were always old;\nTo have it ready made\nThose ancient gleams, convoluting\nThe way they lay among the huge roots,\nAnd I must think in lines of grey\nThe great dark tomes of reverie,\nTo have dim thoughts to be my guide\nThe plumed labyrinth of the sea.\nMust look on blue and green\nAnd never let my eye forget\nWhen our own breath gonne faint and fade\nThat color is my friend\nUp the long stairway of the dead\nAnd purple must surround me too\nWe shall remember well\nThe blindness of the light, the whiteness\nThe yellow of the sun is no more\nOf that white land.\nThe footsteps, and\nIntrusive than the bluish snow\nThe grasses in that light, and in the shade\nThat falls on all of us.\nI must have\nThe menacing life.\nGrey thoughts and blue thoughts walk with me\nIf I am to go away at all.\nWe played, too, once, in the warmest rooms.\nTo one content, there is one content\n-------- Generated Poem 2 --------\nFor all mankind.\nThe forest glades\nAre all the more for being in fable,\nThere is a way in the world.\nAnd wood and town in legend shadowed-\nThat way a man may live in a small town\nOnce more to return.\nThe one content\nHis lifetime and see no man come,\nHas in it every happiness, more brave\nNever the stir of men about a market,\nFor being purposeless; and so\nNever the bobbing of covered carts,\nThose island gardens, deep in the mind,\nNever the creak passed on the stair\nMake of remoteness familiarity\nClosing some other life; an old man\nAnd of the unspoken name a friend.\nCan prosper in that town, can prosper\nWhen his store is seen to close, his wife\n-------- Generated Poem 4 --------\nFades in his house, and the world shouts in vain\nThat no one comes to hear him\nNobody will come to this place.\nIt is a road that leads nowhere.\nWhere his small bell tolls.\nThe solitude is deep.\nThe mountains are high.\nIn that small town\nBut they are desolate, and they turn the traveler’s face\nEverything is known.\nTowards the North.\nAll the sounds of the world are far away.\nThe ways of the young men are known,\nWhen the wind rises above the trees,\nAnd the ways of the young women\nThe boughs bow to the ground.\nAs they walk to the Spring;\nEven the birds that inhabit the tangle of weeds\nThe rumors are all known,\nThat is the roadside cover, are silent.\nOne listens,\nThe quarrels, the hurt, the envy\nBut hears no roar of the forest.\nOne is alone.\nOf the men and the women.\nOne will be taken.\nTill a day comes\nOne will be taken.\nThat way out of the world\nThere is no utterance, there is no conversation,\nThat leads to a wood.\nBut one is uneasy all the same....\nYoung, the young man\nThere is a thin blue mist,\nWalks, a lover now,\nA darkness rising like smoke,\nKnows the streets as they are and is of them.\nAnd within that darkness\nA possession of the heart.\nComes to the wood.\nOne will be taken....\nIt was here, and it will be here again-\nHere, under this sky empty and full of light.\nThere, in the wood, among trees,\nHe sees shadows on the way, hears voices,\nhears the wind and the rustling of leaves;\nThrough an open glade\nHe sees a shape and the shape hears:\nIt waits as he waits,\nWaits as the shadows wait,\nAs the voices wait;\nShadows on the way, voices in the wind."}
{"doc_id": "2005.14165", "para_id": 374, "text": "Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace\nStevens with the title ‘Shadows on the Way’."}
{"doc_id": "2005.14165", "para_id": 375, "text": "The following ﬁgures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from\nthe ground truth datasets in this section, and no samples from GPT-3 are included here."}
{"doc_id": "2005.14165", "para_id": 376, "text": "Context →\nArticle:\nInformal conversation is an important part of any business\nrelationship.Before you start a discussion,however,make sure you understand\nwhich topics are suitable and which are considered taboo in a particular\nculture.\nLatin Americans enjoy sharing information about their local\nhistory, art and customs.You may expect questions about your family,and\nbe sure to show pictures of your children.You may feel free to ask similar\nquestions of your Latin American friends.The French think of conversation\nas an art form,and they enjoy the value of lively discussions as well as\ndisagreements.\nFor them,arguments can be interesting and they can cover\npretty much or any topic ---- as long as they occur in are respectful and\nintelligent manner.\nIn the United States,business people like to discuss a wide range of\ntopics,including opinions about work,family,hobbies,and politics.\nIn\nJapan,China,and Korea,however,people are much more private.They do not\nshare much about their thoughts,feelings,or emotions because they feel\nthat doing so might take away from the harmonious business relationship\nthey’re trying to build.Middle Easterners are also private about their\npersonal lives and family matters.It is considered rude,for example,to ask\na businessman from Saudi Arabia about his wife or children.\nAs a general rule,it’s best not to talk about politics or religion with\nyour business friends.This can get you into trouble,even in the United\nStates,where people hold different religious views.In addition,discussing\none’s salary is usually considered unsuitable.Sports is typically a\nfriendly subject in most parts of the world,although be careful not to\ncriticize national sport.Instead,be friendly and praise your host’s team."}
{"doc_id": "2005.14165", "para_id": 377, "text": "Q: What shouldn’t you do when talking about sports with colleagues from\nanother country?"}
{"doc_id": "2005.14165", "para_id": 378, "text": "A: Criticizing the sports of your colleagues’ country."}
{"doc_id": "2005.14165", "para_id": 379, "text": "Q: Which is typically a friendly topic in most places according to the\nauthor?"}
{"doc_id": "2005.14165", "para_id": 380, "text": "Q: Why are people from Asia more private in their conversation with others?"}
{"doc_id": "2005.14165", "para_id": 381, "text": "A: They don’t want to have their good relationship with others harmed by\ninformal conversation."}
{"doc_id": "2005.14165", "para_id": 382, "text": "Correct Answer →\ntaboo\nIncorrect Answer →\ncheerful topics\nIncorrect Answer →\nrude topics\nIncorrect Answer →\ntopics that can never be talked about"}
{"doc_id": "2005.14165", "para_id": 383, "text": "Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability\nof each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 384, "text": "Context →\nanli 2:\nanli 2:\nThe Gold Coast Hotel & Casino is a hotel and casino\nlocated in Paradise, Nevada.\nThis locals’ casino is owned and operated\nby Boyd Gaming.\nThe Gold Coast is located one mile (∼\n1.6km) west of the\nLas Vegas Strip on West Flamingo Road.\nIt is located across the street\nfrom the Palms Casino Resort and the Rio All Suite Hotel and Casino.\nQuestion:\nThe Gold Coast is a budget-friendly casino.\nTrue, False, or\nNeither?"}
{"doc_id": "2005.14165", "para_id": 385, "text": "Correct Answer →\nNeither\nIncorrect Answer →\nTrue\nIncorrect Answer →\nFalse"}
{"doc_id": "2005.14165", "para_id": 386, "text": "Context →\nArticle:\nMrs.\nSmith is an unusual teacher.\nOnce she told each student to bring\nalong a few potatoes in plastic bag.\nOn each potato the students had to\nwrite a name of a person that they hated And the next day, every child\nbrought some potatoes.\nSome had two potatoes;some three;some up to five.\nMrs.\nSmith then told the children to carry the bags everywhere they went,\neven to the toilet, for two weeks.\nAs day after day passed, the children\nstarted to complain about the awful smell of the rotten potatoes.\nThose children who brought five potatoes began to feel the weight trouble\nof the bags.\nAfter two weeks, the children were happy to hear that the\ngame was finally ended.\nMrs.\nSmith asked,\"How did you feel while carrying\nthe potatoes for two weeks?\" The children started complaining about the\ntrouble loudly.\nThen Mrs.\nSmith told them why she asked them to play the game.\nShe\nsaid,\"This is exactly the situation when you carry your hatred for somebody\ninside your heart.\nThe terrible smell of the hatred will pollute your\nheart and you will carry something unnecessary with you all the time.\nIf\nyou cannot stand the smell of the rotten potatoes for just two weeks, can\nyou imagine how heavy it would be to have the hatred in your heart for your\nlifetime?\nSo throw away any hatred from your heart, and you’ll be really\nhappy.\""}
{"doc_id": "2005.14165", "para_id": 387, "text": "Q: Which of the following is True according to the passage?"}
{"doc_id": "2005.14165", "para_id": 388, "text": "A: If a kid hated four people,he or she had to carry four potatoes."}
{"doc_id": "2005.14165", "para_id": 389, "text": "Q: The children complained about\nbesides the weight trouble."}
{"doc_id": "2005.14165", "para_id": 390, "text": "Q: Mrs.Smith asked her students to write\non the potatoes."}
{"doc_id": "2005.14165", "para_id": 391, "text": "Correct Answer →\nnames\nIncorrect Answer →\nnumbers\nIncorrect Answer →\ntime\nIncorrect Answer →\nplaces"}
{"doc_id": "2005.14165", "para_id": 392, "text": "Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability\nof each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 393, "text": "Correct Answer →\nUsing a brush, brush on sealant onto wood until it is fully saturated with\nthe sealant.\nIncorrect Answer →\nUsing a brush, drip on sealant onto wood until it is fully saturated with\nthe sealant."}
{"doc_id": "2005.14165", "para_id": 394, "text": "Context →\nMy body cast a shadow over the grass because"}
{"doc_id": "2005.14165", "para_id": 395, "text": "Correct Answer →\nthe sun was rising.\nIncorrect Answer →\nthe grass was cut."}
{"doc_id": "2005.14165", "para_id": 396, "text": "Context →\n(CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while\nserving as Prime Minister of Israel, criticized Donald Trump for appealing\nto \"Second Amendment people\" in a speech and warned that the words that\npoliticians use can incite violence and undermine democracy.\n\"Trump’s\nwords are an incitement to the type of political violence that touched\nme personally,\" Rabin wrote in USAToday.\nHe said that Trump’s appeal to\n\"Second Amendment people\" to stop Hillary Clinton -- comments that were\ncriticized as a call for violence against Clinton, something Trump denied\n-- \"were a new level of ugliness in an ugly campaign season.\""}
{"doc_id": "2005.14165", "para_id": 397, "text": "- The son of a former Israeli Prime Minister who was assassinated wrote an\nop ed about the consequence of violent political rhetoric.\n- Warns of \"parallels\" between Israel of the 1990s and the U.S. today."}
{"doc_id": "2005.14165", "para_id": 398, "text": "Correct Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Donald Trump’s\naggressive rhetoric.\nCorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Trump’s aggressive\nrhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Hillary Clinton’s\naggressive rhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned U.S.’s aggressive\nrhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Yitzhak Rabin’s\naggressive rhetoric."}
{"doc_id": "2005.14165", "para_id": 399, "text": "Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single ”problem” because\nthis is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script."}
{"doc_id": "2005.14165", "para_id": 400, "text": "Context →\nanli 1:\nanli 1:\nFulton James MacGregor MSP is a Scottish politician\nwho is a Scottish National Party (SNP) Member of Scottish Parliament\nfor the constituency of Coatbridge and Chryston.\nMacGregor is currently\nParliamentary Liaison Officer to Shona Robison, Cabinet Secretary for\nHealth & Sport.\nHe also serves on the Justice and Education & Skills\ncommittees in the Scottish Parliament.\nQuestion:\nFulton James MacGregor is a Scottish politican who is a Liaison\nofficer to Shona Robison who he swears is his best friend.\nTrue, False, or\nNeither?"}
{"doc_id": "2005.14165", "para_id": 401, "text": "Correct Answer →\nNeither\nIncorrect Answer →\nTrue\nIncorrect Answer →\nFalse"}
{"doc_id": "2005.14165", "para_id": 402, "text": "Context →\nOrganisms require energy in order to do what?"}
{"doc_id": "2005.14165", "para_id": 403, "text": "Correct Answer →\nmature and develop.\nIncorrect Answer →\nrest soundly.\nIncorrect Answer →\nabsorb light.\nIncorrect Answer →\ntake in nutrients."}
{"doc_id": "2005.14165", "para_id": 404, "text": "Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 405, "text": "Context →\nMaking a cake:\nSeveral cake pops are shown on a display.\nA woman and girl\nare shown making the cake pops in a kitchen.\nThey"}
{"doc_id": "2005.14165", "para_id": 406, "text": "Correct Answer →\nbake them, then frost and decorate.\nIncorrect Answer →\ntaste them as they place them on plates.\nIncorrect Answer →\nput the frosting on the cake as they pan it.\nIncorrect Answer →\ncome out and begin decorating the cake as well."}
{"doc_id": "2005.14165", "para_id": 407, "text": "Figure G.9: Formatted dataset example for HellaSwag"}
{"doc_id": "2005.14165", "para_id": 408, "text": "Context →\nanli 3:\nanli 3:\nWe shut the loophole which has American workers actually\nsubsidizing the loss of their own job.\nThey just passed an expansion of\nthat loophole in the last few days:\n$43 billion of giveaways, including\nfavors to the oil and gas industry and the people importing ceiling fans\nfrom China.\nQuestion:\nThe loophole is now gone True, False, or Neither?"}
{"doc_id": "2005.14165", "para_id": 409, "text": "Correct Answer →\nFalse\nIncorrect Answer →\nTrue\nIncorrect Answer →\nNeither"}
{"doc_id": "2005.14165", "para_id": 410, "text": "Figure G.10: Formatted dataset example for ANLI R3"}
{"doc_id": "2005.14165", "para_id": 411, "text": "Context →\nQuestion:\nGeorge wants to warm his hands quickly by rubbing them.\nWhich\nskin surface will produce the most heat?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 412, "text": "Correct Answer →\ndry palms\nIncorrect Answer →\nwet palms\nIncorrect Answer →\npalms covered with oil\nIncorrect Answer →\npalms covered with lotion"}
{"doc_id": "2005.14165", "para_id": 413, "text": "Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 414, "text": "Correct Answer →\ncajole is to compliance\nIncorrect Answer →\nbalk is to fortitude\nIncorrect Answer →\nbetray is to loyalty\nIncorrect Answer →\nhinder is to destination\nIncorrect Answer →\nsoothe is to passion"}
{"doc_id": "2005.14165", "para_id": 415, "text": "Figure G.12: Formatted dataset example for SAT Analogies"}
{"doc_id": "2005.14165", "para_id": 416, "text": "Correct Context →\nGrace was happy to trade me her sweater for my jacket.\nShe thinks the\nsweater\nIncorrect Context →\nGrace was happy to trade me her sweater for my jacket.\nShe thinks the\njacket"}
{"doc_id": "2005.14165", "para_id": 417, "text": "Figure G.13: Formatted dataset example for Winograd. The ‘partial’ evaluation method we use compares the probability\nof the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 418, "text": "Correct Context →\nJohnny likes fruits more than vegetables in his new keto diet because the\nfruits\nIncorrect Context →\nJohnny likes fruits more than vegetables in his new keto diet because the\nvegetables"}
{"doc_id": "2005.14165", "para_id": 419, "text": "Figure G.14: Formatted dataset example for Winogrande. The ‘partial’ evaluation method we use compares the\nprobability of the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 420, "text": "Context →\nREADING COMPREHENSION ANSWER KEY\nWhile this process moved along, diplomacy continued its rounds.\nDirect\npressure on the Taliban had proved unsuccessful.\nAs one NSC staff note\nput it, \"Under the Taliban, Afghanistan is not so much a state sponsor\nof terrorism as it is a state sponsored by terrorists.\" In early 2000,\nthe United States began a high-level effort to persuade Pakistan to use\nits influence over the Taliban.\nIn January 2000, Assistant Secretary\nof State Karl Inderfurth and the State Department’s counterterrorism\ncoordinator, Michael Sheehan, met with General Musharraf in Islamabad,\ndangling before him the possibility of a presidential visit in March as a\nreward for Pakistani cooperation.\nSuch a visit was coveted by Musharraf,\npartly as a sign of his government’s legitimacy.\nHe told the two envoys\nthat he would meet with Mullah Omar and press him on Bin Laden.\nThey\nleft, however, reporting to Washington that Pakistan was unlikely in fact\nto do anything,\" given what it sees as the benefits of Taliban control\nof Afghanistan.\" President Clinton was scheduled to travel to India.\nThe State Department felt that he should not visit India without also\nvisiting Pakistan.\nThe Secret Service and the CIA, however, warned in\nthe strongest terms that visiting Pakistan would risk the President’s\nlife.\nCounterterrorism officials also argued that Pakistan had not done\nenough to merit a presidential visit.\nBut President Clinton insisted\non including Pakistan in the itinerary for his trip to South Asia.\nHis\none-day stopover on March 25, 2000, was the first time a U.S. president\nhad been there since 1969.\nAt his meeting with Musharraf and others,\nPresident Clinton concentrated on tensions between Pakistan and India\nand the dangers of nuclear proliferation, but also discussed Bin Laden.\nPresident Clinton told us that when he pulled Musharraf aside for a brief,\none-on-one meeting, he pleaded with the general for help regarding Bin\nLaden.\" I offered him the moon when I went to see him, in terms of better\nrelations with the United States, if he’d help us get Bin Laden and deal\nwith another issue or two.\" The U.S. effort continued."}
{"doc_id": "2005.14165", "para_id": 421, "text": "Who did The State Department feel should visit both India and Pakistan?"}
{"doc_id": "2005.14165", "para_id": 422, "text": "Correct Answer →\n- [False] Bin Laden\nIncorrect Answer →\n- [True] Bin Laden"}
{"doc_id": "2005.14165", "para_id": 423, "text": "Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the\nquestions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question\nbeing considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use\nK to refer to the number of questions shown within the context."}
{"doc_id": "2005.14165", "para_id": 424, "text": "Context →\nQuestion:\nWhich factor will most likely cause a person to develop a fever?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 425, "text": "Correct Answer →\na bacterial population in the bloodstream\nIncorrect Answer →\na leg muscle relaxing after exercise\nIncorrect Answer →\nseveral viral particles on the skin\nIncorrect Answer →\ncarbohydrates being digested in the stomach"}
{"doc_id": "2005.14165", "para_id": 426, "text": "Figure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 427, "text": "Context →\nBob went to the gas station to fill up his car.\nHis tank was completely\nempty and so was his wallet.\nThe cashier offered to pay for his gas if he\ncame back later to pay.\nBob felt grateful as he drove home."}
{"doc_id": "2005.14165", "para_id": 428, "text": "Correct Answer →\nBob believed that there were good people in the world.\nIncorrect Answer →\nBob contemplated how unfriendly the world was."}
{"doc_id": "2005.14165", "para_id": 429, "text": "Figure G.17: Formatted dataset example for StoryCloze"}
{"doc_id": "2005.14165", "para_id": 430, "text": "Context →\nHelsinki is the capital and largest city of Finland.\nIt is in the region\nof Uusimaa, in southern Finland, on the shore of the Gulf of Finland.\nHelsinki has a population of , an urban population of , and a metropolitan\npopulation of over 1.4 million, making it the most populous municipality\nand urban area in Finland.\nHelsinki is some north of Tallinn, Estonia,\neast of Stockholm, Sweden, and west of Saint Petersburg, Russia.\nHelsinki\nhas close historical connections with these three cities."}
{"doc_id": "2005.14165", "para_id": 431, "text": "The Helsinki metropolitan area includes the urban core of Helsinki, Espoo,\nVantaa, Kauniainen, and surrounding commuter towns.\nIt is the world’s\nnorthernmost metro area of over one million people, and the city is the\nnorthernmost capital of an EU member state.\nThe Helsinki metropolitan\narea is the third largest metropolitan area in the Nordic countries\nafter Stockholm and Copenhagen, and the City of Helsinki is the third\nlargest after Stockholm and Oslo.\nHelsinki is Finland’s major political,\neducational, financial, cultural, and research center as well as one of\nnorthern Europe’s major cities.\nApproximately 75% of foreign companies\nthat operate in Finland have settled in the Helsinki region.\nThe nearby\nmunicipality of Vantaa is the location of Helsinki Airport, with frequent\nservice to various destinations in Europe and Asia."}
{"doc_id": "2005.14165", "para_id": 432, "text": "Q: what is the most populous municipality in Finland?"}
{"doc_id": "2005.14165", "para_id": 433, "text": "Q: what percent of the foreign companies that operate in Finland are in\nHelsinki?"}
{"doc_id": "2005.14165", "para_id": 434, "text": "Q: what towns are a part of the metropolitan area?"}
{"doc_id": "2005.14165", "para_id": 435, "text": "Target Completion →\nHelsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns"}
{"doc_id": "2005.14165", "para_id": 436, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nasinoc ="}
{"doc_id": "2005.14165", "para_id": 437, "text": "Figure G.19: Formatted dataset example for Cycled Letters"}
{"doc_id": "2005.14165", "para_id": 438, "text": "Context →\nPassage:\nSaint Jean de Br´ebeuf was a French Jesuit missionary who\ntravelled to New France in 1625.\nThere he worked primarily with the Huron\nfor the rest of his life, except for a few years in France from 1629 to\n1633.\nHe learned their language and culture, writing extensively about\neach to aid other missionaries.\nIn 1649, Br´ebeuf and another missionary\nwere captured when an Iroquois raid took over a Huron village .\nTogether\nwith Huron captives, the missionaries were ritually tortured and killed\non March 16, 1649.\nBr´ebeuf was beatified in 1925 and among eight Jesuit\nmissionaries canonized as saints in the Roman Catholic Church in 1930.\nQuestion:\nHow many years did Saint Jean de Br´ebeuf stay in New France\nbefore he went back to France for a few years?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 439, "text": "\"A step.\nCut in the rock.\nAbout fifty feet ahead.\" She moved faster.\nThey both moved faster.\n\"In fact,\" she said, raising the torch higher,\n\"there’s more than a\n.\n->"}
{"doc_id": "2005.14165", "para_id": 440, "text": "Figure G.21: Formatted dataset example for LAMBADA"}
{"doc_id": "2005.14165", "para_id": 441, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nskicts ="}
{"doc_id": "2005.14165", "para_id": 442, "text": "Figure G.22: Formatted dataset example for Anagrams 1 (A1)"}
{"doc_id": "2005.14165", "para_id": 443, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nvolwskagen ="}
{"doc_id": "2005.14165", "para_id": 444, "text": "Figure G.23: Formatted dataset example for Anagrams 2"}
{"doc_id": "2005.14165", "para_id": 445, "text": "Context →\nQ: Who played tess on touched by an angel?"}
{"doc_id": "2005.14165", "para_id": 446, "text": "Target Completion →\nDelloreese Patricia Early (July 6, 1931 { November 19, 2017), known\nprofessionally as Della Reese"}
{"doc_id": "2005.14165", "para_id": 447, "text": "Figure G.24: Formatted dataset example for Natural Questions"}
{"doc_id": "2005.14165", "para_id": 448, "text": "Context →\nTITLE: William Perry (American football) - Professional career\nPARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL\nDraft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.\nHowever, defensive coordinator Buddy Ryan, who had a highly acrimonious\nrelationship with Ditka, called Perry a \"wasted draft-pick\".\nPerry\nsoon became a pawn in the political power struggle between Ditka and\nRyan.\nPerry’s \"Refrigerator\" nickname followed him into the NFL and he\nquickly became a favorite of the Chicago Bears fans.\nTeammates called\nhim \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused\nto play Perry, Ditka decided to use Perry as a fullback when the team was\nnear the opponents’ goal line or in fourth and short situations, either\nas a ball carrier or a lead blocker for star running back Walter Payton.\nDitka stated the inspiration for using Perry as a fullback came to him\nduring five-yard sprint exercises.\nDuring his rookie season, Perry\nrushed for two touchdowns and caught a pass for one.\nPerry even had\nthe opportunity to run the ball during Super Bowl XX, as a nod to his\npopularity and contributions to the team’s success.\nThe first time he\ngot the ball, he was tackled for a one-yard loss while attempting to throw\nhis first NFL pass on a halfback option play.\nThe second time he got the\nball, he scored a touchdown (running over Patriots linebacker Larry McGrew\nin the process).\nAbout halfway through his rookie season, Ryan finally\nbegan to play Perry, who soon proved that he was a capable defensive\nlineman.\nHis Super Bowl ring size is the largest of any professional\nfootball player in the history of the event.\nHis ring size is 25, while\nthe ring size for the average adult male is between 10 and 12.\nPerry went\non to play for ten years in the NFL, retiring after the 1994 season.\nIn\nhis ten years as a pro, he regularly struggled with his weight, which\nhampered his performance at times.\nHe played in 138 games, recording\n29.5 sacks and five fumble recoveries, which he returned for a total of\n71 yards.\nIn his offensive career he ran five yards for two touchdowns,\nand had one reception for another touchdown.\nPerry later attempted a\ncomeback, playing an unremarkable 1996 season with the London Monarchs of\nthe World League of American Football (later NFL Europa)."}
{"doc_id": "2005.14165", "para_id": 449, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nr e!c.i p r o.c a/l ="}
{"doc_id": "2005.14165", "para_id": 450, "text": "Figure G.26: Formatted dataset example for Symbol Insertion"}
{"doc_id": "2005.14165", "para_id": 451, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\ntaefed ="}
{"doc_id": "2005.14165", "para_id": 452, "text": "Figure G.27: Formatted dataset example for Reversed Words"}
{"doc_id": "2005.14165", "para_id": 453, "text": "Background:\nFrom the German point of view, March 1941 saw an improvement.\nThe Luftwaffe flew 4,000 sorties that month, including 12 major and\nthree heavy attacks.\nThe electronic war intensified but the Luftwaffe\nflew major inland missions only on moonlit nights.\nPorts were easier to\nfind and made better targets.\nTo confuse the British, radio silence was\nobserved until the bombs fell.\nX- and Y-Ger¨at beams were placed over\nfalse targets and switched only at the last minute.\nRapid frequency\nchanges were introduced for X-Ger¨at, whose wider band of frequencies and\ngreater tactical flexibility ensured it remained effective at a time when\nBritish selective jamming was degrading the effectiveness of Y-Ger¨at."}
{"doc_id": "2005.14165", "para_id": 454, "text": "Figure G.28: Formatted dataset example for SQuADv2"}
{"doc_id": "2005.14165", "para_id": 455, "text": "Context →\nNormal force -- In a simple case such as an object resting upon a table,\nthe normal force on the object is equal but in opposite direction to the\ngravitational force applied on the object (or the weight of the object),\nthat is, N = m g (\\displaystyle N=mg), where m is mass, and g is the\ngravitational field strength (about 9.81 m/s on Earth).\nThe normal force\nhere represents the force applied by the table against the object that\nprevents it from sinking through the table and requires that the table is\nsturdy enough to deliver this normal force without breaking.\nHowever, it\nis easy to assume that the normal force and weight are action-reaction\nforce pairs (a common mistake).\nIn this case, the normal force and\nweight need to be equal in magnitude to explain why there is no upward\nacceleration of the object.\nFor example, a ball that bounces upwards\naccelerates upwards because the normal force acting on the ball is larger\nin magnitude than the weight of the ball.\nquestion:\nis the normal force equal to the force of gravity?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 456, "text": "Context →\nThe trend toward lower rents may seem surprising given that some\ncommunities in New York are bemoaning the loss of favorite local\nbusinesses to high rents.\nBut, despite the recent softening, for many\nof these retailers there’s still been too big a jump from the rental rates\nof the late 1970s, when their leases were signed.\nCertainly, the recent\ndrop in prices doesn’t mean Manhattan comes cheap.\nquestion:\nManhattan comes cheap.\ntrue, false, or neither?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 457, "text": "Context →\nThe bet, which won him dinner for four, was regarding the existence and\nmass of the top quark, an elementary particle discovered in 1995.\nquestion:\nThe Top Quark is the last of six flavors of quarks predicted by\nthe standard model theory of particle physics.\nTrue or False?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 458, "text": "Context →\nAn outfitter provided everything needed for the safari.\nBefore his first walking holiday, he went to a specialist outfitter to buy\nsome boots.\nquestion:\nIs the word ‘outfitter’ used in the same way in the two\nsentences above?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 459, "text": "Context →\nFinal Exam with Answer Key\nInstructions:\nPlease carefully read the following passages.\nFor each\npassage, you must identify which noun the pronoun marked in *bold* refers\nto.\n=====\nPassage:\nMr.\nMoncrieff visited Chester’s luxurious New York apartment,\nthinking that it belonged to his son Edward.\nThe result was that Mr.\nMoncrieff has decided to cancel Edward’s allowance on the ground that\nhe no longer requires *his* financial support.\nQuestion:\nIn the passage above, what does the pronoun \"*his*\" refer to?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 460, "text": "Context →\nQ: ‘Nude Descending A Staircase’ is perhaps the most famous painting by\nwhich 20th century artist?"}
{"doc_id": "2005.14165", "para_id": 461, "text": "Target Completion →\nMARCEL DUCHAMP\nTarget Completion →\nr mutt\nTarget Completion →\nduchamp\nTarget Completion →\nmarcel duchamp\nTarget Completion →\nR.Mutt\nTarget Completion →\nMarcel duChamp\nTarget Completion →\nHenri-Robert-Marcel Duchamp\nTarget Completion →\nMarcel du Champ\nTarget Completion →\nhenri robert marcel duchamp\nTarget Completion →\nDuchampian\nTarget Completion →\nDuchamp\nTarget Completion →\nduchampian\nTarget Completion →\nmarcel du champ\nTarget Completion →\nMarcel Duchamp\nTarget Completion →\nMARCEL DUCHAMP"}
{"doc_id": "2005.14165", "para_id": 462, "text": "Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions."}
{"doc_id": "2005.14165", "para_id": 463, "text": "Context →\nQ: What school did burne hogarth establish?"}
{"doc_id": "2005.14165", "para_id": 464, "text": "Context →\nKeinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden.\n="}
{"doc_id": "2005.14165", "para_id": 465, "text": "Target Completion →\nIn no case may they be used for commercial purposes."}
{"doc_id": "2005.14165", "para_id": 466, "text": "Figure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and\nother langauge tasks, the format for zero-shot learning is “Q: What is the {language} translation of {sentence} A:\n{translation}.”"}
{"doc_id": "2005.14165", "para_id": 467, "text": "Context →\nIn no case may they be used for commercial purposes.\n="}
{"doc_id": "2005.14165", "para_id": 468, "text": "Target Completion →\nKeinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden."}
{"doc_id": "2005.14165", "para_id": 469, "text": "Context →\nAnalysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females.\n="}
{"doc_id": "2005.14165", "para_id": 470, "text": "Target Completion →\nL’analyse de la distribution de fr´equence des stades larvaires d’I.\nverticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves\nm^ales ´etaient `a des stades plus avanc´es que les larves femelles."}
{"doc_id": "2005.14165", "para_id": 471, "text": "Context →\nL’analyse de la distribution de fr´equence des stades larvaires d’I.\nverticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves\nm^ales ´etaient `a des stades plus avanc´es que les larves femelles.\n="}
{"doc_id": "2005.14165", "para_id": 472, "text": "Target Completion →\nAnalysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females."}
{"doc_id": "2005.14165", "para_id": 473, "text": "Context →\nThe truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey’s accession\nto the European Union, despite Turkey’s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill.\n="}
{"doc_id": "2005.14165", "para_id": 474, "text": "Target Completion →\nAdev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei\neuropenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea\nEuropean˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul\n¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort."}
{"doc_id": "2005.14165", "para_id": 475, "text": "Context →\nAdev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei\neuropenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea\nEuropean˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul\n¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort.\n="}
{"doc_id": "2005.14165", "para_id": 476, "text": "Target Completion →\nThe truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey’s accession\nto the European Union, despite Turkey’s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill."}
{"doc_id": "2005.14165", "para_id": 477, "text": "Figure G.42: Formatted dataset example for Arithmetic 1DC"}
{"doc_id": "2005.14165", "para_id": 478, "text": "Figure G.43: Formatted dataset example for Arithmetic 2D-"}
{"doc_id": "2005.14165", "para_id": 479, "text": "Figure G.44: Formatted dataset example for Arithmetic 2D+"}
{"doc_id": "2005.14165", "para_id": 480, "text": "Figure G.45: Formatted dataset example for Arithmetic 2Dx"}
{"doc_id": "2005.14165", "para_id": 481, "text": "Figure G.46: Formatted dataset example for Arithmetic 3D-"}
{"doc_id": "2005.14165", "para_id": 482, "text": "Figure G.47: Formatted dataset example for Arithmetic 3D+"}
{"doc_id": "2005.14165", "para_id": 483, "text": "Figure G.48: Formatted dataset example for Arithmetic 4D-"}
{"doc_id": "2005.14165", "para_id": 484, "text": "Figure G.49: Formatted dataset example for Arithmetic 4D+"}
{"doc_id": "2005.14165", "para_id": 485, "text": "Figure G.50: Formatted dataset example for Arithmetic 5D−"}
{"doc_id": "2005.14165", "para_id": 486, "text": "Figure G.51: Formatted dataset example for Arithmetic 5D+"}
{"doc_id": "2005.14165", "para_id": 487, "text": "Name\nMetric\nSplit\nFine-tune\nSOTA\nK\nSmall Med Large XL 2.7B 6.7B 13B 175B\nSmall Med Large XL 2.7B 6.7B 13B 175B\nSmall Med Large XL 2.7B 6.7B 13B 175B\n175B\n(test server)"}
{"doc_id": "2005.14165", "para_id": 488, "text": "HellaSwag\nacc\ndev\n85.6\n20\n33.7\n43.6 51.0\n54.7 62.8 67.4 70.9 78.9\n33.0\n42.9 50.5\n53.5 61.9 66.5 70.0 78.1\n33.5\n43.1 51.3\n54.9 62.9 67.3 71.3 79.3\nLAMBADA\nacc\ntest\n68.0\n15\n42.7\n54.3 60.4\n63.6 67.1 70.3 72.5 76.2\n22.0\n47.1 52.6\n58.3 61.1 65.4 69.0 72.5\n22.0\n40.4 63.2\n57.0 78.1 79.1 81.3 86.4\nLAMBADA\nppl\ntest\n8.63\n15\n18.6\n9.09 6.53\n5.44 4.60 4.00 3.56 3.00\n165.0 11.6 8.29\n6.46 5.53 4.61 4.06 3.35\n165.0 27.6 6.63\n7.45 2.89 2.56 2.56 1.92\nStoryCloze\nacc\ntest\n91.8\n70\n63.3\n68.5 72.4\n73.4 77.2 77.7 79.5 83.2\n62.3\n68.7 72.3\n74.2 77.3 78.7 79.7 84.7\n62.3\n70.2 73.9\n76.1 80.2 81.2 83.0 87.7"}
{"doc_id": "2005.14165", "para_id": 489, "text": "NQs\nacc\ntest\n44.5\n64\n0.64\n1.75 2.71\n4.40 6.01 5.79 7.84 14.6\n1.19\n3.07 4.79\n5.43 8.73 9.78 13.7 23.0\n1.72\n4.46 7.89\n9.72 13.2 17.0 21.0 29.9\nTriviaQA\nacc\ndev\n68.0\n64\n4.15\n7.61 14.0\n19.7 31.3 38.7 41.8 64.3\n4.19\n12.9 20.5\n26.5 35.9 44.4 51.3 68.0\n6.96\n16.3 26.5\n32.1 42.3 51.6 57.5 71.2\n71.2\nWebQs\nacc\ntest\n45.5\n64\n1.77\n3.20 4.33\n4.63 7.92 7.73 8.22 14.4\n2.56\n6.20 8.51\n9.15 14.5 15.1 19.0 25.3\n5.46\n12.6 15.9\n19.6 24.8 27.7 33.5 41.5"}
{"doc_id": "2005.14165", "para_id": 490, "text": "Ro→En 16\nBLEU-mb test\n39.9\n64\n2.08\n2.71 3.09\n3.15 16.3 8.34 20.2 19.9\n0.55\n15.4 23.0\n26.3 30.6 33.2 35.6 38.6\n1.25\n20.7 25.8\n29.2 33.1 34.8 37.0 39.5\nRo→En 16\nBLEU-sb\ntest\n64\n2.39\n3.08 3.49\n3.56 16.8 8.75 20.8 20.9\n0.65\n15.9 23.6\n26.8 31.3 34.2 36.7 40.0\n1.40\n21.3 26.6\n30.1 34.3 36.2 38.4 41.3\nEn→Ro 16\nBLEU-mb test\n38.5\n64\n2.14\n2.65 2.53\n2.50 3.46 4.24 5.32 14.1\n0.35\n3.30 7.89\n8.72 13.2 15.1 17.3 20.6\n1.25\n5.90 9.33\n10.7 14.3 16.3 18.0 21.0\nEn→Ro 16\nBLEU-sb\ntest\n64\n2.61\n3.11 3.07\n3.09 4.26 5.31 6.43 18.0\n0.55\n3.90 9.15\n10.3 15.7 18.2 20.8 24.9\n1.64\n7.40 10.9\n12.9 17.2 19.6 21.8 25.8\nFr→En 14\nBLEU-mb test\n35.0\n64\n1.81\n2.53 3.47\n3.13 20.6 15.1 21.8 21.2\n1.28\n15.9 23.7\n26.3 29.0 30.5 30.2 33.7\n4.98\n25.5 28.5\n31.1 33.7 34.9 36.6 39.2\nFr→En 14\nBLEU-sb\ntest\n64\n2.29\n2.99 3.90\n3.60 21.2 15.5 22.4 21.9\n1.50\n16.3 24.4\n27.0 30.0 31.6 31.4 35.6\n5.30\n26.2 29.5\n32.2 35.1 36.4 38.3 41.4\nEn→Fr 14\nBLEU-mb test\n45.6\n64\n1.74\n2.16 2.73\n2.15 15.1 8.82 12.0 25.2\n0.49\n8.00 14.8\n15.9 20.3 23.3 24.9 28.3\n4.08\n14.5 19.3\n21.5 24.9 27.3 29.5 32.6\nEn→Fr 14\nBLEU-sb\ntest\n45.9\n64\n2.44\n2.75 3.54\n2.82 19.3 11.4 15.3 31.3\n0.81\n10.0 18.2\n19.3 24.7 28.3 30.1 34.1\n5.31\n18.0 23.6\n26.1 30.3 33.3 35.5 39.9\nDe→En 16\nBLEU-mb test\n40.2\n64\n2.06\n2.87 3.41\n3.63 21.5 17.3 23.0 27.2\n0.83\n16.2 22.5\n24.7 28.2 30.7 33.0 30.4\n3.25\n22.7 26.2\n29.2 32.7 34.8 37.3 40.6\nDe→En 16\nBLEU-sb\ntest\n64\n2.39\n3.27 3.85\n4.04 22.5 18.2 24.4 28.6\n0.93\n17.1 23.4\n25.8 29.2 31.9 34.5 32.1\n3.60\n23.8 27.5\n30.5 34.1 36.5 39.1 43.0\nEn→De 16\nBLEU-mb test\n41.2\n64\n1.70\n2.27 2.31\n2.43 12.9 8.66 10.4 24.6\n0.50\n7.00 12.9\n13.1 18.3 20.9 22.5 26.2\n3.42\n12.3 15.4\n17.1 20.9 23.0 26.6 29.7\nEn→De 16\nBLEU-sb\ntest\n41.2\n64\n2.09\n2.65 2.75\n2.92 13.7 9.36 11.0 25.3\n0.54\n7.40 13.4\n13.4 18.8 21.7 23.3 27.3\n3.78\n12.9 16.1\n17.7 21.7 24.1 27.7 30.9"}
{"doc_id": "2005.14165", "para_id": 491, "text": "Winograd\nacc\ntest\n93.8\n7\n66.3\n72.9 74.7\n76.9 82.4 85.7 87.9 88.3\n63.4\n68.5 72.9\n76.9 82.4 84.6 86.1 89.7\n63.4\n67.4 73.6\n76.9 84.3 85.4 82.4 88.6\nWinogrande\nacc\ndev\n84.6\n50\n52.0\n52.1 57.4\n58.7 62.3 64.5 67.9 70.2\n51.3\n53.0 58.3\n59.1 61.7 65.8 66.9 73.2\n51.3\n52.6 57.5\n59.1 62.6 67.4 70.0 77.7"}
{"doc_id": "2005.14165", "para_id": 492, "text": "PIQA\nacc\ndev\n77.1\n50\n64.6\n70.2 72.9\n75.1 75.6 78.0 78.5 81.0\n64.3\n69.3 71.8\n74.4 74.3 76.3 77.8 80.5\n64.3\n69.4 72.0\n74.3 75.4 77.8 79.9 82.3\n82.8\nARC (Challenge) acc\ntest\n78.5\n50\n26.6\n29.5 31.8\n35.5 38.0 41.4 43.7 51.4\n25.5\n30.2 31.6\n36.4 38.4 41.5 43.1 53.2\n25.5\n28.4 32.3\n36.7 39.5 43.7 44.8 51.5\nARC (Easy)\nacc\ntest\n92.0\n50\n43.6\n46.5 53.0\n53.8 58.2 60.2 63.8 68.8\n42.7\n48.2 54.6\n55.9 60.3 62.6 66.8 71.2\n42.7\n51.0 58.1\n59.1 62.1 65.8 69.1 70.1\nOpenBookQA\nacc\ntest\n87.2\n100\n35.6\n43.2 45.2\n46.8 53.0 50.4 55.6 57.6\n37.0\n39.8 46.2\n46.4 53.4 53.0 55.8 58.8\n37.0\n43.6 48.0\n50.6 55.6 55.2 60.8 65.4"}
{"doc_id": "2005.14165", "para_id": 493, "text": "Quac\nf1\ndev\n74.4\n5\n21.2\n26.8 31.0\n30.1 34.7 36.1 38.4 41.5\n21.1\n26.9 31.9\n32.3 37.4 39.0 40.6 43.4\n21.6\n27.6 32.9\n34.2 38.2 39.9 40.9 44.3\nRACE-h\nacc\ntest\n90.0\n10\n35.2\n37.9 40.1\n40.9 42.4 44.1 44.6 45.5\n34.3\n37.7 40.0\n42.0 43.8 44.3 44.6 45.9\n34.3\n37.0 40.4\n41.4 42.3 44.7 45.1 46.8\nRACE-m\nacc\ntest\n93.1\n10\n42.1\n47.2 52.1\n52.3 54.7 54.4 56.7 58.4\n42.3\n47.3 51.7\n55.2 56.1 54.7 56.9 57.4\n42.3\n47.0 52.7\n53.0 55.6 55.4 58.1 58.1\nSQuADv2\nem\ndev\n90.7\n16\n22.6\n32.8 33.9\n43.1 43.6 45.4 49.0 52.6\n25.1\n37.5 37.9\n47.9 47.9 51.1 56.0 60.1\n27.5\n40.5 39.2\n53.5 50.0 56.6 62.6 64.9\nSQuADv2\nf1\ndev\n93.0\n16\n28.3\n40.2 41.4\n50.3 51.0 52.7 56.3 59.5\n30.1\n43.6 44.1\n54.0 54.1 57.1 61.8 65.4\n32.1\n45.5 44.9\n58.7 55.9 62.1 67.7 69.8\nCoQA\nf1\ndev\n90.7\n5\n34.5\n55.0 61.8\n65.3 71.1 72.8 76.3 81.5\n30.6\n52.1 61.6\n66.1 71.8 75.1 77.9 84.0\n31.1\n52.0 62.7\n66.8 73.2 77.3 79.9 85.0\nDROP\nf1\ndev\n89.1\n20\n9.40\n13.6 14.4\n16.4 19.7 17.0 24.0 23.6\n11.7\n18.1 20.9\n23.0 26.4 27.3 29.2 34.3\n12.9\n18.7 24.0\n25.6 29.7 29.7 32.3 36.5"}
{"doc_id": "2005.14165", "para_id": 494, "text": "BoolQ\nacc\ndev\n91.0\n32\n49.7\n60.3 58.9\n62.4 67.1 65.4 66.2 60.5\n52.6\n61.7 60.4\n63.7 68.4 68.7 69.0 76.7\n43.1\n60.6 62.0\n64.1 70.3 70.0 70.2 77.5\n76.4\nCB\nacc\ndev\n96.9\n32\n0.00\n32.1 8.93\n19.6 19.6 28.6 19.6 46.4\n55.4\n53.6 53.6\n48.2 57.1 33.9 55.4 64.3\n42.9\n58.9 53.6\n69.6 67.9 60.7 66.1 82.1\n75.6\nCB\nf1\ndev\n93.9\n32\n0.00\n29.3 11.4\n17.4 22.4 25.1 20.3 42.8\n60.1\n39.8 45.6\n37.5 45.7 28.5 44.6 52.5\n26.1\n40.4 32.6\n48.3 45.7 44.6 46.0 57.2\n52.0\nCopa\nacc\ndev\n94.8\n32\n66.0\n68.0 73.0\n77.0 76.0 80.0 84.0 91.0\n62.0\n64.0 66.0\n74.0 76.0 82.0 86.0 87.0\n67.0\n64.0 72.0\n77.0 83.0 83.0 86.0 92.0\n92.0\nRTE\nacc\ndev\n92.5\n32\n47.7\n49.8 48.4\n56.0 46.6 55.2 62.8 63.5\n53.1\n47.3 49.5\n49.5 54.9 54.9 56.3 70.4\n52.3\n48.4 46.9\n50.9 56.3 49.5 60.6 72.9\n69.0\nWiC\nacc\ndev\n76.1\n32\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.00\n50.0\n50.3 50.3\n49.2 49.4 50.3 50.0 48.6\n49.8\n55.0 53.0\n53.0 51.6 53.1 51.1 55.3\n49.4\nWSC\nacc\ndev\n93.8\n32\n59.6\n56.7 65.4\n61.5 66.3 60.6 64.4 65.4\n58.7\n58.7 60.6\n62.5 66.3 60.6 66.3 69.2\n58.7\n60.6 54.8\n49.0 62.5 67.3 75.0 75.0\n80.1\nMultiRC\nacc\ndev\n62.3\n32\n4.72\n9.65 12.3\n13.6 14.3 18.4 24.2 27.6\n4.72\n9.65 12.3\n13.6 14.3 18.4 24.2 27.6\n6.09\n11.8 16.8\n20.8 24.7 23.8 25.0 32.5\n30.5\nMultiRC\nf1a\ndev\n88.2\n32\n57.0\n59.7 60.4\n59.9 60.0 64.5 71.4 72.9\n57.0\n59.7 60.4\n59.9 60.0 64.5 71.4 72.9\n45.0\n55.9 64.2\n65.4 69.5 66.4 69.3 74.8\n75.4\nReCoRD\nacc\ndev\n92.5\n32\n70.8\n78.5 82.1\n84.1 86.2 88.6 89.0 90.2\n69.8\n77.0 80.7\n83.0 85.9 88.0 88.8 90.2\n69.8\n77.2 81.3\n83.1 86.6 87.9 88.9 89.0\n90.2\nReCoRD\nf1\ndev\n93.3\n32\n71.9\n79.2 82.8\n85.2 87.3 89.5 90.4 91.0\n70.7\n77.8 81.6\n83.9 86.8 88.8 89.7 91.2\n70.7\n77.9 82.1\n84.0 87.5 88.8 89.8 90.1\n91.1\nSuperGLUE\naverage\ndev\n89.0\n40.6\n47.4 46.8\n49.6 50.1 52.3 54.4 58.2\n54.4\n55.1 56.7\n57.8 61.2 59.7 64.3 68.9\n50.2\n56.2 56.8\n60.0 64.3 63.6 66.9 73.2\n71.8"}
{"doc_id": "2005.14165", "para_id": 495, "text": "ANLI R1\nacc\ntest\n73.8\n50\n33.4\n34.2 33.4\n33.4 34.2 32.3 33.2 34.6\n32.1\n31.6 31.9\n34.6 30.6 31.6 32.7 32.0\n32.1\n32.5 30.9\n32.5 33.5 33.1 33.3 36.8\nANLI R2\nacc\ntest\n50.7\n50\n33.2\n31.9 33.3\n33.3 33.8 33.5 33.5 35.4\n35.7\n33.7 33.2\n32.7 32.7 33.9 33.9 33.9\n35.7\n33.8 32.1\n31.4 32.6 33.3 32.6 34.0\nANLI R3\nacc\ntest\n48.3\n50\n33.6\n34.0 33.8\n33.4 35.3 34.8 34.4 34.5\n35.0\n32.6 33.0\n33.9 34.1 33.1 32.5 35.1\n35.0\n34.4 35.1\n36.0 32.7 33.9 34.5 40.2"}
{"doc_id": "2005.14165", "para_id": 496, "text": "2D+\nacc\nn/a\n50\n0.70\n0.65 0.70\n0.85 1.10 2.54 15.4 76.9\n2.00\n0.55 3.15\n4.00 12.1 19.6 73.0 99.6\n2.00\n4.10 3.50\n4.50 8.90 11.9 55.5 100.0\n2D-\nacc\nn/a\n50\n1.25\n1.25 1.25\n1.25 1.60 7.60 12.6 58.0\n1.15\n0.95 1.45\n1.95 3.85 11.5 44.6 86.4\n1.15\n1.45 2.25\n2.70 7.35 13.6 52.4 98.9\n3D+\nacc\nn/a\n50\n0.10\n0.10 0.05\n0.10 0.10 0.25 1.40 34.2\n0.15\n0.00 0.10\n0.30 0.45 0.95 15.4 65.5\n0.15\n0.45 0.30\n0.55 0.75 0.90 8.40 80.4\n3D-\nacc\nn/a\n50\n0.05\n0.05 0.05\n0.05 0.05 0.45 1.35 48.3\n0.05\n0.15 0.25\n0.30 0.55 1.60 6.15 78.7\n0.05\n0.10 0.15\n0.35 0.65 1.05 9.20 94.2\n4D+\nacc\nn/a\n50\n0.05\n0.05 0.00\n0.00 0.05 0.05 0.15 4.00\n0.00\n0.00 0.10\n0.00 0.00 0.10 0.80 14.0\n0.00\n0.05 0.05\n0.00 0.15 0.15 0.40 25.5\n4D-\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.10 7.50\n0.00\n0.00 0.00\n0.00 0.05 0.00 0.50 14.0\n0.00\n0.05 0.00\n0.00 0.10 0.05 0.40 26.8\n5D+\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.65\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 3.45\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 9.30\n5D-\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.80\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 3.75\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 9.90\n2Dx\nacc\nn/a\n50\n2.20\n2.25 2.65\n2.10 2.55 5.80 6.15 19.8\n1.35\n2.35 3.35\n2.35 4.75 9.15 11.0 27.4\n1.35\n2.90 2.70\n2.85 4.25 6.10 7.05 29.2\n1DC\nacc\nn/a\n50\n1.25\n2.95 2.75\n0.05 0.30 2.35 0.75 9.75\n1.90\n2.80 2.85\n3.65 6.45 9.15 8.20 14.3\n1.70\n2.15 3.90\n5.75 6.20 7.60 9.95 21.3"}
{"doc_id": "2005.14165", "para_id": 497, "text": "Cycled Letters\nacc\nn/a\n100\n0.62\n0.71 2.85\n0.00 0.63 1.35 2.58 3.66\n1.67\n4.36 5.68\n6.46 6.25 9.41 15.1 21.7\n4.63\n9.27 10.7\n14.5 16.7 21.9 27.7 37.9\nAnagrams 1\nacc\nn/a\n100\n0.10\n0.14 0.40\n0.00 0.27 0.69 1.16 2.28\n0.21\n0.61 1.12\n1.27 1.60 2.72 3.72 8.62\n0.50\n1.27 2.13\n3.05 3.81 5.49 8.38 15.1\nAnagrams 2\nacc\nn/a\n100\n0.81\n1.21 2.69\n0.01 1.71 3.75 4.53 8.91\n1.19\n2.62 4.70\n4.77 6.97 10.2 14.6 25.9\n1.94\n4.80 7.59\n9.87 12.6 18.9 25.6 39.7\nSymbol Insertion acc\nn/a\n100\n0.00\n0.00 0.10\n0.00 0.05 0.42 0.89 8.26\n0.03\n0.05 0.57\n1.18 1.67 3.46 6.62 45.4\n0.11\n0.28 2.19\n4.18 6.61 11.0 27.3 67.2\nReversed Words\nacc\nn/a\n100\n0.00\n0.01 0.01\n0.01 0.02 0.03 0.03 0.09\n0.02\n0.01 0.01\n0.00 0.05 0.07 0.11 0.48\n0.00\n0.05 0.00\n0.17 0.24 0.30 0.42 0.44"}
{"doc_id": "2005.14165", "para_id": 498, "text": "SAT Analogies\nacc\nn/a\n20\n35.6\n39.0 45.2\n44.1 50.0 49.2 52.7 53.7\n30.5\n41.2 43.1\n46.5 55.1 54.3 53.5 59.1\n30.5\n40.4 42.8\n40.6 48.4 51.9 53.5 65.2"}
{"doc_id": "2005.14165", "para_id": 499, "text": "Table H.1: Scores for every task, setting and model that we investigate in this paper."}
{"doc_id": "2005.14165", "para_id": 500, "text": "Figure H.2: Results for SAT task.\nFigure H.3: All results for all Winograd tasks."}
{"doc_id": "2005.14165", "para_id": 501, "text": "Figure H.5: All results for all Cloze and Completion tasks."}
{"doc_id": "2005.14165", "para_id": 502, "text": "Figure H.6: All results for all Common Sense Reasoning tasks."}
{"doc_id": "2005.14165", "para_id": 503, "text": "Figure H.8: All results for all Reading Comprehension tasks."}
{"doc_id": "2005.14165", "para_id": 504, "text": "Figure H.11: All results for all Translation tasks."}
{"doc_id": "2005.14165", "para_id": 505, "text": "[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent.\nIn Advances in neural information processing systems, pages 3981–3989, 2016."}
{"doc_id": "2005.14165", "para_id": 506, "text": "[AI19] WeChat AI. Tr-mt (ensemble), December 2019."}
{"doc_id": "2005.14165", "para_id": 507, "text": "[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019."}
{"doc_id": "2005.14165", "para_id": 508, "text": "[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. Language (technology) is power:\nA critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050, 2020."}
{"doc_id": "2005.14165", "para_id": 509, "text": "[BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language\nprocessing, pages 1533–1544, 2013."}
{"doc_id": "2005.14165", "para_id": 510, "text": "[BDD+09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth\nPASCAL recognizing textual entailment challenge. 2009."}
{"doc_id": "2005.14165", "para_id": 511, "text": "[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical\nresource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200–2204, 2010."}
{"doc_id": "2005.14165", "para_id": 512, "text": "[BHDD+06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The second PASCAL recognising textual entailment challenge. 2006."}
{"doc_id": "2005.14165", "para_id": 513, "text": "[BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella\nLapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language.\narXiv preprint arXiv:2004.10151, 2020."}
{"doc_id": "2005.14165", "para_id": 514, "text": "[BLC13] Yoshua Bengio, Nicholas L´eonard, and Aaron C. Courville. Estimating or propagating gradients through\nstochastic neurons for conditional computation. Arxiv, 2013."}
{"doc_id": "2005.14165", "para_id": 515, "text": "[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019."}
{"doc_id": "2005.14165", "para_id": 516, "text": "[Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "2005.14165", "para_id": 517, "text": "[CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language\nConference, 1978."}
{"doc_id": "2005.14165", "para_id": 518, "text": "[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457, 2018."}
{"doc_id": "2005.14165", "para_id": 519, "text": "[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers, 2019."}
{"doc_id": "2005.14165", "para_id": 520, "text": "[CHI+18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. Quac : Question answering in context. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 521, "text": "[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019."}
{"doc_id": "2005.14165", "para_id": 522, "text": "[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740,\n2019."}
{"doc_id": "2005.14165", "para_id": 523, "text": "[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017."}
{"doc_id": "2005.14165", "para_id": 524, "text": "[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."}
{"doc_id": "2005.14165", "para_id": 525, "text": "[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiﬁcation,\nand recognising textual entailment, pages 177–190. Springer, 2006."}
{"doc_id": "2005.14165", "para_id": 526, "text": "[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 527, "text": "[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine\ntranslation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation,\npages 97–104, 2014."}
{"doc_id": "2005.14165", "para_id": 528, "text": "[DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems, 2015."}
{"doc_id": "2005.14165", "para_id": 529, "text": "[DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat-\ning projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung\n23. Data can be found at https://github.com/mcdm/CommitmentBank/."}
{"doc_id": "2005.14165", "para_id": 530, "text": "[DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016."}
{"doc_id": "2005.14165", "para_id": 531, "text": "[DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDrop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161, 2019."}
{"doc_id": "2005.14165", "para_id": 532, "text": "[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. Arxiv, 2019."}
{"doc_id": "2005.14165", "para_id": 533, "text": "[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale.\narXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "2005.14165", "para_id": 534, "text": "[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. ArXiv, abs/1703.03400, 2017."}
{"doc_id": "2005.14165", "para_id": 535, "text": "[Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000."}
{"doc_id": "2005.14165", "para_id": 536, "text": "[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases\nin word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019."}
{"doc_id": "2005.14165", "para_id": 537, "text": "[GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020."}
{"doc_id": "2005.14165", "para_id": 538, "text": "[GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computational Linguistics, 2007."}
{"doc_id": "2005.14165", "para_id": 539, "text": "[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 540, "text": "[GSL+18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A\nSmith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018."}
{"doc_id": "2005.14165", "para_id": 541, "text": "[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza-\ntion of generated text. arXiv preprint arXiv: 1906.04043, 2019."}
{"doc_id": "2005.14165", "para_id": 542, "text": "[GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource\nneural machine translation. arXiv preprint arXiv:1808.08437, 2018."}
{"doc_id": "2005.14165", "para_id": 543, "text": "[HB20] Daniel Hernandez and Tom Brown. Ai and efﬁciency, May 2020."}
{"doc_id": "2005.14165", "para_id": 544, "text": "[HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\nCoRR, abs/1904.09751, 2019."}
{"doc_id": "2005.14165", "para_id": 545, "text": "[HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020."}
{"doc_id": "2005.14165", "para_id": 546, "text": "[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md.\nMostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.\narXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "2005.14165", "para_id": 547, "text": "[HR18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv\npreprint arXiv:1801.06146, 2018."}
{"doc_id": "2005.14165", "para_id": 548, "text": "[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015."}
{"doc_id": "2005.14165", "para_id": 549, "text": "[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent.\nIn International Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001."}
{"doc_id": "2005.14165", "para_id": 550, "text": "[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini,\nDani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual\nevaluation. arXiv preprint arXiv:1911.03064, 2019."}
{"doc_id": "2005.14165", "para_id": 551, "text": "[IBGC+14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. A neural\nnetwork for factoid question answering over paragraphs. In Empirical Methods in Natural Language\nProcessing, 2014."}
{"doc_id": "2005.14165", "para_id": 552, "text": "[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of\ngenerated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019."}
{"doc_id": "2005.14165", "para_id": 553, "text": "[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017."}
{"doc_id": "2005.14165", "para_id": 554, "text": "[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020."}
{"doc_id": "2005.14165", "para_id": 555, "text": "[JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits\nof language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "2005.14165", "para_id": 556, "text": "[JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019."}
{"doc_id": "2005.14165", "para_id": 557, "text": "[JZC+19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on\nconversational question answering. arXiv preprint arXiv:1909.10772, 2019."}
{"doc_id": "2005.14165", "para_id": 558, "text": "[KCR+18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North\nAmerican Chapter of the Association for Computational Linguistics (NAACL), 2018."}
{"doc_id": "2005.14165", "para_id": 559, "text": "[KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi.\nUniﬁedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020."}
{"doc_id": "2005.14165", "para_id": 560, "text": "[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s ﬁt to fabricate: Ai-generated\ntext as a tool of media misinformation, 2020."}
{"doc_id": "2005.14165", "para_id": 561, "text": "[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020."}
{"doc_id": "2005.14165", "para_id": 562, "text": "[KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-\ntions: a benchmark for question answering research. Transactions of the Association of Computational\nLinguistics, 2019."}
{"doc_id": "2005.14165", "para_id": 563, "text": "[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 564, "text": "[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002."}
{"doc_id": "2005.14165", "para_id": 565, "text": "[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019."}
{"doc_id": "2005.14165", "para_id": 566, "text": "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019."}
{"doc_id": "2005.14165", "para_id": 567, "text": "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\nAdversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020."}
{"doc_id": "2005.14165", "para_id": 568, "text": "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint\narXiv:1905.07504, 2019."}
{"doc_id": "2005.14165", "para_id": 569, "text": "[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning, 2012."}
{"doc_id": "2005.14165", "para_id": 570, "text": "[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\narXiv:2001.08210, 2020."}
{"doc_id": "2005.14165", "para_id": 571, "text": "[LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation\nlearning using multi-task deep neural networks for semantic classiﬁcation and information retrieval. In\nProceedings of the 2015 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2015."}
{"doc_id": "2005.14165", "para_id": 572, "text": "[LH17] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017."}
{"doc_id": "2005.14165", "para_id": 573, "text": "[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\nnetworks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482,\n2019."}
{"doc_id": "2005.14165", "para_id": 574, "text": "[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\nnatural language understanding. arXiv preprint arXiv:1901.11504, 2019."}
{"doc_id": "2005.14165", "para_id": 575, "text": "[Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint\narXiv:2005.00955, 2020."}
{"doc_id": "2005.14165", "para_id": 576, "text": "[LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019."}
{"doc_id": "2005.14165", "para_id": 577, "text": "[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017."}
{"doc_id": "2005.14165", "para_id": 578, "text": "[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "2005.14165", "para_id": 579, "text": "[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Kiela Douwe.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401,\n2020."}
{"doc_id": "2005.14165", "para_id": 580, "text": "[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018."}
{"doc_id": "2005.14165", "para_id": 581, "text": "[LWS+20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\nTrain large, then compress: Rethinking model size for efﬁcient training and inference of transformers,\n2020."}
{"doc_id": "2005.14165", "para_id": 582, "text": "[LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017."}
{"doc_id": "2005.14165", "para_id": 583, "text": "[LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy\nLin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020."}
{"doc_id": "2005.14165", "para_id": 584, "text": "[Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation,\n1992."}
{"doc_id": "2005.14165", "para_id": 585, "text": "[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305,\n2017."}
{"doc_id": "2005.14165", "para_id": 586, "text": "[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781, 2013."}
{"doc_id": "2005.14165", "para_id": 587, "text": "[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of\ncommonsense stories. arXiv preprint arXiv:1604.01696, 2016."}
{"doc_id": "2005.14165", "para_id": 588, "text": "[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?\na new dataset for open book question answering. ArXiv, abs/1809.02789, 2018."}
{"doc_id": "2005.14165", "para_id": 589, "text": "[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of\nlarge-batch training, 2018."}
{"doc_id": "2005.14165", "para_id": 590, "text": "[MKM+94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,\nKaren Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure.\nIn Proceedings of the workshop on Human Language Technology, pages 114–119. Association for\nComputational Linguistics, 1994."}
{"doc_id": "2005.14165", "para_id": 591, "text": "[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2005.14165", "para_id": 592, "text": "[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019."}
{"doc_id": "2005.14165", "para_id": 593, "text": "[MWZ+18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018."}
{"doc_id": "2005.14165", "para_id": 594, "text": "[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456, 2020."}
{"doc_id": "2005.14165", "para_id": 595, "text": "[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments.\narXiv preprint arXiv:1907.07355, 2019."}
{"doc_id": "2005.14165", "para_id": 596, "text": "[Nor09] Peter Norvig. Natural language corpus data, 2009."}
{"doc_id": "2005.14165", "para_id": 597, "text": "[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor\nas woman is to doctor. arXiv preprint arXiv:1905.09866, 2019."}
{"doc_id": "2005.14165", "para_id": 598, "text": "[NWD+19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nnli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019."}
{"doc_id": "2005.14165", "para_id": 599, "text": "[PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating\ncontext-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "2005.14165", "para_id": 600, "text": "[PFB18] Jason Phang, Thibault F´evry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary\ntraining on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018."}
{"doc_id": "2005.14165", "para_id": 601, "text": "[PHR+18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. Collecting diverse natural language inference problems for sentence representation\nevaluation. In Proceedings of EMNLP, 2018."}
{"doc_id": "2005.14165", "para_id": 602, "text": "[PKL+16] Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The lambada dataset: Word prediction\nrequiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016."}
{"doc_id": "2005.14165", "para_id": 603, "text": "[PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word\nembeddings: Architecture and representation, 2018."}
{"doc_id": "2005.14165", "para_id": 604, "text": "[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018."}
{"doc_id": "2005.14165", "para_id": 605, "text": "[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning.\nGloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), 2014."}
{"doc_id": "2005.14165", "para_id": 606, "text": "[QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020."}
{"doc_id": "2005.14165", "para_id": 607, "text": "[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019."}
{"doc_id": "2005.14165", "para_id": 608, "text": "[RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011."}
{"doc_id": "2005.14165", "para_id": 609, "text": "[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019."}
{"doc_id": "2005.14165", "para_id": 610, "text": "[RCP+17] Scott Reed, Yutian Chen, Thomas Paine, A¨aron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn\ndistributions. arXiv preprint arXiv:1710.10304, 2017."}
{"doc_id": "2005.14165", "para_id": 611, "text": "[RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for\nsquad. arXiv preprint arXiv:1806.03822, 2018."}
{"doc_id": "2005.14165", "para_id": 612, "text": "[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral),\n2016."}
{"doc_id": "2005.14165", "para_id": 613, "text": "[RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension\nwith numerical reasoning. In Proceedings of EMNLP, 2019."}
{"doc_id": "2005.14165", "para_id": 614, "text": "[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme.\nGender bias in\ncoreference resolution. arXiv preprint arXiv:1804.09301, 2018."}
{"doc_id": "2005.14165", "para_id": 615, "text": "[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training, 2018."}
{"doc_id": "2005.14165", "para_id": 616, "text": "[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012."}
{"doc_id": "2005.14165", "para_id": 617, "text": "[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of\nthe generalization error across scales, 2019."}
{"doc_id": "2005.14165", "para_id": 618, "text": "[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters\nof a language model? arXiv preprint arXiv:2002.08910, 2020."}
{"doc_id": "2005.14165", "para_id": 619, "text": "[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer, 2019."}
{"doc_id": "2005.14165", "para_id": 620, "text": "[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019."}
{"doc_id": "2005.14165", "para_id": 621, "text": "[SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale, 2019."}
{"doc_id": "2005.14165", "para_id": 622, "text": "[SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris\nMcGufﬁe, and Jasmine Wang. Release strategies and the social impacts of language models, 2019."}
{"doc_id": "2005.14165", "para_id": 623, "text": "[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019."}
{"doc_id": "2005.14165", "para_id": 624, "text": "[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."}
{"doc_id": "2005.14165", "para_id": 625, "text": "[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019."}
{"doc_id": "2005.14165", "para_id": 626, "text": "[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709, 2015."}
{"doc_id": "2005.14165", "para_id": 627, "text": "[SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017."}
{"doc_id": "2005.14165", "para_id": 628, "text": "[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2019."}
{"doc_id": "2005.14165", "para_id": 629, "text": "[SS20] Timo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation and natural\nlanguage inference. arXiv preprint arXiv:2001.07676, 2020."}
{"doc_id": "2005.14165", "para_id": 630, "text": "[STQ+19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence\npre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "2005.14165", "para_id": 631, "text": "[TFR+17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain\nrandomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017."}
{"doc_id": "2005.14165", "para_id": 632, "text": "[TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations.\nCoRR, abs/cs/0508103, 2005."}
{"doc_id": "2005.14165", "para_id": 633, "text": "[TL18] Trieu H. Trinh and Quoc V. Le.\nA simple method for commonsense reasoning.\narXiv preprint\narXiv:1806.02847, 2018."}
{"doc_id": "2005.14165", "para_id": 634, "text": "[TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent\nmodules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003."}
{"doc_id": "2005.14165", "para_id": 635, "text": "[Tur20] Project Turing. Microsoft research blog, Feb 2020."}
{"doc_id": "2005.14165", "para_id": 636, "text": "[VBL+16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One\nShot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016."}
{"doc_id": "2005.14165", "para_id": 637, "text": "[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, 2017."}
{"doc_id": "2005.14165", "para_id": 638, "text": "[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand-\ning systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019."}
{"doc_id": "2005.14165", "para_id": 639, "text": "[WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent\ndual learning. ICLR 2019, 2018."}
{"doc_id": "2005.14165", "para_id": 640, "text": "[XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le.\nUnsupervised data\naugmentation for consistency training, 2019."}
{"doc_id": "2005.14165", "para_id": 641, "text": "[YdC+19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,\nLingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating\ngeneral linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019."}
{"doc_id": "2005.14165", "para_id": 642, "text": "[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237,\n2019."}
{"doc_id": "2005.14165", "para_id": 643, "text": "[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019."}
{"doc_id": "2005.14165", "para_id": 644, "text": "[ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin\nChoi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019."}
{"doc_id": "2005.14165", "para_id": 645, "text": "[ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885, 2018."}
{"doc_id": "2005.14165", "para_id": 646, "text": "[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019."}
{"doc_id": "2005.14165", "para_id": 647, "text": "[ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593,\n2019."}
{"doc_id": "2010.11929", "para_id": 0, "text": "AN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"}
{"doc_id": "2010.11929", "para_id": 1, "text": "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†"}
{"doc_id": "2010.11929", "para_id": 2, "text": "∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com"}
{"doc_id": "2010.11929", "para_id": 3, "text": "While the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1"}
{"doc_id": "2010.11929", "para_id": 4, "text": "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance."}
{"doc_id": "2010.11929", "para_id": 5, "text": "In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020)."}
{"doc_id": "2010.11929", "para_id": 6, "text": "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classiﬁcation in supervised fashion."}
{"doc_id": "2010.11929", "para_id": 7, "text": "When trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases"}
{"doc_id": "2010.11929", "para_id": 8, "text": "1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer"}
{"doc_id": "2010.11929", "para_id": 9, "text": "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data."}
{"doc_id": "2010.11929", "para_id": 10, "text": "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks."}
{"doc_id": "2010.11929", "para_id": 11, "text": "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020)."}
{"doc_id": "2010.11929", "para_id": 12, "text": "Naive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefﬁciently on hardware accelerators."}
{"doc_id": "2010.11929", "para_id": 13, "text": "Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well."}
{"doc_id": "2010.11929", "para_id": 14, "text": "There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019)."}
{"doc_id": "2010.11929", "para_id": 15, "text": "Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet."}
{"doc_id": "2010.11929", "para_id": 16, "text": "Our work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works."}
{"doc_id": "2010.11929", "para_id": 17, "text": "Linear Projection of Flattened Patches\n* Extra learnable\n[ cl ass]  embedding"}
{"doc_id": "2010.11929", "para_id": 18, "text": "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017)."}
{"doc_id": "2010.11929", "para_id": 19, "text": "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\ntheir efﬁcient implementations – can be used almost out of the box."}
{"doc_id": "2010.11929", "para_id": 20, "text": "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings."}
{"doc_id": "2010.11929", "para_id": 21, "text": "Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time."}
{"doc_id": "2010.11929", "para_id": 22, "text": "Position embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder."}
{"doc_id": "2010.11929", "para_id": 23, "text": "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019)."}
{"doc_id": "2010.11929", "para_id": 24, "text": "The MLP contains two layers with a GELU non-linearity."}
{"doc_id": "2010.11929", "para_id": 25, "text": "z0 = [xclass; x1\npE; x2\npE; · · · ; xN\np E] + Epos,\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\n(1)"}
{"doc_id": "2010.11929", "para_id": 26, "text": "Inductive bias.\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch."}
{"doc_id": "2010.11929", "para_id": 27, "text": "Hybrid Architecture.\nAs an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classiﬁcation input embedding and position embeddings are added as described above."}
{"doc_id": "2010.11929", "para_id": 28, "text": "Typically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 29, "text": "We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future."}
{"doc_id": "2010.11929", "para_id": 30, "text": "Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 31, "text": "ViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M"}
{"doc_id": "2010.11929", "para_id": 32, "text": "Table 1: Details of Vision Transformer model variants."}
{"doc_id": "2010.11929", "para_id": 33, "text": "We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\nimagery, and Structured – tasks that require geometric understanding like localization."}
{"doc_id": "2010.11929", "para_id": 34, "text": "Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive."}
{"doc_id": "2010.11929", "para_id": 35, "text": "For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model."}
{"doc_id": "2010.11929", "para_id": 36, "text": "Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b)."}
{"doc_id": "2010.11929", "para_id": 37, "text": "Metrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\nwhere ﬁne-tuning would be too costly."}
{"doc_id": "2010.11929", "para_id": 38, "text": "We ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days."}
{"doc_id": "2010.11929", "para_id": 39, "text": "Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this"}
{"doc_id": "2010.11929", "para_id": 40, "text": "Ours-JFT\nOurs-JFT\nOurs-I21k\nBiT-L\nNoisy Student\n(ViT-H/14)\n(ViT-L/16)\n(ViT-L/16)\n(ResNet152x4)\n(EfﬁcientNet-L2)"}
{"doc_id": "2010.11929", "para_id": 41, "text": "ImageNet\n88.55 ± 0.04\n87.76 ± 0.03\n85.30 ± 0.02\n87.54 ± 0.02\n88.4/88.5∗\nImageNet ReaL\n90.72 ± 0.05\n90.54 ± 0.03\n88.62 ± 0.05\n90.54\n90.55\nCIFAR-10\n99.50 ± 0.06\n99.42 ± 0.03\n99.15 ± 0.03\n99.37 ± 0.06\n−\nCIFAR-100\n94.55 ± 0.04\n93.90 ± 0.05\n93.25 ± 0.05\n93.51 ± 0.08\n−\nOxford-IIIT Pets\n97.56 ± 0.03\n97.32 ± 0.11\n94.67 ± 0.15\n96.62 ± 0.23\n−\nOxford Flowers-102\n99.68 ± 0.02\n99.74 ± 0.00\n99.61 ± 0.02\n99.63 ± 0.03\n−\nVTAB (19 tasks)\n77.63 ± 0.23\n76.28 ± 0.46\n72.72 ± 0.21\n76.29 ± 1.70\n−"}
{"doc_id": "2010.11929", "para_id": 42, "text": "Table 2:\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\nin Touvron et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 43, "text": "70\nViT-H/14\nBiT-L (R152x4)\nVIVI-Ex-100% (R50x3)\nS4L (R50x1)"}
{"doc_id": "2010.11929", "para_id": 44, "text": "Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups."}
{"doc_id": "2010.11929", "para_id": 45, "text": "model still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\nproximately 30 days."}
{"doc_id": "2010.11929", "para_id": 46, "text": "Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\nSpecialized the performance of the top two models is similar."}
{"doc_id": "2010.11929", "para_id": 47, "text": "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments."}
{"doc_id": "2010.11929", "para_id": 48, "text": "First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance"}
{"doc_id": "2010.11929", "para_id": 49, "text": "2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\nresolution increase during ﬁne-tuning improves the performance."}
{"doc_id": "2010.11929", "para_id": 50, "text": "ImageNet\nImageNet-21k\nJFT-300M\nPre-training dataset"}
{"doc_id": "2010.11929", "para_id": 51, "text": "10 M\n30 M\n100 M\n300 M\nNumber of JFT pre-training samples"}
{"doc_id": "2010.11929", "para_id": 52, "text": "Figure 3:\nTransfer to ImageNet.\nWhile\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows."}
{"doc_id": "2010.11929", "para_id": 53, "text": "Figure 4: Linear few-shot evaluation on Ima-\ngeNet versus pre-training size. ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved."}
{"doc_id": "2010.11929", "para_id": 54, "text": "Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models."}
{"doc_id": "2010.11929", "para_id": 55, "text": "region spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes."}
{"doc_id": "2010.11929", "para_id": 56, "text": "Second, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is sufﬁcient, even beneﬁcial."}
{"doc_id": "2010.11929", "para_id": 57, "text": "Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work."}
{"doc_id": "2010.11929", "para_id": 58, "text": "We perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone)."}
{"doc_id": "2010.11929", "para_id": 59, "text": "Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts."}
{"doc_id": "2010.11929", "para_id": 60, "text": "To begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The ﬁrst layer of\nthe Vision Transformer linearly projects the ﬂattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding ﬁlters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the ﬁne structure within each patch."}
{"doc_id": "2010.11929", "para_id": 61, "text": "After the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4)."}
{"doc_id": "2010.11929", "para_id": 62, "text": "Self-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speciﬁcally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6)."}
{"doc_id": "2010.11929", "para_id": 63, "text": "Figure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails."}
{"doc_id": "2010.11929", "para_id": 64, "text": "Transformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin"}
{"doc_id": "2010.11929", "para_id": 65, "text": "RGB embedding filters\n(first 28 principal components)"}
{"doc_id": "2010.11929", "para_id": 66, "text": "Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details."}
{"doc_id": "2010.11929", "para_id": 67, "text": "et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work."}
{"doc_id": "2010.11929", "para_id": 68, "text": "We have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\ndatasets, whilst being relatively cheap to pre-train."}
{"doc_id": "2010.11929", "para_id": 69, "text": "While these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\ntraining. Finally, further scaling of ViT would likely lead to improved performance."}
{"doc_id": "2010.11929", "para_id": 70, "text": "The work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions."}
{"doc_id": "2010.11929", "para_id": 71, "text": "Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020."}
{"doc_id": "2010.11929", "para_id": 72, "text": "Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 73, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 74, "text": "I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nIn ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 75, "text": "Lucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\nwe done with imagenet? arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 76, "text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 77, "text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 78, "text": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a."}
{"doc_id": "2010.11929", "para_id": 79, "text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020b."}
{"doc_id": "2010.11929", "para_id": 80, "text": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c."}
{"doc_id": "2010.11929", "para_id": 81, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 82, "text": "Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR, 2020."}
{"doc_id": "2010.11929", "para_id": 83, "text": "J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009."}
{"doc_id": "2010.11929", "para_id": 84, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019."}
{"doc_id": "2010.11929", "para_id": 85, "text": "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\nlutional neural networks. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 86, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016."}
{"doc_id": "2010.11929", "para_id": 87, "text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 88, "text": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 89, "text": "Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 90, "text": "Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 91, "text": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020."}
{"doc_id": "2010.11929", "para_id": 92, "text": "Olivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\nICML, 2020."}
{"doc_id": "2010.11929", "para_id": 93, "text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015."}
{"doc_id": "2010.11929", "para_id": 94, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "2010.11929", "para_id": 95, "text": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 96, "text": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."}
{"doc_id": "2010.11929", "para_id": 97, "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In NIPS, 2012."}
{"doc_id": "2010.11929", "para_id": 98, "text": "Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989."}
{"doc_id": "2010.11929", "para_id": 99, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 100, "text": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019."}
{"doc_id": "2010.11929", "para_id": 101, "text": "Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 102, "text": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 103, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 104, "text": "M. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nICVGIP, 2008."}
{"doc_id": "2010.11929", "para_id": 105, "text": "Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\n2012."}
{"doc_id": "2010.11929", "para_id": 106, "text": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018."}
{"doc_id": "2010.11929", "para_id": 107, "text": "B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838–855, 1992.\ndoi: 10.1137/0330046.\nURL\nhttps://doi.org/10.1137/0330046."}
{"doc_id": "2010.11929", "para_id": 108, "text": "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520, 2019."}
{"doc_id": "2010.11929", "para_id": 109, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018."}
{"doc_id": "2010.11929", "para_id": 110, "text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019."}
{"doc_id": "2010.11929", "para_id": 111, "text": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 112, "text": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, 2017."}
{"doc_id": "2010.11929", "para_id": 113, "text": "Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 114, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 115, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020."}
{"doc_id": "2010.11929", "para_id": 116, "text": "Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020."}
{"doc_id": "2010.11929", "para_id": 117, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "2010.11929", "para_id": 118, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a."}
{"doc_id": "2010.11929", "para_id": 119, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\narXiv preprint\narXiv:2003.07853, 2020b."}
{"doc_id": "2010.11929", "para_id": 120, "text": "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019."}
{"doc_id": "2010.11929", "para_id": 121, "text": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 122, "text": "Dirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 123, "text": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020."}
{"doc_id": "2010.11929", "para_id": 124, "text": "Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 125, "text": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\nimproves imagenet classiﬁcation. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 126, "text": "Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\nSupervised Learning. In ICCV, 2019a."}
{"doc_id": "2010.11929", "para_id": 127, "text": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b."}
{"doc_id": "2010.11929", "para_id": 128, "text": "Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 129, "text": "Models\nDataset\nEpochs\nBase LR\nLR decay\nWeight decay\nDropout"}
{"doc_id": "2010.11929", "para_id": 130, "text": "ViT-B/{16,32}\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nViT-L/32\nJFT-300M\n7\n6 · 10−4\nlinear\n0.1\n0.0\nViT-L/16\nJFT-300M\n7/14\n4 · 10−4\nlinear\n0.1\n0.0\nViT-H/14\nJFT-300M\n14\n3 · 10−4\nlinear\n0.1\n0.0\nR50x{1,2}\nJFT-300M\n7\n10−3\nlinear\n0.1\n0.0\nR101x1\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nR152x{1,2}\nJFT-300M\n7\n6 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-B/{16,32}\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-L/32\nJFT-300M\n7\n2 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-L/16\nJFT-300M\n7/14\n4 · 10−4\nlinear\n0.1\n0.0\nViT-B/{16,32}\nImageNet-21k\n90\n10−3\nlinear\n0.03\n0.1\nViT-L/{16,32}\nImageNet-21k\n30/90\n10−3\nlinear\n0.03\n0.1\nViT-∗\nImageNet\n300\n3 · 10−3\ncosine\n0.3\n0.1"}
{"doc_id": "2010.11929", "para_id": 131, "text": "Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224."}
{"doc_id": "2010.11929", "para_id": 132, "text": "Standard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\ntwo elements of the sequence and their respective query qi and key kj representations."}
{"doc_id": "2010.11929", "para_id": 133, "text": "Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k."}
{"doc_id": "2010.11929", "para_id": 134, "text": "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\nUmsa ∈Rk·Dh×D\n(8)"}
{"doc_id": "2010.11929", "para_id": 135, "text": "Table 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224."}
{"doc_id": "2010.11929", "para_id": 136, "text": "We ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,"}
{"doc_id": "2010.11929", "para_id": 137, "text": "ImageNet\n20 000\n{0.003, 0.01, 0.03, 0.06}\nCIFAR100\n10 000\n{0.001, 0.003, 0.01, 0.03}\nCIFAR10\n10 000\n{0.001, 0.003, 0.01, 0.03}\nOxford-IIIT Pets\n500\n{0.001, 0.003, 0.01, 0.03}\nOxford Flowers-102\n500\n{0.001, 0.003, 0.01, 0.03}\nVTAB (19 tasks)\n2 500\n0.01"}
{"doc_id": "2010.11929", "para_id": 138, "text": "Table 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\nﬁne-tuning resolution is 384."}
{"doc_id": "2010.11929", "para_id": 139, "text": "for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020))."}
{"doc_id": "2010.11929", "para_id": 140, "text": "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer."}
{"doc_id": "2010.11929", "para_id": 141, "text": "For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks."}
{"doc_id": "2010.11929", "para_id": 142, "text": "We employ the masked patch prediction objective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations."}
{"doc_id": "2010.11929", "para_id": 143, "text": "We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics."}
{"doc_id": "2010.11929", "para_id": 144, "text": "Lastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet."}
{"doc_id": "2010.11929", "para_id": 145, "text": "We report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to"}
{"doc_id": "2010.11929", "para_id": 146, "text": "ImageNet\nCIFAR-10\n98.13\n97.77\n97.86\n97.94\n-\nCIFAR-100\n87.13\n86.31\n86.35\n87.07\n-\nImageNet\n77.91\n73.38\n76.53\n71.16\n-\nImageNet ReaL\n83.57\n79.56\n82.19\n77.83\n-\nOxford Flowers-102\n89.49\n85.43\n89.66\n86.36\n-\nOxford-IIIT-Pets\n93.81\n92.04\n93.64\n91.35\n-"}
{"doc_id": "2010.11929", "para_id": 147, "text": "ImageNet-21k\nCIFAR-10\n98.95\n98.79\n99.16\n99.13\n99.27\nCIFAR-100\n91.67\n91.97\n93.44\n93.04\n93.82\nImageNet\n83.97\n81.28\n85.15\n80.99\n85.13\nImageNet ReaL\n88.35\n86.63\n88.40\n85.65\n88.70\nOxford Flowers-102\n99.38\n99.11\n99.61\n99.19\n99.51\nOxford-IIIT-Pets\n94.43\n93.02\n94.73\n93.09\n94.82"}
{"doc_id": "2010.11929", "para_id": 148, "text": "JFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.49\n94.04\n92.52\n94.55\nImageNet\n84.15\n80.73\n87.12\n84.37\n88.04\nImageNet ReaL\n88.85\n86.27\n89.99\n88.28\n90.33\nOxford Flowers-102\n99.56\n99.27\n99.56\n99.45\n99.68\nOxford-IIIT-Pets\n95.80\n93.40\n97.11\n95.83\n97.56"}
{"doc_id": "2010.11929", "para_id": 149, "text": "Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2."}
{"doc_id": "2010.11929", "para_id": 150, "text": "Epochs\nImageNet\nImageNet ReaL\nCIFAR-10\nCIFAR-100\nPets\nFlowers\nexaFLOPs\nname"}
{"doc_id": "2010.11929", "para_id": 151, "text": "ViT-B/32\n7\n80.73\n86.27\n98.61\n90.49\n93.40\n99.27\n55\nViT-B/16\n7\n84.15\n88.85\n99.00\n91.87\n95.80\n99.56\n224\nViT-L/32\n7\n84.37\n88.28\n99.19\n92.52\n95.83\n99.45\n196\nViT-L/16\n7\n86.30\n89.43\n99.38\n93.46\n96.81\n99.66\n783\nViT-L/16\n14\n87.12\n89.99\n99.38\n94.04\n97.11\n99.56\n1567\nViT-H/14\n14\n88.08\n90.36\n99.50\n94.71\n97.11\n99.71\n4262"}
{"doc_id": "2010.11929", "para_id": 152, "text": "ResNet50x1\n7\n77.54\n84.56\n97.67\n86.07\n91.11\n94.26\n50\nResNet50x2\n7\n82.12\n87.94\n98.29\n89.20\n93.43\n97.02\n199\nResNet101x1\n7\n80.67\n87.07\n98.48\n89.17\n94.08\n95.95\n96\nResNet152x1\n7\n81.88\n87.96\n98.82\n90.22\n94.17\n96.94\n141\nResNet152x2\n7\n84.97\n89.69\n99.06\n92.05\n95.37\n98.62\n563\nResNet152x2\n14\n85.56\n89.89\n99.24\n91.92\n95.75\n98.75\n1126\nResNet200x3\n14\n87.22\n90.15\n99.34\n93.53\n96.32\n99.04\n3306"}
{"doc_id": "2010.11929", "para_id": 153, "text": "R50x1+ViT-B/32\n7\n84.90\n89.15\n99.01\n92.24\n95.75\n99.46\n106\nR50x1+ViT-B/16\n7\n85.58\n89.65\n99.14\n92.63\n96.65\n99.40\n274\nR50x1+ViT-L/32\n7\n85.68\n89.04\n99.24\n92.93\n96.97\n99.43\n246\nR50x1+ViT-L/16\n7\n86.60\n89.72\n99.18\n93.64\n97.03\n99.40\n859\nR50x1+ViT-L/16\n14\n87.12\n89.76\n99.31\n93.89\n97.36\n99.11\n1668"}
{"doc_id": "2010.11929", "para_id": 154, "text": "Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\naFLOPs)."}
{"doc_id": "2010.11929", "para_id": 155, "text": "Figure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training."}
{"doc_id": "2010.11929", "para_id": 156, "text": "ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning"}
{"doc_id": "2010.11929", "para_id": 157, "text": "ImageNet\n77.54\n78.24\n84.97\n84.37\nCIFAR10\n97.67\n97.46\n99.06\n99.07\nCIFAR100\n86.07\n85.17\n92.05\n91.06\nOxford-IIIT Pets\n91.11\n91.00\n95.37\n94.79\nOxford Flowers-102\n94.26\n92.06\n98.62\n99.32\nAverage\n89.33\n88.79\n94.01\n93.72"}
{"doc_id": "2010.11929", "para_id": 158, "text": "Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD."}
{"doc_id": "2010.11929", "para_id": 159, "text": "Figure 8: Scaling different model dimensions of the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 160, "text": "performance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor 7 epochs, not 30."}
{"doc_id": "2010.11929", "para_id": 161, "text": "We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\nall dimensions proportionally results in robust improvements."}
{"doc_id": "2010.11929", "para_id": 162, "text": "In order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then trans-\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer."}
{"doc_id": "2010.11929", "para_id": 163, "text": "This design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,"}
{"doc_id": "2010.11929", "para_id": 164, "text": "Figure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\nwell, but require different learning-rates."}
{"doc_id": "2010.11929", "para_id": 165, "text": "Pos. Emb.\nDefault/Stem\nEvery Layer\nEvery Layer-Shared"}
{"doc_id": "2010.11929", "para_id": 166, "text": "No Pos. Emb.\n0.61382\nN/A\nN/A\n1-D Pos. Emb.\n0.64206\n0.63964\n0.64292\n2-D Pos. Emb.\n0.64001\n0.64046\n0.64022\nRel. Pos. Emb.\n0.64032\nN/A\nN/A"}
{"doc_id": "2010.11929", "para_id": 167, "text": "Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear."}
{"doc_id": "2010.11929", "para_id": 168, "text": "the difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9."}
{"doc_id": "2010.11929", "para_id": 169, "text": "We ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:"}
{"doc_id": "2010.11929", "para_id": 170, "text": "• Providing no positional information: Considering the inputs as a bag of patches."}
{"doc_id": "2010.11929", "para_id": 171, "text": "• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper)."}
{"doc_id": "2010.11929", "para_id": 172, "text": "• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\nembedding for that patch."}
{"doc_id": "2010.11929", "para_id": 173, "text": "• Relative positional embeddings: Considering the relative distance between patches to en-\ncode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the log-\nits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax."}
{"doc_id": "2010.11929", "para_id": 174, "text": "In addition to different ways of encoding spatial information, we also tried different ways of in-\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after"}
{"doc_id": "2010.11929", "para_id": 175, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 176, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 177, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 178, "text": "Figure 10: Position embeddings of models trained with different hyperparameters."}
{"doc_id": "2010.11929", "para_id": 179, "text": "the stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers)."}
{"doc_id": "2010.11929", "para_id": 180, "text": "Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10)."}
{"doc_id": "2010.11929", "para_id": 181, "text": "Figure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels."}
{"doc_id": "2010.11929", "para_id": 182, "text": "We are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,"}
{"doc_id": "2010.11929", "para_id": 183, "text": "we perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor."}
{"doc_id": "2010.11929", "para_id": 184, "text": "Figure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions."}
{"doc_id": "2010.11929", "para_id": 185, "text": "Another quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\nmodels."}
{"doc_id": "2010.11929", "para_id": 186, "text": "Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient."}
{"doc_id": "2010.11929", "para_id": 187, "text": "Axial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3."}
{"doc_id": "2010.11929", "para_id": 188, "text": "Moreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP."}
{"doc_id": "2010.11929", "para_id": 189, "text": "Figure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at"}
{"doc_id": "2010.11929", "para_id": 190, "text": "3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation."}
{"doc_id": "2010.11929", "para_id": 191, "text": "Figure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left)."}
{"doc_id": "2010.11929", "para_id": 192, "text": "the cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right)."}
{"doc_id": "2010.11929", "para_id": 193, "text": "To understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens."}
{"doc_id": "2010.11929", "para_id": 194, "text": "To compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers."}
{"doc_id": "2010.11929", "para_id": 195, "text": "We also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy."}
{"doc_id": "2010.11929", "para_id": 196, "text": "Table 9 shows the scores attained on each of the VTAB-1k tasks."}
{"doc_id": "2010.11929", "para_id": 197, "text": "Figure 14: Further example attention maps as in Figure 6 (random selection)."}
{"doc_id": "2010.11929", "para_id": 198, "text": "Table 9: Breakdown of VTAB-1k performance across tasks."}
{"doc_id": "2010.11929", "para_id": 199, "text": "ViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7"}
{"doc_id": "2103.00020", "para_id": 0, "text": "Learning Transferable Visual Models From Natural Language Supervision"}
{"doc_id": "2103.00020", "para_id": 1, "text": "Alec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1"}
{"doc_id": "2103.00020", "para_id": 2, "text": "Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1"}
{"doc_id": "2103.00020", "para_id": 3, "text": "Task-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\nnitude in compute, model capacity, and data, steadily im-\nproving capabilities. The development of “text-to-text” as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\nagnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset speciﬁc customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspeciﬁc training data."}
{"doc_id": "2103.00020", "para_id": 4, "text": "State-of-the-art computer vision systems are\ntrained to predict a ﬁxed set of predetermined\nobject categories. This restricted form of super-\nvision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im-\nage is an efﬁcient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark-\ning on over 30 different existing computer vi-\nsion datasets, spanning tasks such as OCR, ac-\ntion recognition in videos, geo-localization, and\nmany types of ﬁne-grained object classiﬁcation.\nThe model transfers non-trivially to most tasks\nand is often competitive with a fully supervised\nbaseline without the need for any dataset spe-\nciﬁc training. For instance, we match the ac-\ncuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28\nmillion training examples it was trained on. We\nrelease our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP."}
{"doc_id": "2103.00020", "para_id": 5, "text": "These results suggest that the aggregate supervision acces-\nsible to modern pre-training methods within web-scale col-\nlections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other ﬁelds such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging."}
{"doc_id": "2103.00020", "para_id": 6, "text": "Over 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre-\ndict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi-\nble to learn more data efﬁcient image representations via\nmanifold learning in the weight space of classiﬁers trained\nto predict words in captions associated with images. Sri-\nvastava & Salakhutdinov (2012) explored deep represen-\ntation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag features.\nJoulin et al. (2016) modernized this line of work and demon-\nstrated that CNNs trained to predict words in image cap-\ntions learn useful image representations. They converted\nthe title, description, and hashtag metadata of images in the\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\nwords multi-label classiﬁcation task and showed that pre-\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\nlabels learned representations which preformed similarly\nto ImageNet-based pre-training on transfer tasks. Li et al.\n(2017) then extended this approach to predicting phrase n-\ngrams in addition to individual words and demonstrated the\nability of their system to zero-shot transfer to other image"}
{"doc_id": "2103.00020", "para_id": 7, "text": "Pre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 8, "text": "*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <{alec, jongwook}@openai.com>."}
{"doc_id": "2103.00020", "para_id": 9, "text": "Learning Transferable Visual Models From Natural Language Supervision\n2"}
{"doc_id": "2103.00020", "para_id": 10, "text": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classiﬁer to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the\ntarget dataset’s classes."}
{"doc_id": "2103.00020", "para_id": 11, "text": "classiﬁcation datasets by scoring target classes based on\ntheir dictionary of learned visual n-grams and predicting the\none with the highest score. Adopting more recent architec-\ntures and pre-training approaches, VirTex (Desai & Johnson,\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\nVIRT (Zhang et al., 2020) have recently demonstrated the\npotential of transformer-based language modeling, masked\nlanguage modeling, and contrastive objectives to learn im-\nage representations from text."}
{"doc_id": "2103.00020", "para_id": 12, "text": "mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general-\nity. Both approaches also use static softmax classiﬁers to\nperform prediction and lack a mechanism for dynamic out-\nputs. This severely curtails their ﬂexibility and limits their\n“zero-shot” capabilities."}
{"doc_id": "2103.00020", "para_id": 13, "text": "A crucial difference between these weakly supervised mod-\nels and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classiﬁers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im-\nage, text) pairs and demonstrate that a simpliﬁed version of\nConVIRT trained from scratch, which we call CLIP, for Con-\ntrastive Language-Image Pre-training, is an efﬁcient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob-\nserve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We ﬁnd that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and ﬁnd"}
{"doc_id": "2103.00020", "para_id": 14, "text": "While exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com-\nmon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com-\nputer vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet-related hashtags on Instagram im-\nages is an effective pre-training task. When ﬁne-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset."}
{"doc_id": "2103.00020", "para_id": 15, "text": "This line of work represents the current pragmatic middle\nground between learning from a limited amount of super-\nvised “gold-labels” and learning from practically unlimited\namounts of raw text. However, it is not without compro-"}
{"doc_id": "2103.00020", "para_id": 16, "text": "Learning Transferable Visual Models From Natural Language Supervision\n3"}
{"doc_id": "2103.00020", "para_id": 17, "text": "vision. Although early work wrestled with the complexity\nof natural language when using topic model and n-gram\nrepresentations, improvements in deep contextual represen-\ntation learning suggest we now have the tools to effectively\nleverage this abundant source of supervision (McCann et al.,\n2017)."}
{"doc_id": "2103.00020", "para_id": 18, "text": "Learning from natural language has several potential\nstrengths over other training methods. It’s much easier\nto scale natural language supervision compared to standard\ncrowd-sourced labeling for image classiﬁcation since it does\nnot require annotations to be in a classic “machine learning\ncompatible format” such as the canonical 1-of-N majority\nvote “gold label”. Instead, methods which work on natural\nlanguage can learn passively from the supervision contained\nin the vast amount of text on the internet. Learning from\nnatural language also has an important advantage over most\nunsupervised or self-supervised learning approaches in that\nit doesn’t “just” learn a representation but also connects that\nrepresentation to language which enables ﬂexible zero-shot\ntransfer. In the following subsections, we detail the speciﬁc\napproach we settled on."}
{"doc_id": "2103.00020", "para_id": 19, "text": "Bag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Language Model"}
{"doc_id": "2103.00020", "para_id": 20, "text": "Figure 2. CLIP is much more efﬁcient at zero-shot transfer\nthan our image caption baseline. Although highly expressive,\nwe found that transformer-based language models are relatively\nweak at zero-shot ImageNet classiﬁcation. Here, we see that it\nlearns 3x slower than a baseline which predicts a bag-of-words\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\nprediction objective for the contrastive objective of CLIP further\nimproves efﬁciency another 4x."}
{"doc_id": "2103.00020", "para_id": 21, "text": "it can be competitive with prior task-speciﬁc supervised\nmodels. We also conﬁrm these ﬁndings with linear-probe\nrepresentation learning analysis and show that CLIP out-\nperforms the best publicly available ImageNet model while\nalso being more computationally efﬁcient. We additionally\nﬁnd that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model’s capability. These re-\nsults have signiﬁcant policy and ethical implications, which\nwe consider in Section 7."}
{"doc_id": "2103.00020", "para_id": 22, "text": "Existing work has mainly used three datasets, MS-COCO\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\nYFCC100M (Thomee et al., 2016). While MS-COCO and\nVisual Genome are high quality crowd-labeled datasets, they\nare small by modern standards with approximately 100,000\ntraining photos each. By comparison, other computer vision\nsystems are trained on up to 3.5 billion Instagram photos\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\nis a possible alternative, but the metadata for each image is\nsparse and of varying quality. Many images use automati-\ncally generated ﬁlenames like 20160716 113957.JPG\nas “titles” or contain “descriptions” of camera exposure\nsettings. After ﬁltering to keep only images with natural\nlanguage titles and/or descriptions in English, the dataset\nshrunk by a factor of 6 to only 15 million photos. This is\napproximately the same size as ImageNet."}
{"doc_id": "2103.00020", "para_id": 23, "text": "At the core of our approach is the idea of learning percep-\ntion from supervision contained in natural language. As\ndiscussed in the introduction, this is not at all a new idea,\nhowever terminology used to describe work in this space\nis varied, even seemingly contradictory, and stated motiva-\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\nduce methods which learn visual representations from text\npaired with images but describe their approaches as unsuper-\nvised, self-supervised, weakly supervised, and supervised\nrespectively."}
{"doc_id": "2103.00020", "para_id": 24, "text": "A major motivation for natural language supervision is the\nlarge quantities of data of this form available publicly on the\ninternet. Since existing datasets do not adequately reﬂect\nthis possibility, considering results only on them would un-\nderestimate the potential of this line of research. To address\nthis, we constructed a new dataset of 400 million (image,\ntext) pairs collected form a variety of publicly available\nsources on the Internet. To attempt to cover as broad a set\nof visual concepts as possible, we search for (image, text)\npairs as part of the construction process whose text includes\none of a set of 500,000 queries.1 We approximately class"}
{"doc_id": "2103.00020", "para_id": 25, "text": "We emphasize that what is common across this line of work\nis not any of the details of the particular methods used but\nthe appreciation of natural language as a training signal. All\nthese approaches are learning from natural language super-"}
{"doc_id": "2103.00020", "para_id": 26, "text": "1The base query list is all words occurring at least 100 times in\nthe English version of Wikipedia. This is augmented with bi-grams"}
{"doc_id": "2103.00020", "para_id": 27, "text": "Learning Transferable Visual Models From Natural Language Supervision\n4"}
{"doc_id": "2103.00020", "para_id": 28, "text": "balance the results by including up to 20,000 (image, text)\npairs per query. The resulting dataset has a similar total\nword count as the WebText dataset used to train GPT-2. We\nrefer to this dataset as WIT for WebImageText."}
{"doc_id": "2103.00020", "para_id": 29, "text": "multi-modal embedding space by jointly training an image\nencoder and text encoder to maximize the cosine similar-\nity of the image and text embeddings of the N real pairs\nin the batch while minimizing the cosine similarity of the\nembeddings of the N 2 −N incorrect pairings. We opti-\nmize a symmetric cross entropy loss over these similarity\nscores. In Figure 3 we include pseudocode of the core of an\nimplementation of CLIP. To our knowledge this batch con-\nstruction technique and objective was ﬁrst introduced in the\narea of deep metric learning as the multi-class N-pair loss\nSohn (2016), was popularized for contrastive representation\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\nrecently adapted for contrastive (text, image) representation\nlearning in the domain of medical imaging by Zhang et al.\n(2020)."}
{"doc_id": "2103.00020", "para_id": 30, "text": "State-of-the-art computer vision systems use very large\namounts of compute. Mahajan et al. (2018) required 19\nGPU years to train their ResNeXt101-32x48d and Xie et al.\n(2020) required 33 TPUv3 core-years to train their Noisy\nStudent EfﬁcientNet-L2. When considering that both these\nsystems were trained to predict only 1000 ImageNet classes,\nthe task of learning an open set of visual concepts from\nnatural language seems daunting. In the course of our ef-\nforts, we found training efﬁciency was key to successfully\nscaling natural language supervision and we selected our\nﬁnal pre-training method based on this metric."}
{"doc_id": "2103.00020", "para_id": 31, "text": "Due to the large size of our pre-training dataset, over-ﬁtting\nis not a major concern and the details of training CLIP are\nsimpliﬁed compared to the implementation of Zhang et al.\n(2020). We train CLIP from scratch without initializing the\nimage encoder with ImageNet weights or the text encoder\nwith pre-trained weights. We do not use the non-linear\nprojection between the representation and the contrastive\nembedding space, a change which was introduced by Bach-\nman et al. (2019) and popularized by Chen et al. (2020b).\nWe instead use only a linear projection to map from each en-\ncoder’s representation to the multi-modal embedding space.\nWe did not notice a difference in training efﬁciency between\nthe two versions and speculate that non-linear projections\nmay be co-adapted with details of current image only in\nself-supervised representation learning methods. We also\nremove the text transformation function tu from Zhang et al.\n(2020) which samples a single sentence at uniform from\nthe text since many of the (image, text) pairs in CLIP’s pre-\ntraining dataset are only a single sentence. We also simplify\nthe image transformation function tv. A random square\ncrop from resized images is the only data augmentation\nused during training. Finally, the temperature parameter\nwhich controls the range of the logits in the softmax, τ, is\ndirectly optimized during training as a log-parameterized\nmultiplicative scalar to avoid turning as a hyper-parameter."}
{"doc_id": "2103.00020", "para_id": 32, "text": "Our initial approach, similar to VirTex, jointly trained an\nimage CNN and text transformer from scratch to predict the\ncaption of an image. However, we encountered difﬁculties\nefﬁciently scaling this method. In Figure 2 we show that a\n63 million parameter transformer language model, which\nalready uses twice the compute of its ResNet-50 image\nencoder, learns to recognize ImageNet classes three times\nslower than a much simpler baseline that predicts a bag-of-\nwords encoding of the same text."}
{"doc_id": "2103.00020", "para_id": 33, "text": "Both these approaches share a key similarity. They try to pre-\ndict the exact words of the text accompanying each image.\nThis is a difﬁcult task due to the wide variety of descriptions,\ncomments, and related text that co-occur with images. Re-\ncent work in contrastive representation learning for images\nhas found that contrastive objectives can learn better repre-\nsentations than their equivalent predictive objective (Tian\net al., 2019). Other work has found that although generative\nmodels of images can learn high quality image representa-\ntions, they require over an order of magnitude more compute\nthan contrastive models with the same performance (Chen\net al., 2020a). Noting these ﬁndings, we explored training\na system to solve the potentially easier proxy task of pre-\ndicting only which text as a whole is paired with which\nimage and not the exact words of that text. Starting with\nthe same bag-of-words encoding baseline, we swapped the\npredictive objective for a contrastive objective in Figure 2\nand observed a further 4x efﬁciency improvement in the rate\nof zero-shot transfer to ImageNet."}
{"doc_id": "2103.00020", "para_id": 34, "text": "We consider two different architectures for the image en-\ncoder. For the ﬁrst, we use ResNet-50 (He et al., 2016a)\nas the base architecture for the image encoder due to its\nwidespread adoption and proven performance. We make sev-\neral modiﬁcations to the original version using the ResNet-\nD improvements from He et al. (2019) and the antialiased\nrect-2 blur pooling from Zhang (2019). We also replace\nthe global average pooling layer with an attention pooling\nmechanism. The attention pooling is implemented as a sin-\ngle layer of “transformer-style” multi-head QKV attention\nwhere the query is conditioned on the global average-pooled"}
{"doc_id": "2103.00020", "para_id": 35, "text": "Given a batch of N (image, text) pairs, CLIP is trained to\npredict which of the N × N possible (image, text) pairings\nacross a batch actually occurred. To do this, CLIP learns a"}
{"doc_id": "2103.00020", "para_id": 36, "text": "with high pointwise mutual information as well as the names of\nall Wikipedia articles above a certain search volume. Finally all\nWordNet synsets not already in the query list are added."}
{"doc_id": "2103.00020", "para_id": 37, "text": "Learning Transferable Visual Models From Natural Language Supervision\n5"}
{"doc_id": "2103.00020", "para_id": 38, "text": "# image_encoder - ResNet or Vision Transformer\n# text_encoder  - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l]       - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t             - learned temperature parameter"}
{"doc_id": "2103.00020", "para_id": 39, "text": "one dimension of the model. While Tan & Le (2019) tune\nthe ratio of compute allocated to each dimension for their\nEfﬁcientNet architecture, we use a simple baseline of allo-\ncating additional compute equally to increasing the width,\ndepth, and resolution of the model. For the text encoder, we\nonly scale the width of the model to be proportional to the\ncalculated increase in width of the ResNet and do not scale\nthe depth at all, as we found CLIP’s performance to be less\nsensitive to the capacity of the text encoder."}
{"doc_id": "2103.00020", "para_id": 40, "text": "# extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T)  #[n, d_t]"}
{"doc_id": "2103.00020", "para_id": 41, "text": "# joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)"}
{"doc_id": "2103.00020", "para_id": 42, "text": "We train a series of 5 ResNets and 3 Vision Transformers.\nFor the ResNets we train a ResNet-50, a ResNet-101, and\nthen 3 more which follow EfﬁcientNet-style model scaling\nand use approximately 4x, 16x, and 64x the compute of a\nResNet-50. They are denoted as RN50x4, RN50x16, and\nRN50x64 respectively. For the Vision Transformers we\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\nmodels for 32 epochs. We use the Adam optimizer (Kingma\n& Ba, 2014) with decoupled weight decay regularization\n(Loshchilov & Hutter, 2017) applied to all weights that are\nnot gains or biases, and decay the learning rate using a\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\nparameters were set using a combination of grid searches,\nrandom search, and manual tuning on the baseline ResNet-\n50 model when trained for 1 epoch. Hyper-parameters were\nthen adapted heuristically for larger models due to compu-\ntational constraints. The learnable temperature parameter\nτ was initialized to the equivalent of 0.07 from (Wu et al.,\n2018) and clipped to prevent scaling the logits by more\nthan 100 which we found necessary to prevent training in-\nstability. We use a very large minibatch size of 32,768.\nMixed-precision (Micikevicius et al., 2017) was used to ac-\ncelerate training and save memory. To save additional mem-\nory, gradient checkpointing (Griewank & Walther, 2000;\nChen et al., 2016), half-precision Adam statistics (Dhariwal\net al., 2020), and half-precision stochastically rounded text\nencoder weights were used. The calculation of embedding\nsimilarities was also sharded with individual GPUs comput-\ning only the subset of the pairwise similarities necessary for\ntheir local batch of embeddings. The largest ResNet model,\nRN50x64, took 18 days to train on 592 V100 GPUs while\nthe largest Vision Transformer took 12 days on 256 V100\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\npixel resolution for one additional epoch to boost perfor-\nmance similar to FixRes (Touvron et al., 2019). We denote\nthis model as ViT-L/14@336px. Unless otherwise speciﬁed,\nall results reported in this paper as “CLIP” use this model\nwhich we found to perform best."}
{"doc_id": "2103.00020", "para_id": 43, "text": "# scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)"}
{"doc_id": "2103.00020", "para_id": 44, "text": "# symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss   = (loss_i + loss_t)/2"}
{"doc_id": "2103.00020", "para_id": 45, "text": "Figure 3. Numpy-like pseudocode for the core of an implementa-\ntion of CLIP."}
{"doc_id": "2103.00020", "para_id": 46, "text": "representation of the image. For the second architecture, we\nexperiment with the recently introduced Vision Transformer\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\nimplementation with only the minor modiﬁcation of adding\nan additional layer normalization to the combined patch\nand position embeddings before the transformer and use a\nslightly different initialization scheme."}
{"doc_id": "2103.00020", "para_id": 47, "text": "The text encoder is a Transformer (Vaswani et al., 2017)\nwith the architecture modiﬁcations described in Radford\net al. (2019). As a base size we use a 63M-parameter 12-\nlayer 512-wide model with 8 attention heads. The trans-\nformer operates on a lower-cased byte pair encoding (BPE)\nrepresentation of the text with a 49,152 vocab size (Sen-\nnrich et al., 2015). For computational efﬁciency, the max\nsequence length was capped at 76. The text sequence is\nbracketed with [SOS] and [EOS] tokens and the activa-\ntions of the highest layer of the transformer at the [EOS]\ntoken are treated as the feature representation of the text\nwhich is layer normalized and then linearly projected into\nthe multi-modal embedding space. Masked self-attention\nwas used in the text encoder to preserve the ability to ini-\ntialize with a pre-trained language model or add language\nmodeling as an auxiliary objective, though exploration of\nthis is left as future work."}
{"doc_id": "2103.00020", "para_id": 48, "text": "While previous computer vision research has often scaled\nmodels by increasing the width (Mahajan et al., 2018) or\ndepth (He et al., 2016a) in isolation, for the ResNet image\nencoders we adapt the approach of Tan & Le (2019) which\nfound that allocating additional compute across all of width,\ndepth, and resolution outperforms only allocating it to only"}
{"doc_id": "2103.00020", "para_id": 49, "text": "Learning Transferable Visual Models From Natural Language Supervision\n6"}
{"doc_id": "2103.00020", "para_id": 50, "text": "training as a transfer learning method to improve supervised\nﬁne-tuning, it also included an ablation study demonstrat-\ning that the performance of four heuristic zero-shot transfer\nmethods improved steadily over the course of pre-training,\nwithout any supervised adaption. This analysis served as the\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\nsively on studying the task-learning capabilities of language\nmodels via zero-shot transfer."}
{"doc_id": "2103.00020", "para_id": 51, "text": "In computer vision, zero-shot learning usually refers to the\nstudy of generalizing to unseen object categories in image\nclassiﬁcation (Lampert et al., 2009). We instead use the\nterm in a broader sense and study generalization to unseen\ndatasets. We motivate this as a proxy for performing un-\nseen tasks, as aspired to in the zero-data learning paper of\nLarochelle et al. (2008). While much research in the ﬁeld of\nunsupervised learning focuses on the representation learn-\ning capabilities of machine learning systems, we motivate\nstudying zero-shot transfer as a way of measuring the task-\nlearning capabilities of machine learning systems. In this\nview, a dataset evaluates performance on a task on a spe-\nciﬁc distribution. However, many popular computer vision\ndatasets were created by the research community primarily\nas benchmarks to guide the development of generic image\nclassiﬁcation methods rather than measuring performance\non a speciﬁc task. While it is reasonable to say that the\nSVHN dataset measures the task of street number transcrip-\ntion on the distribution of Google Street View photos, it is\nunclear what “real” task the CIFAR-10 dataset measures.\nIt is clear, however, what distribution CIFAR-10 is drawn\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\nrobustness to distribution shift and domain generalization\nrather than task generalization. Please see Section 3.3 for\nanalysis focused on this."}
{"doc_id": "2103.00020", "para_id": 52, "text": "CLIP is pre-trained to predict if an image and a text snippet\nare paired together in its dataset. To perform zero-shot clas-\nsiﬁcation, we reuse this capability. For each dataset, we use\nthe names of all the classes in the dataset as the set of poten-\ntial text pairings and predict the most probable (image, text)\npair according to CLIP. In a bit more detail, we ﬁrst compute\nthe feature embedding of the image and the feature embed-\nding of the set of possible texts by their respective encoders.\nThe cosine similarity of these embeddings is then calculated,\nscaled by a temperature parameter τ, and normalized into a\nprobability distribution via a softmax. Note that this predic-\ntion layer is a multinomial logistic regression classiﬁer with\nL2-normalized inputs, L2-normalized weights, no bias, and\ntemperature scaling. When interpreted this way, the image\nencoder is the computer vision backbone which computes a\nfeature representation for the image and the text encoder is a\nhypernetwork (Ha et al., 2016) which generates the weights\nof a linear classiﬁer based on the text specifying the visual\nconcepts that the classes represent. Lei Ba et al. (2015) ﬁrst\nintroduced a zero-shot image classiﬁer of this form while\nthe idea of generating a classiﬁer from natural language\ndates back to at least Elhoseiny et al. (2013). Continuing\nwith this interpretation, every step of CLIP pre-training can\nbe viewed as optimizing the performance of a randomly\ncreated proxy to a computer vision dataset which contains 1\nexample per class and has 32,768 total classes deﬁned via\nnatural language descriptions. For zero-shot evaluation, we\ncache the zero-shot classiﬁer once it has been computed by\nthe text encoder and reuse it for all subsequent predictions.\nThis allows the cost of generating it to be amortized across\nall the predictions in a dataset."}
{"doc_id": "2103.00020", "para_id": 53, "text": "To our knowledge, Visual N-Grams (Li et al., 2017) ﬁrst\nstudied zero-shot transfer to existing image classiﬁcation\ndatasets in the manner described above. It is also the only\nother work we are aware of that has studied zero-shot trans-\nfer to standard image classiﬁcation datasets using a gener-\nically pre-trained model and serves as the best reference\npoint for contextualizing CLIP. Their approach learns the\nparameters of a dictionary of 142,806 visual n-grams (span-\nning 1- to 5- grams) and optimizes these n-grams using a\ndifferential version of Jelinek-Mercer smoothing to maxi-\nmize the probability of all text n-grams for a given image.\nIn order to perform zero-shot transfer, they ﬁrst convert the\ntext of each of the dataset’s class names into its n-gram\nrepresentation and then compute its probability according\nto their model, predicting the one with the highest score."}
{"doc_id": "2103.00020", "para_id": 54, "text": "In Table 1 we compare Visual N-Grams to CLIP. The best\nCLIP model improves accuracy on ImageNet from a proof\nof concept 11.5% to 76.2% and matches the performance\nof the original ResNet-50 despite using none of the 1.28\nmillion crowd-labeled training examples available for this\ndataset. Additionally, the top-5 accuracy of CLIP models\nare noticeably higher than their top-1, and this model has a\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\n2016). The ability to match the performance of a strong,\nfully supervised baselines in a zero-shot setting suggests"}
{"doc_id": "2103.00020", "para_id": 55, "text": "Our focus on studying zero-shot transfer as an evaluation of\ntask learning is inspired by work demonstrating task learn-\ning in the ﬁeld of NLP. To our knowledge Liu et al. (2018)\nﬁrst identiﬁed task learning as an “unexpected side-effect”\nwhen a language model trained to generate Wikipedia ar-\nticles learned to reliably transliterate names between lan-\nguages. While GPT-1 (Radford et al., 2018) focused on pre-"}
{"doc_id": "2103.00020", "para_id": 56, "text": "Learning Transferable Visual Models From Natural Language Supervision\n7"}
{"doc_id": "2103.00020", "para_id": 57, "text": "Table 1. Comparing CLIP to prior zero-shot transfer image classi-\nﬁcation results. CLIP improves performance on all three datasets\nby a large amount. This improvement reﬂects many differences\nin the 4 years since the development of Visual N-Grams (Li et al.,\n2017)."}
{"doc_id": "2103.00020", "para_id": 58, "text": "CLIP is a signiﬁcant step towards ﬂexible and practical\nzero-shot computer vision classiﬁers. As mentioned above,\nthe comparison to Visual N-Grams is meant for contextu-\nalizing the performance of CLIP and should not be inter-\npreted as a direct methods comparison between CLIP and\nVisual N-Grams as many performance relevant differences\nbetween the two systems were not controlled for. For in-\nstance, we train on a dataset that is 10x larger, use a vision\nmodel that requires nearly 100x more compute per predic-\ntion, likely used over 1000x their training compute, and\nuse a transformer-based model which did not exist when\nVisual N-Grams was published. As a closer comparison, we\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\nthat Visual N-Grams was trained on and found it matched\ntheir reported ImageNet performance within a V100 GPU\nday. This baseline was also trained from scratch instead of\nbeing initialized from pre-trained ImageNet weights as in\nVisual N-Grams."}
{"doc_id": "2103.00020", "para_id": 59, "text": "Prompt engineering and ensembling\nContextless class names (Li et al. 2017)"}
{"doc_id": "2103.00020", "para_id": 60, "text": "Figure 4. Prompt engineering and ensembling improve zero-\nshot performance. Compared to the baseline of using contextless\nclass names, prompt engineering and ensembling boost zero-shot\nclassiﬁcation performance by almost 5 points on average across\n36 datasets. This improvement is similar to the gain from using\n4 times more compute with the baseline zero-shot method but is\n“free” when amortized over many predictions."}
{"doc_id": "2103.00020", "para_id": 61, "text": "chosen somewhat haphazardly and do not anticipate issues\nrelated to zero-shot transfer which relies on task description\nin order to transfer successfully."}
{"doc_id": "2103.00020", "para_id": 62, "text": "CLIP also outperforms Visual N-Grams on the other 2 re-\nported datasets. On aYahoo, CLIP achieves a 95% reduction\nin the number of errors, and on SUN, CLIP more than dou-\nbles the accuracy of Visual N-Grams. To conduct a more\ncomprehensive analysis and stress test, we implement a\nmuch larger evaluation suite detailed in Appendix A. In\ntotal we expand from the 3 datasets reported in Visual N-\nGrams to include over 30 datasets and compare to over 50\nexisting computer vision systems to contextualize results."}
{"doc_id": "2103.00020", "para_id": 63, "text": "A common issue is polysemy. When the name of a class\nis the only information provided to CLIP’s text encoder it\nis unable to differentiate which word sense is meant due to\nthe lack of context. In some cases multiple meanings of the\nsame word might be included as different classes in the same\ndataset! This happens in ImageNet which contains both\nconstruction cranes and cranes that ﬂy. Another example is\nfound in classes of the Oxford-IIIT Pet dataset where the\nword boxer is, from context, clearly referring to a breed of\ndog, but to a text encoder lacking context could just as likely\nrefer to a type of athlete."}
{"doc_id": "2103.00020", "para_id": 64, "text": "Most standard image classiﬁcation datasets treat the infor-\nmation naming or describing classes which enables natural\nlanguage based zero-shot transfer as an afterthought. The\nvast majority of datasets annotate images with just a numeric\nid of the label and contain a ﬁle mapping these ids back to\ntheir names in English. Some datasets, such as Flowers102\nand GTSRB, don’t appear to include this mapping at all\nin their released versions preventing zero-shot transfer en-\ntirely.2 For many datasets, we observed these labels may be"}
{"doc_id": "2103.00020", "para_id": 65, "text": "Another issue we encountered is that it’s relatively rare in\nour pre-training dataset for the text paired with the image\nto be just a single word. Usually the text is a full sentence\ndescribing the image in some way. To help bridge this\ndistribution gap, we found that using the prompt template\n“A photo of a {label}.” to be a good default that\nhelps specify the text is about the content of the image. This\noften improves performance over the baseline of using only\nthe label text. For instance, just using this prompt improves\naccuracy on ImageNet by 1.3%."}
{"doc_id": "2103.00020", "para_id": 66, "text": "2Alec learned much more about ﬂower species and German\ntrafﬁc signs over the course of this project than he originally antic-\nipated."}
{"doc_id": "2103.00020", "para_id": 67, "text": "Learning Transferable Visual Models From Natural Language Supervision\n8"}
{"doc_id": "2103.00020", "para_id": 68, "text": "Similar to the “prompt engineering” discussion around GPT-\n3 (Brown et al., 2020; Gao et al., 2020), we have also\nobserved that zero-shot performance can be signiﬁcantly\nimproved by customizing the prompt text to each task. A\nfew, non exhaustive, examples follow. We found on several\nﬁne-grained image classiﬁcation datasets that it helped to\nspecify the category. For example on Oxford-IIIT Pets, us-\ning “A photo of a {label}, a type of pet.”\nto help provide context worked well. Likewise, on Food101\nspecifying a type of food and on FGVC Aircraft a type of\naircraft helped too. For OCR datasets, we found that putting\nquotes around the text or number to be recognized improved\nperformance. Finally, we found that on satellite image classi-\nﬁcation datasets it helped to specify that the images were of\nthis form and we use variants of “a satellite photo\nof a {label}.”."}
{"doc_id": "2103.00020", "para_id": 69, "text": "We also experimented with ensembling over multiple zero-\nshot classiﬁers as another way of improving performance.\nThese classiﬁers are computed by using different context\nprompts such as ‘A photo of a big {label}” and\n“A photo of a small {label}”. We construct the\nensemble over the embedding space instead of probability\nspace. This allows us to cache a single set of averaged text\nembeddings so that the compute cost of the ensemble is the\nsame as using a single classiﬁer when amortized over many\npredictions. We’ve observed ensembling across many gen-\nerated zero-shot classiﬁers to reliably improve performance\nand use it for the majority of datasets. On ImageNet, we\nensemble 80 different context prompts and this improves\nperformance by an additional 3.5% over the single default\nprompt discussed above. When considered together, prompt\nengineering and ensembling improve ImageNet accuracy\nby almost 5%. In Figure 4 we visualize how prompt engi-\nneering and ensembling change the performance of a set of\nCLIP models compared to the contextless baseline approach\nof directly embedding the class name as done in Li et al.\n(2017)."}
{"doc_id": "2103.00020", "para_id": 70, "text": "40\n30\n20\n10\n0\n10\n20\n30\n40\nScore (%)\nZero-Shot CLIP vs. Linear Probe on ResNet50"}
{"doc_id": "2103.00020", "para_id": 71, "text": "Figure 5. Zero-shot CLIP is competitive with a fully super-\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\nclassiﬁer outperforms a fully supervised linear classiﬁer ﬁtted on\nResNet-50 features on 16 datasets, including ImageNet."}
{"doc_id": "2103.00020", "para_id": 72, "text": "ten than not and wins on 16 of the 27 datasets. Looking at\nindividual datasets reveals some interesting behavior. On\nﬁne-grained classiﬁcation tasks, we observe a wide spread\nin performance. On two of these datasets, Stanford Cars and\nFood101, zero-shot CLIP outperforms logistic regression\non ResNet-50 features by over 20% while on two others,\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\nforms by over 10%. On OxfordPets and Birdsnap, per-\nformance is much closer. We suspect these difference are\nprimarily due to varying amounts of per-task supervision\nbetween WIT and ImageNet. On “general” object classiﬁca-\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\nPascalVOC2007 performance is relatively similar with a\nslight advantage for zero-shot CLIP in all cases. On STL10,\nCLIP achieves 99.3% overall which appears to be a new\nstate of the art despite not using any training examples. Zero-\nshot CLIP signiﬁcantly outperforms a ResNet-50 on two\ndatasets measuring action recognition in videos. On Kinet-\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\non UCF101. We speculate this is due to natural language\nproviding wider supervision for visual concepts involving\nverbs, compared to the noun-centric object supervision in\nImageNet."}
{"doc_id": "2103.00020", "para_id": 73, "text": "Since task-agnostic zero-shot classiﬁers for computer vision\nhave been understudied, CLIP provides a promising oppor-\ntunity to gain a better understanding of this type of model.\nIn this section, we conduct a study of various properties of\nCLIP’s zero-shot classiﬁers. As a ﬁrst question, we look\nsimply at how well zero-shot classiﬁers perform. To con-\ntextualize this, we compare to the performance of a simple\noff-the-shelf baseline: ﬁtting a fully supervised, regularized,\nlogistic regression classiﬁer on the features of the canonical\nResNet-50. In Figure 5 we show this comparison across 27\ndatasets. Please see Appendix A for details of datasets and\nsetup."}
{"doc_id": "2103.00020", "para_id": 74, "text": "Zero-shot CLIP outperforms this baseline slightly more of-"}
{"doc_id": "2103.00020", "para_id": 75, "text": "Looking at where zero-shot CLIP notably underperforms,"}
{"doc_id": "2103.00020", "para_id": 76, "text": "Learning Transferable Visual Models From Natural Language Supervision\n9"}
{"doc_id": "2103.00020", "para_id": 77, "text": "expect zero-shot to underperform one-shot, we instead ﬁnd\nthat zero-shot CLIP matches the performance of 4-shot lo-\ngistic regression on the same feature space. This is likely\ndue to an important difference between the zero-shot and\nfew-shot approach. First, CLIP’s zero-shot classiﬁer is gen-\nerated via natural language which allows for visual concepts\nto be directly speciﬁed (“communicated”). By contrast,\n“normal” supervised learning must infer concepts indirectly\nfrom training examples. Context-less example-based learn-\ning has the drawback that many different hypotheses can\nbe consistent with the data, especially in the one-shot case.\nA single image often contains many different visual con-\ncepts. Although a capable learner is able to exploit visual\ncues and heuristics, such as assuming that the concept being\ndemonstrated is the primary object in an image, there is no\nguarantee."}
{"doc_id": "2103.00020", "para_id": 78, "text": "A potential resolution of this discrepancy between zero-\nshot and few-shot performance is to use CLIP’s zero-shot\nclassiﬁer as a prior for the weights of the few-shot classiﬁer.\nWhile adding an L2 penalty towards the generated weights\nis a straightforward implementation of this idea, we found\nthat hyperparameter optimization would often select for\nsuch a large value of this regularizer that the resulting few-\nshot classiﬁer was “just” the zero-shot classiﬁer. Research\ninto better methods of combining the strength of zero-shot\ntransfer with ﬂexibility of few-shot learning is a promising\ndirection for future work."}
{"doc_id": "2103.00020", "para_id": 79, "text": "0\n1\n2\n4\n8\n16\n# of labeled training examples per class"}
{"doc_id": "2103.00020", "para_id": 80, "text": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes.\nZero-shot CLIP matches the average performance of a 4-shot linear\nclassiﬁer trained on the same feature space and nearly matches the\nbest results of a 16-shot linear classiﬁer across publicly available\nmodels. For both BiT-M and SimCLRv2, the best performing\nmodel is highlighted. Light gray lines are other models in the eval\nsuite. The 20 datasets with at least 16 examples per class were\nused in this analysis."}
{"doc_id": "2103.00020", "para_id": 81, "text": "When comparing zero-shot CLIP to few-shot logistic re-\ngression on the features of other models, zero-shot CLIP\nroughly matches the performance of the best performing\n16-shot classiﬁer in our evaluation suite, which uses the fea-\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\nWe are certain that a BiT-L model trained on JFT-300M\nwould perform even better but these models have not been\npublicly released. That a BiT-M ResNet-152x2 performs\nbest in a 16-shot setting is somewhat surprising since, as\nanalyzed in Section 3.2, the Noisy Student EfﬁcientNet-L2\noutperforms it in a fully supervised setting by almost 5% on\naverage across 27 datasets."}
{"doc_id": "2103.00020", "para_id": 82, "text": "we see that zero-shot CLIP is quite weak on several spe-\ncialized, complex, or abstract tasks such as satellite image\nclassiﬁcation (EuroSAT and RESISC45), lymph node tumor\ndetection (PatchCamelyon), counting objects in synthetic\nscenes (CLEVRCounts), self-driving related tasks such as\nGerman trafﬁc sign recognition (GTSRB), recognizing dis-\ntance to the nearest car (KITTI Distance). These results\nhighlight the poor capability of zero-shot CLIP on more\ncomplex tasks. By contrast, non-expert humans can robustly\nperform several of these tasks, such as counting, satellite\nimage classiﬁcation, and trafﬁc sign recognition, suggesting\nsigniﬁcant room for improvement. However, we caution\nthat it is unclear whether measuring zero-shot transfer, as\nopposed to few-shot transfer, is a meaningful evaluation for\ndifﬁcult tasks that a learner has no prior experience with,\nsuch as lymph node tumor classiﬁcation for almost all hu-\nmans (and possibly CLIP)."}
{"doc_id": "2103.00020", "para_id": 83, "text": "In addition to studying the average performance of zero-shot\nCLIP and few-shot logistic regression, we also examine\nperformance on individual datasets. In Figure 7, we show\nestimates for the number of labeled examples per class that\na logistic regression classiﬁer on the same feature space\nrequires to match the performance of zero-shot CLIP. Since\nzero-shot CLIP is also a linear classiﬁer, this estimates the\neffective data efﬁciency of zero-shot transfer in this setting.\nIn order to avoid training thousands of linear classiﬁers,\nwe estimate the effective data efﬁciency based on a log-\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\nshot (when possible), and a fully supervised linear classiﬁer\ntrained on each dataset. We ﬁnd that zero-shot transfer can"}
{"doc_id": "2103.00020", "para_id": 84, "text": "While comparing zero-shot performance to fully supervised\nmodels contextualizes the task-learning capabilities of CLIP,\ncomparing to few-shot methods is a more direct compari-\nson, since zero-shot is its limit. In Figure 6, we visualize\nhow zero-shot CLIP compares to few-shot logistic regres-\nsion on the features of many image models including the\nbest publicly available ImageNet models, self-supervised\nlearning methods, and CLIP itself. While it is intuitive to"}
{"doc_id": "2103.00020", "para_id": 85, "text": "Learning Transferable Visual Models From Natural Language Supervision\n10"}
{"doc_id": "2103.00020", "para_id": 86, "text": "0\n25\n50\n75\n100\n125\n150\n175\n200\n# of labeled examples per class"}
{"doc_id": "2103.00020", "para_id": 87, "text": "20\n30\n40\n50\n60\n70\n80\n90\n100\nLinear Probe CLIP Performance"}
{"doc_id": "2103.00020", "para_id": 88, "text": "Figure 7. The data efﬁciency of zero-shot transfer varies\nwidely. Calculating the number of labeled examples per class\na linear classiﬁer on the same CLIP feature space requires to match\nthe performance of the zero-shot classiﬁer contextualizes the ef-\nfectiveness of zero-shot transfer. Values are estimated based on\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\nresults. Performance varies widely from still underperforming a\none-shot classiﬁer on two datasets to matching an estimated 184\nlabeled examples per class."}
{"doc_id": "2103.00020", "para_id": 89, "text": "Figure 8. Zero-shot performance is correlated with linear\nprobe performance but still mostly sub-optimal. Comparing\nzero-shot and linear probe performance across datasets shows a\nstrong correlation with zero-shot performance mostly shifted 10 to\n25 points lower. On only 5 datasets does zero-shot performance\napproach linear probe performance (≤3 point difference)."}
{"doc_id": "2103.00020", "para_id": 90, "text": "mance, suggesting that CLIP is relatively consistent at con-\nnecting underlying representation and task learning to zero-\nshot transfer. However, zero-shot CLIP only approaches\nfully supervised performance on 5 datasets: STL10, CI-\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\ndatasets, both zero-shot accuracy and fully supervised accu-\nracy are over 90%. This suggests that CLIP may be more\neffective at zero-shot transfer for tasks where its underly-\ning representations are also high quality. The slope of a\nlinear regression model predicting zero-shot performance\nas a function of fully supervised performance estimates that\nfor every 1% improvement in fully supervised performance,\nzero-shot performance improves by 1.28%. However, the\n95th-percentile conﬁdence intervals still include values of\nless than 1 (0.93-1.79)."}
{"doc_id": "2103.00020", "para_id": 91, "text": "have widely varying efﬁciency per dataset from less than 1\nlabeled example per class to 184. Two datasets, Flowers102\nand EuroSAT underperform one-shot models. Half of the\ndatasets require less than 5 examples per class with a median\nof 5.4. However, the mean estimated data efﬁciency is 20.8\nexamples per class. This is due to the 20% of datasets\nwhere supervised classiﬁers require many labeled examples\nper class in order to match performance. On ImageNet,\nzero-shot CLIP matches the performance of a 16-shot linear\nclassiﬁer trained on the same feature space."}
{"doc_id": "2103.00020", "para_id": 92, "text": "If we assume that evaluation datasets are large enough that\nthe parameters of linear classiﬁers trained on them are well\nestimated, then, because CLIP’s zero-shot classiﬁer is also\na linear classiﬁer, the performance of the fully supervised\nclassiﬁers roughly sets an upper bound for what zero-shot\ntransfer can achieve. In Figure 8 we compare CLIP’s zero-\nshot performance with fully supervised linear classiﬁers\nacross datasets. The dashed, y = x line represents an “op-\ntimal” zero-shot classiﬁer that matches the performance of\nits fully supervised equivalent. For most datasets, the per-\nformance of zero-shot classiﬁers still underperform fully su-\npervised classiﬁers by 10% to 25%, suggesting that there is\nstill plenty of headroom for improving CLIP’s task-learning\nand zero-shot transfer capabilities."}
{"doc_id": "2103.00020", "para_id": 93, "text": "Over the past few years, empirical studies of deep learning\nsystems have documented that performance is predictable as\na function of important quantities such as training compute\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\nThe GPT family of models has so far demonstrated consis-\ntent improvements in zero-shot performance across a 1000x\nincrease in training compute. In Figure 9, we check whether\nthe zero-shot performance of CLIP follows a similar scaling\npattern. We plot the average error rate of the 5 ResNet CLIP\nmodels across 39 evaluations on 36 different datasets and\nﬁnd that a similar log-log linear scaling trend holds for CLIP\nacross a 44x increase in model compute. While the overall\ntrend is smooth, we found that performance on individual\nevaluations can be much noisier. We are unsure whether"}
{"doc_id": "2103.00020", "para_id": 94, "text": "There is a positive correlation of 0.82 (p-value < 10−6)\nbetween zero-shot performance and fully supervised perfor-"}
{"doc_id": "2103.00020", "para_id": 95, "text": "Learning Transferable Visual Models From Natural Language Supervision\n11"}
{"doc_id": "2103.00020", "para_id": 96, "text": "classiﬁers has the added beneﬁt of being very similar to the\napproach used for its zero-shot classiﬁers which enables\nextensive comparisons and analysis in Section 3.1. Finally,\nwe aim to compare CLIP to a comprehensive set of existing\nmodels across many tasks. Studying 66 different models on\n27 different datasets requires tuning 1782 different evalua-\ntions. Fine-tuning opens up a much larger design and hyper-\nparameter space, which makes it difﬁcult to fairly evaluate\nand computationally expensive to compare a diverse set of\ntechniques as discussed in other large scale empirical studies\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\nclassiﬁers require minimal hyper-parameter tuning and have\nstandardized implementations and evaluation procedures.\nPlease see Appendix A for further details on evaluation."}
{"doc_id": "2103.00020", "para_id": 97, "text": "Figure 9. Zero-shot CLIP performance scales smoothly as a\nfunction of model compute. Across 39 evals on 36 different\ndatasets, average zero-shot error is well modeled by a log-log lin-\near trend across a 44x range of compute spanning 5 different CLIP\nmodels. Lightly shaded lines are performance on individual evals,\nshowing that performance is much more varied despite the smooth\noverall trend."}
{"doc_id": "2103.00020", "para_id": 98, "text": "Figure 10 summarizes our ﬁndings. To minimize selection\neffects that could raise concerns of conﬁrmation or reporting\nbias, we ﬁrst study performance on the 12 dataset evaluation\nsuite from Kornblith et al. (2019). While small CLIP mod-\nels such as a ResNet-50 and ResNet-101 outperform other\nResNets trained on ImageNet-1K (BiT-S and the originals),\nthey underperform ResNets trained on ImageNet-21K (BiT-\nM). These small CLIP models also underperform models\nin the EfﬁcientNet family with similar compute require-\nments. However, models trained with CLIP scale very well\nand the largest model we trained (ResNet-50x64) slightly\noutperforms the best performing existing model (a Noisy\nStudent EfﬁcientNet-L2) on both overall score and compute\nefﬁciency. We also ﬁnd that CLIP vision transformers are\nabout 3x more compute efﬁcient than CLIP ResNets, which\nallows us to reach higher overall performance within our\ncompute budget. These results qualitatively replicate the\nﬁndings of Dosovitskiy et al. (2020) which reported that\nvision transformers are more compute efﬁcient than con-\nvnets when trained on sufﬁciently large datasets. Our best\noverall model is a ViT-L/14 that is ﬁne-tuned at a higher res-\nolution of 336 pixels on our dataset for 1 additional epoch.\nThis model outperforms the best existing model across this\nevaluation suite by an average of 2.6%."}
{"doc_id": "2103.00020", "para_id": 99, "text": "this is caused by high variance between individual training\nruns on sub-tasks (as documented in D’Amour et al. (2020))\nmasking a steadily improving trend or whether performance\nis actually non-monotonic as a function of compute on some\ntasks."}
{"doc_id": "2103.00020", "para_id": 100, "text": "While we have extensively analyzed the task-learning ca-\npabilities of CLIP through zero-shot transfer in the previ-\nous section, it is more common to study the representation\nlearning capabilities of a model. There exist many ways to\nevaluate the quality of representations as well as disagree-\nments over what properties an “ideal” representation should\nhave (Locatello et al., 2020). Fitting a linear classiﬁer on\na representation extracted from the model and measuring\nits performance on various datasets is a common approach.\nAn alternative is measuring the performance of end-to-end\nﬁne-tuning of the model. This increases ﬂexibility, and\nprior work has convincingly demonstrated that ﬁne-tuning\noutperforms linear classiﬁcation on most image classiﬁ-\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\nWhile the high performance of ﬁne-tuning motivates its\nstudy for practical reasons, we still opt for linear classiﬁer\nbased evaluation for several reasons. Our work is focused\non developing a high-performing task and dataset-agnostic\npre-training approach. Fine-tuning, because it adapts rep-\nresentations to each dataset during the ﬁne-tuning phase,\ncan compensate for and potentially mask failures to learn\ngeneral and robust representations during the pre-training\nphase. Linear classiﬁers, because of their limited ﬂexibility,\ninstead highlight these failures and provide clear feedback\nduring development. For CLIP, training supervised linear"}
{"doc_id": "2103.00020", "para_id": 101, "text": "As Figure 21 qualitatively shows, CLIP models learn a wider\nset of tasks than has previously been demonstrated in a sin-\ngle computer vision model trained end-to-end from random\ninitialization. These tasks include geo-localization, optical\ncharacter recognition, facial emotion recognition, and action\nrecognition. None of these tasks are measured in the evalua-\ntion suite of Kornblith et al. (2019). This could be argued\nto be a form of selection bias in Kornblith et al. (2019)’s\nstudy towards tasks that overlap with ImageNet. To address\nthis, we also measure performance on a broader 27 dataset\nevaluation suite. This evaluation suite, detailed in Appendix\nA includes datasets representing the aforementioned tasks,\nGerman Trafﬁc Signs Recognition Benchmark (Stallkamp\net al., 2011), as well as several other datasets adapted from\nVTAB (Zhai et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 102, "text": "Learning Transferable Visual Models From Natural Language Supervision\n12"}
{"doc_id": "2103.00020", "para_id": 103, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 104, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet"}
{"doc_id": "2103.00020", "para_id": 105, "text": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\nEfﬁcientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ﬁne-tuned or\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset."}
{"doc_id": "2103.00020", "para_id": 106, "text": "On this broader evaluation suite, the beneﬁts of CLIP are\nmore clear. All CLIP models, regardless of scale, outper-\nform all evaluated systems in terms of compute efﬁciency.\nThe improvement in average score of the best model over\nprevious systems increases from 2.6% to 5%. We also ﬁnd\nthat self-supervised systems do noticeably better on our\nbroader evaluation suite. For instance, while SimCLRv2\nstill underperforms BiT-M on average on the 12 datasets\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\non our 27 dataset evaluation suite. These ﬁndings suggest\ncontinuing to expand task diversity and coverage in order\nto better understand the “general” performance of systems.\nWe suspect additional evaluation efforts along the lines of\nVTAB to be valuable."}
{"doc_id": "2103.00020", "para_id": 107, "text": "and HatefulMemes), geo-localization and scene recognition\n(Country211, SUN397), and activity recognition in videos\n(Kinetics700 and UCF101). In addition CLIP also does\nmuch better on ﬁne-grained car and trafﬁc sign recognition\n(Stanford Cars and GTSRB). This may reﬂect a problem\nwith overly narrow supervision in ImageNet. A result such\nas the 14.7% improvement on GTSRB could be indicative\nof an issue with ImageNet-1K, which has only a single la-\nbel for all trafﬁc and street signs. This could encourage\na supervised representation to collapse intra-class details\nand hurt accuracy on a ﬁne-grained downstream task. As\nmentioned, CLIP still underperforms the EfﬁcientNet on\nseveral datasets. Unsurprisingly, the dataset that the Efﬁ-\ncientNet does best relative to CLIP on is the one it was\ntrained on: ImageNet. The EffcientNet also slightly outper-\nforms CLIP on low-resolution datasets such as CIFAR10\nand CIFAR100. We suspect this is at least partly due to the\nlack of scale-based data augmentation in CLIP. The Efﬁ-\ncientNet also does slightly better on PatchCamelyon and\nCLEVRCounts, datasets where overall performance is still"}
{"doc_id": "2103.00020", "para_id": 108, "text": "In addition to the aggregate analysis above, we visualize\nper-dataset differences in the performance of the best CLIP\nmodel and the best model in our evaluation suite across\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\nStudent EfﬁcientNet-L2 on 21 of the 27 datasets. CLIP\nimproves the most on tasks which require OCR (SST2"}
{"doc_id": "2103.00020", "para_id": 109, "text": "Learning Transferable Visual Models From Natural Language Supervision\n13"}
{"doc_id": "2103.00020", "para_id": 110, "text": "combination of the two? CLIP models, which are trained via\nnatural language supervision on a very large dataset and are\ncapable of high zero-shot performance, are an opportunity\nto investigate this question from a different angle."}
{"doc_id": "2103.00020", "para_id": 111, "text": "Taori et al. (2020) is a recent comprehensive study mov-\ning towards quantifying and understanding these behaviors\nfor ImageNet models. Taori et al. (2020) study how the\nperformance of ImageNet models change when evaluated\non natural distribution shifts. They measure performance\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\nand ImageNet Rendition (Hendrycks et al., 2020a). They\ndistinguish these datasets, which all consist of novel images\ncollected from a variety of sources, from synthetic distri-\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\nsarial attacks (Goodfellow et al., 2014) which are created by\nperturbing existing images in various ways. They propose\nthis distinction because in part because they ﬁnd that while\nseveral techniques have been demonstrated to improve per-\nformance on synthetic distribution shifts, they often fail to\nyield consistent improvements on natural distributions.3"}
{"doc_id": "2103.00020", "para_id": 112, "text": "10\n5\n0\n5\n10\n15\n20\n25\nScore (%)\nLogistic Regression on CLIP vs. EfficientNet L2 NS"}
{"doc_id": "2103.00020", "para_id": 113, "text": "Figure 11. CLIP’s features outperform the features of the best\nImageNet model on a wide variety of datasets. Fitting a linear\nclassiﬁer on CLIP’s features outperforms using the Noisy Student\nEfﬁcientNet-L2 on 21 out of 27 datasets."}
{"doc_id": "2103.00020", "para_id": 114, "text": "Across these collected datasets, the accuracy of ImageNet\nmodels drop well below the expectation set by the Ima-\ngeNet validation set. For the following summary discussion\nwe report average accuracy across all 7 natural distribution\nshift datasets and average accuracy across the correspond-\ning class subsets of ImageNet unless otherwise speciﬁed.\nAdditionally, for Youtube-BB and ImageNet-Vid, which\nhave two different evaluation settings, we use the average\nof pm-0 and pm-10 accuracy."}
{"doc_id": "2103.00020", "para_id": 115, "text": "In 2015, it was announced that a deep learning model ex-\nceeded human performance on the ImageNet test set (He\net al., 2015). However, research in the subsequent years\nhas repeatedly found that these models still make many sim-\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\nAlcorn et al., 2019), and new benchmarks testing these sys-\ntems has often found their performance to be much lower\nthan both their ImageNet accuracy and human accuracy\n(Recht et al., 2019; Barbu et al., 2019). What explains this\ndiscrepancy? Various ideas have been suggested and stud-\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\ntheme of proposed explanations is that deep learning models\nare exceedingly adept at ﬁnding correlations and patterns\nwhich hold across their training dataset and thus improve\nin-distribution performance. However many of these corre-\nlations and patterns are actually spurious and do not hold for\nother distributions and result in large drops in performance\non other datasets."}
{"doc_id": "2103.00020", "para_id": 116, "text": "A ResNet-101 makes 5 times as many mistakes when eval-\nuated on these natural distribution shifts compared to the\nImageNet validation set. Encouragingly however, Taori et al.\n(2020) ﬁnd that accuracy under distribution shift increases\npredictably with ImageNet accuracy and is well modeled\nas a linear function of logit-transformed accuracy. Taori\net al. (2020) use this ﬁnding to propose that robustness\nanalysis should distinguish between effective and relative\nrobustness. Effective robustness measures improvements\nin accuracy under distribution shift above what is predicted\nby the documented relationship between in-distribution and\nout-of-distribution accuracy. Relative robustness captures\nany improvement in out-of-distribution accuracy. Taori et al.\n(2020) argue that robustness techniques should aim to im-\nprove both effective robustness and relative robustness."}
{"doc_id": "2103.00020", "para_id": 117, "text": "We caution that, to date, most of these studies limit their\nevaluation to models trained on ImageNet. Recalling the\ntopic of discussion, it may be a mistake to generalize too\nfar from these initial ﬁndings. To what degree are these\nfailures attributable to deep learning, ImageNet, or some"}
{"doc_id": "2103.00020", "para_id": 118, "text": "Almost all models studied in Taori et al. (2020) are trained"}
{"doc_id": "2103.00020", "para_id": 119, "text": "3We refer readers to Hendrycks et al. (2020a) for additional\nexperiments and discussion on this claim."}
{"doc_id": "2103.00020", "para_id": 120, "text": "Learning Transferable Visual Models From Natural Language Supervision\n14"}
{"doc_id": "2103.00020", "para_id": 121, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 122, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet"}
{"doc_id": "2103.00020", "para_id": 123, "text": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overﬁt to their task."}
{"doc_id": "2103.00020", "para_id": 124, "text": "or ﬁne-tuned on the ImageNet dataset. Returning to the\ndiscussion in the introduction to this section - is training\nor adapting to the ImageNet dataset distribution the cause\nof the observed robustness gap? Intuitively, a zero-shot\nmodel should not be able to exploit spurious correlations\nor patterns that hold only on a speciﬁc distribution, since it\nis not trained on that distribution. 4 Thus it is reasonable\nto expect zero-shot models to have much higher effective\nrobustness. In Figure 13, we compare the performance of\nzero-shot CLIP with existing ImageNet models on natural\ndistribution shifts. All zero-shot CLIP models improve\neffective robustness by a large amount and reduce the size\nof the gap between ImageNet accuracy and accuracy under\ndistribution shift by up to 75%."}
{"doc_id": "2103.00020", "para_id": 125, "text": "in much more robust models regardless of whether they\nare zero-shot or ﬁne-tuned. As an initial experiment to\npotentially begin narrowing this down, we also measure\nhow the performance of CLIP models change after adapting\nto the ImageNet distribution via a L2 regularized logistic\nregression classiﬁer ﬁt to CLIP features on the ImageNet\ntraining set. We visualize how performance changes from\nthe zero-shot classiﬁer in Figure 14. Although adapting\nCLIP to the ImageNet distribution increases its ImageNet\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\nof the 2018 SOTA from Mahajan et al. (2018), average\naccuracy under distribution shift slightly decreases."}
{"doc_id": "2103.00020", "para_id": 126, "text": "It is surprising to see a 9.2% increase in accuracy, which cor-\nresponds to roughly 3 years of improvement in SOTA, fail\nto translate into any improvement in average performance\nunder distribution shift. We also break down the differences\nbetween zero-shot accuracy and linear classiﬁer accuracy\nper dataset in Figure 14 and ﬁnd performance still increases\nsigniﬁcantly on one dataset, ImageNetV2. ImageNetV2\nclosely followed the creation process of the original Ima-\ngeNet dataset which suggests that gains in accuracy from\nsupervised adaptation are closely concentrated around the\nImageNet distribution. Performance decreases by 4.7% on"}
{"doc_id": "2103.00020", "para_id": 127, "text": "While these results show that zero-shot models can be much\nmore robust, they do not necessarily mean that supervised\nlearning on ImageNet causes a robustness gap. Other details\nof CLIP, such as its large and diverse pre-training dataset\nor use of natural language supervision could also result"}
{"doc_id": "2103.00020", "para_id": 128, "text": "4We caution that a zero-shot model can still exploit spurious\ncorrelations that are shared between the pre-training and evaluation\ndistributions."}
{"doc_id": "2103.00020", "para_id": 129, "text": "Learning Transferable Visual Models From Natural Language Supervision\n15"}
{"doc_id": "2103.00020", "para_id": 130, "text": "Ideal robust model (y = x)\nZero-Shot CLIP\nStandard ImageNet training\nExisiting robustness techniques\nImageNet"}
{"doc_id": "2103.00020", "para_id": 131, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 132, "text": "65\n70\n75\n80\n85\n90\n95\n100\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 133, "text": "Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\nthis “robustness gap” by up to 75%. Linear ﬁts on logit transformed values are shown with bootstrap estimated 95% conﬁdence intervals.\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\nset, ResNet-101."}
{"doc_id": "2103.00020", "para_id": 134, "text": "pooling predictions across all sub-classes according to the\nImageNet class hierarchy. Sometimes this mapping is much\nless than perfect. For the person class in Youtube-BB, pre-\ndictions are made by pooling over the ImageNet classes for\na baseball player, a bridegroom, and a scuba diver. With\nCLIP we can instead generate a custom zero-shot classi-\nﬁer for each dataset directly based on its class names. In\nFigure 14 we see that this improves average effective ro-\nbustness by 5% but is concentrated in large improvements\non only a few datasets. Curiously, accuracy on ObjectNet\nalso increases by 2.3%. Although the dataset was designed\nto closely overlap with ImageNet classes, using the names\nprovided for each class by ObjectNet’s creators still helps a\nsmall amount compared to using ImageNet class names and\npooling predictions when necessary."}
{"doc_id": "2103.00020", "para_id": 135, "text": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\nand 1.9% on ImageNet-A. The change in accuracy on the\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\nsigniﬁcant."}
{"doc_id": "2103.00020", "para_id": 136, "text": "How is it possible to improve accuracy by 9.2% on the Im-\nageNet dataset with little to no increase in accuracy under\ndistribution shift? Is the gain primarily from “exploiting\nspurious correlations”? Is this behavior unique to some com-\nbination of CLIP, the ImageNet datatset, and the distribution\nshifts studied, or a more general phenomena? Does it hold\nfor end-to-end ﬁnetuning as well as linear classiﬁers? We\ndo not have conﬁdent answers to these questions at this time.\nPrior work has also pre-trained models on distributions other\nthan ImageNet, but it is common to study and release mod-\nels only after they have been ﬁne-tuned to ImageNet. As a\nstep towards understanding whether pre-trained zero-shot\nmodels consistently have higher effective robustness than\nﬁne-tuned models, we encourage the authors of Mahajan\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\n(2020) to, if possible, study these questions on their models\nas well."}
{"doc_id": "2103.00020", "para_id": 137, "text": "While zero-shot CLIP improves effective robustness, Figure\n14 shows that the beneﬁt is almost entirely gone in a fully\nsupervised setting. To better understand this difference, we\ninvestigate how effective robustness changes on the contin-\nuum from zero-shot to fully supervised. In Figure 15 we\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\n..., 128-shot, and fully supervised logistic regression classi-\nﬁers on the best CLIP model’s features. We see that while\nfew-shot models also show higher effective robustness than\nexisting models, this beneﬁt fades as in-distribution per-\nformance increases with more training data and is mostly,\nthough not entirely, gone for the fully supervised model.\nAdditionally, zero-shot CLIP is notably more robust than\na few-shot model with equivalent ImageNet performance."}
{"doc_id": "2103.00020", "para_id": 138, "text": "We also investigate another robustness intervention enabled\nby ﬂexible zero-shot natural-language-based image classi-\nﬁers. The target classes across the 7 transfer datasets are\nnot always perfectly aligned with those of ImageNet. Two\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\nclasses of ImageNet. This presents a problem when trying\nto use the ﬁxed 1000-way classiﬁer of an ImageNet model\nto make predictions. Taori et al. (2020) handle this by max-"}
{"doc_id": "2103.00020", "para_id": 139, "text": "Learning Transferable Visual Models From Natural Language Supervision\n16"}
{"doc_id": "2103.00020", "para_id": 140, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 141, "text": "10\n5\n0\n5\n10\n15\n20\n25\n30\nChange from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 142, "text": "Ideal robust model (y = x)\nAdaptive Zero-Shot CLIP\nImageNet Zero-Shot CLIP\nLogistic Regression CLIP\nStandard ImageNet training\nRobustness intervention\nTrained with more data"}
{"doc_id": "2103.00020", "para_id": 143, "text": "10\n5\n0\n5\n10\n15\n20\n25\n30\nChange from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 144, "text": "70\n75\n80\n85\n90\n95\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 145, "text": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classiﬁer\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset speciﬁc zero-shot\nclassiﬁers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\nwith ImageNet categories."}
{"doc_id": "2103.00020", "para_id": 146, "text": "humans on one of our tasks. We wanted to get a sense of\nhow strong human zero-shot performance is at these tasks,\nand how much human performance is improved if they are\nshown one or two image samples. This can help us to\ncompare task difﬁculty for humans and CLIP, and identify\ncorrelations and differences between them."}
{"doc_id": "2103.00020", "para_id": 147, "text": "Across our experiments, high effective robustness seems to\nresult from minimizing the amount of distribution speciﬁc\ntraining data a model has access to, but this comes at a cost\nof reducing dataset-speciﬁc performance."}
{"doc_id": "2103.00020", "para_id": 148, "text": "Taken together, these results suggest that the recent shift\ntowards large-scale task and dataset agnostic pre-training\ncombined with a reorientation towards zero-shot and few-\nshot benchmarking on broad evaluation suites (as advocated\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\ndevelopment of more robust systems and provides a more\naccurate assessment of performance. We are curious to see\nif the same results hold for zero-shot models in the ﬁeld\nof NLP such as the GPT family. While Hendrycks et al.\n(2020b) has reported that pre-training improves relative ro-\nbustness on sentiment analysis, Miller et al. (2020)’s study\nof the robustness of question answering models under nat-\nural distribution shift ﬁnds, similar to Taori et al. (2020),\nlittle evidence of effective robustness improvements to date."}
{"doc_id": "2103.00020", "para_id": 149, "text": "We had ﬁve different humans look at each of 3669 images\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\n2012) and select which of the 37 cat or dog breeds best\nmatched the image (or ‘I don’t know’ if they were com-\npletely uncertain). In the zero-shot case the humans were\ngiven no examples of the breeds and asked to label them\nto the best of their ability without an internet search. In\nthe one-shot experiment the humans were given one sample\nimage of each breed and in the two-shot experiment they\nwere given two sample images of each breed.5"}
{"doc_id": "2103.00020", "para_id": 150, "text": "One possible concern was that the human workers were not\nsufﬁciently motivated in the zero-shot task. High human\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)"}
{"doc_id": "2103.00020", "para_id": 151, "text": "5There is not a perfect correspondence between the human\nfew-shot tasks and the model’s few-shot performance since the\nmodel cannot refer to sample images in the way that the humans\ncan."}
{"doc_id": "2103.00020", "para_id": 152, "text": "How does CLIP compare to human performance and human\nlearning? To get a better understanding of how well humans\nperform in similar evaluation settings to CLIP, we evaluated"}
{"doc_id": "2103.00020", "para_id": 153, "text": "Learning Transferable Visual Models From Natural Language Supervision\n17"}
{"doc_id": "2103.00020", "para_id": 154, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 155, "text": "Accuracy\nMajority Vote\non Full Dataset\nAccuracy\non Guesses"}
{"doc_id": "2103.00020", "para_id": 156, "text": "Zero-shot human\n53.7\n57.0\n69.7\n63.9\nZero-shot CLIP\n93.5\n93.5\n93.5\n93.5\nOne-shot human\n75.7\n80.3\n78.5\n81.2\nTwo-shot human\n75.7\n85.0\n79.2\n86.1"}
{"doc_id": "2103.00020", "para_id": 157, "text": "Table 2. Comparison of human performance on Oxford IIT Pets.\nAs in Parkhi et al. (2012), the metric is average per-class classiﬁca-\ntion accuracy. Most of the gain in performance when going from\nthe human zero shot case to the human one shot case is on images\nthat participants were highly uncertain on. “Guesses” refers to\nrestricting the dataset to where participants selected an answer\nother than “I don’t know”, the “majority vote” is taking the most\nfrequent (exclusive of ties) answer per image."}
{"doc_id": "2103.00020", "para_id": 158, "text": "Ideal robust model (y = x)\nFew-Shot CLIP (best model)\nZero-Shot CLIP (best model)\nStandard ImageNet training\nRobustness intervention\nTrained with more data"}
{"doc_id": "2103.00020", "para_id": 159, "text": "quality pre-trained model is near state-of-the-art for few\nshot learning (Tian et al., 2020), which suggests that there is\na gap between the best few-shot machine learning methods\nand human few-shot learning."}
{"doc_id": "2103.00020", "para_id": 160, "text": "65\n70\n75\n80\n85\n90\n95\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 161, "text": "Figure 15. Few-shot CLIP also increases effective robustness\ncompared to existing ImageNet models but is less robust than\nzero-shot CLIP. Minimizing the amount of ImageNet training\ndata used for adaption increases effective robustness at the cost of\ndecreasing relative robustness. 16-shot logistic regression CLIP\nmatches zero-shot CLIP on ImageNet, as previously reported in\nFigure 7, but is less robust."}
{"doc_id": "2103.00020", "para_id": 162, "text": "If we plot human accuracy vs CLIP’s zero shot accuracy\n(Figure 16), we see that the hardest problems for CLIP are\nalso hard for humans. To the extent that errors are consistent,\nour hypothesis is that this is due to at least a two factors:\nnoise in the dataset (including mislabeled images) and out of\ndistribution images being hard for both humans and models."}
{"doc_id": "2103.00020", "para_id": 163, "text": "and 97-100% accuracy on the subset of attention check\nimages increased our trust in the human workers."}
{"doc_id": "2103.00020", "para_id": 164, "text": "A concern with pre-training on a very large internet dataset\nis unintentional overlap with downstream evals. This is\nimportant to investigate since, in a worst-case scenario, a\ncomplete copy of an evaluation dataset could leak into the\npre-training dataset and invalidate the evaluation as a mean-\ningful test of generalization. One option to prevent this is to\nidentify and remove all duplicates before training a model.\nWhile this guarantees reporting true hold-out performance,\nit requires knowing all possible data which a model might\nbe evaluated on ahead of time. This has the downside of\nlimiting the scope of benchmarking and analysis. Adding a\nnew evaluation would require an expensive re-train or risk\nreporting an un-quantiﬁed beneﬁt due to overlap."}
{"doc_id": "2103.00020", "para_id": 165, "text": "Interestingly, humans went from a performance average of\n54% to 76% with just one training example per class, and\nthe marginal gain from an additional training example is\nminimal. The gain in accuracy going from zero to one shot\nis almost entirely on images that humans were uncertain\nabout. This suggests that humans “know what they don’t\nknow” and are able to update their priors on the images they\nare most uncertain in based on a single example. Given this,\nit seems that while CLIP is a promising training strategy\nfor zero-shot performance (Figure 5) and does well on tests\nof natural distribution shift (Figure 13), there is a large\ndifference between how humans learn from a few examples\nand the few-shot methods in this paper."}
{"doc_id": "2103.00020", "para_id": 166, "text": "Instead, we document how much overlap occurs and how\nperformance changes due to these overlaps. To do this, we\nuse the following procedure:"}
{"doc_id": "2103.00020", "para_id": 167, "text": "This suggests that there are still algorithmic improvements\nwaiting to be made to decrease the gap between machine\nand human sample efﬁciency, as noted by Lake et al. (2016)\nand others. Because these few-shot evaluations of CLIP\ndon’t make effective use of prior knowledge and the humans\ndo, we speculate that ﬁnding a method to properly integrate\nprior knowledge into few-shot learning is an important step\nin algorithmic improvements to CLIP. To our knowledge,\nusing a linear classiﬁer on top of the features of a high-"}
{"doc_id": "2103.00020", "para_id": 168, "text": "1) For each evaluation dataset, we run a duplicate detector\n(see Appendix C) on its examples. We then manually inspect\nthe found nearest neighbors and set a per dataset threshold\nto keep high precision while maximizing recall. Using\nthis threshold, we then create two new subsets, Overlap,\nwhich contains all examples which have a similarity to a\ntraining example above the threshold, and Clean, which"}
{"doc_id": "2103.00020", "para_id": 169, "text": "Learning Transferable Visual Models From Natural Language Supervision\n18"}
{"doc_id": "2103.00020", "para_id": 170, "text": "our analysis. There is a median overlap of 2.2% and an av-\nerage overlap of 3.2%. Due to this small amount of overlap,\noverall accuracy is rarely shifted by more than 0.1% with\nonly 7 datasets above this threshold. Of these, only 2 are\nstatistically signiﬁcant after Bonferroni correction. The max\ndetected improvement is only 0.6% on Birdsnap which has\nthe second largest overlap at 12.1%. The largest overlap is\nfor Country211 at 21.5%. This is due to it being constructed\nout of YFCC100M, which our pre-training dataset contains\na ﬁltered subset of. Despite this large overlap there is only\na 0.2% increase in accuracy on Country211. This may be\nbecause the training text accompanying an example is often\nnot related to the speciﬁc task a downstream eval measures.\nCountry211 measures geo-localization ability, but inspect-\ning the training text for these duplicates showed they often\ndo not mention the location of the image."}
{"doc_id": "2103.00020", "para_id": 171, "text": "We are aware of two potential concerns with our analysis.\nFirst our detector is not perfect. While it achieves near\n100% accuracy on its proxy training task and manual in-\nspection + threshold tuning results in very high precision\nwith good recall among the found nearest-neighbors, we can\nnot tractably check its recall across 400 million examples.\nAnother potential confounder of our analysis is that the un-\nderlying data distribution may shift between the Overlap\nand Clean subsets. For example, on Kinetics-700 many\n“overlaps” are in fact all black transition frames. This ex-\nplains why Kinetics-700 has an apparent 20% accuracy drop\non Overlap. We suspect more subtle distribution shifts\nlikely exist. One possibility we noticed on CIFAR-100 is\nthat, due to the very low resolution of its images, many\nduplicates were false positives of small objects such as birds\nor planes. Changes in accuracy could instead be due to\nchanges in the class distribution or difﬁculty of the dupli-\ncates. Unfortunately, these distribution and difﬁculty shifts\ncould also mask the effects of over-ﬁtting."}
{"doc_id": "2103.00020", "para_id": 172, "text": "Figure 16. The hardest problems for CLIP also tend to be the hard-\nest problems for humans. Here we rank image categories by difﬁ-\nculty for CLIP as measured as probability of the correct label."}
{"doc_id": "2103.00020", "para_id": 173, "text": "contains all examples that are below this threshold. We\ndenote the unaltered full dataset All for reference. From\nthis we ﬁrst record the degree of data contamination as the\nratio of the number of examples in Overlap to the size of\nAll."}
{"doc_id": "2103.00020", "para_id": 174, "text": "2) We then compute the zero-shot accuracy of CLIP\nRN50x64 on the three splits and report All - Clean\nas our main metric. This is the difference in accuracy due\nto contamination. When positive it is our estimate of how\nmuch the overall reported accuracy on the dataset was in-\nﬂated by over-ﬁtting to overlapping data."}
{"doc_id": "2103.00020", "para_id": 175, "text": "However, these results closely follow the ﬁndings of simi-\nlar duplicate analysis in previous work on large scale pre-\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\ndetected similar overlap rates and found minimal changes in\noverall performance. Importantly, Kolesnikov et al. (2019)\nalso compared the alternative de-duplication strategy dis-\ncussed in the introduction to this section with the approach\nwe settled on and observed little difference between the two\napproaches."}
{"doc_id": "2103.00020", "para_id": 176, "text": "3) The amount of overlap is often small so we also run a\nbinomial signiﬁcance test where we use the accuracy on\nClean as the null hypothesis and compute the one-tailed\n(greater) p-value for the Overlap subset. We also calculate\n99.5% Clopper-Pearson conﬁdence intervals on Dirty as\nanother check."}
{"doc_id": "2103.00020", "para_id": 177, "text": "A summary of this analysis is presented in Figure 17. Out\nof 35 datasets studied, 9 datasets have no detected overlap\nat all. Most of these datasets are synthetic or specialized\nmaking them unlikely to be posted as normal images on\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\nare guaranteed to have no overlap due to containing novel\ndata from after the date our dataset was created (ObjectNet\nand Hateful Memes). This demonstrates our detector has\na low-false positive rate which is important as false posi-\ntives would under-estimate the effect of contamination in"}
{"doc_id": "2103.00020", "para_id": 178, "text": "There are still many limitations to CLIP. While several of\nthese are discussed as part of analysis in various sections,\nwe summarize and collect them here."}
{"doc_id": "2103.00020", "para_id": 179, "text": "On datasets with training splits, the performance of zero-\nshot CLIP is on average competitive with the simple su-"}
{"doc_id": "2103.00020", "para_id": 180, "text": "Learning Transferable Visual Models From Natural Language Supervision\n19"}
{"doc_id": "2103.00020", "para_id": 181, "text": "Difference in Accuracy on Overlapping vs. Clean Data (%)"}
{"doc_id": "2103.00020", "para_id": 182, "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nDetected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 183, "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nDetected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 184, "text": "Figure 17. Few statistically signiﬁcant improvements in accuracy due to detected data overlap. (Left) While several datasets have\nup to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\n99.5% Clopper-Pearson conﬁdence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\nimprovements statistically signiﬁcant when calculated using a one-sided binomial test."}
{"doc_id": "2103.00020", "para_id": 185, "text": "pervised baseline of a linear classiﬁer on top of ResNet-50\nfeatures. On most of these datasets, the performance of\nthis baseline is now well below the overall state of the art.\nSigniﬁcant work is still needed to improve the task learning\nand transfer capabilities of CLIP. While scaling has so far\nsteadily improved performance and suggests a route for con-\ntinued improvement, we estimate around a 1000x increase\nin compute is required for zero-shot CLIP to reach overall\nstate-of-the-art performance. This is infeasible to train with\ncurrent hardware. Further research into improving upon the\ncomputational and data efﬁciency of CLIP will be necessary."}
{"doc_id": "2103.00020", "para_id": 186, "text": "CLIP learns a high quality semantic OCR representation that\nperforms well on digitally rendered text, which is common\nin its pre-training dataset, as evidenced by performance on\nRendered SST2. However, CLIP only achieves 88% accu-\nracy on the handwritten digits of MNIST. An embarrassingly\nsimple baseline of logistic regression on raw pixels outper-\nforms zero-shot CLIP. Both semantic and near-duplicate\nnearest-neighbor retrieval verify that there are almost no im-\nages that resemble MNIST digits in our pre-training dataset.\nThis suggests CLIP does little to address the underlying\nproblem of brittle generalization of deep learning models.\nInstead CLIP tries to circumvent the problem and hopes that\nby training on such a large and varied dataset that all data\nwill be effectively in-distribution. This is a naive assumption\nthat, as MNIST demonstrates, is easy to violate."}
{"doc_id": "2103.00020", "para_id": 187, "text": "Analysis in Section 3.1 found that CLIP’s zero-shot perfor-\nmance is still quite weak on several kinds of tasks. When\ncompared to task-speciﬁc models, the performance of CLIP\nis poor on several types of ﬁne-grained classiﬁcation such\nas differentiating models of cars, species of ﬂowers, and\nvariants of aircraft. CLIP also struggles with more abstract\nand systematic tasks such as counting the number of objects\nin an image. Finally for novel tasks which are unlikely to be\nincluded in CLIP’s pre-training dataset, such as classifying\nthe distance to the nearest car in a photo, CLIP’s perfor-\nmance can be near random. We are conﬁdent that there are\nstill many, many, tasks where CLIP’s zero-shot performance\nis near chance level."}
{"doc_id": "2103.00020", "para_id": 188, "text": "Although CLIP can ﬂexibly generate zero-shot classiﬁers\nfor a wide variety of tasks and datasets, CLIP is still limited\nto choosing from only those concepts in a given zero-shot\nclassiﬁer. This is a signiﬁcant restriction compared to a\ntruly ﬂexible approach like image captioning which could\ngenerate novel outputs. Unfortunately, as described in Sec-\ntion 2.3 we found the computational efﬁciency of the image\ncaption baseline we tried to be much lower than CLIP. A\nsimple idea worth trying is joint training of a contrastive\nand generative objective with the hope of combining the\nefﬁciency of CLIP with the ﬂexibility of a caption model.\nAs another alternative, search could be performed at infer-\nence time over many natural language explanations of a\ngiven image, similar to approach proposed in Learning with\nLatent Language Andreas et al. (2017)."}
{"doc_id": "2103.00020", "para_id": 189, "text": "While zero-shot CLIP generalizes well to many natural im-\nage distributions as investigated in Section 3.3, we’ve ob-\nserved that zero-shot CLIP still generalizes poorly to data\nthat is truly out-of-distribution for it. An illustrative exam-\nple occurs for the task of OCR as reported in Appendix E."}
{"doc_id": "2103.00020", "para_id": 190, "text": "Learning Transferable Visual Models From Natural Language Supervision\n20"}
{"doc_id": "2103.00020", "para_id": 191, "text": "CLIP also does not address the poor data efﬁciency of deep\nlearning. Instead CLIP compensates by using a source of\nsupervision that can be scaled to hundreds of millions of\ntraining examples. If every image seen during training of\na CLIP model was presented at a rate of one per second,\nit would take 405 years to iterate through the 12.8 billion\nimages seen over 32 training epochs. Combining CLIP\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\nself-training (Lee; Xie et al., 2020) methods is a promising\ndirection given their demonstrated ability to improve data\nefﬁciency over standard supervised learning."}
{"doc_id": "2103.00020", "para_id": 192, "text": "CLIP has a wide range of capabilities due to its ability to\ncarry out arbitrary image classiﬁcation tasks. One can give\nit images of cats and dogs and ask it to classify cats, or give\nit images taken in a department store and ask it to classify\nshoplifters–a task with signiﬁcant social implications and\nfor which AI may be unﬁt. Like any image classiﬁcation\nsystem, CLIP’s performance and ﬁtness for purpose need to\nbe evaluated, and its broader impacts analyzed in context.\nCLIP also introduces a capability that will magnify and alter\nsuch issues: CLIP makes it possible to easily create your\nown classes for categorization (to ‘roll your own classiﬁer’)\nwithout a need for re-training. This capability introduces\nchallenges similar to those found in characterizing other,\nlarge-scale generative models like GPT-3 (Brown et al.,\n2020); models that exhibit non-trivial zero-shot (or few-\nshot) generalization can have a vast range of capabilities,\nmany of which are made clear only after testing for them."}
{"doc_id": "2103.00020", "para_id": 193, "text": "Our methodology has several signiﬁcant limitations. De-\nspite our focus on zero-shot transfer, we repeatedly queried\nperformance on full validation sets to guide the develop-\nment of CLIP. These validation sets often have thousands\nof examples, which is unrealistic for true zero-shot sce-\nnarios. Similar concerns have been raised in the ﬁeld of\nsemi-supervised learning (Oliver et al., 2018). Another po-\ntential issue is our selection of evaluation datasets. While\nwe have reported results on Kornblith et al. (2019)’s 12\ndataset evaluation suite as a standardized collection, our\nmain results use a somewhat haphazardly assembled col-\nlection of 27 datasets that is undeniably co-adapted with\nthe development and capabilities of CLIP. Creating a new\nbenchmark of tasks designed explicitly to evaluate broad\nzero-shot transfer capabilities, rather than re-using existing\nsupervised datasets, would help address these issues."}
{"doc_id": "2103.00020", "para_id": 194, "text": "Our studies of CLIP in a zero-shot setting show that the\nmodel displays signiﬁcant promise for widely-applicable\ntasks like image retrieval or search. For example, it can ﬁnd\nrelevant images in a database given text, or relevant text\ngiven an image. Further, the relative ease of steering CLIP\ntoward bespoke applications with little or no additional data\nor training could unlock a variety of novel applications that\nare hard for us to envision today, as has occurred with large\nlanguage models over the past few years."}
{"doc_id": "2103.00020", "para_id": 195, "text": "CLIP is trained on text paired with images on the internet.\nThese image-text pairs are unﬁltered and uncurated and\nresult in CLIP models learning many social biases. This\nhas been previously demonstrated for image caption models\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\nfor detailed analysis and quantiﬁcation of these behaviors for\nCLIP as well as discussion of potential mitigation strategies."}
{"doc_id": "2103.00020", "para_id": 196, "text": "In addition to the more than 30 datasets studied in earlier\nsections of this paper, we evaluate CLIP’s performance on\nthe FairFace benchmark and undertake exploratory bias\nprobes. We then characterize the model’s performance in\na downstream task, surveillance, and discuss its usefulness\nas compared with other available systems. Many of CLIP’s\ncapabilities are omni-use in nature (e.g. OCR can be used\nto make scanned documents searchable, to power screen\nreading technologies, or to read license plates). Several\nof the capabilities measured, from action recognition, ob-\nject classiﬁcation, and geo-localization, to facial emotion\nrecognition, can be used in surveillance. Given its social\nimplications, we address this domain of use speciﬁcally in\nthe Surveillance section."}
{"doc_id": "2103.00020", "para_id": 197, "text": "While we have emphasized throughout this work that speci-\nfying image classiﬁers through natural language is a ﬂexible\nand general interface, it has its own limitations. Many com-\nplex tasks and visual concepts can be difﬁcult to specify\njust through text. Actual training examples are undeniably\nuseful but CLIP does not optimize for few-shot performance\ndirectly. In our work, we fall back to ﬁtting linear classiﬁers\non top of CLIP’s features. This results in a counter-intuitive\ndrop in performance when transitioning from a zero-shot\nto a few-shot setting. As discussed in Section 4, this is\nnotably different from human performance which shows a\nlarge increase from a zero to a one shot setting. Future work\nis needed to develop methods that combine CLIP’s strong\nzero-shot performance with efﬁcient few-shot learning."}
{"doc_id": "2103.00020", "para_id": 198, "text": "We have also sought to characterize the social biases inher-\nent to the model. Our bias tests represent our initial efforts\nto probe aspects of how the model responds in different sce-\nnarios, and are by nature limited in scope. CLIP and models\nlike it will need to be analyzed in relation to their speciﬁc\ndeployments to understand how bias manifests and iden-\ntify potential interventions. Further community exploration\nwill be required to develop broader, more contextual, and\nmore robust testing schemes so that AI developers can bet-\nter characterize biases in general purpose computer vision\nmodels."}
{"doc_id": "2103.00020", "para_id": 199, "text": "Learning Transferable Visual Models From Natural Language Supervision\n21"}
{"doc_id": "2103.00020", "para_id": 200, "text": "FairFace Model\n75.4\n94.4\n60.7\nLinear Probe CLIP\n92.8\n97.7\n63.1\nZero-Shot CLIP\n91.3\n97.2\n54.3\nLinear Probe Instagram\n87.2\n93.9\n54.1"}
{"doc_id": "2103.00020", "para_id": 201, "text": "FairFace Model\n93.7\n94.2\n59.7\nLinear Probe CLIP\n93.4\n96.5\n63.8\nZero-Shot CLIP\n58.3\n95.9\n57.1\nLinear Probe Instagram\n90.8\n93.2\n54.2"}
{"doc_id": "2103.00020", "para_id": 202, "text": "Table 4. Percent accuracy on Race, Gender, and Age classiﬁcation\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\ngether as FairFace category ‘Non-White’)"}
{"doc_id": "2103.00020", "para_id": 203, "text": "Table 3. Percent accuracy on Race, Gender, and Age classiﬁcation\nof images in FairFace category ‘White’"}
{"doc_id": "2103.00020", "para_id": 204, "text": "Middle Southeast East\nModel\nGender Black White Indian Latino Eastern\nAsian\nAsian Average"}
{"doc_id": "2103.00020", "para_id": 205, "text": "Male\n96.9\n96.4\n98.7\n96.5\n98.9\n96.2\n96.9\n97.2\nLinear Probe CLIP\nFemale\n97.9\n96.7\n97.9\n99.2\n97.2\n98.5\n97.3\n97.8\n97.4\n96.5\n98.3\n97.8\n98.4\n97.3\n97.1\n97.5"}
{"doc_id": "2103.00020", "para_id": 206, "text": "Male\n96.3\n96.4\n97.7\n97.2\n98.3\n95.5\n96.8\n96.9\nZero-Shot CLIP\nFemale\n97.1\n95.3\n98.3\n97.8\n97.5\n97.2\n96.4\n97.0\n96.7\n95.9\n98.0\n97.5\n98.0\n96.3\n96.6"}
{"doc_id": "2103.00020", "para_id": 207, "text": "Male\n92.5\n94.8\n96.2\n93.1\n96.0\n92.7\n93.4\n94.1\nLinear Probe Instagram Female\n90.1\n91.4\n95.0\n94.8\n95.0\n94.1\n94.3\n93.4\n91.3\n93.2\n95.6\n94.0\n95.6\n93.4\n93.9"}
{"doc_id": "2103.00020", "para_id": 208, "text": "Table 5. Percent accuracy on gender classiﬁcation of images by FairFace race category"}
{"doc_id": "2103.00020", "para_id": 209, "text": "as an initial bias probe, then probe the model further to\nsurface additional biases and sources of biases, including\nclass design."}
{"doc_id": "2103.00020", "para_id": 210, "text": "Algorithmic decisions, training data, and choices about how\nclasses are deﬁned and taxonomized (which we refer to in-\nformally as “class design”) can all contribute to and amplify\nsocial biases and inequalities resulting from the use of AI\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\n& Star, 2000). Class design is particularly relevant to mod-\nels like CLIP, since any developer can deﬁne a class and the\nmodel will provide some result."}
{"doc_id": "2103.00020", "para_id": 211, "text": "We evaluated two versions of CLIP on the FairFace dataset:\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\nsion classiﬁer ﬁtted to FairFace’s dataset on top of CLIP’s\nfeatures (“LR CLIP”). We ﬁnd that LR CLIP gets higher\naccuracy on the FairFace dataset than both the ResNext-101\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\nhajan et al., 2018) and FairFace’s own model on most of the\nclassiﬁcation tests we ran7. ZS CLIP’s performance varies\nby category and is worse than that of FairFace’s model for a\nfew categories, and better for others. (See Table 3 and Table\n4)."}
{"doc_id": "2103.00020", "para_id": 212, "text": "In this section, we provide preliminary analysis of some\nof the biases in CLIP, using bias probes inspired by those\noutlined in Buolamwini & Gebru (2018) and K¨arkk¨ainen\n& Joo (2019). We also conduct exploratory bias research\nintended to ﬁnd speciﬁc examples of biases in the model,\nsimilar to that conducted by Solaiman et al. (2019)."}
{"doc_id": "2103.00020", "para_id": 213, "text": "and Keyes (2018) have shown. While FairFace’s dataset reduces\nthe proportion of White faces, it still lacks representation of entire\nlarge demographic groups, effectively erasing such categories. We\nuse the 2 gender categories and 7 race categories deﬁned in the\nFairFace dataset in a number of our experiments not in order to\nreinforce or endorse the use of such reductive categories, but in\norder to enable us to make comparisons to prior work.\n7One challenge with this comparison is that the FairFace model\nuses binary classes for race (“White” and “Non-White”), instead\nof breaking down races into ﬁner-grained sub-groups."}
{"doc_id": "2103.00020", "para_id": 214, "text": "We start by analyzing the performance of Zero-Shot CLIP on\nthe face image dataset FairFace (K¨arkk¨ainen & Joo, 2019)6"}
{"doc_id": "2103.00020", "para_id": 215, "text": "6FairFace is a face image dataset designed to balance age, gen-\nder, and race, in order to reduce asymmetries common in previous\nface datasets. It categorizes gender into 2 groups: female and male\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\nAsian, Middle Eastern, and Latino. There are inherent problems\nwith race and gender classiﬁcations, as e.g. Bowker & Star (2000)"}
{"doc_id": "2103.00020", "para_id": 216, "text": "Learning Transferable Visual Models From Natural Language Supervision\n22"}
{"doc_id": "2103.00020", "para_id": 217, "text": "Middle\nSoutheast\nEast\nCategory\nBlack\nWhite\nIndian\nLatino\nEastern\nAsian\nAsian"}
{"doc_id": "2103.00020", "para_id": 218, "text": "Crime-related Categories\n16.4\n24.9\n24.4\n10.8\n19.7\n4.4\n1.3\nNon-human Categories\n14.4\n5.5\n7.6\n3.7\n2.0\n1.9\n0.0"}
{"doc_id": "2103.00020", "para_id": 219, "text": "Table 6. Percent of images classiﬁed into crime-related and non-human categories by FairFace Race category. The label set included 7\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 220, "text": "Category Label Set\n0-2\n3-9\n10-19\n20-29\n30-39\n40-49\n50-59\n60-69\nover 70"}
{"doc_id": "2103.00020", "para_id": 221, "text": "Default Label Set\n30.3\n35.0\n29.5\n16.3\n13.9\n18.5\n19.1\n16.2\n10.4\nDefault Label Set + ‘child’ category\n2.3\n4.3\n14.7\n15.0\n13.4\n18.2\n18.6\n15.5\n9.4"}
{"doc_id": "2103.00020", "para_id": 222, "text": "Table 7. Percent of images classiﬁed into crime-related and non-human categories by FairFace Age category, showing comparison between\nresults obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 223, "text": "We found that 4.9% (conﬁdence intervals between 4.6%\nand 5.4%) of the images were misclassiﬁed into one of\nthe non-human classes we used in our probes (‘animal’,\n‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\nimages had the highest misclassiﬁcation rate (approximately\n14%; conﬁdence intervals between [12.6% and 16.4%])\nwhile all other races had misclassiﬁcation rates under 8%.\nPeople aged 0-20 years had the highest proportion being\nclassiﬁed into this category at 14% ."}
{"doc_id": "2103.00020", "para_id": 224, "text": "Additionally, we test the performance of the LR CLIP and\nZS CLIP models across intersectional race and gender cate-\ngories as they are deﬁned in the FairFace dataset. We ﬁnd\nthat model performance on gender classiﬁcation is above\n95% for all race categories. Table 5 summarizes these re-\nsults."}
{"doc_id": "2103.00020", "para_id": 225, "text": "While LR CLIP achieves higher accuracy than the Linear\nProbe Instagram model on the FairFace benchmark dataset\nfor gender, race and age classiﬁcation of images by intersec-\ntional categories, accuracy on benchmarks offers only one\napproximation of algorithmic fairness, as Raji et al. (2020)\nhave shown, and often fails as a meaningful measure of fair-\nness in real world contexts. Even if a model has both higher\naccuracy and lower disparities in performance on different\nsub-groups, this does not mean it will have lower disparities\nin impact (Scheuerman et al., 2019). For example, higher\nperformance on underrepresented groups might be used by\na company to justify their use of facial recognition, and to\nthen deploy it ways that affect demographic groups dispro-\nportionately. Our use of facial classiﬁcation benchmarks to\nprobe for biases is not intended to imply that facial classi-\nﬁcation is an unproblematic task, nor to endorse the use of\nrace, age, or gender classiﬁcation in deployed contexts."}
{"doc_id": "2103.00020", "para_id": 226, "text": "We also found that 16.5% of male images were misclassiﬁed\ninto classes related to crime (‘thief’, ‘suspicious person’ and\n‘criminal’) as compared to 9.8% of female images. Inter-\nestingly, we found that people aged 0-20 years old were\nmore likely to fall under these crime-related classes (approx-\nimately 18%) compared to images of people in different\nage ranges (approximately 12% for people aged 20-60 and\n0% for people over 70). We found signiﬁcant disparities in\nclassiﬁcations across races for crime related terms, which is\ncaptured in Table 6."}
{"doc_id": "2103.00020", "para_id": 227, "text": "Given that we observed that people under 20 were the most\nlikely to be classiﬁed in both the crime-related and non-\nhuman animal categories, we carried out classiﬁcation for\nthe images with the same classes but with an additional\ncategory ‘child’ added to the categories. Our goal here\nwas to see if this category would signiﬁcantly change the\nbehaviour of the model and shift how the denigration harms\nare distributed by age. We found that this drastically reduced\nthe number of images of people under 20 classiﬁed in either\ncrime-related categories or non-human animal categories\n(Table 7). This points to how class design has the potential\nto be a key factor determining both the model performance\nand the unwanted biases or behaviour the model may exhibit\nwhile also asks overarching questions about the use of face"}
{"doc_id": "2103.00020", "para_id": 228, "text": "We also probed the model using classiﬁcation terms with\nhigh potential to cause representational harm, focusing on\ndenigration harms in particular (Crawford, 2017). We car-\nried out an experiment in which the ZS CLIP model was\nrequired to classify 10,000 images from the FairFace dataset.\nIn addition to the FairFace classes, we added in the follow-\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\n‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\nexperiment was to check if harms of denigration dispropor-\ntionately impact certain demographic subgroups."}
{"doc_id": "2103.00020", "para_id": 229, "text": "Learning Transferable Visual Models From Natural Language Supervision\n23"}
{"doc_id": "2103.00020", "para_id": 230, "text": "images to automatically classify people along such lines\n(y Arcas et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 231, "text": "When given the combined set of labels that Google Cloud\nVision (GCV), Amazon Rekognition and Microsoft returned\nfor all the images, similar to the biases Schwemmer et al.\n(2020) found in GCV systems, we found our system also\ndisproportionately attached labels to do with hair and ap-\npearance in general to women more than men. For ex-\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\nappeared signiﬁcantly more often for women. Additionally,\nCLIP attached some labels that described high status occu-\npations disproportionately more often to men such as ‘ex-\necutive’ and ‘doctor’. Out of the only four occupations that\nit attached more often to women, three were ‘newscaster’,\n‘television presenter’ and ‘newsreader’ and the fourth was\n‘Judge’. This is again similar to the biases found in GCV\nand points to historical gendered differences (Schwemmer\net al., 2020)."}
{"doc_id": "2103.00020", "para_id": 232, "text": "The results of these probes can change based on the class\ncategories one chooses to include as well as the speciﬁc\nlanguage one uses to describe each class. Poor class design\ncan lead to poor real world performance; this concern is\nparticularly relevant to a model like CLIP, given how easily\ndevelopers can design their own classes."}
{"doc_id": "2103.00020", "para_id": 233, "text": "We also carried out experiments similar to those outlined by\nSchwemmer et al. (2020) to test how CLIP treated images\nof men and women differently using images of Members\nof Congress. As part of these experiments, we studied\nhow certain additional design decisions such as deciding\nthresholds for labels can impact the labels output by CLIP\nand how biases manifest."}
{"doc_id": "2103.00020", "para_id": 234, "text": "We carried out three experiments - we tested for accuracy\non gender classiﬁcation and we tested for how labels were\ndifferentially distributed across two different label sets. For\nour ﬁrst label set, we used a label set of 300 occupations and\nfor our second label set we used a combined set of labels that\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\nAzure Computer Vision returned for all the images."}
{"doc_id": "2103.00020", "para_id": 235, "text": "Interestingly, when we lowered the threshold to 0.5% for\nthis set of labels, we found that the labels disproportionately\ndescribing men also shifted to appearance oriented words\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\ntion oriented words such as ‘military person’ and ‘executive’\n- which were not used to describe images of women at the\nhigher 4% threshold - were used for both men and women\nat the lower 0.5% threshold, which could have caused the\nchange in labels for men. The reverse was not true. Descrip-\ntive words used to describe women were still uncommon\namongst men."}
{"doc_id": "2103.00020", "para_id": 236, "text": "We ﬁrst simply looked into gender prediction performance\nof the model on the images of Members of Congress, in\norder to check to see if the model correctly recognized\nmen as men and women as women given the image of a\nperson who appeared to be in an ofﬁcial setting/position of\npower. We found that the model got 100% accuracy on the\nimages. This is slightly better performance than the model’s\nperformance on the FairFace dataset. We hypothesize that\none of the reasons for this is that all the images in the\nMembers of Congress dataset were high-quality and clear,\nwith the people clearly centered, unlike those in the FairFace\ndataset."}
{"doc_id": "2103.00020", "para_id": 237, "text": "Design decisions at every stage of building a model impact\nhow biases manifest and this is especially true for CLIP\ngiven the ﬂexibility it offers. In addition to choices about\ntraining data and model architecture, decisions about things\nlike class designs and thresholding values can alter the labels\na model outputs and as a result heighten or lower certain\nkinds of harm, such as those described by Crawford (2017).\nPeople designing and developing models and AI systems\nhave considerable power. Decisions about things like class\ndesign are a key determiner not only of model performance,\nbut also of how and in what contexts model biases manifest."}
{"doc_id": "2103.00020", "para_id": 238, "text": "In order to study how the biases in returned labels depend on\nthe thresholds set for label probability, we did an experiment\nin which we set threshold values at 0.5% and 4.0%. We\nfound that the lower threshold led to lower quality of labels.\nHowever, even the differing distributions of labels under\nthis threshold can hold signals for bias. For example, we\nﬁnd that under the 0.5% threshold labels such as ‘nanny’\nand ‘housekeeper’ start appearing for women whereas labels\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\nThis points to gendered associations similar to those that\nhave previously been found for occupations (Schwemmer\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016)."}
{"doc_id": "2103.00020", "para_id": 239, "text": "These experiments are not comprehensive.\nThey illus-\ntrate potential issues stemming from class design and other\nsources of bias, and are intended to spark inquiry."}
{"doc_id": "2103.00020", "para_id": 240, "text": "We next sought to characterize model performance in re-\nlation to a downstream task for which there is signiﬁcant\nsocietal sensitivity: surveillance. Our analysis aims to better\nembody the characterization approach described above and\nto help orient the research community towards the potential\nfuture impacts of increasingly general purpose computer\nvision models and aid the development of norms and checks"}
{"doc_id": "2103.00020", "para_id": 241, "text": "At the higher 4% threshold, the labels with the highest prob-\nability across both genders include “lawmaker”, “legislator”\nand “congressman”. However, the presence of these biases\namongst lower probability labels nonetheless point to larger\nquestions about what ‘sufﬁciently’ safe behaviour may look"}
{"doc_id": "2103.00020", "para_id": 242, "text": "Learning Transferable Visual Models From Natural Language Supervision\n24"}
{"doc_id": "2103.00020", "para_id": 243, "text": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\nidentiﬁed with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\ncertain label by gender."}
{"doc_id": "2103.00020", "para_id": 244, "text": "around such systems. Our inclusion of surveillance is not\nintended to indicate enthusiasm for this domain - rather, we\nthink surveillance is an important domain to try to make\npredictions about given its societal implications (Zuboff,\n2015; Browne, 2015)."}
{"doc_id": "2103.00020", "para_id": 245, "text": "the model to choose from. Additionally, we carried out a\n‘stress test’ where the class set included at least one more\ncaption for something that was ‘close’ to the image (for\nexample, ‘parking lot with white car’ vs. ‘parking lot with\nred car’). We found that the model had a top-1 accuracy\nof 91.8% on the CCTV images for the initial evaluation.\nThe accuracy dropped signiﬁcantly to 51.1% for the second\nevaluation, with the model incorrectly choosing the ‘close’\nanswer 40.7% of the time."}
{"doc_id": "2103.00020", "para_id": 246, "text": "We measure the model’s performance on classiﬁcation of\nimages from CCTV cameras and zero-shot celebrity identiﬁ-\ncation. We ﬁrst tested model performance on low-resolution\nimages captured from surveillance cameras (e.g. CCTV\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\ndata captured by Varadarajan & Odobez (2009), which both\nconsist of real world outdoor scenes with non-actors."}
{"doc_id": "2103.00020", "para_id": 247, "text": "For ﬁne-grained detection, the zero-shot model performed\npoorly, with results near random. Note that this experiment\nwas targeted only towards detecting the presence or absence\nof small objects in image sequences."}
{"doc_id": "2103.00020", "para_id": 248, "text": "Given CLIP’s ﬂexible class construction, we tested 515\nsurveillance images captured from 12 different video se-\nquences on self-constructed general classes for coarse and\nﬁne grained classiﬁcation. Coarse classiﬁcation required the\nmodel to correctly identify the main subject of the image (i.e.\ndetermine if the image was a picture of an empty parking\nlot, school campus, etc.). For ﬁne-grained classiﬁcation, the\nmodel had to choose between two options constructed to\ndetermine if the model could identify the presence/absence\nof smaller features in the image such as a person standing\nin the corner."}
{"doc_id": "2103.00020", "para_id": 249, "text": "We also tested CLIP’s zero-shot performance for ‘in the\nwild’ identity detection using the CelebA dataset8. We did\nthis to evaluate the model’s performance for identity detec-\ntion using just the publicly available data it was pre-trained\non. While we tested this on a dataset of celebrities who have\na larger number of images on the internet, we hypothesize\nthat the number of images in the pre-training data needed\nfor the model to associate faces with names will keep de-\ncreasing as models get more powerful (see Table 8), which\nhas signiﬁcant societal implications (Garvie, 2019). This"}
{"doc_id": "2103.00020", "para_id": 250, "text": "For coarse classiﬁcation, we constructed the classes by hand-\ncaptioning the images ourselves to describe the contents\nof the image and there were always at least 6 options for"}
{"doc_id": "2103.00020", "para_id": 251, "text": "8Note: The CelebA dataset is more representative of faces with\nlighter skin tones. Due to the nature of the dataset, we were not\nable to control for race, gender, age, etc."}
{"doc_id": "2103.00020", "para_id": 252, "text": "Learning Transferable Visual Models From Natural Language Supervision\n25"}
{"doc_id": "2103.00020", "para_id": 253, "text": "We hope that this work motivates future research on the\ncharacterization of the capabilities, shortcomings, and biases\nof such models, and we are excited to engage with the\nresearch community on such questions."}
{"doc_id": "2103.00020", "para_id": 254, "text": "CLIP L/14\n59.2\n43.3\n42.2\nCLIP RN50x64\n56.4\n39.5\n38.4\nCLIP RN50x16\n52.7\n37.4\n36.3\nCLIP RN50x4\n52.8\n38.1\n37.3"}
{"doc_id": "2103.00020", "para_id": 255, "text": "We believe one good step forward is community exploration\nto further characterize the capabilities of models like CLIP\nand - crucially - identify application areas where they have\npromising performance and areas where they may have\nreduced performance9. This process of characterization can\nhelp researchers increase the likelihood models are used\nbeneﬁcially by:"}
{"doc_id": "2103.00020", "para_id": 256, "text": "Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy"}
{"doc_id": "2103.00020", "para_id": 257, "text": "mirrors recent developments in natural language processing,\nin which recent large language models trained on Internet\ndata often exhibit a surprising ability to provide informa-\ntion related to relatively minor public ﬁgures (Brown et al.,\n2020)."}
{"doc_id": "2103.00020", "para_id": 258, "text": "• Identifying potentially beneﬁcial downstream uses of\nmodels early in the research process, enabling other\nresearchers to think about applications."}
{"doc_id": "2103.00020", "para_id": 259, "text": "We found that the model had 59.2% top-1 accuracy out\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\nages. However, this performance dropped to 43.3% when\nwe increased our class sizes to 1k celebrity names. This\nperformance is not competitive when compared to produc-\ntion level models such as Google’s Celebrity Recognition\n(Google). However, what makes these results noteworthy is\nthat this analysis was done using only zero-shot identiﬁca-\ntion capabilities based on names inferred from pre-training\ndata - we didn’t use any additional task-speciﬁc dataset, and\nso the (relatively) strong results further indicate that before\ndeploying multimodal models, people will need to carefully\nstudy them for behaviors in a given context and domain."}
{"doc_id": "2103.00020", "para_id": 260, "text": "• Surfacing tasks with signiﬁcant sensitivity and a large\nset of societal stakeholders, which may call for inter-\nvention by policymakers."}
{"doc_id": "2103.00020", "para_id": 261, "text": "• Better characterizing biases in models, alerting other\nresearchers to areas of concern and areas for interven-\ntions."}
{"doc_id": "2103.00020", "para_id": 262, "text": "• Creating suites of tests to evaluate systems like CLIP\non, so we can better characterize model capabilities\nearlier in the development cycle."}
{"doc_id": "2103.00020", "para_id": 263, "text": "• Identifying potential failure modes and areas for further\nwork."}
{"doc_id": "2103.00020", "para_id": 264, "text": "CLIP offers signiﬁcant beneﬁt for tasks that have relatively\nlittle data given its zero-shot capabilities. However, large\ndatasets and high performing supervised models exist for\nmany in-demand surveillance tasks such as facial recogni-\ntion. As a result, CLIP’s comparative appeal for such uses\nis low. Additionally, CLIP is not designed for common\nsurveillance-relevant tasks like object detection and seman-\ntic segmentation. This means it has limited use for certain\nsurveillance tasks when models that are designed with these\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\navailable."}
{"doc_id": "2103.00020", "para_id": 265, "text": "We plan to contribute to this work, and hope this analysis\nprovides some motivating examples for subsequent research."}
{"doc_id": "2103.00020", "para_id": 266, "text": "Any model that leverages written, spoken, signed or any\nother form of human language as part of its training signal\nis arguably using natural language as a source of supervi-\nsion. This is an admittedly extremely broad area and covers\nmost work in the ﬁeld of distributional semantics including\ntopic models (Blei et al., 2003), word, sentence, and para-\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\nMikolov, 2014), and language models (Bengio et al., 2003).\nIt also includes much of the broader ﬁeld of NLP that deals\nwith predicting or modeling sequences of natural language\nin some way. Work in NLP intentionally leveraging natural\nlanguage supervision in the form of explanations, feedback,\ninstructions, and advice for tasks such as classiﬁcation (as\nopposed to the commonly used representation of supervision\nas a set of arbitrarily encoded discrete category labels) has"}
{"doc_id": "2103.00020", "para_id": 267, "text": "However, CLIP does unlock a certain aspect of usability\ngiven how it removes the need for training data. Thus, CLIP\nand similar models could enable bespoke, niche surveillance\nuse cases for which no well-tailored models or datasets exist,\nand could lower the skill requirements to build such appli-\ncations. As our experiments show, ZS CLIP displays non-\ntrivial, but not exceptional, performance on a few surveil-\nlance relevant tasks today."}
{"doc_id": "2103.00020", "para_id": 268, "text": "This preliminary analysis is intended to illustrate some of\nthe challenges that general purpose computer vision models\npose and to give a glimpse into their biases and impacts."}
{"doc_id": "2103.00020", "para_id": 269, "text": "9A model could be unﬁt for use due to inadequate performance\nor due to the inappropriateness of AI use in the application area\nitself."}
{"doc_id": "2103.00020", "para_id": 270, "text": "Learning Transferable Visual Models From Natural Language Supervision\n26"}
{"doc_id": "2103.00020", "para_id": 271, "text": "large scale representation learning by training a system to\npair descriptive text with videos instead of images. Several\nworks have explored using dense spoken natural language\nsupervision for videos (Miech et al., 2019; 2020b). When\nconsidered together with CLIP, these works suggest that\nlarge scale natural language supervision is a promising way\nto learn high quality perceptual systems for many domains.\nAlayrac et al. (2020) extended this line of work to an addi-\ntional modality by adding raw audio as an additional super-\nvision source and demonstrated beneﬁts from combining all\nthree sources of supervision."}
{"doc_id": "2103.00020", "para_id": 272, "text": "been explored in many creative and advanced ways. Dialog\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\n2019) develops techniques to learn from interactive natural\nlanguage feedback in dialog. Several papers have leveraged\nsemantic parsing to convert natural language explanations\ninto features (Srivastava et al., 2017) or additional training\nlabels (Hancock et al., 2018). More recently, ExpBERT\n(Murty et al., 2020) uses feature representations produced\nby conditioning a deep contextual language model on nat-\nural language explanations and descriptions of relations to\nimprove performance on the task of relation extraction."}
{"doc_id": "2103.00020", "para_id": 273, "text": "CLIP is an example of using natural language as a training\nsignal for learning about a domain other than language. In\nthis context, the earliest use of the term natural language\nsupervision that we are aware of is the work of Ramanathan\net al. (2013) which showed that natural language descrip-\ntions could be used along side other sources of supervision\nto improve performance on the task of video event under-\nstanding. However, as mentioned in the introduction and\napproach section, methods of leveraging natural language\ndescriptions in computer vision well predate the use of this\nspeciﬁc term, especially for image retrieval (Mori et al.,\n1999) and object classiﬁcation (Wang et al., 2009). Other\nearly work leveraged tags (but not natural language) asso-\nciated with images for the task of semantic segmentation\n(Barnard et al., 2003). More recently, He & Peng (2017)\nand Liang et al. (2020) demonstrated using natural language\ndescriptions and explanations to improve ﬁne-grained vi-\nsual classiﬁcation of birds. Others have investigated how\ngrounded language can be used to improve visual represen-\ntations and classiﬁers on the ShapeWorld dataset (Kuhnle\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\nFinally, techniques which combine natural language with\nreinforcement learning environments (Narasimhan et al.,\n2015) have demonstrated exciting emergent behaviors such\nas systematically accomplishing zero-shot tasks (Hill et al.,\n2019)."}
{"doc_id": "2103.00020", "para_id": 274, "text": "As part of our work on CLIP we also construct a new dataset\nof image-text pairs. Modern work on image-text retrieval\nhas relied on a set of crowd-sourced sentence level im-\nage caption evaluation datasets like Pascal1K (Rashtchian\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\n(Young et al., 2014). However, these datasets are still rel-\natively small and limit achievable performance. Several\nmethods have been proposed to create larger datasets au-\ntomatically with Ordonez et al. (2011) as a notable early\nexample. In the deep learning era, Mithun et al. (2018)\ndemonstrated an additional set of (image, text) pairs col-\nlected from the internet could improve retrieval performance\nand several new automatically constructed datasets such as\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\n2020), and OCR-CC (Yang et al., 2020) have been created.\nHowever, these datasets still use signiﬁcantly more aggres-\nsive ﬁltering or are designed for a speciﬁc task such as OCR\nand as a result are still much smaller than WIT with between\n1 and 10 million training examples."}
{"doc_id": "2103.00020", "para_id": 275, "text": "A related idea to CLIP is webly supervised learning. This\nline of work queries image search engines to build image\ndatasets by querying for terms and uses the queries as the\nlabels for the returned images (Fergus et al., 2005). Classi-\nﬁers trained on these large but noisily labeled datasets can\nbe competitive with those trained on smaller carefully la-\nbeled datasets. These image-query pairs are also often used\nto improve performance on standard datasets as additional\ntraining data (Chen & Gupta, 2015). CLIP also uses search\nqueries as part of its dataset creation process. However\nCLIP only uses full text sequences co-occuring with images\nas supervision rather than just the queries, which are often\nonly a single word or short n-gram. We also restrict this step\nin CLIP to text only querying for sub-string matches while\nmost webly supervised work uses standard image search\nengines which have their own complex retrieval and ﬁlter-\ning pipelines that often involve computer vision systems.\nOf this line of work, Learning Everything about Anything:\nWebly-Supervised Visual Concept Learning (Divvala et al.,"}
{"doc_id": "2103.00020", "para_id": 276, "text": "CLIP’s pre-training task optimizes for text-image retrieval.\nThis areas of research dates back to the mid-90s with the\npreviously mentioned Mori et al. (1999) as representative of\nearly work. While initial efforts focused primarily on predic-\ntive objectives over time research shifted towards learning\njoint multi-modal embedding spaces with techniques like\nkernel Canonical Correlation Analysis and various ranking\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\nHodosh et al., 2013). Over time work explored many combi-\nnations of training objective, transfer, and more expressive\nmodels and steadily improved performance (Frome et al.,\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\n2014; Faghri et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 277, "text": "2014) has a notably similar ambition and goal as CLIP."}
{"doc_id": "2103.00020", "para_id": 278, "text": "Other work has leveraged natural language supervision for\ndomains other than images. Stroud et al. (2020) explores"}
{"doc_id": "2103.00020", "para_id": 279, "text": "Finally, CLIP is related to a recent burst of activity on learn-\ning joint models of vision and language (Lu et al., 2019; Tan"}
{"doc_id": "2103.00020", "para_id": 280, "text": "Learning Transferable Visual Models From Natural Language Supervision\n27"}
{"doc_id": "2103.00020", "para_id": 281, "text": "& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\n2020). This line of work focuses on richly connecting vision\nand language in order to solve complex downstream tasks\nsuch as visual question answering, visual commonsense\nreasoning, or multimodal entailment. These approaches\nleverage impressively engineered models which combine 3\n(or more) pre-trained subsystems, typically an image feature\nmodel, a region proposal / object detection model, and a\npre-trained masked language model such as BERT. These\nsystems are then jointly ﬁne-tuned via various training objec-\ntives on image-text pairs and applied to the aforementioned\ntasks and achieve impressive results. CLIP is instead fo-\ncused on learning visual models from scratch via natural\nlanguage supervision and does not densely connect the two\ndomains with a joint attention model. The only interaction\nin a CLIP model between the image and text domain is a\nsingle dot product in a learned joint embedding space. We\nare excited to see CLIP hybridized with this line of work."}
{"doc_id": "2103.00020", "para_id": 282, "text": "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorﬂow: A system for large-scale machine learning. In\n12th {USENIX} symposium on operating systems design\nand implementation ({OSDI} 16), pp. 265–283, 2016."}
{"doc_id": "2103.00020", "para_id": 283, "text": "Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi´c,\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\nand Zisserman, A. Self-supervised multimodal versatile\nnetworks. arXiv preprint arXiv:2006.16228, 2020."}
{"doc_id": "2103.00020", "para_id": 284, "text": "Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\nS., and Nguyen, A. Strike (with) a pose: Neural networks\nare easily fooled by strange poses of familiar objects. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 4845–4854, 2019."}
{"doc_id": "2103.00020", "para_id": 285, "text": "Andreas, J., Klein, D., and Levine, S. Learning with latent\nlanguage. arXiv preprint arXiv:1711.00482, 2017."}
{"doc_id": "2103.00020", "para_id": 286, "text": "We have investigated whether it is possible to transfer the\nsuccess of task-agnostic web-scale pre-training in NLP to\nanother domain. We ﬁnd that adopting this formula re-\nsults in similar behaviors emerging in the ﬁeld of computer\nvision and discuss the social implications of this line of\nresearch. In order to optimize their training objective, CLIP\nmodels learn to perform a wide variety of tasks during pre-\ntraining. This task learning can then be leveraged via natural\nlanguage prompting to enable zero-shot transfer to many\nexisting datasets. At sufﬁcient scale, the performance of this\napproach can be competitive with task-speciﬁc supervised\nmodels although there is still room for much improvement."}
{"doc_id": "2103.00020", "para_id": 287, "text": "Assiri, Y. Stochastic optimization of plain convolutional\nneural networks with simple methods. arXiv preprint\narXiv:2001.08856, 2020."}
{"doc_id": "2103.00020", "para_id": 288, "text": "Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning\nrepresentations by maximizing mutual information across\nviews. In Advances in Neural Information Processing\nSystems, pp. 15535–15545, 2019."}
{"doc_id": "2103.00020", "para_id": 289, "text": "Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\nlarge-scale bias-controlled dataset for pushing the lim-\nits of object recognition models. In Advances in Neural\nInformation Processing Systems, pp. 9453–9463, 2019."}
{"doc_id": "2103.00020", "para_id": 290, "text": "We’d like to thank the millions of people involved in creating\nthe data CLIP is trained on. We’d also like to thank Susan\nZhang for her work on image conditional language models\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\nGillian Hadﬁeld for their thoughtful feedback on the broader\nimpacts section of the paper. We are also grateful to the\nAcceleration and Supercomputing teams at OpenAI for their\ncritical work on software and hardware infrastructure this\nproject used. Finally, we’d also like to thank the developers\nof the many software packages used throughout this project\nincluding, but not limited, to Numpy (Harris et al., 2020),\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\npandas (pandas development team, 2020), and scikit-learn\n(Pedregosa et al., 2011)."}
{"doc_id": "2103.00020", "para_id": 291, "text": "Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\nD. M., and Jordan, M. I. Matching words and pictures.\nJournal of machine learning research, 3(Feb):1107–1135,\n2003."}
{"doc_id": "2103.00020", "para_id": 292, "text": "Bechmann, A. and Bowker, G. C. Unsupervised by any\nother name: Hidden layers of knowledge production in\nartiﬁcial intelligence on social media. Big Data & Society,\n6(1):205395171881956, January 2019. doi: 10.1177/\n2053951718819569. URL https://doi.org/10.\n1177/2053951718819569."}
{"doc_id": "2103.00020", "para_id": 293, "text": "Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research, 3(Feb):1137–1155, 2003."}
{"doc_id": "2103.00020", "para_id": 294, "text": "Bhargava, S. and Forsyth, D. Exposing and correcting the\ngender bias in image captioning datasets and models.\narXiv preprint arXiv:1912.00578, 2019."}
{"doc_id": "2103.00020", "para_id": 295, "text": "Learning Transferable Visual Models From Natural Language Supervision\n28"}
{"doc_id": "2103.00020", "para_id": 296, "text": "Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\nallocation. Journal of machine Learning research, 3(Jan):\n993–1022, 2003."}
{"doc_id": "2103.00020", "para_id": 297, "text": "Chen, X., Fan, H., Girshick, R., and He, K. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020d."}
{"doc_id": "2103.00020", "para_id": 298, "text": "Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\nCheng, Y., and Liu, J. Uniter: Learning universal image-\ntext representations. arXiv preprint arXiv:1909.11740,\n2019."}
{"doc_id": "2103.00020", "para_id": 299, "text": "Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\nKalai, A. T. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings. Advances\nin neural information processing systems, 29:4349–4357,\n2016."}
{"doc_id": "2103.00020", "para_id": 300, "text": "Cheng, G., Han, J., and Lu, X. Remote sensing image scene\nclassiﬁcation: Benchmark and state of the art. Proceed-\nings of the IEEE, 105(10):1865–1883, 2017."}
{"doc_id": "2103.00020", "para_id": 301, "text": "Bowker, G. C. and Star, S. L. Sorting things out: Classiﬁca-\ntion and its consequences. MIT press, 2000."}
{"doc_id": "2103.00020", "para_id": 302, "text": "Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\nand Dahl, G. E. On empirical comparisons of optimiz-\ners for deep learning. arXiv preprint arXiv:1910.05446,\n2019."}
{"doc_id": "2103.00020", "para_id": 303, "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020."}
{"doc_id": "2103.00020", "para_id": 304, "text": "Coates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\nartiﬁcial intelligence and statistics, pp. 215–223, 2011."}
{"doc_id": "2103.00020", "para_id": 305, "text": "Browne, S. Dark Matters: Surveillance of Blackness. Duke\nUniversity Press, 2015."}
{"doc_id": "2103.00020", "para_id": 306, "text": "Bulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\nvisual representations with caption annotations. arXiv\ne-prints, pp. arXiv–2008, 2020."}
{"doc_id": "2103.00020", "para_id": 307, "text": "Crawford, K.\nThe trouble with bias.\nNIPS 2017\nKeynote, 2017. URL https://www.youtube.com/\nwatch?v=fMym_BKWQzk."}
{"doc_id": "2103.00020", "para_id": 308, "text": "Buolamwini, J. and Gebru, T. Gender shades: Intersec-\ntional accuracy disparities in commercial gender classi-\nﬁcation. In Conference on fairness, accountability and\ntransparency, pp. 77–91, 2018."}
{"doc_id": "2103.00020", "para_id": 309, "text": "Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.\nIn Advances in neural information processing systems,\npp. 3079–3087, 2015."}
{"doc_id": "2103.00020", "para_id": 310, "text": "Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A\nshort note on the kinetics-700 human action dataset. arXiv\npreprint arXiv:1907.06987, 2019."}
{"doc_id": "2103.00020", "para_id": 311, "text": "D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\nJ., Hoffman, M. D., et al. Underspeciﬁcation presents\nchallenges for credibility in modern machine learning.\narXiv preprint arXiv:2011.03395, 2020."}
{"doc_id": "2103.00020", "para_id": 312, "text": "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\nD., and Sutskever, I. Generative pretraining from pixels.\nIn International Conference on Machine Learning, pp.\n1691–1703. PMLR, 2020a."}
{"doc_id": "2103.00020", "para_id": 313, "text": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09, 2009."}
{"doc_id": "2103.00020", "para_id": 314, "text": "Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016."}
{"doc_id": "2103.00020", "para_id": 315, "text": "Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\nimage-net.org/challenges/LSVRC/2012/."}
{"doc_id": "2103.00020", "para_id": 316, "text": "Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. arXiv preprint arXiv:2002.05709, 2020b."}
{"doc_id": "2103.00020", "para_id": 317, "text": "Desai, K. and Johnson, J. Virtex: Learning visual rep-\nresentations from textual annotations. arXiv preprint\narXiv:2006.06666, 2020."}
{"doc_id": "2103.00020", "para_id": 318, "text": "Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. Big self-supervised models are strong semi-\nsupervised learners. arXiv preprint arXiv:2006.10029,\n2020c."}
{"doc_id": "2103.00020", "para_id": 319, "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018."}
{"doc_id": "2103.00020", "para_id": 320, "text": "Chen, X. and Gupta, A.\nWebly supervised learning of\nconvolutional networks.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 1431–\n1439, 2015."}
{"doc_id": "2103.00020", "para_id": 321, "text": "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv preprint arXiv:2005.00341, 2020."}
{"doc_id": "2103.00020", "para_id": 322, "text": "Learning Transferable Visual Models From Natural Language Supervision\n29"}
{"doc_id": "2103.00020", "para_id": 323, "text": "Divvala, S. K., Farhadi, A., and Guestrin, C. Learning\neverything about anything: Webly-supervised visual con-\ncept learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 3270–\n3277, 2014."}
{"doc_id": "2103.00020", "para_id": 324, "text": "biased towards texture; increasing shape bias improves ac-\ncuracy and robustness. arXiv preprint arXiv:1811.12231,\n2018."}
{"doc_id": "2103.00020", "para_id": 325, "text": "Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\ncut learning in deep neural networks. arXiv preprint\narXiv:2004.07780, 2020."}
{"doc_id": "2103.00020", "para_id": 326, "text": "Dodge, S. and Karam, L. A study and comparison of human\nand deep learning recognition performance under visual\ndistortions. In 2017 26th international conference on\ncomputer communication and networks (ICCCN), pp. 1–\n7. IEEE, 2017."}
{"doc_id": "2103.00020", "para_id": 327, "text": "Gomez, L., Patel, Y., Rusi˜nol, M., Karatzas, D., and Jawahar,\nC. Self-supervised learning of visual features through\nembedding images into text topic spaces. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 4230–4239, 2017."}
{"doc_id": "2103.00020", "para_id": 328, "text": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020."}
{"doc_id": "2103.00020", "para_id": 329, "text": "Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572, 2014."}
{"doc_id": "2103.00020", "para_id": 330, "text": "Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\nﬁer: Zero-shot learning using purely textual descriptions.\nIn Proceedings of the IEEE International Conference on\nComputer Vision, pp. 2584–2591, 2013."}
{"doc_id": "2103.00020", "para_id": 331, "text": "Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\nD., Lee, D.-H., et al. Challenges in representation learn-\ning: A report on three machine learning contests. Neural\nNetworks, 64:59–63, 2015."}
{"doc_id": "2103.00020", "para_id": 332, "text": "Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\nproving visual-semantic embeddings with hard negatives.\narXiv preprint arXiv:1707.05612, 2017."}
{"doc_id": "2103.00020", "para_id": 333, "text": "Google. Google cloud api: Celebrity recognition. URL"}
{"doc_id": "2103.00020", "para_id": 334, "text": "https://cloud.google.com/vision/docs/\ncelebrity-recognition."}
{"doc_id": "2103.00020", "para_id": 335, "text": "Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\ning object categories from google’s image search. In\nTenth IEEE International Conference on Computer Vision\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\n2005."}
{"doc_id": "2103.00020", "para_id": 336, "text": "Griewank, A. and Walther, A. Algorithm 799: revolve: an\nimplementation of checkpointing for the reverse or ad-\njoint mode of computational differentiation. ACM Trans-\nactions on Mathematical Software (TOMS), 26(1):19–45,\n2000."}
{"doc_id": "2103.00020", "para_id": 337, "text": "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\nRanzato, M., and Mikolov, T. Devise: A deep visual-\nsemantic embedding model. In Advances in neural infor-\nmation processing systems, pp. 2121–2129, 2013."}
{"doc_id": "2103.00020", "para_id": 338, "text": "Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020."}
{"doc_id": "2103.00020", "para_id": 339, "text": "Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\nLarge-scale adversarial training for vision-and-language\nrepresentation learning. arXiv preprint arXiv:2006.06195,\n2020."}
{"doc_id": "2103.00020", "para_id": 340, "text": "Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv\npreprint arXiv:1609.09106, 2016."}
{"doc_id": "2103.00020", "para_id": 341, "text": "Hancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\nS., and R´e, C. Training classiﬁers with natural language\nexplanations. In Proceedings of the conference. Associ-\nation for Computational Linguistics. Meeting, volume\n2018, pp. 1884. NIH Public Access, 2018."}
{"doc_id": "2103.00020", "para_id": 342, "text": "Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-\nguage models better few-shot learners. arXiv preprint\narXiv:2012.15723, 2020."}
{"doc_id": "2103.00020", "para_id": 343, "text": "Garvie,\nC.,\nMay\n2019.\nURL\nhttps://www.\nflawedfacedata.com/."}
{"doc_id": "2103.00020", "para_id": 344, "text": "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\nLearning from dialogue after deployment: Feed yourself,\nchatbot! arXiv preprint arXiv:1901.05415, 2019."}
{"doc_id": "2103.00020", "para_id": 345, "text": "Geiger, A., Lenz, P., and Urtasun, R. Are we ready for\nautonomous driving? the kitti vision benchmark suite. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2012."}
{"doc_id": "2103.00020", "para_id": 346, "text": "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\nKerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del"}
{"doc_id": "2103.00020", "para_id": 347, "text": "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\nmann, F. A., and Brendel, W. Imagenet-trained cnns are"}
{"doc_id": "2103.00020", "para_id": 348, "text": "Learning Transferable Visual Models From Natural Language Supervision\n30"}
{"doc_id": "2103.00020", "para_id": 349, "text": "R´ıo, J., Wiebe, M., Peterson, P., G´erard-Marchant, P.,\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\nGohlke, C., and Oliphant, T. E. Array programming\nwith NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\ns41586-020-2649-2."}
{"doc_id": "2103.00020", "para_id": 350, "text": "Hendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016."}
{"doc_id": "2103.00020", "para_id": 351, "text": "Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\nSong, D. Natural adversarial examples. arXiv preprint\narXiv:1907.07174, 2019."}
{"doc_id": "2103.00020", "para_id": 352, "text": "Hays, J. and Efros, A. A. Im2gps: estimating geographic\ninformation from a single image. In 2008 ieee confer-\nence on computer vision and pattern recognition, pp. 1–8.\nIEEE, 2008."}
{"doc_id": "2103.00020", "para_id": 353, "text": "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\net al. The many faces of robustness: A critical analy-\nsis of out-of-distribution generalization. arXiv preprint\narXiv:2006.16241, 2020a."}
{"doc_id": "2103.00020", "para_id": 354, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectiﬁers: Surpassing human-level performance on\nimagenet classiﬁcation. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 1026–1034,\n2015."}
{"doc_id": "2103.00020", "para_id": 355, "text": "Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\nR., and Song, D. Pretrained transformers improve out-of-\ndistribution robustness. arXiv preprint arXiv:2004.06100,\n2020b."}
{"doc_id": "2103.00020", "para_id": 356, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016a."}
{"doc_id": "2103.00020", "para_id": 357, "text": "Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\nY. Deep learning scaling is predictable, empirically. arXiv\npreprint arXiv:1712.00409, 2017."}
{"doc_id": "2103.00020", "para_id": 358, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016b."}
{"doc_id": "2103.00020", "para_id": 359, "text": "Hill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\nM., McClelland, J. L., and Santoro, A. Environmental\ndrivers of systematicity and generalization in a situated\nagent. In International Conference on Learning Repre-\nsentations, 2019."}
{"doc_id": "2103.00020", "para_id": 360, "text": "He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9729–\n9738, 2020."}
{"doc_id": "2103.00020", "para_id": 361, "text": "Hodosh, M., Young, P., and Hockenmaier, J. Framing image\ndescription as a ranking task: Data, models and evaluation\nmetrics. Journal of Artiﬁcial Intelligence Research, 47:\n853–899, 2013."}
{"doc_id": "2103.00020", "para_id": 362, "text": "Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\nEnhancing image geolocalization by combinatorial parti-\ntioning of maps. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pp. 536–551, 2018."}
{"doc_id": "2103.00020", "para_id": 363, "text": "He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\nBag of tricks for image classiﬁcation with convolutional\nneural networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 558–\n567, 2019."}
{"doc_id": "2103.00020", "para_id": 364, "text": "Howard, J. and Ruder, S.\nUniversal language model\nﬁne-tuning for text classiﬁcation.\narXiv preprint\narXiv:1801.06146, 2018."}
{"doc_id": "2103.00020", "para_id": 365, "text": "He, X. and Peng, Y. Fine-grained image classiﬁcation via\ncombining vision and language. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 5994–6002, 2017."}
{"doc_id": "2103.00020", "para_id": 366, "text": "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\nB., and Madry, A. Adversarial examples are not bugs,\nthey are features. In Advances in Neural Information\nProcessing Systems, pp. 125–136, 2019."}
{"doc_id": "2103.00020", "para_id": 367, "text": "Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classiﬁcation. IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing, 12(7):2217–2226, 2019."}
{"doc_id": "2103.00020", "para_id": 368, "text": "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167, 2015."}
{"doc_id": "2103.00020", "para_id": 369, "text": "Henaff, O. Data-efﬁcient image recognition with contrastive\npredictive coding. In International Conference on Ma-\nchine Learning, pp. 4182–4192. PMLR, 2020."}
{"doc_id": "2103.00020", "para_id": 370, "text": "Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\nA. Deep structured output learning for unconstrained text\nrecognition. arXiv preprint arXiv:1412.5903, 2014."}
{"doc_id": "2103.00020", "para_id": 371, "text": "Hendrycks, D. and Dietterich, T. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261, 2019."}
{"doc_id": "2103.00020", "para_id": 372, "text": "Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\ntransformer networks. Advances in neural information\nprocessing systems, 28:2017–2025, 2015."}
{"doc_id": "2103.00020", "para_id": 373, "text": "Learning Transferable Visual Models From Natural Language Supervision\n31"}
{"doc_id": "2103.00020", "para_id": 374, "text": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 123(1):32–73,\n2017."}
{"doc_id": "2103.00020", "para_id": 375, "text": "Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\nnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pp.\n2901–2910, 2017."}
{"doc_id": "2103.00020", "para_id": 376, "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in neural information processing systems,\npp. 1097–1105, 2012."}
{"doc_id": "2103.00020", "para_id": 377, "text": "Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\nLearning visual features from large weakly supervised\ndata. In European Conference on Computer Vision, pp.\n67–84. Springer, 2016."}
{"doc_id": "2103.00020", "para_id": 378, "text": "Kuhnle, A. and Copestake, A.\nShapeworld-a new test\nmethodology for multimodal language understanding.\narXiv preprint arXiv:1704.04517, 2017."}
{"doc_id": "2103.00020", "para_id": 379, "text": "Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\nmodeling in 3d cnn architectures with bert for action\nrecognition. arXiv preprint arXiv:2008.01232, 2020."}
{"doc_id": "2103.00020", "para_id": 380, "text": "K¨arkk¨ainen, K. and Joo, J. Fairface: Face attribute dataset\nfor balanced race, gender, and age, 2019."}
{"doc_id": "2103.00020", "para_id": 381, "text": "Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020."}
{"doc_id": "2103.00020", "para_id": 382, "text": "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\nman, S. J. Building machines that learn and think like\npeople, 2016."}
{"doc_id": "2103.00020", "para_id": 383, "text": "Karpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\nembeddings for bidirectional image sentence mapping.\nIn Advances in neural information processing systems,\npp. 1889–1897, 2014."}
{"doc_id": "2103.00020", "para_id": 384, "text": "Lampert, C. H., Nickisch, H., and Harmeling, S. Learning\nto detect unseen object classes by between-class attribute\ntransfer. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 951–958. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 385, "text": "Keyes, O. The misgendering machines: Trans/hci implica-\ntions of automatic gender recognition. Proceedings of the\nACM on Human-Computer Interaction, 2(CSCW):1–22,\n2018."}
{"doc_id": "2103.00020", "para_id": 386, "text": "Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\nof new tasks. 2008."}
{"doc_id": "2103.00020", "para_id": 387, "text": "Le, Q. and Mikolov, T. Distributed representations of sen-\ntences and documents. In International conference on\nmachine learning, pp. 1188–1196, 2014."}
{"doc_id": "2103.00020", "para_id": 388, "text": "Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\nRingshia, P., and Testuggine, D. The hateful memes\nchallenge: Detecting hate speech in multimodal memes.\narXiv preprint arXiv:2005.04790, 2020."}
{"doc_id": "2103.00020", "para_id": 389, "text": "LeCun, Y.\nThe mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/."}
{"doc_id": "2103.00020", "para_id": 390, "text": "Kingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014."}
{"doc_id": "2103.00020", "para_id": 391, "text": "Lee, D.-H. Pseudo-label: The simple and efﬁcient semi-\nsupervised learning method for deep neural networks."}
{"doc_id": "2103.00020", "para_id": 392, "text": "Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. arXiv preprint arXiv:1411.2539, 2014."}
{"doc_id": "2103.00020", "para_id": 393, "text": "Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\nzero-shot convolutional neural networks using textual\ndescriptions. In Proceedings of the IEEE International\nConference on Computer Vision, pp. 4247–4255, 2015."}
{"doc_id": "2103.00020", "para_id": 394, "text": "Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\nAdvances in neural information processing systems, 28:\n3294–3302, 2015."}
{"doc_id": "2103.00020", "para_id": 395, "text": "Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\nvisual n-grams from web data. In Proceedings of the\nIEEE International Conference on Computer Vision, pp.\n4183–4192, 2017."}
{"doc_id": "2103.00020", "para_id": 396, "text": "Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\nJ., Gelly, S., and Houlsby, N. Large scale learning of\ngeneral visual representations for transfer. arXiv preprint\narXiv:1912.11370, 2019."}
{"doc_id": "2103.00020", "para_id": 397, "text": "Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\nUnicoder-vl: A universal encoder for vision and language\nby cross-modal pre-training. 2020a."}
{"doc_id": "2103.00020", "para_id": 398, "text": "Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\nmodels transfer better?\nIn Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2661–2671, 2019."}
{"doc_id": "2103.00020", "para_id": 399, "text": "Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\nLearning through dialogue interactions by asking ques-\ntions. arXiv preprint arXiv:1612.04936, 2016."}
{"doc_id": "2103.00020", "para_id": 400, "text": "Learning Transferable Visual Models From Natural Language Supervision\n32"}
{"doc_id": "2103.00020", "para_id": 401, "text": "Proceedings of the European Conference on Computer\nVision (ECCV), pp. 181–196, 2018."}
{"doc_id": "2103.00020", "para_id": 402, "text": "Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., et al.\nOscar: Object-\nsemantics aligned pre-training for vision-language tasks.\narXiv preprint arXiv:2004.06165, 2020b."}
{"doc_id": "2103.00020", "para_id": 403, "text": "McCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors. In\nAdvances in neural information processing systems, pp.\n6294–6305, 2017."}
{"doc_id": "2103.00020", "para_id": 404, "text": "Liang, W., Zou, J., and Yu, Z. Alice: Active learning with\ncontrastive natural language explanations. arXiv preprint\narXiv:2009.10259, 2020."}
{"doc_id": "2103.00020", "para_id": 405, "text": "McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\nnatural language decathlon: Multitask learning as ques-\ntion answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2103.00020", "para_id": 406, "text": "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In European conference on\ncomputer vision, pp. 740–755. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 407, "text": "Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740, 2017."}
{"doc_id": "2103.00020", "para_id": 408, "text": "Linzen, T.\nHow can we accelerate progress towards\nhuman-like linguistic generalization?\narXiv preprint\narXiv:2005.00955, 2020."}
{"doc_id": "2103.00020", "para_id": 409, "text": "Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\nI., and Sivic, J. Howto100m: Learning a text-video em-\nbedding by watching hundred million narrated video clips.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pp. 2630–2640, 2019."}
{"doc_id": "2103.00020", "para_id": 410, "text": "Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\ntimodal framework for the detection of hateful memes.\narXiv preprint arXiv:2012.12871, 2020."}
{"doc_id": "2103.00020", "para_id": 411, "text": "Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\nssi, R., Kaiser, L., and Shazeer, N.\nGenerating\nwikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018."}
{"doc_id": "2103.00020", "para_id": 412, "text": "Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\nman, A. Rareact: A video dataset of unusual interactions.\narXiv preprint arXiv:2008.01018, 2020a."}
{"doc_id": "2103.00020", "para_id": 413, "text": "Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\nand Zisserman, A. End-to-end learning of visual represen-\ntations from uncurated instructional videos. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 9879–9889, 2020b."}
{"doc_id": "2103.00020", "para_id": 414, "text": "Locatello, F., Bauer, S., Lucic, M., R¨atsch, G., Gelly, S.,\nSch¨olkopf, B., and Bachem, O.\nA sober look at the\nunsupervised learning of disentangled representations\nand their evaluation. arXiv preprint arXiv:2010.14766,\n2020."}
{"doc_id": "2103.00020", "para_id": 415, "text": "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. Advances in neural informa-\ntion processing systems, 26:3111–3119, 2013."}
{"doc_id": "2103.00020", "para_id": 416, "text": "Loshchilov, I. and Hutter, F.\nSgdr:\nStochastic gra-\ndient descent with warm restarts.\narXiv preprint\narXiv:1608.03983, 2016."}
{"doc_id": "2103.00020", "para_id": 417, "text": "Loshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017."}
{"doc_id": "2103.00020", "para_id": 418, "text": "Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\nof natural distribution shift on question answering models.\narXiv preprint arXiv:2004.14444, 2020."}
{"doc_id": "2103.00020", "para_id": 419, "text": "Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. In Advances in Neural Information\nProcessing Systems, pp. 13–23, 2019."}
{"doc_id": "2103.00020", "para_id": 420, "text": "Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\ntion using higher order language priors. 2012."}
{"doc_id": "2103.00020", "para_id": 421, "text": "Mithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\nChowdhury, A. K. Webly supervised joint embedding for\ncross-modal image-text retrieval. In Proceedings of the\n26th ACM international conference on Multimedia, pp.\n1856–1864, 2018."}
{"doc_id": "2103.00020", "para_id": 422, "text": "Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\nweakly supervised data and pose representation for action\nrecognition, 2020.\nURL https://www.youtube.\ncom/watch?v=KOQFxbPPLOE&t=1390s."}
{"doc_id": "2103.00020", "para_id": 423, "text": "Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\nquet, O. Are gans created equal? a large-scale study.\nAdvances in neural information processing systems, 31:\n700–709, 2018."}
{"doc_id": "2103.00020", "para_id": 424, "text": "Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\nformation based on dividing and vector quantizing images\nwith words. Citeseer, 1999."}
{"doc_id": "2103.00020", "para_id": 425, "text": "Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\nM., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\nploring the limits of weakly supervised pretraining. In"}
{"doc_id": "2103.00020", "para_id": 426, "text": "Mu, J., Liang, P., and Goodman, N. Shaping visual represen-\ntations with language for few-shot classiﬁcation. arXiv\npreprint arXiv:1911.02683, 2019."}
{"doc_id": "2103.00020", "para_id": 427, "text": "Learning Transferable Visual Models From Natural Language Supervision\n33"}
{"doc_id": "2103.00020", "para_id": 428, "text": "Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\ncation estimation of photos using a hierarchical model\nand scene classiﬁcation. In Proceedings of the European\nConference on Computer Vision (ECCV), pp. 563–579,\n2018."}
{"doc_id": "2103.00020", "para_id": 429, "text": "Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances\nin Neural Information Processing Systems 32, pp. 8024–\n8035, 2019."}
{"doc_id": "2103.00020", "para_id": 430, "text": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in Python. Journal of\nMachine Learning Research, 12:2825–2830, 2011."}
{"doc_id": "2103.00020", "para_id": 431, "text": "Murty, S., Koh, P. W., and Liang, P. Expbert: Representation\nengineering with natural language explanations. arXiv\npreprint arXiv:2005.01932, 2020."}
{"doc_id": "2103.00020", "para_id": 432, "text": "Narasimhan, K., Kulkarni, T., and Barzilay, R. Language\nunderstanding for text-based games using deep reinforce-\nment learning. arXiv preprint arXiv:1506.08941, 2015."}
{"doc_id": "2103.00020", "para_id": 433, "text": "Pennington, J., Socher, R., and Manning, C. D. Glove:\nGlobal vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pp. 1532–1543, 2014."}
{"doc_id": "2103.00020", "para_id": 434, "text": "Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y. Reading digits in natural images with\nunsupervised feature learning. 2011."}
{"doc_id": "2103.00020", "para_id": 435, "text": "Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365,\n2018."}
{"doc_id": "2103.00020", "para_id": 436, "text": "Noble, S. U. Algorithms of oppression: How search engines\nreinforce racism. 2018."}
{"doc_id": "2103.00020", "para_id": 437, "text": "Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\ning implicit group attitudes and beliefs from a demonstra-\ntion web site. Group Dynamics: Theory, Research, and\nPractice, 6(1):101, 2002."}
{"doc_id": "2103.00020", "para_id": 438, "text": "Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\nA.\nImagebert: Cross-modal pre-training with large-\nscale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966, 2020."}
{"doc_id": "2103.00020", "para_id": 439, "text": "Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\nA large-scale benchmark dataset for event recognition in\nsurveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\n2011."}
{"doc_id": "2103.00020", "para_id": 440, "text": "Quattoni, A., Collins, M., and Darrell, T. Learning visual\nrepresentations using images with captions. In 2007 IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 1–8. IEEE, 2007."}
{"doc_id": "2103.00020", "para_id": 441, "text": "Radford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining, 2018."}
{"doc_id": "2103.00020", "para_id": 442, "text": "Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\nfellow, I. Realistic evaluation of deep semi-supervised\nlearning algorithms. Advances in neural information pro-\ncessing systems, 31:3235–3246, 2018."}
{"doc_id": "2103.00020", "para_id": 443, "text": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019."}
{"doc_id": "2103.00020", "para_id": 444, "text": "Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018."}
{"doc_id": "2103.00020", "para_id": 445, "text": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "2103.00020", "para_id": 446, "text": "Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\nimages using 1 million captioned photographs. Advances\nin neural information processing systems, 24:1143–1151,\n2011."}
{"doc_id": "2103.00020", "para_id": 447, "text": "Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\nJ., and Denton, E. Saving face: Investigating the ethical\nconcerns of facial recognition auditing, 2020."}
{"doc_id": "2103.00020", "para_id": 448, "text": "pandas development team, T.\npandas-dev/pandas: Pan-\ndas, February 2020. URL https://doi.org/10.\n5281/zenodo.3509134."}
{"doc_id": "2103.00020", "para_id": 449, "text": "Ramanathan, V., Liang, P., and Fei-Fei, L. Video event\nunderstanding using natural language descriptions. In\nProceedings of the IEEE International Conference on\nComputer Vision, pp. 905–912, 2013."}
{"doc_id": "2103.00020", "para_id": 450, "text": "Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\nC. V. Cats and dogs. In IEEE Conference on Computer\nVision and Pattern Recognition, 2012."}
{"doc_id": "2103.00020", "para_id": 451, "text": "Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\nCollecting image annotations using amazon’s mechanical\nturk. In Proceedings of the NAACL HLT 2010 Workshop\non Creating Speech and Language Data with Amazon’s\nMechanical Turk, pp. 139–147, 2010."}
{"doc_id": "2103.00020", "para_id": 452, "text": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,"}
{"doc_id": "2103.00020", "para_id": 453, "text": "Learning Transferable Visual Models From Natural Language Supervision\n34"}
{"doc_id": "2103.00020", "para_id": 454, "text": "Sohn, K. Improved deep metric learning with multi-class\nn-pair loss objective. In Advances in neural information\nprocessing systems, pp. 1857–1865, 2016."}
{"doc_id": "2103.00020", "para_id": 455, "text": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\nagenet classiﬁers generalize to imagenet? arXiv preprint\narXiv:1902.10811, 2019."}
{"doc_id": "2103.00020", "para_id": 456, "text": "Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\nVoss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\nMcGufﬁe, K., and Wang, J. Release strategies and the\nsocial impacts of language models, 2019."}
{"doc_id": "2103.00020", "para_id": 457, "text": "Salimans, T. and Kingma, D. P. Weight normalization: A\nsimple reparameterization to accelerate training of deep\nneural networks. In Advances in neural information pro-\ncessing systems, pp. 901–909, 2016."}
{"doc_id": "2103.00020", "para_id": 458, "text": "Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\ncomputers see gender: An evaluation of gender classiﬁca-\ntion in commercial facial analysis services. Proceedings\nof the ACM on Human-Computer Interaction, 3(CSCW):\n1–33, 2019."}
{"doc_id": "2103.00020", "para_id": 459, "text": "Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\nof 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402, 2012."}
{"doc_id": "2103.00020", "para_id": 460, "text": "Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/\n10.5281/zenodo.2591652. Version 5.5."}
{"doc_id": "2103.00020", "para_id": 461, "text": "Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\ngender bias in image recognition systems. Socius, 6:\n2378023120967171, 2020."}
{"doc_id": "2103.00020", "para_id": 462, "text": "Srivastava, N. and Salakhutdinov, R. Multimodal learning\nwith deep boltzmann machines. In NIPS, 2012."}
{"doc_id": "2103.00020", "para_id": 463, "text": "Srivastava, S., Labutov, I., and Mitchell, T. Joint concept\nlearning and semantic parsing from natural language ex-\nplanations. In Proceedings of the 2017 conference on\nempirical methods in natural language processing, pp.\n1527–1536, 2017."}
{"doc_id": "2103.00020", "para_id": 464, "text": "Sennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units.\narXiv\npreprint arXiv:1508.07909, 2015."}
{"doc_id": "2103.00020", "para_id": 465, "text": "Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\nand Schmidt, L. Do image classiﬁers generalize across\ntime? arXiv preprint arXiv:1906.02168, 2019."}
{"doc_id": "2103.00020", "para_id": 466, "text": "Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Trafﬁc Sign Recognition Benchmark: A multi-\nclass classiﬁcation competition. In IEEE International\nJoint Conference on Neural Networks, pp. 1453–1460,\n2011."}
{"doc_id": "2103.00020", "para_id": 467, "text": "Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\n2565, 2018."}
{"doc_id": "2103.00020", "para_id": 468, "text": "Stroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937,\n2020."}
{"doc_id": "2103.00020", "para_id": 469, "text": "Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pp.\n8317–8326, 2019."}
{"doc_id": "2103.00020", "para_id": 470, "text": "Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\nA.\nInception-v4, inception-resnet and the impact\nof residual connections on learning.\narXiv preprint\narXiv:1602.07261, 2016."}
{"doc_id": "2103.00020", "para_id": 471, "text": "Socher, R. and Fei-Fei, L. Connecting modalities: Semi-\nsupervised segmentation and annotation of images using\nunaligned text corpora. In 2010 IEEE Computer Society\nConference on Computer Vision and Pattern Recognition,\npp. 966–973. IEEE, 2010."}
{"doc_id": "2103.00020", "para_id": 472, "text": "Tan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490, 2019."}
{"doc_id": "2103.00020", "para_id": 473, "text": "Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946, 2019."}
{"doc_id": "2103.00020", "para_id": 474, "text": "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing, pp. 1631–1642, 2013."}
{"doc_id": "2103.00020", "para_id": 475, "text": "Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classiﬁcation. arXiv preprint\narXiv:2007.00644, 2020."}
{"doc_id": "2103.00020", "para_id": 476, "text": "Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\nA. Y. Grounded compositional semantics for ﬁnding and\ndescribing images with sentences. Transactions of the\nAssociation for Computational Linguistics, 2:207–218,\n2014."}
{"doc_id": "2103.00020", "para_id": 477, "text": "Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM, 59(2):64–73, 2016."}
{"doc_id": "2103.00020", "para_id": 478, "text": "Learning Transferable Visual Models From Natural Language Supervision\n35"}
{"doc_id": "2103.00020", "para_id": 479, "text": "Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849, 2019."}
{"doc_id": "2103.00020", "para_id": 480, "text": "Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\nM., Wang, Y., and Liu, W. All you need is boundary: To-\nward arbitrary-shaped text spotting. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34,\npp. 12160–12167, 2020."}
{"doc_id": "2103.00020", "para_id": 481, "text": "Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classiﬁcation: a\ngood embedding is all you need?\narXiv preprint\narXiv:2003.11539, 2020."}
{"doc_id": "2103.00020", "para_id": 482, "text": "Wang, J., Markert, K., and Everingham, M. Learning mod-\nels for object recognition from natural language descrip-\ntions. In BMVC, volume 1, pp. 2, 2009."}
{"doc_id": "2103.00020", "para_id": 483, "text": "Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 30(11):1958–1970, 2008."}
{"doc_id": "2103.00020", "para_id": 484, "text": "Weston, J., Bengio, S., and Usunier, N. Large scale im-\nage annotation: learning to rank with joint word-image\nembeddings. Machine learning, 81(1):21–35, 2010."}
{"doc_id": "2103.00020", "para_id": 485, "text": "Touvron, H., Vedaldi, A., Douze, M., and J´egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems, pp. 8252–8262,\n2019."}
{"doc_id": "2103.00020", "para_id": 486, "text": "Weston, J. E. Dialog-based language learning. In Advances\nin Neural Information Processing Systems, pp. 829–837,\n2016."}
{"doc_id": "2103.00020", "para_id": 487, "text": "Varadarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops, pp. 1338–1345. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 488, "text": "Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\ncation with convolutional neural networks. In European\nConference on Computer Vision, pp. 37–55. Springer,\n2016."}
{"doc_id": "2103.00020", "para_id": 489, "text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017."}
{"doc_id": "2103.00020", "para_id": 490, "text": "Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\nshick, R.\nDetectron2.\nhttps://github.com/\nfacebookresearch/detectron2, 2019."}
{"doc_id": "2103.00020", "para_id": 491, "text": "Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018."}
{"doc_id": "2103.00020", "para_id": 492, "text": "Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\nlearning via non-parametric instance-level discrimination.\narXiv preprint arXiv:1805.01978, 2018."}
{"doc_id": "2103.00020", "para_id": 493, "text": "Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,\nFeng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scientiﬁc Computing\nin Python. Nature Methods, 17:261–272, 2020. doi:\n10.1038/s41592-019-0686-2."}
{"doc_id": "2103.00020", "para_id": 494, "text": "Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\nwith noisy student improves imagenet classiﬁcation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10687–10698, 2020."}
{"doc_id": "2103.00020", "para_id": 495, "text": "y\nArcas,\nB.\nA.,\nMitchell,\nM.,\nand\nTodorov,\nA.\nPhysiognomy’s\nnew\nclothes.\n2017.\nURL\nhttps://medium.com/@blaisea/\nphysiognomys-new-clothes-f2d4b59fdd6a."}
{"doc_id": "2103.00020", "para_id": 496, "text": "Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\npre-training for text-vqa and text-caption. arXiv preprint\narXiv:2012.04638, 2020."}
{"doc_id": "2103.00020", "para_id": 497, "text": "Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\ndeep learning era. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pp. 2621–2630,\n2017."}
{"doc_id": "2103.00020", "para_id": 498, "text": "Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\nYu, L., Dyer, C., et al. Learning and evaluating general\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\n2019."}
{"doc_id": "2103.00020", "para_id": 499, "text": "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018."}
{"doc_id": "2103.00020", "para_id": 500, "text": "Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\nimage descriptions to visual denotations: New similarity\nmetrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Lin-\nguistics, 2:67–78, 2014."}
{"doc_id": "2103.00020", "para_id": 501, "text": "Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\nbust global representations by penalizing local predictive\npower. In Advances in Neural Information Processing\nSystems, pp. 10506–10518, 2019."}
{"doc_id": "2103.00020", "para_id": 502, "text": "Learning Transferable Visual Models From Natural Language Supervision\n36"}
{"doc_id": "2103.00020", "para_id": 503, "text": "Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv\npreprint arXiv:2006.16934, 2020."}
{"doc_id": "2103.00020", "para_id": 504, "text": "Zeiler, M. D. and Fergus, R. Visualizing and understand-\ning convolutional networks. In European conference on\ncomputer vision, pp. 818–833. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 505, "text": "Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\nmann, M., Dosovitskiy, A., et al. A large-scale study of\nrepresentation learning with the visual task adaptation\nbenchmark. arXiv preprint arXiv:1910.04867, 2019."}
{"doc_id": "2103.00020", "para_id": 506, "text": "Zhang, R. Making convolutional networks shift-invariant\nagain. arXiv preprint arXiv:1904.11486, 2019."}
{"doc_id": "2103.00020", "para_id": 507, "text": "Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\nglotz, C. P. Contrastive learning of medical visual repre-\nsentations from paired images and text. arXiv preprint\narXiv:2010.00747, 2020."}
{"doc_id": "2103.00020", "para_id": 508, "text": "Zuboff, S.\nBig other: surveillance capitalism and the\nprospects of an information civilization. Journal of Infor-\nmation Technology, 30(1):75–89, 2015."}
{"doc_id": "2103.00020", "para_id": 509, "text": "Learning Transferable Visual Models From Natural Language Supervision\n37"}
{"doc_id": "2103.00020", "para_id": 510, "text": "the ResNet-50 architecture as in the smallest contrastive\nmodel. To do so, the output from the CNN is projected into\nfour tokens, which are then fed as a preﬁx to a language\nmodel autoregressively predicting the text tokens. Apart\nfrom the training objective, the model was trained on the\nsame dataset for the same number of epochs as other CLIP\nmodels."}
{"doc_id": "2103.00020", "para_id": 511, "text": "We provide additional details for linear probe experiments\npresented in this paper, including the list of the datasets and\nmodels used for evaluation."}
{"doc_id": "2103.00020", "para_id": 512, "text": "We use the 12 datasets from the well-studied evaluation\nsuite introduced by (Kornblith et al., 2019) and add 15\nadditional datasets in order to assess the performance of\nmodels on a wider variety of distributions and tasks. These\ndatasets include MNIST, the Facial Expression Recognition\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\nﬁc Sign Recognition Benchmark (GTSRB) dataset (Stal-\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\nreira et al., 2019), 2,500 random samples of the CLEVR\ndataset (Johnson et al., 2017), the Hateful Memes dataset\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\net al., 2012). For the two video datasets (UCF101 and Ki-\nnetics700), we use the middle frame of each video clip as\nthe input image. STL-10 and UCF101 have multiple pre-\ndeﬁned train/validation/test splits, 10 and 3 respectively, and\nwe report the average over all splits. Details on each dataset\nand the corresponding evaluation metrics are provided in\nTable 9."}
{"doc_id": "2103.00020", "para_id": 513, "text": "CLIP-RN\nFive ResNet-based contrastive CLIP models\nare included. As discussed in the paper, the ﬁrst two models\nfollow ResNet-50 and ResNet-101, and we use EfﬁcientNet-\nstyle (Tan & Le, 2019) scaling for the next three models\nwhich simultaneously scale the model width, the number\nof layers, and the input resolution to obtain models with\nroughly 4x, 16x, and 64x computation."}
{"doc_id": "2103.00020", "para_id": 514, "text": "CLIP-ViT\nWe include four CLIP models that use the Vi-\nsion Transformer (Dosovitskiy et al., 2020) architecture as\nthe image encoder. We include three models trained on 224-\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\nthe ViT-L/14 model ﬁne-tuned on 336-by-336 pixel input\nimages."}
{"doc_id": "2103.00020", "para_id": 515, "text": "EfﬁcietNet\nWe use the nine models (B0-B8) from the\noriginal EfﬁcientNet paper (Tan & Le, 2019), as well as\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\ntake the input resolutions of 475x475 and 800x800 pixels,\nrespectively."}
{"doc_id": "2103.00020", "para_id": 516, "text": "Additionally, we created two datasets that we call Coun-\ntry211 and Rendered SST2. The Country211 dataset is\ndesigned to assess the geolocation capability of visual rep-\nresentations. We ﬁltered the YFCC100m dataset (Thomee\net al., 2016) to ﬁnd 211 countries (deﬁned as having an\nISO-3166 country code) that have at least 300 photos with\nGPS coordinates, and we built a balanced dataset with 211\ncategories, by sampling 200 photos for training and 100\nphotos for testing, for each country."}
{"doc_id": "2103.00020", "para_id": 517, "text": "Instagram-pretrained ResNeXt\nWe use the four models\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\net al., 2018), as well as their two FixRes variants which use\nhigher input resolutions (Touvron et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 518, "text": "Big Transfer (BiT)\nWe use BiT-S and BiT-M models\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\nImageNet-21k datasets. The model weights for BiT-L is not\npublicly available."}
{"doc_id": "2103.00020", "para_id": 519, "text": "The Rendered SST2 dataset is designed to measure the opti-\ncal character recognition capability of visual representations.\nTo do so, we used the sentences from the Stanford Sentiment\nTreebank dataset (Socher et al., 2013) and rendered them\ninto images, with black texts on a white background, in a\n448×448 resolution. Two example images from this dataset\nare shown in Figure 19."}
{"doc_id": "2103.00020", "para_id": 520, "text": "Vision Transformer (ViT)\nWe also include four ViT\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\nL/16, and ViT-H/14. We note that their best-performing\nmodels, trained on the JFT-300M dataset, are not available\npublicly."}
{"doc_id": "2103.00020", "para_id": 521, "text": "SimCLRv2\nThe SimCLRv2 (Chen et al., 2020c) project\nreleased pre-trained and ﬁne-tuned models in various set-\ntings. We use the seven pretrain-only checkpoints with\nselective kernels."}
{"doc_id": "2103.00020", "para_id": 522, "text": "In combination with the datasets listed above, we evaluate\nthe following series of models using linear probes."}
{"doc_id": "2103.00020", "para_id": 523, "text": "LM RN50\nThis is a multimodal model that uses an au-\ntoregressive loss instead of a contrastive loss, while using"}
{"doc_id": "2103.00020", "para_id": 524, "text": "BYOL\nWe use the recently released model weights of\nBYOL (Grill et al., 2020), speciﬁcally their 50x1 and 200x2"}
{"doc_id": "2103.00020", "para_id": 525, "text": "Learning Transferable Visual Models From Natural Language Supervision\n38"}
{"doc_id": "2103.00020", "para_id": 526, "text": "Figure 19. Two example images from the Rendered SST2 dataset"}
{"doc_id": "2103.00020", "para_id": 527, "text": "a test split, we use the provided validation set to perform\nthe hyperparameter search, and for the datasets that do not\nprovide a validation split or have not published labels for\nthe test data, we split the training dataset to perform the\nhyperparameter search. For the ﬁnal result, we combine the\nvalidation split back with the training split and report the\nperformance on the unused split."}
{"doc_id": "2103.00020", "para_id": 528, "text": "Momentum Contrast (MoCo)\nWe include the MoCo-v1\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\ncheckpoints."}
{"doc_id": "2103.00020", "para_id": 529, "text": "VirTex\nWe use the pretrained model of VirTex (Desai &\nJohnson, 2020). We note that VirTex has a similar model\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\nof high-quality captions from MSCOCO."}
{"doc_id": "2103.00020", "para_id": 530, "text": "The individual linear probe scores are provided in Table 10\nand plotted in Figure 20. The best-performing CLIP model,\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\nachieved the state of the art in 21 of the 27 datasets, i.e.\nincluded in the Clopper-Pearson 99.5% conﬁdence interval\naround each dataset’s top score. For many datasets, CLIP\nperforms signiﬁcantly better than other models, demonstrat-\ning the advantage of natural language supervision over tradi-\ntional pre-training approaches based on image classiﬁcation.\nSee Section 3.2 for more discussions on the linear probe\nresults."}
{"doc_id": "2103.00020", "para_id": 531, "text": "ResNet\nWe add the original ResNet checkpoints released\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\nResNet152."}
{"doc_id": "2103.00020", "para_id": 532, "text": "We use image features taken from the penultimate layer of\neach model, ignoring any classiﬁcation layer provided. For\nCLIP-ViT models, we used the features before the linear\nprojection to the embedding space, which corresponds to\nI f in Figure 3. We train a logistic regression classiﬁer\nusing scikit-learn’s L-BFGS implementation, with maxi-\nmum 1,000 iterations, and report the corresponding met-\nric for each dataset. We determine the L2 regularization\nstrength λ using a hyperparameter sweep on the validation\nsets over the range between 10−6 and 106, with 96 log-\narithmically spaced steps. To save compute required for\nthe sweeps, we perform a parametric binary search that\nstarts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\neratively halves the interval around the peak until it reaches\na resolution of 8 steps per decade. The hyperparameter\nsweeps are performed on a validation split of each dataset.\nFor the datasets that contain a validation split in addition to"}
{"doc_id": "2103.00020", "para_id": 533, "text": "Learning Transferable Visual Models From Natural Language Supervision\n39"}
{"doc_id": "2103.00020", "para_id": 534, "text": "Dataset\nClasses\nTrain size\nTest size\nEvaluation metric"}
{"doc_id": "2103.00020", "para_id": 535, "text": "Food-101\n102\n75,750\n25,250\naccuracy\nCIFAR-10\n10\n50,000\n10,000\naccuracy\nCIFAR-100\n100\n50,000\n10,000\naccuracy\nBirdsnap\n500\n42,283\n2,149\naccuracy\nSUN397\n397\n19,850\n19,850\naccuracy\nStanford Cars\n196\n8,144\n8,041\naccuracy\nFGVC Aircraft\n100\n6,667\n3,333\nmean per class\nPascal VOC 2007 Classiﬁcation\n20\n5,011\n4,952\n11-point mAP\nDescribable Textures\n47\n3,760\n1,880\naccuracy\nOxford-IIIT Pets\n37\n3,680\n3,669\nmean per class\nCaltech-101\n102\n3,060\n6,085\nmean-per-class\nOxford Flowers 102\n102\n2,040\n6,149\nmean per class"}
{"doc_id": "2103.00020", "para_id": 536, "text": "MNIST\n10\n60,000\n10,000\naccuracy\nFacial Emotion Recognition 2013\n8\n32,140\n3,574\naccuracy\nSTL-10\n10\n1000\n8000\naccuracy\nEuroSAT\n10\n10,000\n5,000\naccuracy\nRESISC45\n45\n3,150\n25,200\naccuracy\nGTSRB\n43\n26,640\n12,630\naccuracy\nKITTI\n4\n6,770\n711\naccuracy\nCountry211\n211\n43,200\n21,100\naccuracy\nPatchCamelyon\n2\n294,912\n32,768\naccuracy\nUCF101\n101\n9,537\n1,794\naccuracy\nKinetics700\n700\n494,801\n31,669\nmean(top1, top5)\nCLEVR Counts\n8\n2,000\n500\naccuracy\nHateful Memes\n2\n8,500\n500\nROC AUC\nRendered SST2\n2\n7,792\n1,821\naccuracy\nImageNet\n1000\n1,281,167\n50,000\naccuracy"}
{"doc_id": "2103.00020", "para_id": 537, "text": "Table 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\navailable online at the time of this writing."}
{"doc_id": "2103.00020", "para_id": 538, "text": "Learning Transferable Visual Models From Natural Language Supervision\n40"}
{"doc_id": "2103.00020", "para_id": 539, "text": "LM RN50\n81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2"}
{"doc_id": "2103.00020", "para_id": 540, "text": "50\n86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\n101\n88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\n50x4\n91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\n50x16\n93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\n50x64\n94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6"}
{"doc_id": "2103.00020", "para_id": 541, "text": "B/32\n88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\nB/16\n92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\nL/14\n95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\nL/14-336px\n95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4"}
{"doc_id": "2103.00020", "para_id": 542, "text": "B0\n74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\nB1\n74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\nB2\n75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\nB3\n77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\nB4\n79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\nB5\n81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\nB6\n82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\nB7\n84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\nB8\n84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3"}
{"doc_id": "2103.00020", "para_id": 543, "text": "B0\n78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\nB1\n80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\nB2\n80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\nB3\n82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\nB4\n85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\nB5\n87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\nB6\n87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\nB7\n88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\nL2-475\n91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\nL2-800\n92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4"}
{"doc_id": "2103.00020", "para_id": 544, "text": "32x8d\n84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\n32x16d\n85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\n32x32d\n86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\n32x48d\n86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\nFixRes-v1\n88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\nFixRes-v2\n88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0"}
{"doc_id": "2103.00020", "para_id": 545, "text": "R50x1\n72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\nR50x3\n75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\nR101x1\n73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\nR101x3\n74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\nR152x2\n74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\nR152x4\n74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5"}
{"doc_id": "2103.00020", "para_id": 546, "text": "R50x1\n83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\nR50x3\n86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\nR101x1\n85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\nR101x3\n87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\nR152x2\n88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\nR152x4\n87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5"}
{"doc_id": "2103.00020", "para_id": 547, "text": "B/32\n81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\nB/16\n86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\nL/16\n87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\nH/14\n83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9"}
{"doc_id": "2103.00020", "para_id": 548, "text": "R50x1\n76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\nR50x3\n81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\nR101x1\n77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\nR101x3\n82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\nR152x1\n78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\nR152x2\n82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\nR152x3\n83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0"}
{"doc_id": "2103.00020", "para_id": 549, "text": "50x1\n74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\n200x2\n78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2"}
{"doc_id": "2103.00020", "para_id": 550, "text": "v1\n65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\nv2\n72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1"}
{"doc_id": "2103.00020", "para_id": 551, "text": "VirTex\n57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7"}
{"doc_id": "2103.00020", "para_id": 552, "text": "50\n71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\n101\n72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\n152\n73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1"}
{"doc_id": "2103.00020", "para_id": 553, "text": "Table 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson conﬁdence\ninterval of each dataset’s top score are shown in bold."}
{"doc_id": "2103.00020", "para_id": 554, "text": "⋆We updated the STL10 scores from the previous version of this paper after ﬁxing a CUDA-related bug."}
{"doc_id": "2103.00020", "para_id": 555, "text": "Learning Transferable Visual Models From Natural Language Supervision\n41"}
{"doc_id": "2103.00020", "para_id": 556, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet\nInstagram-pretrained\nSimCLRv2\nBYOL\nMoCo\nViT (ImageNet-21k)\nBiT-M\nBiT-S\nResNet"}
{"doc_id": "2103.00020", "para_id": 557, "text": "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10."}
{"doc_id": "2103.00020", "para_id": 558, "text": "Learning Transferable Visual Models From Natural Language Supervision\n42"}
{"doc_id": "2103.00020", "para_id": 559, "text": "correct rank: 1/101    correct probability: 90.15%"}
{"doc_id": "2103.00020", "para_id": 560, "text": "correct rank: 1/397    correct probability: 90.22%"}
{"doc_id": "2103.00020", "para_id": 561, "text": "a centered satellite photo of permanent crop land."}
{"doc_id": "2103.00020", "para_id": 562, "text": "a centered satellite photo of brushland or shrubland."}
{"doc_id": "2103.00020", "para_id": 563, "text": "correct rank: 1/101    correct probability: 99.30%"}
{"doc_id": "2103.00020", "para_id": 564, "text": "correct rank: 1/102    correct probability: 99.81%"}
{"doc_id": "2103.00020", "para_id": 565, "text": "correct rank: 1/200    correct probability: 76.02%"}
{"doc_id": "2103.00020", "para_id": 566, "text": "correct rank: 1/100    correct probability: 38.02%"}
{"doc_id": "2103.00020", "para_id": 567, "text": "correct rank: 1/1000    correct probability: 88.27%"}
{"doc_id": "2103.00020", "para_id": 568, "text": "a photo of a mcdonnell douglas md-90, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 569, "text": "a photo of a mcdonnell douglas dc-9-30, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 570, "text": "correct rank: 1/196    correct probability: 63.30%"}
{"doc_id": "2103.00020", "para_id": 571, "text": "correct rank: 1/723    correct probability: 98.63%"}
{"doc_id": "2103.00020", "para_id": 572, "text": "correct rank: 1/700    correct probability: 98.98%"}
{"doc_id": "2103.00020", "para_id": 573, "text": "correct rank: 1/102    correct probability: 74.25%"}
{"doc_id": "2103.00020", "para_id": 574, "text": "correct rank: 1/1000    correct probability: 91.61%"}
{"doc_id": "2103.00020", "para_id": 575, "text": "correct rank: 4/500    correct probability: 12.00%"}
{"doc_id": "2103.00020", "para_id": 576, "text": "a photo of a broad tailed hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 577, "text": "a photo of a bishop of llandaff, a type of flower."}
{"doc_id": "2103.00020", "para_id": 578, "text": "a photo of a calliope hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 579, "text": "a photo of a black chinned hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 580, "text": "a photo of a prince of wales feathers, a type of flower."}
{"doc_id": "2103.00020", "para_id": 581, "text": "correct rank: 1/113    correct probability: 98.34%"}
{"doc_id": "2103.00020", "para_id": 582, "text": "correct rank: 1/1000    correct probability: 79.54%"}
{"doc_id": "2103.00020", "para_id": 583, "text": "correct rank: 83/2000    correct probability: 0.27%"}
{"doc_id": "2103.00020", "para_id": 584, "text": "correct rank: 1/1000    correct probability: 79.56%"}
{"doc_id": "2103.00020", "para_id": 585, "text": "correct label: red and white triangle with exclamation mark warning"}
{"doc_id": "2103.00020", "para_id": 586, "text": "a zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 587, "text": "a zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 588, "text": "a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 589, "text": "a zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 590, "text": "a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 591, "text": "Figure 21. Visualization of predictions from 36 CLIP zero-shot classiﬁers. All examples are random with the exception of reselecting\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\nthe class. When more than one template is used, the ﬁrst template is shown. The ground truth label is colored green while an incorrect\nprediction is colored orange."}
{"doc_id": "2103.00020", "para_id": 592, "text": "Learning Transferable Visual Models From Natural Language Supervision\n43"}
{"doc_id": "2103.00020", "para_id": 593, "text": "RN50\n81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\nRN101\n83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\nRN50x4\n86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\nRN50x16\n90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\nRN50x64\n91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6"}
{"doc_id": "2103.00020", "para_id": 594, "text": "B/32\n84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\nB/16\n89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\nL/14\n92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\nL/14-336px\n93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2"}
{"doc_id": "2103.00020", "para_id": 595, "text": "Table 11. Zero-shot performance of CLIP models over 27 datasets."}
{"doc_id": "2103.00020", "para_id": 596, "text": "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance"}
{"doc_id": "2103.00020", "para_id": 597, "text": "Learning Transferable Visual Models From Natural Language Supervision\n44"}
{"doc_id": "2103.00020", "para_id": 598, "text": "Linear Classiﬁer\nZero Shot\nDataset\nYFCC\nWIT\n∆\nYFCC\nWIT\n∆"}
{"doc_id": "2103.00020", "para_id": 599, "text": "To provide a qualitative summary / overview of CLIP’s zero-\nshot performance we visualize a randomly selected predic-\ntion for 36 different zero-shot CLIP classiﬁers in Figure\n21. In addition, Table 11 and Figure 22 show the individual\nzero-shot performance scores for each dataset."}
{"doc_id": "2103.00020", "para_id": 600, "text": "Birdsnap\n47.4\n35.3\n+12.1\n19.9\n4.5\n+15.4\nCountry211\n23.1\n17.3\n+5.8\n5.2\n5.3\n+0.1\nFlowers102\n94.4\n89.8\n+4.6\n48.6\n21.7\n+26.9\nGTSRB\n66.8\n72.5\n−5.7\n6.9\n7.0\n−0.1\nUCF101\n69.2\n74.9\n−5.7\n22.9\n32.0\n−9.1\nStanford Cars\n31.4\n50.3\n−18.9\n3.8\n10.9\n−7.1"}
{"doc_id": "2103.00020", "para_id": 601, "text": "ImageNet\n62.0\n60.8\n+1.2\n31.3\n27.6\n+3.7\nDataset Average\n65.5\n66.6\n−1.1\n29.6\n30.0\n−0.4\nDataset “Wins”\n10\n15\n−5\n19\n18\n+1"}
{"doc_id": "2103.00020", "para_id": 602, "text": "Our early attempts at duplicate detection and analysis used\nnearest neighbors in the model’s learned embedding space.\nWhile it is intuitive to use a model’s own notion of similar-\nity, we encountered issues. We found the model’s feature\nspace is weighted very heavily towards semantic similar-\nity. Many false positives occurred due to distinct objects\nthat would be described similarly (soccer balls, ﬂowers of\nthe same species, etc...) having almost perfect similarity.\nWe also observed the model was quite poor at assigning\ncertain kinds of near-duplicates high similarity scores. We\nnoticed repeatedly that images with high-frequency textures\n(such as fur or stripe patterns) pre-processed by different\nresizing algorithms (nearest neighbor vs bi-linear) could\nhave surprisingly low similarity. This resulted in many false\nnegatives."}
{"doc_id": "2103.00020", "para_id": 603, "text": "Table 12. CLIP performs similarly when trained on only\nYFCC100M.\nComparing\na\nResNet-50\ntrained\non\nonly\nYFCC100M with a same sized subset of WIT shows simi-\nlar average performance and number of wins on zero shot and\nlinear classiﬁer evals.\nHowever, large differences in dataset\nspeciﬁc performance occur. We include performance on the 3\ndatasets where YFCC does best and worst compared to WIT\naccording to a linear probe in order to highlight this as well as\naggregate performance across all linear and zero-shot evals and\nthe canonical ImageNet dataset."}
{"doc_id": "2103.00020", "para_id": 604, "text": "To study whether our custom dataset is critical to the perfor-\nmance of CLIP, we trained a model on a ﬁltered subset of\nthe YFCC100M dataset (details described in Section 2.2)\nand compared its performance to the same model trained\non an equally sized subset of WIT. We train each model for\n32 epochs at which point transfer performance begins to\nplateau due to overﬁtting. Results are shown in Table 12.\nAcross our whole eval suite, YFCC and WIT perform simi-\nlarly on average for both zero-shot and linear probe settings.\nHowever, performance on speciﬁc ﬁne-grained classiﬁca-\ntion datasets can vary widely - sometimes by over 10%.\nOur speculation is that these differences in performance re-\nﬂect the relative density of relevant data in each pre-training\ndataset. For instance, pre-training on YFCC100M, which\nmight contain many photos of birds and ﬂowers (common\nsubjects for photographers), results in better performance on\nBirdsnap and Flowers102, while pre-training on WIT results\nin better car and pet classiﬁers (which appear common in\nour dataset)."}
{"doc_id": "2103.00020", "para_id": 605, "text": "We built our own near-duplicate detector to ﬁx this issue.\nWe created a synthetic data augmentation pipeline that com-\nbined a variety of common image manipulations. The aug-\nmentation pipeline combines random cropping and zooming,\naspect ratio distortion, downsizing and upscaling to different\nresolutions, minor rotations, jpeg compression, and HSV\ncolor jitter. The pipeline also randomly selects from differ-\nent interpolation algorithms for all relevant steps. We then\ntrained a model to maximize the similarity of an image and\nits transformed variant while minimizing similarity to all\nother images in a training batch. We used the same n-pair /\nInfoNCE loss as CLIP but with a ﬁxed temperature of 0.07."}
{"doc_id": "2103.00020", "para_id": 606, "text": "We selected a ResNet-50 as the model architecture. We\nmodiﬁed the base ResNet-50 with the anti-alias improve-\nments from (Zhang, 2019) and used weight norm (Sali-\nmans & Kingma, 2016) instead of batch norm (Ioffe &\nSzegedy, 2015) to avoid leaking information about dupli-\ncates via batch statistics - a problem previously noted in\n(Henaff, 2020). We also found the GELU activation func-\ntion (Hendrycks & Gimpel, 2016) to perform better for this\ntask. We trained the model with a total batch size of 1,712\nfor approximately 30 million images sampled from our pre-\ntraining dataset. At the end of training it achieves nearly\n100% accuracy on its proxy training task."}
{"doc_id": "2103.00020", "para_id": 607, "text": "Overall, these results are encouraging as they suggest our\napproach can use any reasonably ﬁltered collection of paired\n(text, image) data. This mirrors recent work which reported\npositive results using the same contrastive pre-training ob-\njective on the relatively different domain of medical imaging\n(Zhang et al., 2020). It also is similar to the ﬁndings of noisy\nstudent self-training which reported only slight improve-\nments when using their JFT300M dataset over YFCC100M\n(Xie et al., 2020). We suspect the major advantage of our\ndataset over the already existing YFCC100M is its much\nlarger size."}
{"doc_id": "2103.00020", "para_id": 608, "text": "Learning Transferable Visual Models From Natural Language Supervision\n45"}
{"doc_id": "2103.00020", "para_id": 609, "text": "Finally, we caution that WIT includes this ﬁltered subset\nof YFCC100M. This could result in our ablation under-\nestimating the size of performance differences between\nYFCC100M and the rest of WIT. We do not think this is\nlikely as YFCC100M is only 3.7% of the overall WIT data\nblend and it did not noticeably change the performance of\nmodels when it was added to the existing data blend during\nthe creation of WIT."}
{"doc_id": "2103.00020", "para_id": 610, "text": "on 5 datasets requiring the direct and indirect use of OCR.\nThree of these datasets MNIST (LeCun), SVHN (Netzer\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\nthe ability of a model to perform low-level character and\nword recognition, while Hateful Memes (Kiela et al., 2020)\nand SST-2 (Socher et al., 2013) check the ability of a model\nto use OCR to perform a semantic task. Results are reported\nin Table 14."}
{"doc_id": "2103.00020", "para_id": 611, "text": "CLIP’s performance is still highly variable and appears to\nbe sensitive to some combination of the domain (rendered or\nnatural images) and the type of text to be recognized (num-\nbers or words). CLIP’s OCR performance is strongest Hate-\nful Memes and SST-2 - datasets where the text is digitally\nrendered and consists mostly of words. On IIIT5K, which\nis natural images of individually cropped words, zero-shot\nCLIP performs a bit more respectively and its performance\nis similar to Jaderberg et al. (2014) early work combining\ndeep learning and structured prediction to perform open-\nvocabulary OCR. However, performance is noticeably lower\non two datasets involving recognition of hand written and\nstreet view numbers. CLIP’s 51% accuracy on full number\nSVHN is well below any published results. Inspection sug-\ngests CLIP struggles with repeated characters as well as the\nlow resolution and blurry images of SVHN. CLIP’s zero-\nshot MNIST performance is also poor and is outperformed\nby supervised logistic regression on raw pixels, one of the\nsimplest possible machine learning baselines."}
{"doc_id": "2103.00020", "para_id": 612, "text": "Due to the large variety of datasets and experiments consid-\nered in this work, the main body focuses on summarizing\nand analyzing overall results. In the following subsections\nwe report details of performance for speciﬁc groups of tasks,\ndatasets, and evaluation settings."}
{"doc_id": "2103.00020", "para_id": 613, "text": "CLIP pre-trains for the task of image-text retrieval on our\nnoisy web-scale dataset. Although the focus of this paper\nis on representation learning and task learning for the pur-\npose of transfer to a wide variety of downstream datasets,\nvalidating that CLIP is able to achieve high transfer perfor-\nmance transfer on exactly what it is pre-trained for is an\nimportant sanity check / proof of concept. In Table 13 we\ncheck the zero-shot transfer performance of CLIP for both\ntext and image retrieval on the Flickr30k and MSCOCO\ndatsets. Zero-shot CLIP matches or outperforms all prior\nzero-shot results on these two datasets. Zero-shot CLIP is\nalso competitive with the current overall SOTA for the task\nof text retrieval on Flickr30k. On image retrieval, CLIP’s\nperformance relative to the overall state of the art is notice-\nably lower. However, zero-shot CLIP is still competitive\nwith a ﬁne-tuned Unicoder-VL. On the larger MS-COCO\ndataset ﬁne-tuning improves performance signiﬁcantly and\nzero-shot CLIP is not competitive with the most recent work.\nFor both these datasets we prepend the prompt “a photo\nof” to the description of each image which we found boosts\nCLIP’s zero-shot R@1 performance between 1 and 2 points."}
{"doc_id": "2103.00020", "para_id": 614, "text": "SST-2 is a sentence level NLP dataset which we render into\nimages. We include SST-2 in order to check whether CLIP\nis able to convert low level OCR capability into a higher\nlevel representation. Fitting a linear classiﬁer on CLIP’s rep-\nresentation of rendered sentences achives 80.5% accuracy.\nThis is on par with the 80% accuracy of a continuous bag\nof words baseline using GloVe word vectors pre-trained on\n840 billion tokens (Pennington et al., 2014). While this is a\nsimple NLP baseline by today’s standard, and well below\nthe 97.5% of the current SOTA, it is encouraging to see\nthat CLIP is able to turn an image of rendered text into a\nnon-trivial sentence level representation. Fully supervised\nCLIP is also surprisingly strong on Hateful Meme detec-\ntion, where CLIP is only 0.7 points behind the current single\nmodel SOTA and several points above the best baseline from\nthe original paper. Similar to SST-2, these other results on\nHateful Memes use the ground truth text which CLIP does\nnot have access to. Finally, we note that zero-shot CLIP\noutperforms the best results using fully supervised linear\nprobes across all other 56 models included in our evaluation\nsuite. This suggests CLIP’s OCR capability is at least some-\nwhat unique compared to existing work on self-supervised\nand supervised representation learning."}
{"doc_id": "2103.00020", "para_id": 615, "text": "Although visualizations have shown that ImageNet models\ncontain features that respond to the presence of text in an\nimage (Zeiler & Fergus, 2014), these representations are\nnot sufﬁciently ﬁne-grained to use for the task of optical\ncharacter recognition (OCR). To compensate, models are\naugmented with the outputs of custom OCR engines and\nfeatures to boost performance on tasks where this capability\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\ning the development of CLIP, we noticed that CLIP began to\nlearn primitive OCR capabilities which appeared to steadily\nimprove over the course of the project. To evaluate this\nqualitatively noticed behavior, we measured performance"}
{"doc_id": "2103.00020", "para_id": 616, "text": "Learning Transferable Visual Models From Natural Language Supervision\n46"}
{"doc_id": "2103.00020", "para_id": 617, "text": "Text Retrieval\nImage Retrieval\nFlickr30k\nMSCOCO\nFlickr30k\nMSCOCO\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10"}
{"doc_id": "2103.00020", "para_id": 618, "text": "Unicoder-VLa\n86.2\n96.3\n99.0\n62.3\n87.1\n92.8\n71.5\n90.9\n94.9\n46.7\n76.0\n85.3\nUniterb\n87.3\n98.0\n99.2\n65.7\n88.6\n93.8\n75.6\n94.1\n96.8\n52.9\n79.9\n88.0\nVILLAc\n87.9\n97.5\n98.8\n-\n-\n-\n76.3\n94.2\n96.8\n-\n-\n-\nOscard\n-\n-\n-\n73.5\n92.2\n96.0\n-\n-\n-\n57.5\n82.8\n89.8\nERNIE-ViLe\n88.7\n98.0\n99.2\n-\n-\n-\n76.7\n93.6\n96.4\n-\n-\n-"}
{"doc_id": "2103.00020", "para_id": 619, "text": "Visual N-Gramsf\n15.4\n35.7\n45.1\n8.7\n23.1\n33.3\n8.8\n21.2\n29.9\n5.0\n14.5\n21.9\nImageBERTg\n-\n-\n-\n44.0\n71.2\n80.4\n-\n-\n-\n32.3\n59.0\n70.2\nUnicoder-VLa\n64.3\n86.8\n92.3\n-\n-\n-\n48.4\n76.0\n85.2\n-\n-\n-\nUniterb\n83.6\n95.7\n97.7\n-\n-\n-\n68.7\n89.2\n93.9\n-\n-\n-\nCLIP\n88.0\n98.7\n99.4\n58.4\n81.5\n88.1\n68.7\n90.6\n95.2\n37.8\n62.4\n72.2"}
{"doc_id": "2103.00020", "para_id": 620, "text": "Table 13. CLIP improves zero-shot retrieval and is competitive with the best ﬁne-tuned result on Flickr30k text retrieval. Bold\nindicates best overall performance while an underline indicates best in category performance (zero-shot or ﬁne-tuned). For all other\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\na(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f(Li et al., 2017) g(Qi et al., 2020)"}
{"doc_id": "2103.00020", "para_id": 621, "text": "R(2+1)D-BERTa\n98.7\n-\n-\n-\nNS ENet-L2b\n-\n84.8\n-\n-\nHT100M S3Dd\n91.3\n-\n-\n-\nBaseline I3De\n-\n70.2\n-\n-"}
{"doc_id": "2103.00020", "para_id": 622, "text": "Raw Pixels\n92.5\n-\n-\n-\n-\nES Best\n98.9h\n-\n-\n58.6h\n59.0i"}
{"doc_id": "2103.00020", "para_id": 623, "text": "MMV FACf\n91.8\n-\n-\n-\nNS ENet-L2c\n89.4c\n68.2c\n-\n-\nCLIP\n92.0\n73.0\n-\n-"}
{"doc_id": "2103.00020", "para_id": 624, "text": "HT100M S3Dd\n-\n-\n30.5\n34.8\nCLIP\n80.3\n69.6\n40.7\n44.8"}
{"doc_id": "2103.00020", "para_id": 625, "text": "Table 14. OCR performance on 5 datasets. All metrics are accuracy\non the test set except for Hateful Memes which reports ROC AUC\non the dev set. Single model SOTA reported to best of knowledge.\nES Best reports the best performance across the 56 non-CLIP\nmodels in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al.,\n2015) c(Wang et al., 2020) d(Lippe et al., 2020) f(Jaderberg et al.,\n2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\n2018)"}
{"doc_id": "2103.00020", "para_id": 626, "text": "Table 15. Action recognition performance on 3 video datasets. Sin-\ngle model SOTA reported to best of knowledge. Note that linear\nCLIP and linear NS ENet-L2 are trained and evaluated on a single\nframe subsampled version of each dataset and not directly compa-\nrable to prior work. On Kinetics-700, we report the ActivityNet\ncompetition metric which is the average of top-1 and top-5 per-\nformance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al.,\n2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f(Alayrac\net al., 2020)"}
{"doc_id": "2103.00020", "para_id": 627, "text": "For the purpose of learning, a potentially important aspect\nof natural language is its ability to express, and therefore su-\npervise, an extremely wide set of concepts. A CLIP model,\nsince it is trained to pair semi-arbitrary text with images, is\nlikely to receive supervision for a wide range of visual con-\ncepts involving both common and proper nouns, verbs, and\nadjectives. ImageNet-1K, by contrast, only labels common\nnouns. Does the lack of broader supervision in ImageNet\nresult in weaker transfer of ImageNet models to tasks involv-\ning the recognition of visual concepts that are not nouns?"}
{"doc_id": "2103.00020", "para_id": 628, "text": "action classiﬁcation datasets which measure the ability of a\nmodel to recognize verbs. In Table 15 we report results on\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\net al., 2019), two common datasets for the task. Unfortu-\nnately, our CPU based linear classiﬁer takes a prohibitively\nlong time to evaluate on a video dataset due to the very large\nnumber of training frames. To deal with this, we aggres-\nsively sub-sample each video to only a single center frame,\neffectively turning it into an image classiﬁcation dataset.\nAs a result, our reported performance in a linear evaluation\nsetting likely under estimates performance by a moderate\namount."}
{"doc_id": "2103.00020", "para_id": 629, "text": "To investigate this, we measure and compare the perfor-\nmance of CLIP and ImageNet models on several video"}
{"doc_id": "2103.00020", "para_id": 630, "text": "Learning Transferable Visual Models From Natural Language Supervision\n47"}
{"doc_id": "2103.00020", "para_id": 631, "text": "IN\nIN-V2\nIN-A\nIN-R\nObjectNet\nIN-Sketch\nIN-Vid\nYTBB\nTop-1\nTop-1\nTop-1\nTop-1\nTop-1\nTop-1\nPM0\nPM10\nPM0\nPM10"}
{"doc_id": "2103.00020", "para_id": 632, "text": "NS EfﬁcientNet-L2a\n88.3\n80.2\n84.9\n74.7\n68.5\n47.6\n88.0\n82.1\n67.7\n63.5\nFixResNeXt101-32x48d V2b\n86.4\n78.0\n68.4\n80.0\n57.8\n59.1\n85.8\n72.2\n68.9\n57.7\nLinear Probe CLIP\n85.4\n75.9\n75.3\n84.2\n66.2\n57.4\n89.1\n77.2\n68.7\n63.1\nZero-Shot CLIP\n76.2\n70.1\n77.2\n88.9\n72.3\n60.2\n95.3\n89.2\n95.2\n88.5"}
{"doc_id": "2103.00020", "para_id": 633, "text": "Table 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)"}
{"doc_id": "2103.00020", "para_id": 634, "text": "Despite this handicap, CLIP features transfer surprisingly\nwell to this task. CLIP matches the best prior result on UCF-\n101 in a linear probe evaluation setting and also outperforms\nall other models in our evaluation suite. On Kinetics-700,\nCLIP also outperforms the ﬁne-tuned I3D baseline from the\noriginal paper. Since it does not require a training stage,\nwe report CLIP’s zero-shot performance when averaging\npredictions across all frames. CLIP also performs well in\nthis setting and on Kinetics-700 its performance is within\n1% of the fully supervised I3D baseline which is trained\non 545000 labeled videos. Encouraged by these results, we\nalso measure CLIP’s performance on the recently introduced\nRareAct dataset (Miech et al., 2020a) which was designed\nto measure zero-shot recognition of unusual actions like\n“hammering a phone” and “drilling an egg”. CLIP improves\nover the prior state of the art, a S3D model trained on auto-\nmatically extracted captions from 100 million instructional\nvideos, by 10 points."}
{"doc_id": "2103.00020", "para_id": 635, "text": "Another behavior we noticed during the development of\nCLIP was its ability to recognize many places and locations.\nTo quantify this we created the Country211 dataset as de-\nscribed in Appendix A and report results on it throughout\nthe paper. However it is a new benchmark so to compare\nwith prior work on geolocalization we also report results\non the IM2GPS test set from Hays & Efros (2008) in Table\n17. Since IM2GPS is a regression benchmark, we guess the\nGPS coordinates of the nearest image in a set of reference\nimages using CLIP’s embedding space. This is not a zero-\nshot result since it uses nearest-neighbor regression. Despite\nquerying only 1 million images, which is much less than\nprior work, CLIP performs similarly to several task speciﬁc\nmodels. It is not, however, competitive with the current state\nof the art."}
{"doc_id": "2103.00020", "para_id": 636, "text": "While CLIP has encouragingly strong performance on the\ntask of action recognition, we note that there are many differ-\nences between the models being compared beyond just their\nform of supervision such as model architecture, training\ndata distribution, dataset size, and compute used. Further\nwork is needed to more precisely determine what speciﬁc\ndesign decisions contribute to achieving high performance\non this task."}
{"doc_id": "2103.00020", "para_id": 637, "text": "Section 3.3 provides a high level summary and analysis of\nImageNet-related robustness results. We brieﬂy provide\nsome additional numerical details in this appendix. Per-\nformance results per dataset are provided in Table 16 and\ncompared with the current state of the art results reported\nin Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP’s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its ﬂexible zero-shot capability and on\nImageNet-R, which likely reﬂects CLIP’s pre-training dis-\ntribution including signiﬁcant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020)."}
{"doc_id": "2103.00020", "para_id": 638, "text": "ISNsa\n16.9\n43.0\n51.9\n66.7\n80.2\nCPlaNetb\n16.5\n37.1\n46.4\n62.0\n78.5\nCLIP\n13.9\n32.9\n43.0\n62.0\n79.3\nDeep-Ret+c\n14.4\n33.3\n47.7\n61.6\n73.4\nPlaNetd\n8.4\n24.5\n37.6\n53.6\n71.3"}
{"doc_id": "2103.00020", "para_id": 639, "text": "Table 17. Geolocalization performance on the IM2GPS test set.\nMetric is percent of images localized within a given radius. Models\nare ordered by average performance. a(Muller-Budack et al., 2018)\nb(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al.,\n2016)"}
{"doc_id": "2103.00020", "para_id": 640, "text": "Learning Transferable Visual Models From Natural Language Supervision\n48"}
{"doc_id": "2103.00020", "para_id": 641, "text": "Batch size\n32768\nVocabulary size\n49408\nTraining epochs\n32\nMaximum temperature\n100.0\nWeight decay\n0.2\nWarm-up iterations\n2000\nAdam β1\n0.9\nAdam β2\n0.999 (ResNet), 0.98 (ViT)\nAdam ϵ\n10−8 (ResNet), 10−6 (ViT)"}
{"doc_id": "2103.00020", "para_id": 642, "text": "Learning\nEmbedding\nInput\nResNet\nText Transformer\nModel\nrate\ndimension\nresolution\nblocks\nwidth\nlayers\nwidth\nheads"}
{"doc_id": "2103.00020", "para_id": 643, "text": "RN50\n5 × 10−4\n1024\n224\n(3, 4, 6, 3)\n2048\n12\n512\n8\nRN101\n5 × 10−4\n512\n224\n(3, 4, 23, 3)\n2048\n12\n512\n8\nRN50x4\n5 × 10−4\n640\n288\n(4, 6, 10, 6)\n2560\n12\n640\n10\nRN50x16\n4 × 10−4\n768\n384\n(6, 8, 18, 8)\n3072\n12\n768\n12\nRN50x64\n3.6 × 10−4\n1024\n448\n(3, 15, 36, 10)\n4096\n12\n1024\n16"}
{"doc_id": "2103.00020", "para_id": 644, "text": "Learning\nEmbedding\nInput\nVision Transformer\nText Transformer\nModel\nrate\ndimension\nresolution\nlayers\nwidth\nheads\nlayers\nwidth\nheads"}
{"doc_id": "2103.00020", "para_id": 645, "text": "ViT-B/32\n5 × 10−4\n512\n224\n12\n768\n12\n12\n512\n8\nViT-B/16\n5 × 10−4\n512\n224\n12\n768\n12\n12\n512\n8\nViT-L/14\n4 × 10−4\n768\n224\n24\n1024\n16\n12\n768\n12\nViT-L/14-336px\n2 × 10−5\n768\n336\n24\n1024\n16\n12\n768\n12"}
{"doc_id": "1910.10683", "para_id": 0, "text": "Journal of Machine Learning Research 21 (2020) 1-67\nSubmitted 1/20; Revised 6/20; Published 6/20"}
{"doc_id": "1910.10683", "para_id": 1, "text": "Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer"}
{"doc_id": "1910.10683", "para_id": 2, "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-\ntuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.1"}
{"doc_id": "1910.10683", "para_id": 3, "text": "Keywords:\ntransfer learning, natural language processing, multi-task learning, attention-\nbased models, deep learning"}
{"doc_id": "1910.10683", "para_id": 4, "text": "Training a machine learning model to perform natural language processing (NLP) tasks\noften requires that the model can process text in a way that is amenable to downstream\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\nthe model to “understand” text. This knowledge can range from low-level (e.g. the spelling"}
{"doc_id": "1910.10683", "para_id": 5, "text": "∗. Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence\nto craffel@gmail.com.\n1. https://github.com/google-research/text-to-text-transfer-transformer"}
{"doc_id": "1910.10683", "para_id": 6, "text": "©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu."}
{"doc_id": "1910.10683", "para_id": 7, "text": "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v21/20-074.html."}
{"doc_id": "1910.10683", "para_id": 8, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 9, "text": "or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks).\nIn modern machine learning practice, providing this knowledge is rarely done explicitly;\ninstead, it is often learned as part of an auxiliary task. For example, a historically common\napproach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map\nword identities to a continuous representation where, ideally, similar words map to similar\nvectors. These vectors are often learned through an objective that, for example, encourages\nco-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\nRecently, it has become increasingly common to pre-train the entire model on a data-rich\ntask. Ideally, this pre-training causes the model to develop general-purpose abilities and\nknowledge that can then be transferred to downstream tasks. In applications of transfer\nlearning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014), pre-training is typically done via supervised learning on a large labeled data set\nlike ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques\nfor transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.\nThis approach has recently been used to obtain state-of-the-art results in many of the most\ncommon NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu\net al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training\nfor NLP is particularly attractive because unlabeled text data is available en masse thanks\nto the Internet—for example, the Common Crawl project2 produces about 20TB of text\ndata extracted from web pages each month. This is a natural fit for neural networks, which\nhave been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better\nperformance simply by training a larger model on a larger data set (Hestness et al., 2017;\nShazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019;\nShazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).\nThis synergy has resulted in a great deal of recent work developing transfer learning\nmethodology for NLP, which has produced a wide landscape of pre-training objectives\n(Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled\ndata sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al.,\n2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018;\nHoulsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity\nof techniques in this burgeoning field can make it difficult to compare different algorithms,\ntease apart the effects of new contributions, and understand the space of existing methods for\ntransfer learning. Motivated by a need for more rigorous understanding, we leverage a unified\napproach to transfer learning that allows us to systematically study different approaches\nand push the current limits of the field.\nThe basic idea underlying our work is to treat every text processing problem as a\n“text-to-text” problem, i.e. taking text as input and producing new text as output. This\napproach is inspired by previous unifying frameworks for NLP tasks, including casting all text\nproblems as question answering (McCann et al., 2018), language modeling (Radford et al.,\n2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework\nallows us to directly apply the same model, objective, training procedure, and decoding\nprocess to every task we consider. We leverage this flexibility by evaluating performance\non a wide variety of English-based NLP problems, including question answering, document"}
{"doc_id": "1910.10683", "para_id": 10, "text": "\"summarize: state authorities\ndispatched emergency crews tuesday to"}
{"doc_id": "1910.10683", "para_id": 11, "text": "Figure 1: A diagram of our text-to-text framework. Every task we consider—including\ntranslation, question answering, and classification—is cast as feeding our model\ntext as input and training it to generate some target text. This allows us to use the\nsame model, loss function, hyperparameters, etc. across our diverse set of tasks. It\nalso provides a standard testbed for the methods included in our empirical survey.\n“T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”."}
{"doc_id": "1910.10683", "para_id": 12, "text": "summarization, and sentiment classification, to name a few. With this unified approach,\nwe can compare the effectiveness of different transfer learning objectives, unlabeled data\nsets, and other factors, while exploring the limits of transfer learning for NLP by scaling up\nmodels and data sets beyond what has previously been considered."}
{"doc_id": "1910.10683", "para_id": 13, "text": "We emphasize that our goal is not to propose new methods but instead to provide a\ncomprehensive perspective on where the field stands. As such, our work primarily comprises\na survey, exploration, and empirical comparison of existing techniques. We also explore the\nlimits of current approaches by scaling up the insights from our systematic study (training\nmodels up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks\nwe consider. In order to perform experiments at this scale, we introduce the “Colossal Clean\nCrawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text\nscraped from the web. Recognizing that the main utility of transfer learning is the possibility\nof leveraging pre-trained models in data-scarce settings, we release our code, data sets, and\npre-trained models.1"}
{"doc_id": "1910.10683", "para_id": 14, "text": "The remainder of the paper is structured as follows: In the following section, we discuss\nour base model and its implementation, our procedure for formulating every text processing\nproblem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a\nlarge set of experiments that explore the field of transfer learning for NLP. At the end of the\nsection (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art\nresults on a wide variety of benchmarks. Finally, we provide a summary of our results and\nwrap up with a look towards the future in Section 4."}
{"doc_id": "1910.10683", "para_id": 15, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 16, "text": "Before presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our “Colossal Clean Crawled\nCorpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the “Text-to-Text Transfer Transformer”\n(T5)."}
{"doc_id": "1910.10683", "para_id": 17, "text": "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the “Transformer” architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention"}
{"doc_id": "1910.10683", "para_id": 18, "text": "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/"}
{"doc_id": "1910.10683", "para_id": 19, "text": "mechanism after each self-attention layer that attends to the output of the encoder. The\nself-attention mechanism in the decoder also uses a form of autoregressive or causal self-\nattention, which only allows the model to attend to past outputs. The output of the final\ndecoder block is fed into a dense layer with a softmax output, whose weights are shared with\nthe input embedding matrix. All attention mechanisms in the Transformer are split up into\nindependent “heads” whose outputs are concatenated before being further processed.\nSince self-attention is order-independent (i.e. it is an operation on sets), it is common\nto provide an explicit position signal to the Transformer. While the original Transformer\nused a sinusoidal position signal or learned position embeddings, it has recently become\nmore common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\nInstead of using a fixed embedding for each position, relative position embeddings produce\na different learned embedding according to the offset between the “key” and “query” being\ncompared in the self-attention mechanism. We use a simplified form of position embeddings\nwhere each “embedding” is simply a scalar that is added to the corresponding logit used\nfor computing the attention weights. For efficiency, we also share the position embedding\nparameters across all layers in our model, though within a given layer each attention head\nuses a different learned position embedding. Typically, a fixed number of embeddings are\nlearned, each corresponding to a range of possible key-query offsets. In this work, we use 32\nembeddings for all of our models with ranges that increase in size logarithmically up to an\noffset of 128 beyond which we assign all relative positions to the same embedding. Note\nthat a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\ncan build a sensitivity to larger offsets by combining local information from previous layers.\nTo summarize, our model is roughly equivalent to the original Transformer proposed by\nVaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer\nnormalization outside the residual path, and using a different position embedding scheme.\nSince these architectural changes are orthogonal to the experimental factors we consider in\nour empirical survey of transfer learning, we leave the ablation of their impact for future\nwork.\nAs part of our study, we experiment with the scalability of these models, i.e. how their\nperformance changes as they are made to have more parameters or layers. Training large\nmodels can be non-trivial since they might not fit on a single machine and require a great deal\nof computation. As a result, we use a combination of model and data parallelism and train\nmodels on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers\nthat contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with\nsupporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,\n2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky,\n2014)."}
{"doc_id": "1910.10683", "para_id": 20, "text": "Much of the previous work on transfer learning for NLP makes use of large unlabeled data\nsets for unsupervised learning. In this paper, we are interested in measuring the effect of the\nquality, characteristics, and size of this unlabeled data. To generate data sets that satisfy\nour needs, we leverage Common Crawl as a source of text scraped from the web. Common"}
{"doc_id": "1910.10683", "para_id": 21, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 22, "text": "Crawl has previously been used as a source of text data for NLP, for example to train an\nn-gram language model (Buck et al., 2014), as training data for commonsense reasoning\n(Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013),\nas a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and\neven simply as a giant text corpus for testing optimizers (Anil et al., 2019).\nCommon Crawl is a publicly-available web archive that provides “web extracted text”\nby removing markup and other non-text content from the scraped HTML files. This process\nproduces around 20TB of scraped text data each month. Unfortunately, the majority of the\nresulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate\ntext like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped\ntext contains content that is unlikely to be helpful for any of the tasks we consider (offensive\nlanguage, placeholder text, source code, etc.). To address these issues, we used the following\nheuristics for cleaning up Common Crawl’s web extracted text:"}
{"doc_id": "1910.10683", "para_id": 23, "text": "• We only retained lines that ended in a terminal punctuation mark (i.e. a period,\nexclamation mark, question mark, or end quotation mark)."}
{"doc_id": "1910.10683", "para_id": 24, "text": "• We discarded any page with fewer than 3 sentences and only retained lines that\ncontained at least 5 words."}
{"doc_id": "1910.10683", "para_id": 25, "text": "• We removed any page that contained any word on the “List of Dirty, Naughty, Obscene\nor Otherwise Bad Words”.6"}
{"doc_id": "1910.10683", "para_id": 26, "text": "• Many of the scraped pages contained warnings stating that Javascript should be\nenabled so we removed any line with the word Javascript."}
{"doc_id": "1910.10683", "para_id": 27, "text": "• Some pages had placeholder “lorem ipsum” text; we removed any page where the\nphrase “lorem ipsum” appeared."}
{"doc_id": "1910.10683", "para_id": 28, "text": "• Some pages inadvertently contained code. Since the curly bracket “{” appears in\nmany programming languages (such as Javascript, widely used on the web) but not in\nnatural text, we removed any pages that contained a curly bracket."}
{"doc_id": "1910.10683", "para_id": 29, "text": "• Since some of the scraped pages were sourced from Wikipedia and had citation markers\n(e.g. [1], [citation needed], etc.), we removed any such markers."}
{"doc_id": "1910.10683", "para_id": 30, "text": "• Many pages had boilerplate policy notices, so we removed any lines containing the\nstrings “terms of use”, “privacy policy”, “cookie policy”, “uses cookies”, “use of\ncookies”, or “use cookies”."}
{"doc_id": "1910.10683", "para_id": 31, "text": "• To deduplicate the data set, we discarded all but one of any three-sentence span\noccurring more than once in the data set."}
{"doc_id": "1910.10683", "para_id": 32, "text": "Additionally, since most of our downstream tasks are focused on English-language text,\nwe used langdetect7 to filter out any pages that were not classified as English with a\nprobability of at least 0.99. Our heuristics are inspired by past work on using Common"}
{"doc_id": "1910.10683", "para_id": 33, "text": "6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n7. https://pypi.org/project/langdetect/"}
{"doc_id": "1910.10683", "para_id": 34, "text": "Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an\nautomatic language detector and discard short lines and Smith et al. (2013); Grave et al.\n(2018) both perform line-level deduplication. However, we opted to create a new data set\nbecause prior data sets use a more limited set of filtering heuristics, are not publicly available,\nand/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,\n2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on\nparallel training data for machine translation (Smith et al., 2013)).\nTo assemble our base data set, we downloaded the web extracted text from April 2019\nand applied the aforementioned filtering. This produces a collection of text that is not only\norders of magnitude larger than most data sets used for pre-training (about 750 GB) but also\ncomprises reasonably clean and natural English text. We dub this data set the “Colossal\nClean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8"}
{"doc_id": "1910.10683", "para_id": 35, "text": "We consider the impact of using various alternative versions of this data set in Section 3.4."}
{"doc_id": "1910.10683", "para_id": 36, "text": "Our goal in this paper is to measure general language learning abilities. As such, we study\ndownstream performance on a diverse set of benchmarks, including machine translation,\nquestion answering, abstractive summarization, and text classification. Specifically, we\nmeasure performance on the GLUE and SuperGLUE text classification meta-benchmarks;\nCNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English\nto German, French, and Romanian translation. All data was sourced from TensorFlow\nDatasets.9"}
{"doc_id": "1910.10683", "para_id": 37, "text": "GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a\ncollection of text classification tasks meant to test general language understanding abilities:"}
{"doc_id": "1910.10683", "para_id": 38, "text": "• Sentence acceptability judgment (CoLA (Warstadt et al., 2018))"}
{"doc_id": "1910.10683", "para_id": 39, "text": "• Sentiment analysis (SST-2 (Socher et al., 2013))"}
{"doc_id": "1910.10683", "para_id": 40, "text": "• Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer\net al., 2017), QQP (Iyer et al., 2017))"}
{"doc_id": "1910.10683", "para_id": 41, "text": "• Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,"}
{"doc_id": "1910.10683", "para_id": 42, "text": "2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 43, "text": "• Coreference resolution (WNLI and WSC (Levesque et al., 2012))"}
{"doc_id": "1910.10683", "para_id": 44, "text": "• Sentence completion (COPA (Roemmele et al., 2011))"}
{"doc_id": "1910.10683", "para_id": 45, "text": "• Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))"}
{"doc_id": "1910.10683", "para_id": 46, "text": "• Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018),\nBoolQ (Clark et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 47, "text": "8. https://www.tensorflow.org/datasets/catalog/c4\n9. https://www.tensorflow.org/datasets"}
{"doc_id": "1910.10683", "para_id": 48, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 49, "text": "We use the data sets as distributed by the GLUE and SuperGLUE benchmarks.\nFor\nsimplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly\nfor SuperGLUE) as a single task by concatenating all of the constituent data sets. As\nsuggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR)\ndata set (Rahman and Ng, 2012) in the combined SuperGLUE task.\nThe CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question-\nanswering task but was adapted for text summarization by Nallapati et al. (2016); we\nuse the non-anonymized version from See et al. (2017) as an abstractive summarization\ntask. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our\nexperiments, the model is fed the question and its context and asked to generate the answer\ntoken-by-token. For WMT English to German, we use the same training data as (Vaswani\net al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013\nas a validation set (Bojar et al., 2014). For English to French, we use the standard training\ndata from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to\nRomanian, which is a standard lower-resource machine translation benchmark, we use the\ntrain and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train\non English data, so in order to learn to translate a given model will need to learn to generate\ntext in a new language."}
{"doc_id": "1910.10683", "para_id": 50, "text": "In order to train a single model on the diverse set of tasks described above, we cast all of\nthe tasks we consider into a “text-to-text” format—that is, a task where the model is fed\nsome text for context or conditioning and is then asked to produce some output text. This\nframework provides a consistent training objective both for pre-training and fine-tuning.\nSpecifically, the model is trained with a maximum likelihood objective (using “teacher forcing”\n(Williams and Zipser, 1989)) regardless of the task. To specify which task the model should\nperform, we add a task-specific (text) prefix to the original input sequence before feeding it\nto the model."}
{"doc_id": "1910.10683", "para_id": 51, "text": "As an example, to ask the model to translate the sentence “That is good.” from English\nto German, the model would be fed the sequence “translate English to German: That is\ngood.” and would be trained to output “Das ist gut.” For text classification tasks, the\nmodel simply predicts a single word corresponding to the target label. For example, on the\nMNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies\n(“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With\nour preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis:\nMy feelings towards pigeons are filled with animosity.” with the corresponding target word\n“entailment”. Note that an issue arises if our model outputs text on a text classification\ntask that does not correspond to any of the possible labels (for example if the model\noutputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”,\nor “contradiction”). In this case, we always count the model’s output as wrong, though we\nnever observed this behavior in any of our trained models. Note that the choice of text prefix\nused for a given task is essentially a hyperparameter; we found that changing the exact\nwording of the prefix had limited impact and so did not perform extensive experiments into\ndifferent prefix choices. A diagram of our text-to-text framework with a few input/output"}
{"doc_id": "1910.10683", "para_id": 52, "text": "examples is shown in Figure 1. We provide full examples of preprocessed inputs for every\ntask we studied in Appendix D.\nOur text-to-text framework follows previous work that casts multiple NLP tasks into\na common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a\nbenchmark that uses a consistent question-answering format for a suite of ten NLP tasks.\nThe Natural Language Decathlon also stipulates that all models must be multi-task, i.e.\nare able to simultaneously tackle all of the tasks at once. We instead allow for separately\nfine-tuning the model on each individual task and use short task prefixes instead of an explicit\nquestion-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of\nlanguage models by feeding some input to the model as a prefix and then autoregressively\nsampling an output. For example, automatic summarization is done by feeding in a document\nfollowed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation)\nand then the summary is predicted via autoregressive decoding. We mainly consider models\nthat explicitly process an input with an encoder before generating an output with a separate\ndecoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar\net al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to\npossible output choices are appended to the input and the model is trained to extract the\ninput span corresponding to the correct choice. In contrast, our framework also allows for\ngenerative tasks like machine translation and abstractive summarization where it is not\npossible to enumerate all possible output choices.\nWe were able to straightforwardly cast all of the tasks we considered into a text-to-text\nformat with the exception of STS-B, which is a regression task where the goal is to predict\na similarity score between 1 and 5. We found that most of these scores were annotated\nin increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and\nconverted the result to a literal string representation of the number (e.g. the floating-point\nvalue 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a\nstring corresponding to a number between 1 and 5, we convert it to a floating-point value;\notherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B\nregression problem as a 21-class classification problem.\nSeparately, we also convert the Winograd tasks (WNLI from GLUE, WSC from Super-\nGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more\namenable to the text-to-text framework. Examples from the Winograd tasks consist of a\ntext passage containing an ambiguous pronoun that could refer to more than one of the noun\nphrases in the passage. For example, the passage might be “The city councilmen refused\nthe demonstrators a permit because they feared violence.”, which contains the ambiguous\npronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI,\nWSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in\nthe text passage and asking the model to predict the noun that it refers to. The example\nmentioned above would be transformed to the input “The city councilmen refused the\ndemonstrators a permit because *they* feared violence.” and the model would be trained to\npredict the target text “The city councilmen”.\nFor WSC, examples contain the passage, the ambiguous pronoun, a candidate noun,\nand a True/False label reflecting whether the candidate matches the pronoun (ignoring any\narticles). We only train on examples with a “True” label since we do not know the correct\nnoun targets for examples with a “False” label. For evaluation, we assign a “True” label if"}
{"doc_id": "1910.10683", "para_id": 53, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 54, "text": "the words in the model’s output are a subset of the words in the candidate noun phrase\n(or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC\ntraining set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples\nfrom DPR are annotated with the correct referent noun, making it easy to use this data set\nin the format listed above."}
{"doc_id": "1910.10683", "para_id": 55, "text": "The WNLI training and validation sets have a significant overlap with the WSC training\nset. To avoid leaking validation examples into our training data (a particular issue in the\nmulti-task experiments of Section 3.5.2), we therefore never train on WNLI and never report\nresults on the WNLI validation set. Omitting results on the WNLI validation set is standard\npractice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the\ntraining set, i.e. validation examples are all slightly-perturbed versions of training examples\nwith the opposite label. As such, we do not include WNLI in the average GLUE score\nwhenever we report on the validation set (all sections except Section 3.7 where results\nare presented on the test sets). Converting examples from WNLI to the “referent noun\nprediction” variant described above is a little more involved; we describe this process in\nAppendix B."}
{"doc_id": "1910.10683", "para_id": 56, "text": "Recent advances in transfer learning for NLP have come from a wide variety of developments,\nsuch as new pre-training objectives, model architectures, unlabeled data sets, and more.\nIn this section, we carry out an empirical survey of these techniques in hopes of teasing\napart their contribution and significance. We then combine the insights gained to attain\nstate-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly\ngrowing area of research, it is not feasible for us to cover every possible technique or idea\nin our empirical study. For a broader literature review, we recommend a recent survey by\nRuder et al. (2019)."}
{"doc_id": "1910.10683", "para_id": 57, "text": "We systematically study these contributions by taking a reasonable baseline (described\nin Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3\nwe measure the performance of different unsupervised objectives while keeping the rest of\nour experimental pipeline fixed. This “coordinate ascent” approach might miss second-order\neffects (for example, some particular unsupervised objective may work best on a model\nlarger than our baseline setting), but performing a combinatorial exploration of all of the\nfactors in our study would be prohibitively expensive. In future work, we expect it could be\nfruitful to more thoroughly consider combinations of the approaches we study."}
{"doc_id": "1910.10683", "para_id": 58, "text": "Our goal is to compare a variety of different approaches on a diverse set of tasks while\nkeeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do\nnot exactly replicate existing approaches. For example, “encoder-only” models like BERT\n(Devlin et al., 2018) are designed to produce a single prediction per input token or a single\nprediction for an entire input sequence. This makes them applicable for classification or span\nprediction tasks but not for generative tasks like translation or abstractive summarization.\nAs such, none of the model architectures we consider are identical to BERT or consist of an\nencoder-only structure. Instead, we test approaches that are similar in spirit—for example,\nwe consider an analogous objective to BERT’s “masked language modeling” objective in"}
{"doc_id": "1910.10683", "para_id": 59, "text": "Section 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7)."}
{"doc_id": "1910.10683", "para_id": 60, "text": "Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections."}
{"doc_id": "1910.10683", "para_id": 61, "text": "For our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single “stack” (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of 12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention\nmechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\nthis results in a model with about 220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied\nin the model."}
{"doc_id": "1910.10683", "para_id": 62, "text": "As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,"}
{"doc_id": "1910.10683", "para_id": 63, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 64, "text": "we “pack” multiple sequences into each entry of the batch10 so that our batches contain\nroughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds\nto pre-training on 235 ≈34B tokens. This is considerably less than BERT (Devlin et al.,\n2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly\n2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still\nproviding a sufficient amount of pre-training for acceptable performance. We consider the\neffect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\nDuring pre-training, we use an “inverse square root” learning rate schedule: 1\n\u000ep"}
{"doc_id": "1910.10683", "para_id": 65, "text": "max(n, k)\nwhere n is the current training iteration and k is the number of warm-up steps (set to 104"}
{"doc_id": "1910.10683", "para_id": 66, "text": "in all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps,\nthen exponentially decays the learning rate until pre-training is over. We also experimented\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\nbetter results but requires knowing the total number of training steps ahead of time. Since\nwe will be varying the number of training steps in some of our experiments, we opt for the\nmore generic inverse square root schedule.\nOur models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen\nas a trade-off between the high-resource tasks (i.e. those with large data sets), which\nbenefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit\nquickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.\n216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save\na checkpoint every 5,000 steps and report results on the model checkpoint corresponding\nto the highest validation performance. For models fine-tuned on multiple tasks, we choose\nthe best checkpoint for each task independently. For all of the experiments except those in\nSection 3.7, we report results in the validation set to avoid performing model selection on\nthe test set."}
{"doc_id": "1910.10683", "para_id": 67, "text": "We use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens\n(Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000\nwordpieces. Since we ultimately fine-tune our model on English to German, French, and\nRomanian translation, we also require that our vocabulary covers these non-English languages.\nTo address this, we classified pages from the Common Crawl scrape used in C4 as German,\nFrench, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts\nof English C4 data with 1 part each of data classified as German, French or Romanian.\nThis vocabulary was shared across both the input and output of our model. Note that\nour vocabulary makes it so that our model can only process a predetermined, fixed set of\nlanguages."}
{"doc_id": "1910.10683", "para_id": 68, "text": "Leveraging unlabeled data to pre-train our model necessitates an objective that does not\nrequire labels but (loosely speaking) teaches the model generalizable knowledge that will be"}
{"doc_id": "1910.10683", "para_id": 69, "text": "10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\nindex.html#data_generators.generator_utils.pack_examples"}
{"doc_id": "1910.10683", "para_id": 70, "text": "Figure 2: Schematic of the objective we use in our baseline model. In this example, we\nprocess the sentence “Thank you for inviting me to your party last week.” The\nwords “for”, “inviting” and “last” (marked with an ×) are randomly chosen for\ncorruption. Each consecutive span of corrupted tokens is replaced by a sentinel\ntoken (shown as <X> and <Y>) that is unique over the example. Since “for” and\n“inviting” occur consecutively, they are replaced by a single sentinel <X>. The\noutput sequence then consists of the dropped-out spans, delimited by the sentinel\ntokens used to replace them in the input plus a final sentinel token <Z>."}
{"doc_id": "1910.10683", "para_id": 71, "text": "useful in downstream tasks. Preliminary work that applied the transfer learning paradigm\nof pre-training and fine-tuning all of the model’s parameters to NLP problems used a\ncausal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that\n“denoising” objectives (Devlin et al., 2018; Taylor, 1953) (also called “masked language\nmodeling”) produce better performance and as a result they have quickly become standard.\nIn a denoising objective, the model is trained to predict missing or otherwise corrupted\ntokens in the input. Inspired by BERT’s “masked language modeling” objective and the\n“word dropout” regularization technique (Bowman et al., 2015), we design an objective that\nrandomly samples and then drops out 15% of tokens in the input sequence. All consecutive\nspans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token\nis assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens\nwhich are added to our vocabulary and do not correspond to any wordpiece. The target\nthen corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel\ntokens used in the input sequence plus a final sentinel token to mark the end of the target\nsequence. Our choices to mask consecutive spans of tokens and only predict dropped-out\ntokens were made to reduce the computational cost of pre-training. We perform thorough\ninvestigation into pre-training objectives in Section 3.3. An example of the transformation\nresulting from applying this objective is shown in Figure 2. We empirically compare this\nobjective to many other variants in Section 3.3."}
{"doc_id": "1910.10683", "para_id": 72, "text": "In this section, we present results using the baseline experimental procedure described above\nto get a sense of what kind of performance to expect on our suite of downstream tasks.\nIdeally, we would repeat every experiment in our study multiple times to get a confidence\ninterval on our results. Unfortunately, this would be prohibitively expensive due to the large"}
{"doc_id": "1910.10683", "para_id": 73, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 74, "text": "⋆Baseline average\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline standard deviation\n0.235\n0.065\n0.343\n0.416\n0.112\n0.090\n0.108\nNo pre-training\n66.22\n17.60\n50.31\n53.04\n25.86\n39.77\n24.04"}
{"doc_id": "1910.10683", "para_id": 75, "text": "Table 1: Average and standard deviation of scores achieved by our baseline model and\ntraining procedure. For comparison, we also report performance when training on\neach task from scratch (i.e. without any pre-training) for the same number of steps\nused to fine-tune the baseline model. All scores in this table (and every table in\nour paper except Table 14) are reported on the validation sets of each data set."}
{"doc_id": "1910.10683", "para_id": 76, "text": "number of experiments we run. As a cheaper alternative, we train our baseline model 10\ntimes from scratch (i.e. with different random initializations and data set shuffling) and\nassume that the variance over these runs of the base model also applies to each experimental\nvariant. We don’t expect most of the changes we make to have a dramatic effect on the\ninter-run variance, so this should provide a reasonable indication of the significance of\ndifferent changes. Separately, we also measure the performance of training our model for 218"}
{"doc_id": "1910.10683", "para_id": 77, "text": "steps (the same number we use for fine-tuning) on all downstream tasks without pre-training.\nThis gives us an idea of how much pre-training benefits our model in the baseline setting.\nWhen reporting results in the main text, we only report a subset of the scores across all\nthe benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we\nreport the average score across all subtasks (as stipulated by the official benchmarks) under\nthe headings “GLUE” and “SGLUE”. For all translation tasks, we report the BLEU score\n(Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with “exp” smoothing\nand “intl” tokenization. We refer to scores for WMT English to German, English to French,\nand English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail,\nwe find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F\nmetrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under\nthe heading “CNNDM”. Similarly, for SQuAD we find the performance of the “exact match”\nand “F1” scores to be highly correlated so we report the “exact match” score alone. We\nprovide every score achieved on every task for all experiments in Table 16, Appendix E.\nOur results tables are all formatted so that each row corresponds to a particular experi-\nmental configuration with columns giving the scores for each benchmark. We will include\nthe mean performance of the baseline configuration in most tables. Wherever a baseline\nconfiguration appears, we will mark it with a ⋆(as in the first row of Table 1). We also\nwill boldface any score that is within two standard deviations of the maximum (best) in a\ngiven experiment.\nOur baseline results are shown in Table 1. Overall, our results are comparable to existing\nmodels of similar size. For example, BERTBASE achieved an exact match score of 80.8\non SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and\n84.24, respectively (see Table 16). Note that we cannot directly compare our baseline to\nBERTBASE because ours is an encoder-decoder model and was pre-trained for roughly 1⁄4\nas many steps. Unsurprisingly, we find that pre-training provides significant gains across\nalmost all benchmarks. The only exception is WMT English to French, which is a large"}
{"doc_id": "1910.10683", "para_id": 78, "text": "enough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and “no pre-training” emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data efficiency in this paper, we emphasize that this is one of the primary benefits of the\ntransfer learning paradigm.\nAs for inter-run variance, we find that for most tasks the standard deviation across runs\nis smaller than 1% of the task’s baseline score. Exceptions to this rule include CoLA, CB,\nand COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks.\nFor example, on CB our baseline model had an average F1 score of 91.22 with a standard\ndeviation of 3.237 (see Table 16), which may be partly due to the fact that CB’s validation\nset contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed\nas the average of scores across the tasks comprising each benchmark. As a result, we caution\nthat the high inter-run variance of CoLA, CB, and COPA can make it harder to compare\nmodels using the GLUE and SuperGLUE scores alone."}
{"doc_id": "1910.10683", "para_id": 79, "text": "While the Transformer was originally introduced with an encoder-decoder architecture, much\nmodern work on transfer learning for NLP uses alternative architectures. In this section, we\nreview and compare these architectural variants."}
{"doc_id": "1910.10683", "para_id": 80, "text": "A major distinguishing factor for different architectures is the “mask” used by different\nattention mechanisms in the model. Recall that the self-attention operation in a Transformer\ntakes a sequence as input and outputs a new sequence of the same length. Each entry of\nthe output sequence is produced by computing a weighted average of entries of the input\nsequence. Specifically, let yi refer to the ith element of the output sequence and xj refer to\nthe jth entry of the input sequence. yi is computed as P\nj wi,jxj, where wi,j is the scalar\nweight produced by the self-attention mechanism as a function of xi and xj. The attention\nmask is then used to zero out certain weights in order to constrain which entries of the input\ncan be attended to at a given output timestep. Diagrams of the masks we will consider are\nshown in Figure 3. For example, the causal mask (Figure 3, middle) sets any wi,j to zero if\nj > i.\nThe first model structure we consider is an an encoder-decoder Transformer, which\nconsists of two layer stacks: The encoder, which is fed an input sequence, and the decoder,\nwhich produces a new output sequence. A schematic of this architectural variant is shown\nin the left panel of Figure 4.\nThe encoder uses a “fully-visible” attention mask. Fully-visible masking allows a self-\nattention mechanism to attend to any entry of the input when producing each entry of\nits output. We visualize this masking pattern in Figure 3, left. This form of masking is\nappropriate when attending over a “prefix”, i.e. some context provided to the model that\nis later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible\nmasking pattern and appends a special “classification” token to the input. BERT’s output"}
{"doc_id": "1910.10683", "para_id": 81, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 82, "text": "Figure 3: Matrices representing different attention mask patterns. The input and output\nof the self-attention mechanism are denoted x and y respectively. A dark cell\nat row i and column j indicates that the self-attention mechanism is allowed to\nattend to input element j at output timestep i. A light cell indicates that the\nself-attention mechanism is not allowed to attend to the corresponding i and j\ncombination. Left: A fully-visible mask allows the self-attention mechanism to\nattend to the full input at every output timestep. Middle: A causal mask prevents\nthe ith output element from depending on any input elements from “the future”.\nRight: Causal masking with a prefix allows the self-attention mechanism to use\nfully-visible masking on a portion of the input sequence."}
{"doc_id": "1910.10683", "para_id": 83, "text": "at the timestep corresponding to the classification token is then used to make a prediction\nfor classifying the input sequence.\nThe self-attention operations in the Transformer’s decoder use a “causal” masking pattern.\nWhen producing the ith entry of the output sequence, causal masking prevents the model\nfrom attending to the jth entry of the input sequence for j > i. This is used during training\nso that the model can’t “see into the future” as it produces its output. An attention matrix\nfor this masking pattern is shown in Figure 3, middle.\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an\noutput sequence. That is, at each output timestep, a token is sampled from the model’s\npredicted distribution and the sample is fed back into the model to produce a prediction for\nthe next output timestep, and so on. As such, a Transformer decoder (without an encoder)\ncan be used as a language model (LM), i.e. a model trained solely for next-step prediction\n(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second\nmodel structure we consider. A schematic of this architecture is shown in Figure 4, middle.\nIn fact, early work on transfer learning for NLP used this architecture with a language\nmodeling objective as a pre-training method (Radford et al., 2018).\nLanguage models are typically used for compression or sequence generation (Graves,\n2013). However, they can also be used in the text-to-text framework simply by concatenating\nthe inputs and targets. As an example, consider the case of English to German translation:\nIf we have a training datapoint with input sentence “That is good.” and target “Das ist\ngut.”, we would simply train the model on next-step prediction over the concatenated input\nsequence “translate English to German: That is good. target: Das ist gut.” If we wanted to"}
{"doc_id": "1910.10683", "para_id": 84, "text": "Figure 4: Schematics of the Transformer architecture variants we consider. In this diagram,\nblocks represent elements of a sequence and lines represent attention visibility.\nDifferent colored groups of blocks indicate different Transformer layer stacks. Dark\ngrey lines correspond to fully-visible masking and light grey lines correspond\nto causal masking. We use “.” to denote a special end-of-sequence token that\nrepresents the end of a prediction. The input and output sequences are represented\nas x and y respectively. Left: A standard encoder-decoder architecture uses fully-\nvisible masking in the encoder and the encoder-decoder attention, with causal\nmasking in the decoder. Middle: A language model consists of a single Transformer\nlayer stack and is fed the concatenation of the input and target, using a causal\nmask throughout. Right: Adding a prefix to a language model corresponds to\nallowing fully-visible masking over the input."}
{"doc_id": "1910.10683", "para_id": 85, "text": "obtain the model’s prediction for this example, the model would be fed the prefix “translate\nEnglish to German: That is good. target:” and would be asked to generate the remainder\nof the sequence autoregressively. In this way, the model can predict an output sequence\ngiven an input, which satisfies the needs of text-to-text tasks. This approach was recently\nused to show that language models can learn to perform some text-to-text tasks without\nsupervision (Radford et al., 2019)."}
{"doc_id": "1910.10683", "para_id": 86, "text": "A fundamental and frequently cited drawback of using a language model in the text-\nto-text setting is that causal masking forces the model’s representation of the ith entry of\nthe input sequence to only depend on the entries up until i. To see why this is potentially\ndisadvantageous, consider the text-to-text framework where the model is provided with a\nprefix/context before being asked to make predictions (e.g., the prefix is an English sentence\nand the model is asked to predict the German translation). With fully causal masking, the\nmodel’s representation of a prefix state can only depend on prior entries of the prefix. So,\nwhen predicting an entry of the output, the model will attend to a representation of the\nprefix that is unnecessarily limited. Similar arguments have been made against using a\nunidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau\net al., 2015)."}
{"doc_id": "1910.10683", "para_id": 87, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 88, "text": "This issue can be avoided in a Transformer-based language model simply by changing\nthe masking pattern. Instead of using a causal mask, we use fully-visible masking during\nthe prefix portion of the sequence. This masking pattern and a schematic of the resulting\n“prefix LM” (the third model structure we consider) are illustrated in the rightmost panels of\nFigures 3 and 4, respectively. In the English to German translation example mentioned above,\nfully-visible masking would be applied to the prefix “translate English to German: That is\ngood. target:” and causal masking would be used during training for predicting the target\n“Das ist gut.” Using a prefix LM in the text-to-text framework was originally proposed by"}
{"doc_id": "1910.10683", "para_id": 89, "text": "Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective\non a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder\nmodel with parameters shared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target sequence."}
{"doc_id": "1910.10683", "para_id": 90, "text": "We note that when following our text-to-text framework, the prefix LM architecture\nclosely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an\nexample from the MNLI benchmark where the premise is “I hate pigeons.”, the hypothesis is\n“My feelings towards pigeons are filled with animosity.” and the correct label is “entailment”.\nTo feed this example into a language model, we would transform it into the sequence “mnli\npremise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\ntarget: entailment”. In this case, the fully-visible prefix would correspond to the entire input\nsequence up to the word “target:”, which can be seen as being analogous to the “classification”\ntoken used in BERT. So, our model would have full visibility over the entire input, and then\nwould be tasked with making a classification by outputting the word “entailment”. It is easy\nfor the model to learn to output one of the valid class labels given the task prefix (“mnli” in\nthis case). As such, the main difference between a prefix LM and the BERT architecture is\nthat the classifier is simply integrated into the output layer of the Transformer decoder in\nthe prefix LM."}
{"doc_id": "1910.10683", "para_id": 91, "text": "In the interest of experimentally comparing these architectural variants, we would like each\nmodel we consider to be equivalent in some meaningful way. We might say that two models\nare equivalent if they either have the same number of parameters or they require roughly\nthe same amount of computation to process a given (input-sequence, target-sequence) pair.\nUnfortunately, it is not possible to compare an encoder-decoder model to a language model\narchitecture (comprising a single Transformer stack) according to both of these criteria\nat the same time. To see why, first note an encoder-decoder model with L layers in the\nencoder and L layers in the decoder has approximately the same number of parameters as a\nlanguage model with 2L layers. However, the same L + L encoder-decoder model will have\napproximately the same computational cost as a language model with only L layers. This\nis a consequence of the fact that the L layers in the language model must be applied to\nboth the input and output sequence, while the encoder is only applied to the input sequence\nand the decoder is only applied to the output sequence. Note that these equivalences are\napproximate—there are some extra parameters in the decoder due to the encoder-decoder\nattention and there are also some computational costs in the attention layers that are\nquadratic in the sequence lengths. In practice, however, we observed nearly identical step"}
{"doc_id": "1910.10683", "para_id": 92, "text": "times for L-layer language models versus L + L-layer encoder-decoder models, suggesting a\nroughly equivalent computational cost. Further, for the model sizes we consider, the number\nof parameters in the encoder-decoder attention layers is about 10% of the total parameter\ncount, so we make the simplifying assumption that an L + L-layer encoder-decoder model\nhas the same number of parameters as an 2L-layer language model.\nTo provide a reasonable means of comparison, we consider multiple configurations for\nour encoder-decoder model. We will refer to the number of layers and parameters in a\nBERTBASE-sized layer stack as L and P, respectively. We will use M to refer to the number\nof FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model\nto process a given input-target pair. In total, we will compare:"}
{"doc_id": "1910.10683", "para_id": 93, "text": "• An encoder-decoder model with L layers in the encoder and L layers in the decoder.\nThis model has 2P parameters and a computation cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 94, "text": "• An equivalent model, but with parameters shared across the encoder and decoder,\nresulting in P parameters and an M-FLOP computational cost."}
{"doc_id": "1910.10683", "para_id": 95, "text": "• An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P\nparameters and an M/2-FLOP cost."}
{"doc_id": "1910.10683", "para_id": 96, "text": "• A decoder-only language model with L layers and P parameters and a resulting\ncomputational cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 97, "text": "• A decoder-only prefix LM with the same architecture (and thus the same number\nof parameters and computational cost), but with fully-visible self-attention over the\ninput."}
{"doc_id": "1910.10683", "para_id": 98, "text": "As an unsupervised objective, we will consider both a basic language modeling objective as\nwell as our baseline denoising objective described in Section 3.1.4. We include the language\nmodeling objective due to its historic use as a pre-training objective (Dai and Le, 2015;\nRamachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,\n2018) as well as its natural fit for the language model architectures we consider. For models\nthat ingest a prefix before making predictions (the encoder-decoder model and prefix LM),\nwe sample a span of text from our unlabeled data set and choose a random point to split\nit into prefix and target portions. For the standard language model, we train the model\nto predict the entire span from beginning to end. Our unsupervised denoising objective is\ndesigned for text-to-text models; to adapt it for use with a language model we concatenate\nthe inputs and targets as described in Section 3.2.1."}
{"doc_id": "1910.10683", "para_id": 99, "text": "The scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count (2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in"}
{"doc_id": "1910.10683", "para_id": 100, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 101, "text": "Architecture\nObjective\nParams\nCost\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 102, "text": "⋆Encoder-decoder\nDenoising\n2P\nM\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nEnc-dec, shared\nDenoising\nP\nM\n82.81\n18.78\n80.63\n70.73\n26.72\n39.03\n27.46\nEnc-dec, 6 layers\nDenoising\nP\nM/2\n80.88\n18.97\n77.59\n68.42\n26.38\n38.40\n26.95\nLanguage model\nDenoising\nP\nM\n74.70\n17.93\n61.14\n55.02\n25.09\n35.28\n25.86\nPrefix LM\nDenoising\nP\nM\n81.82\n18.61\n78.94\n68.11\n26.43\n37.98\n27.39"}
{"doc_id": "1910.10683", "para_id": 103, "text": "Encoder-decoder\nLM\n2P\nM\n79.56\n18.59\n76.02\n64.29\n26.27\n39.17\n26.86\nEnc-dec, shared\nLM\nP\nM\n79.60\n18.13\n76.35\n63.50\n26.62\n39.17\n27.05\nEnc-dec, 6 layers\nLM\nP\nM/2\n78.67\n18.26\n75.32\n64.06\n26.13\n38.42\n26.89\nLanguage model\nLM\nP\nM\n73.78\n17.54\n53.81\n56.51\n25.23\n34.31\n25.38\nPrefix LM\nLM\nP\nM\n79.68\n17.84\n76.87\n64.86\n26.28\n37.51\n26.76"}
{"doc_id": "1910.10683", "para_id": 104, "text": "Table 2: Performance of the different architectural variants described in Section 3.2.2. We\nuse P to refer to the number of parameters in a 12-layer base Transformer layer\nstack and M to refer to the FLOPs required to process a sequence using the encoder-\ndecoder model. We evaluate each architectural variant using a denoising objective\n(described in Section 3.1.4) and an autoregressive objective (as is commonly used\nto train language models)."}
{"doc_id": "1910.10683", "para_id": 105, "text": "the encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al.,\n2019) also found that sharing parameters across Transformer blocks can be an effective means\nof lowering the total parameter count without sacrificing much performance. XLNet also\nbears some resemblance to the shared encoder-decoder approach with a denoising objective\n(Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms\nthe decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder\nattention is beneficial. Finally, we confirm the widely-held conception that using a denoising\nobjective always results in better downstream task performance compared to a language\nmodeling objective. This observation has been previously made by Devlin et al. (2018),\nVoita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more\ndetailed exploration of unsupervised objectives in the following section."}
{"doc_id": "1910.10683", "para_id": 106, "text": "The choice of unsupervised objective is of central importance as it provides the mechanism\nthrough which the model gains general-purpose knowledge to apply to downstream tasks.\nThis has led to the development of a wide variety of pre-training objectives (Dai and Le,\n2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019;\nLiu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019).\nIn this section, we perform a procedural exploration of the space of unsupervised objectives.\nIn many cases, we will not replicate an existing objective exactly—some will be modified to\nfit our text-to-text encoder-decoder framework and, in other cases, we will use objectives\nthat combine concepts from multiple common approaches.\nOverall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized\nspan of text from our unlabeled text data set. The token sequence is processed to produce a\n(corrupted) input sequence and a corresponding target. Then, the model is trained as usual"}
{"doc_id": "1910.10683", "para_id": 107, "text": "Prefix language modeling\nThank you for inviting\nme to your party last week .\nBERT-style Devlin et al. (2018)\nThank you <M> <M> me to your party apple week .\n(original text)\nDeshuffling\nparty me for your to . last fun you inviting week Thank\n(original text)\nMASS-style Song et al. (2019)\nThank you <M> <M> me to your party <M> week .\n(original text)\nI.i.d. noise, replace spans\nThank you <X> me to your party <Y> week .\n<X> for inviting <Y> last <Z>\nI.i.d. noise, drop tokens\nThank you me to your party week .\nfor inviting last\nRandom spans\nThank you <X> to <Y> week .\n<X> for inviting me <Y> your party last <Z>"}
{"doc_id": "1910.10683", "para_id": 108, "text": "Table 3: Examples of inputs and targets produced by some of the unsupervised objectives\nwe consider applied to the input text “Thank you for inviting me to your party last\nweek .” Note that all of our objectives process tokenized text. For this particular\nsentence, all words were mapped to a single token by our vocabulary. We write\n(original text) as a target to denote that the model is tasked with reconstructing the\nentire input text. <M> denotes a shared mask token and <X>, <Y>, and <Z> denote\nsentinel tokens that are assigned unique token IDs. The BERT-style objective\n(second row) includes a corruption where some tokens are replaced by a random\ntoken ID; we show this via the greyed-out word apple."}
{"doc_id": "1910.10683", "para_id": 109, "text": "with maximum likelihood to predict the target sequence. We provide illustrative examples\nof many of the objectives we consider in Table 3."}
{"doc_id": "1910.10683", "para_id": 110, "text": "To begin with, we compare three techniques that are inspired by commonly-used objectives\nbut differ significantly in their approach. First, we include a basic “prefix language modeling”\nobjective as was used in Section 3.2.3.\nThis technique splits a span of text into two\ncomponents, one to use as inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder. Second, we consider an objective inspired by the “masked\nlanguage modeling” (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a\nspan of text and corrupts 15% of the tokens. 90% of the corrupted tokens are replaced\nwith a special mask token and 10% are replaced with a random token. Since BERT is an\nencoder-only model, its goal during pre-training is to reconstruct masked tokens at the\noutput of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted\nsequence as the target. Note that this differs from our baseline objective, which uses only\nthe corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally,\nwe also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was\napplied to a denoising sequential autoencoder. This approach takes a sequence of tokens,\nshuffles it, and then uses the original deshuffled sequence as a target. We provide examples\nof the inputs and targets for these three methods in the first three rows of Table 3.\nThe performance of these three objectives is shown in Table 4. Overall, we find that the\nBERT-style objective performs best, though the prefix language modeling objective attains\nsimilar performance on the translation tasks. Indeed, the motivation for the BERT objective\nwas to outperform language model-based pre-training. The deshuffling objective performs\nconsiderably worse than both prefix language modeling and the BERT-style objective."}
{"doc_id": "1910.10683", "para_id": 111, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 112, "text": "Prefix language modeling\n80.69\n18.94\n77.99\n65.27\n26.86\n39.73\n27.49\nBERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nDeshuffling\n73.17\n18.59\n67.61\n58.47\n26.11\n39.30\n25.62"}
{"doc_id": "1910.10683", "para_id": 113, "text": "Table 4: Performance of the three disparate pre-training objectives described in Section 3.3.1."}
{"doc_id": "1910.10683", "para_id": 114, "text": "BERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nMASS-style (Song et al., 2019)\n82.32\n19.16\n80.10\n69.28\n26.79\n39.89\n27.55\n⋆Replace corrupted spans\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nDrop corrupted tokens\n84.44\n19.31\n80.52\n68.67\n27.07\n39.76\n27.82"}
{"doc_id": "1910.10683", "para_id": 115, "text": "Table 5: Comparison of variants of the BERT-style pre-training objective. In the first two\nvariants, the model is trained to reconstruct the original uncorrupted text segment.\nIn the latter two, the model only predicts the sequence of corrupted tokens."}
{"doc_id": "1910.10683", "para_id": 116, "text": "Based on the results in the prior section, we will now focus on exploring modifications to\nthe BERT-style denoising objective. This objective was originally proposed as a pre-training\ntechnique for an encoder-only model trained for classification and span prediction. As\nsuch, it may be possible to modify it so that it performs better or is more efficient in our\nencoder-decoder text-to-text setup.\nFirst, we consider a simple variant of the BERT-style objective where we don’t include the\nrandom token swapping step. The resulting objective simply replaces 15% of the tokens in\nthe input with a mask token and the model is trained to reconstruct the original uncorrupted\nsequence. A similar masking objective was used by Song et al. (2019) where it was referred to\nas “MASS”, so we call this variant the “MASS-style” objective. Second, we were interested\nto see if it was possible to avoid predicting the entire uncorrupted text span since this\nrequires self-attention over long sequences in the decoder. We consider two strategies to\nachieve this: First, instead of replacing each corrupted token with a mask token, we replace\nthe entirety of each consecutive span of corrupted tokens with a unique mask token. Then,\nthe target sequence becomes the concatenation of the “corrupted” spans, each prefixed by\nthe mask token used to replace it in the input. This is the pre-training objective we use in\nour baseline, described in Section 3.1.4. Second, we also consider a variant where we simply\ndrop the corrupted tokens from the input sequence completely and task the model with\nreconstructing the dropped tokens in order. Examples of these approaches are shown in the\nfifth and sixth rows of Table 3.\nAn empirical comparison of the original BERT-style objective to these three alternatives\nis shown in Table 5. We find that in our setting, all of these variants perform similarly. The\nonly exception was that dropping corrupted tokens completely produced a small improvement\nin the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our"}
{"doc_id": "1910.10683", "para_id": 117, "text": "Corruption rate\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 118, "text": "10%\n82.82\n19.00\n80.38\n69.55\n26.87\n39.28\n27.44\n⋆15%\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n25%\n83.00\n19.54\n80.96\n70.48\n27.04\n39.83\n27.47\n50%\n81.27\n19.32\n79.80\n70.33\n27.01\n39.90\n27.49"}
{"doc_id": "1910.10683", "para_id": 119, "text": "Table 6:\nPerformance of the i.i.d. corruption objective with different corruption rates."}
{"doc_id": "1910.10683", "para_id": 120, "text": "baseline average of 53.84, see Table 16). This may be due to the fact that CoLA involves\nclassifying whether a given sentence is grammatically and syntactically acceptable, and\nbeing able to determine when tokens are missing is closely related to detecting acceptability.\nHowever, dropping tokens completely performed worse than replacing them with sentinel\ntokens on SuperGLUE. The two variants that do not require predicting the full original\nsequence (“replace corrupted spans” and “drop corrupted spans”) are both potentially\nattractive since they make the target sequences shorter and consequently make training\nfaster. Going forward, we will explore variants where we replace corrupted spans with\nsentinel tokens and only predict the corrupted tokens (as in our baseline objective)."}
{"doc_id": "1910.10683", "para_id": 121, "text": "So far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al.,\n2018). Again, since our text-to-text framework differs from BERT’s, we are interested to\nsee if a different corruption rate works better for us. We compare corruption rates of 10%,\n15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited\neffect on the model’s performance. The only exception is that the largest corruption rate we\nconsider (50%) results in a significant degradation of performance on GLUE and SQuAD.\nUsing a larger corruption rate also results in longer targets, which can potentially slow down\ntraining. Based on these results and the historical precedent set by BERT, we will use a\ncorruption rate of 15% going forward."}
{"doc_id": "1910.10683", "para_id": 122, "text": "We now turn towards the goal of speeding up training by predicting shorter targets. The\napproach we have used so far makes an i.i.d. decision for each input token as to whether\nto corrupt it or not. When multiple consecutive tokens have been corrupted, they are\ntreated as a “span” and a single unique mask token is used to replace the entire span.\nReplacing entire spans with a single token results in unlabeled text data being processed into\nshorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case\nthat a significant number of corrupted tokens appear consecutively. As a result, we might\nobtain additional speedup by specifically corrupting spans of tokens rather than corrupting\nindividual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a\npre-training objective for BERT, where it was found to improve performance (Joshi et al.,\n2019).\nTo test this idea, we consider an objective that specifically corrupts contiguous, randomly-\nspaced spans of tokens. This objective can be parametrized by the proportion of tokens to\nbe corrupted and the total number of corrupted spans. The span lengths are then chosen"}
{"doc_id": "1910.10683", "para_id": 123, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 124, "text": "⋆Baseline (i.i.d.)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n2\n83.54\n19.39\n82.09\n72.20\n26.76\n39.99\n27.63\n3\n83.49\n19.62\n81.84\n72.53\n26.86\n39.65\n27.62\n5\n83.40\n19.24\n82.05\n72.23\n26.88\n39.40\n27.53\n10\n82.85\n19.33\n81.84\n70.44\n26.79\n39.49\n27.69"}
{"doc_id": "1910.10683", "para_id": 125, "text": "Table 7: Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for\ndifferent average span lengths. In all cases, we corrupt 15% of the original text\nsequence."}
{"doc_id": "1910.10683", "para_id": 126, "text": "randomly to satisfy these specified parameters. For example, if we are processing a sequence\nof 500 tokens and we have specified that 15% of tokens should be corrupted and that there\nshould be 25 total spans, then the total number of corrupted tokens would be 500×0.15 = 75\nand the average span length would be 75/25 = 3. Note that given the original sequence\nlength and corruption rate, we can equivalently parametrize this objective either by the\naverage span length or the total number of spans.\nWe compare the span-corruption objective to the i.i.d-corruption objective in Table 7.\nWe use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3,\n5 and 10. Again, we find a limited difference between these objectives, though the version\nwith an average span length of 10 slightly underperforms the other values in some cases.\nWe also find in particular that using an average span length of 3 slightly (but significantly)\noutperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the\nspan-corruption objective also provides some speedup during training compared to the i.i.d.\nnoise approach because span corruption produces shorter sequences on average."}
{"doc_id": "1910.10683", "para_id": 127, "text": "Figure 5 shows a flow chart of the choices made during our exploration of unsupervised\nobjectives. Overall, the most significant difference in performance we observed was that\ndenoising objectives outperformed language modeling and deshuffling for pre-training. We\ndid not observe a remarkable difference across the many variants of the denoising objectives\nwe explored. However, different objectives (or parameterizations of objectives) can lead to\ndifferent sequence lengths and thus different training speeds. This implies that choosing\namong the denoising objectives we considered here should mainly be done according to\ntheir computational cost. Our results also suggest that additional exploration of objectives\nsimilar to the ones we consider here may not lead to significant gains for the tasks and model\nwe consider. Instead, it may be fortuitous to explore entirely different ways of leveraging\nunlabeled data."}
{"doc_id": "1910.10683", "para_id": 128, "text": "Like the unsupervised objective, the pre-training data set itself is a crucial component of\nthe transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training\ndata sets are usually not treated as significant contributions on their own and are often not"}
{"doc_id": "1910.10683", "para_id": 129, "text": "Figure 5: A flow chart of our exploration of unsupervised objectives. We first consider a\nfew disparate approaches in Section 3.3.1 and find that a BERT-style denoising\nobjective performs best. Then, we consider various methods for simplifying the\nBERT objective so that it produces shorter target sequences in Section 3.3.2.\nGiven that replacing dropped-out spans with sentinel tokens performs well and\nresults in short target sequences, in Section 3.3.3 we experiment with different\ncorruption rates. Finally, we evaluate an objective that intentionally corrupts\ncontiguous spans of tokens in Section 3.3.4."}
{"doc_id": "1910.10683", "para_id": 130, "text": "released alongside pre-trained models and code. Instead, they are typically introduced in\nthe course of presenting a new method or model. As a result, there has been relatively little\ncomparison of different pre-training data sets as well as a lack of a “standard” data set used\nfor pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c;\nYang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced)\ndata set to using a smaller preexisting data set (often Wikipedia). To probe more deeply\ninto the impact of the pre-training data set on performance, in this section we compare\nvariants of our C4 data set and other potential sources of pre-training data. We release all\nof the C4 data set variants we consider as part of TensorFlow Datasets.11"}
{"doc_id": "1910.10683", "para_id": 131, "text": "In creating C4, we developed various heuristics to filter the web-extracted text from Common\nCrawl (see Section 2.2 for a description). We are interested in measuring whether this\nfiltering results in improved performance on downstream tasks, in addition to comparing\nit to other filtering approaches and common pre-training data sets. Towards this end, we\ncompare the performance of our baseline model after pre-training on the following data sets:"}
{"doc_id": "1910.10683", "para_id": 132, "text": "C4 As a baseline, we first consider pre-training on our proposed unlabeled data set as\ndescribed in Section 2.2."}
{"doc_id": "1910.10683", "para_id": 133, "text": "Unfiltered C4 To measure the effect of the heuristic filtering we used in creating C4\n(deduplication, removing bad words, only retaining sentences, etc.), we also generate\nan alternate version of C4 that forgoes this filtering. Note that we still use langdetect"}
{"doc_id": "1910.10683", "para_id": 134, "text": "11. https://www.tensorflow.org/datasets/catalog/c4"}
{"doc_id": "1910.10683", "para_id": 135, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 136, "text": "to extract English text. As a result, our “unfiltered” variant still includes some filtering\nbecause langdetect sometimes assigns a low probability to non-natural English text."}
{"doc_id": "1910.10683", "para_id": 137, "text": "RealNews-like Recent work has used text data extracted from news websites (Zellers\net al., 2019; Baevski et al., 2019). To compare to this approach, we generate another\nunlabeled data set by additionally filtering C4 to only include content from one of the\ndomains used in the “RealNews” data set (Zellers et al., 2019). Note that for ease of\ncomparison, we retain the heuristic filtering methods used in C4; the only difference is\nthat we have ostensibly omitted any non-news content."}
{"doc_id": "1910.10683", "para_id": 138, "text": "WebText-like Similarly, the WebText data set (Radford et al., 2019) only uses content\nfrom webpages that were submitted to the content aggregation website Reddit and\nreceived a “score” of at least 3. The score for a webpage submitted to Reddit is\ncomputed based on the proportion of users who endorse (upvote) or oppose (downvote)\nthe webpage. The idea behind using the Reddit score as a quality signal is that users\nof the site would only upvote high-quality text content. To generate a comparable data\nset, we first tried removing all content from C4 that did not originate from a URL that\nappeared in the list prepared by the OpenWebText effort.12 However, this resulted in\ncomparatively little content—only about 2 GB—because most pages never appear on\nReddit. Recall that C4 was created based on a single month of Common Crawl data.\nTo avoid using a prohibitively small data set, we therefore downloaded 12 months\nof data from Common Crawl from August 2018 to July 2019, applied our heuristic\nfiltering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like\ndata set, which is of comparable size to the original 40GB WebText data set (Radford\net al., 2019)."}
{"doc_id": "1910.10683", "para_id": 139, "text": "Wikipedia The website Wikipedia consists of millions of encyclopedia articles written\ncollaboratively. The content on the site is subject to strict quality guidelines and\ntherefore has been used as a reliable source of clean and natural text. We use the\nEnglish Wikipedia text data from TensorFlow Datasets,13 which omits any markup or\nreference sections from the articles."}
{"doc_id": "1910.10683", "para_id": 140, "text": "Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia\nis that it represents only one possible domain of natural text (encyclopedia articles).\nTo mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the\nToronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from\neBooks, which represents a different domain of natural language. BERT’s popularity\nhas led to the Wikipedia + TBC combination being used in many subsequent works."}
{"doc_id": "1910.10683", "para_id": 141, "text": "The results achieved after pre-training on each of these data sets is shown in Table 8. A\nfirst obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades\nperformance and makes the unfiltered variant perform the worst in every task. Beyond\nthis, we found that in some cases a pre-training data set with a more constrained domain\noutperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus"}
{"doc_id": "1910.10683", "para_id": 142, "text": "12. https://github.com/jcpeterson/openwebtext\n13. https://www.tensorflow.org/datasets/catalog/wikipedia"}
{"doc_id": "1910.10683", "para_id": 143, "text": "Data set\nSize\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 144, "text": "⋆C4\n745GB\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nC4, unfiltered\n6.1TB\n81.46\n19.14\n78.78\n68.04\n26.55\n39.34\n27.21\nRealNews-like\n35GB\n83.83\n19.23\n80.39\n72.38\n26.75\n39.90\n27.48\nWebText-like\n17GB\n84.03\n19.31\n81.42\n71.40\n26.80\n39.74\n27.59\nWikipedia\n16GB\n81.85\n19.31\n81.29\n68.01\n26.94\n39.69\n27.67\nWikipedia + TBC\n20GB\n83.65\n19.28\n82.08\n73.24\n26.77\n39.63\n27.57"}
{"doc_id": "1910.10683", "para_id": 145, "text": "Table 8: Performance resulting from pre-training on different data sets. The first four\nvariants are based on our new C4 data set."}
{"doc_id": "1910.10683", "para_id": 146, "text": "produced a SuperGLUE score of 73.24, beating our baseline’s score (using C4) of 71.36.\nThis is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to\n50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC\nis a reading comprehension data set whose largest source of data comes from fiction books,\nwhich is exactly the domain covered by TBC. Similarly, using the RealNews-like data set\nfor pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for\nReCoRD, a data set that measures reading comprehension on news articles. As a final\nexample, using data from Wikipedia produced significant (but less dramatic) gains on\nSQuAD, which is a question-answering data set with passages sourced from Wikipedia.\nSimilar observations have been made in prior work, e.g. Beltagy et al. (2019) found that\npre-training BERT on text from research papers improved its performance on scientific tasks.\nThe main lesson behind these findings is that pre-training on in-domain unlabeled data can\nimprove performance on downstream tasks. This is unsurprising but also unsatisfying if\nour goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary\ndomains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded\nimprovements on downstream tasks. This observation also motivates the parallel line of\nresearch on domain adaptation for natural language processing; for surveys of this field see\ne.g. Ruder (2019); Li (2012).\nA drawback to only pre-training on a single domain is that the resulting data sets are\noften substantially smaller. Similarly, while the WebText-like variant performed as well or\nbetter than the C4 data set in our baseline setting, the Reddit-based filtering produced a\ndata set that was about 40× smaller than C4 despite being based on 12× more data from\nCommon Crawl. Note, however, that in our baseline setup we only pre-train on 235 ≈34B\ntokens, which is only about 8 times larger than the smallest pre-training data set we consider.\nWe investigate at what point using a smaller pre-training data sets poses an issue in the\nfollowing section."}
{"doc_id": "1910.10683", "para_id": 147, "text": "The pipeline we use to create C4 was designed to be able to create extremely large pre-\ntraining data sets. The access to so much data allows us to pre-train our models without\nrepeating examples. It is not clear whether repeating examples during pre-training would\nbe helpful or harmful to downstream performance because our pre-training objective is itself\nstochastic and can help prevent the model from seeing the same exact data multiple times."}
{"doc_id": "1910.10683", "para_id": 148, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 149, "text": "Number of tokens\nRepeats\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 150, "text": "⋆Full data set\n0\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n229\n64\n82.87\n19.19\n80.97\n72.03\n26.83\n39.74\n27.63\n227\n256\n82.62\n19.20\n79.78\n69.97\n27.02\n39.71\n27.33\n225\n1,024\n79.55\n18.57\n76.27\n64.76\n26.38\n39.56\n26.80\n223\n4,096\n76.34\n18.33\n70.92\n59.29\n26.37\n38.84\n25.81"}
{"doc_id": "1910.10683", "para_id": 151, "text": "Table 9: Measuring the effect of repeating data during pre-training. In these experiments,\nwe only use the first N tokens from C4 (with varying values of N shown in the\nfirst column) but still pre-train over 235 tokens. This results in the data set being\nrepeated over the course of pre-training (with the number of repeats for each\nexperiment shown in the second column), which may result in memorization (see\nFigure 6)."}
{"doc_id": "1910.10683", "para_id": 152, "text": "To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model\non artificially truncated versions of C4. Recall that we pre-train our baseline model on\n235 ≈34B tokens (a small fraction of the total size of C4). We consider training on truncated\nvariants of C4 consisting of 229, 227, 225 and 223 tokens. These sizes correspond to repeating\nthe data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training."}
{"doc_id": "1910.10683", "para_id": 153, "text": "The resulting downstream performance is shown in Table 9. As expected, performance\ndegrades as the data set size shrinks. We suspect this may be due to the fact that the model\nbegins to memorize the pre-training data set. To measure if this is true, we plot the training\nloss for each of these data set sizes in Figure 6. Indeed, the model attains significantly\nsmaller training losses as the size of the pre-training data set shrinks, suggesting possible\nmemorization. Baevski et al. (2019) similarly observed that truncating the pre-training data\nset size can degrade downstream task performance."}
{"doc_id": "1910.10683", "para_id": 154, "text": "We note that these effects are limited when the pre-training data set is repeated only\n64 times. This suggests that some amount of repetition of pre-training data might not be\nharmful. However, given that additional pre-training can be beneficial (as we will show in\nSection 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest\nusing large pre-training data sets whenever possible. We also note that this effect may be\nmore pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting\nto a smaller pre-training data set."}
{"doc_id": "1910.10683", "para_id": 155, "text": "So far we have considered the setting where all parameters of a model are pre-trained on\nan unsupervised task before being fine-tuned on individual supervised tasks. While this\napproach is straightforward, various alternative methods for training the model on down-\nstream/supervised tasks have been proposed. In this section, we compare different schemes\nfor fine-tuning the model in addition to the approach of training the model simultaneously\non multiple tasks."}
{"doc_id": "1910.10683", "para_id": 156, "text": "Figure 6: Pre-training loss for our original C4 data set as well as 4 artificially truncated\nversions. The sizes listed refer to the number of tokens in each data set. The four\nsizes considered correspond to repeating the data set between 64 and 4,096 times\nover the course of pre-training. Using a smaller data set size results in smaller\ntraining loss values, which may suggest some memorization of the unlabeled data\nset."}
{"doc_id": "1910.10683", "para_id": 157, "text": "It has been argued that fine-tuning all of the model’s parameters can lead to suboptimal\nresults, particularly on low-resource tasks (Peters et al., 2019). Early results on transfer\nlearning for text classification tasks advocated fine-tuning only the parameters of a small\nclassifier that was fed sentence embeddings produced by a fixed pre-trained model (Subra-\nmanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau\net al., 2017). This approach is less applicable to our encoder-decoder model because the\nentire decoder must be trained to output the target sequences for a given task. Instead, we\nfocus on two alternative fine-tuning approaches that update only a subset of the parameters\nof our encoder-decoder model.\nThe first, “adapter layers” (Houlsby et al., 2019; Bapna et al., 2019), is motivated by\nthe goal of keeping most of the original model fixed while fine-tuning. Adapter layers are\nadditional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward\nnetworks in each block of the Transformer. These new feed-forward networks are designed\nso that their output dimensionality matches their input. This allows them to be inserted\ninto the network with no additional changes to the structure or parameters. When fine-\ntuning, only the adapter layer and layer normalization parameters are updated. The main\nhyperparameter of this approach is the inner dimensionality d of the feed-forward network,\nwhich changes the number of new parameters added to the model. We experiment with\nvarious values for d.\nThe second alternative fine-tuning method we consider is “gradual unfreezing” (Howard\nand Ruder, 2018). In gradual unfreezing, more and more of the model’s parameters are fine-\ntuned over time. Gradual unfreezing was originally applied to a language model architecture\nconsisting of a single stack of layers. In this setting, at the start of fine-tuning only the"}
{"doc_id": "1910.10683", "para_id": 158, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 159, "text": "Fine-tuning method\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 160, "text": "⋆All parameters\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nAdapter layers, d = 32\n80.52\n15.08\n79.32\n60.40\n13.84\n17.88\n15.54\nAdapter layers, d = 128\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nAdapter layers, d = 512\n81.54\n17.78\n79.18\n64.30\n23.45\n33.98\n25.81\nAdapter layers, d = 2048\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nGradual unfreezing\n82.50\n18.95\n79.17\n70.79\n26.71\n39.02\n26.93"}
{"doc_id": "1910.10683", "para_id": 161, "text": "Table 10: Comparison of different alternative fine-tuning methods that only update a subset\nof the model’s parameters. For adapter layers, d refers to the inner dimensionality\nof the adapters."}
{"doc_id": "1910.10683", "para_id": 162, "text": "parameters of the final layer are updated, then after training for a certain number of updates\nthe parameters of the second-to-last layer are also included, and so on until the entire\nnetwork’s parameters are being fine-tuned. To adapt this approach to our encoder-decoder\nmodel, we gradually unfreeze layers in the encoder and decoder in parallel, starting from\nthe top in both cases. Since the parameters of our input embedding matrix and output\nclassification matrix are shared, we update them throughout fine-tuning. Recall that our\nbaseline model consists of 12 layers each in the encoder and decoder and is fine-tuned for\n218 steps. As such, we subdivide the fine-tuning process into 12 episodes of 218/12 steps each\nand train from layers 12 −n to 12 in the nth episode. We note that Howard and Ruder\n(2018) suggested fine-tuning an additional layer after each epoch of training. However, since\nour supervised data sets vary so much in size and since some of our downstream tasks are\nactually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler\nstrategy of fine-tuning an additional layer after every 218/12 steps.\nA comparison of the performance of these fine-tuning approaches is shown in Table 10.\nFor adapter layers, we report the performance using an inner dimensionality d of 32, 128,\n512, 2048. Pursuant with past results (Houlsby et al., 2019; Bapna et al., 2019) we find that\nlower-resource tasks like SQuAD work well with a small value of d whereas higher resource\ntasks require a large dimensionality to achieve reasonable performance. This suggests that\nadapter layers could be a promising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size. Note that in our case we treat\nGLUE and SuperGLUE each as a single “task” by concatenating their constituent data\nsets, so although they comprise some low-resource data sets the combined data set is large\nenough that it necessitates a large value of d. We found that gradual unfreezing caused\na minor degradation in performance across all tasks, though it did provide some speedup\nduring fine-tuning. Better results may be attainable by more carefully tuning the unfreezing\nschedule."}
{"doc_id": "1910.10683", "para_id": 163, "text": "So far, we have been pre-training our model on a single unsupervised learning task before\nfine-tuning it individually on each downstream task. An alternative approach, called “multi-\ntask learning” (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a\ntime. This approach typically has the goal of training a single model that can simultaneously"}
{"doc_id": "1910.10683", "para_id": 164, "text": "perform many tasks at once, i.e. the model and most of its parameters are shared across all\ntasks. We relax this goal somewhat and instead investigate methods for training on multiple\ntasks at once in order to eventually produce separate parameter settings that perform well\non each individual task. For example, we might train a single model on many tasks, but\nwhen reporting performance we are allowed to select a different checkpoint for each task.\nThis loosens the multi-task learning framework and puts it on more even footing compared\nto the pre-train-then-fine-tune approach we have considered so far. We also note that in our\nunified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets\ntogether. It follows that we can still train on unlabeled data when using multi-task learning\nby treating the unsupervised task as one of the tasks being mixed together. In contrast,\nmost applications of multi-task learning to NLP add task-specific classification networks or\nuse different loss functions for each task (Liu et al., 2019b).\nAs pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task\nlearning is how much data from each task the model should be trained on. Our goal is to not\nunder- or over-train the model—that is, we want the model to see enough data from a given\ntask that it can perform the task well, but not to see so much data that it memorizes the\ntraining set. How exactly to set the proportion of data coming from each task can depend on\nvarious factors including data set sizes, the “difficulty” of learning the task (i.e. how much\ndata the model must see before being able to perform the task effectively), regularization,\netc. An additional issue is the potential for “task interference” or “negative transfer”, where\nachieving good performance on one task can hinder performance on another. Given these\nconcerns, we begin by exploring various strategies for setting the proportion of data coming\nfrom each task. A similar exploration was performed by Wang et al. (2019a)."}
{"doc_id": "1910.10683", "para_id": 165, "text": "Examples-proportional mixing A major factor in how quickly a model will overfit to\na given task is the task’s data set size. As such, a natural way to set the mixing\nproportions is to sample in proportion to the size of each task’s data set. This is\nequivalent to concatenating the data sets for all tasks and randomly sampling examples\nfrom the combined data set. Note, however, that we are including our unsupervised\ndenoising task, which uses a data set that is orders of magnitude larger than every\nother task’s. It follows that if we simply sample in proportion to each data set’s size,\nthe vast majority of the data the model sees will be unlabeled, and it will undertrain\non all of the supervised tasks. Even without the unsupervised task, some tasks (e.g.\nWMT English to French) are so large that they would similarly crowd out most of\nthe batches. To get around this issue, we set an artificial “limit” on the data set sizes\nbefore computing the proportions. Specifically, if the number of examples in each of\nour N task’s data sets is en, n ∈{1, . . . , N} then we set probability of sampling an\nexample from the mth task during training to rm = min(em, K)/ P min(en, K) where\nK is the artificial data set size limit."}
{"doc_id": "1910.10683", "para_id": 166, "text": "Temperature-scaled mixing An alternative way of mitigating the huge disparity between\ndata set sizes is to adjust the “temperature” of the mixing rates. This approach was\nused by multilingual BERT to ensure that the model was sufficiently trained on low-\nresource languages.14 To implement temperature scaling with temperature T, we raise"}
{"doc_id": "1910.10683", "para_id": 167, "text": "14. https://github.com/google-research/bert/blob/master/multilingual.md"}
{"doc_id": "1910.10683", "para_id": 168, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 169, "text": "each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they\nsum to 1. When T = 1, this approach is equivalent to examples-proportional mixing\nand as T increases the proportions become closer to equal mixing. We retain the data\nset size limit K (applied to obtain rm before temperature scaling) but set it to a large\nvalue of K = 221. We use a large value of K because increasing the temperature will\ndecrease the mixing rate of the largest data sets."}
{"doc_id": "1910.10683", "para_id": 170, "text": "Equal mixing In this case, we sample examples from each task with equal probability.\nSpecifically, each example in each batch is sampled uniformly at random from one of\nthe data sets we train on. This is most likely a suboptimal strategy, as the model will\noverfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly\ninclude it as a point of reference of what might go wrong when the proportions are set\nsuboptimally."}
{"doc_id": "1910.10683", "para_id": 171, "text": "To compare these mixing strategies on equal footing with our baseline pre-train-then-\nfine-tune results, we train multi-task models for the same total number of steps: 219 + 218 =\n786,432. The results are shown in Table 11.\nIn general, we find that multi-task training underperforms pre-training followed by\nfine-tuning on most tasks. The “equal” mixing strategy in particular results in dramatically\ndegraded performance, which may be because the low-resource tasks have overfit, the high-\nresource tasks have not seen enough data, or the model has not seen enough unlabeled data to\nlearn general-purpose language capabilities. For examples-proportional mixing, we find that\nfor most tasks there is a “sweet spot” for K where the model obtains the best performance,\nand larger or smaller values of K tend to result in worse performance. The exception (for the\nrange of K values we considered) was WMT English to French translation, which is such a\nhigh-resource task that it always benefits from a higher mixing proportion. Finally, we note\nthat temperature-scaled mixing also provides a means of obtaining reasonable performance\nfrom most tasks, with T = 2 performing the best in most cases. The finding that a multi-task\nmodel is outperformed by separate models trained on each individual task has previously\nbeen observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has\nbeen shown that the multi-task setup can confer benefits across very similar tasks Liu et al.\n(2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap\nbetween multi-task training and the pre-train-then-fine-tune approach."}
{"doc_id": "1910.10683", "para_id": 172, "text": "3.5.3 Combining Multi-Task Learning with Fine-Tuning"}
{"doc_id": "1910.10683", "para_id": 173, "text": "Recall that we are studying a relaxed version of multi-task learning where we train a single\nmodel on a mixture of tasks but are allowed to evaluate performance using different parameter\nsettings (checkpoints) for the model. We can extend this approach by considering the case\nwhere the model is pre-trained on all tasks at once but is then fine-tuned on the individual\nsupervised tasks. This is the method used by the “MT-DNN” (Liu et al., 2015, 2019b),\nwhich achieved state-of-the-art performance on GLUE and other benchmarks when it was\nintroduced. We consider three variants of this approach: In the first, we simply pre-train the\nmodel on an examples-proportional mixture with an artificial data set size limit of K = 219"}
{"doc_id": "1910.10683", "para_id": 174, "text": "before fine-tuning it on each individual downstream task. This helps us measure whether\nincluding the supervised tasks alongside the unsupervised objective during pre-training"}
{"doc_id": "1910.10683", "para_id": 175, "text": "Mixing strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 176, "text": "⋆Baseline (pre-train/fine-tune)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nEqual\n76.13\n19.02\n76.51\n63.37\n23.89\n34.31\n26.78\nExamples-proportional, K = 216\n80.45\n19.04\n77.25\n69.95\n24.35\n34.99\n27.10\nExamples-proportional, K = 217\n81.56\n19.12\n77.00\n67.91\n24.36\n35.00\n27.25\nExamples-proportional, K = 218\n81.67\n19.07\n78.17\n67.94\n24.57\n35.19\n27.39\nExamples-proportional, K = 219\n81.42\n19.24\n79.78\n67.30\n25.21\n36.30\n27.76\nExamples-proportional, K = 220\n80.80\n19.24\n80.36\n67.38\n25.66\n36.93\n27.68\nExamples-proportional, K = 221\n79.83\n18.79\n79.50\n65.10\n25.82\n37.22\n27.13\nTemperature-scaled, T = 2\n81.90\n19.28\n79.42\n69.92\n25.42\n36.72\n27.20\nTemperature-scaled, T = 4\n80.56\n19.22\n77.99\n69.54\n25.04\n35.82\n27.45\nTemperature-scaled, T = 8\n77.21\n19.10\n77.14\n66.07\n24.55\n35.35\n27.17"}
{"doc_id": "1910.10683", "para_id": 177, "text": "Table 11: Comparison of multi-task training using different mixing strategies. Examples-\nproportional mixing refers to sampling examples from each data set according to\nthe total size of each data set, with an artificial limit (K) on the maximum data set\nsize. Temperature-scaled mixing re-scales the sampling rates by a temperature T.\nFor temperature-scaled mixing, we use an artificial data set size limit of K = 221."}
{"doc_id": "1910.10683", "para_id": 178, "text": "gives the model some beneficial early exposure to the downstream tasks. We might also\nhope that mixing in many sources of supervision could help the pre-trained model obtain a\nmore general set of “skills” (loosely speaking) before it is adapted to an individual task. To\nmeasure this directly, we consider a second variant where we pre-train the model on the same\nexamples-proportional mixture (with K = 219) except that we omit one of the downstream\ntasks from this pre-training mixture. Then, we fine-tune the model on the task that was\nleft out during pre-training. We repeat this for each of the downstream tasks we consider.\nWe call this approach “leave-one-out” multi-task training. This simulates the real-world\nsetting where a pre-trained model is fine-tuned on a task it had not seen during pre-training.\nNote that multi-task pre-training provides a diverse mixture of supervised tasks. Since other\nfields (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014)) use a supervised data set for pre-training, we were interested to see whether\nomitting the unsupervised task from the multi-task pre-training mixture still produced good\nresults. For our third variant we therefore pre-train on an examples-proportional mixture of\nall of the supervised tasks we consider with K = 219. In all of these variants, we follow our\nstandard procedure of pre-training for 219 steps before fine-tuning for 218 steps.\nWe compare the results of these approaches in Table 12. For comparison, we also include\nresults for our baseline (pre-train then fine-tune) and for standard multi-task learning\n(without fine-tuning) on an examples-proportional mixture with K = 219. We find that\nfine-tuning after multi-task pre-training results in comparable performance to our baseline.\nThis suggests that using fine-tuning after multi-task learning can help mitigate some of\nthe trade-offs between different mixing rates described in Section 3.5.2. Interestingly, the\nperformance of “leave-one-out” training was only slightly worse, suggesting that a model\nthat was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pre-\ntraining might not result in a dramatic task interference). Finally, supervised multi-task\npre-training performed significantly worse in every case except for the translation tasks. This"}
{"doc_id": "1910.10683", "para_id": 179, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 180, "text": "Training strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 181, "text": "⋆Unsupervised pre-training + fine-tuning\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nMulti-task training\n81.42\n19.24\n79.78\n67.30\n25.21\n36.30\n27.76\nMulti-task pre-training + fine-tuning\n83.11\n19.12\n80.26\n71.03\n27.08\n39.80\n28.07\nLeave-one-out multi-task training\n81.98\n19.05\n79.97\n71.68\n26.93\n39.79\n27.87\nSupervised multi-task pre-training\n79.93\n18.96\n77.38\n65.36\n26.81\n40.13\n28.04"}
{"doc_id": "1910.10683", "para_id": 182, "text": "Table 12: Comparison of unsupervised pre-training, multi-task learning, and various forms\nof multi-task pre-training."}
{"doc_id": "1910.10683", "para_id": 183, "text": "could suggest that the translation tasks benefit less from (English) pre-training, whereas\nunsupervised pre-training is an important factor in the other tasks."}
{"doc_id": "1910.10683", "para_id": 184, "text": "The “bitter lesson” of machine learning research argues that general methods that can\nleverage additional computation ultimately win out against methods that rely on human\nexpertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016;\nMahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a).\nRecent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c;\nRadford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown\nthat scaling up produces improved performance compared to more carefully-engineered\nmethods. However, there are a variety of possible ways to scale, including using a bigger\nmodel, training the model for more steps, and ensembling. In this section, we compare these\ndifferent approaches by addressing the following premise: “You were just given 4× more\ncompute. How should you use it?”"}
{"doc_id": "1910.10683", "para_id": 185, "text": "We start with our baseline model, which has 220M parameters and is pre-trained and\nfine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized\nsimilarly to “BERTBASE”. To experiment with increased model size, we follow the guidelines\nof “BERTLARGE” Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and\n16-head attention mechanisms. We then generate two variants with 16 and 32 layers each in\nthe encoder and decoder, producing models with 2× and 4× as many parameters as our\noriginal model. These two variants also have a roughly 2× and 4× the computational cost.\nUsing our baseline and these two larger models, we consider three ways of using 4× as much\ncomputation: Training for 4× as many steps, training for 2× as many steps with the 2×\nbigger model, and training the 4× bigger model for the “baseline” number of training steps.\nWhen we increase the training steps, we scale both the pre-train and fine-tune steps for\nsimplicity. Note that when increasing the number of pre-training steps, we are effectively\nincluding more pre-training data as C4 is so large that we do not complete one pass over\nthe data even when training for 223 steps."}
{"doc_id": "1910.10683", "para_id": 186, "text": "An alternative way for the model to see 4× as much data is to increase the batch size by a\nfactor of 4. This can potentially result in faster training due to more efficient parallelization.\nHowever, training with a 4× larger batch size can yield a different outcome than training"}
{"doc_id": "1910.10683", "para_id": 187, "text": "Scaling strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 188, "text": "⋆Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n1× size, 4× training steps\n85.33\n19.33\n82.45\n74.72\n27.08\n40.66\n27.93\n1× size, 4× batch size\n84.60\n19.42\n82.52\n74.64\n27.07\n40.60\n27.84\n2× size, 2× training steps\n86.18\n19.66\n84.18\n77.18\n27.52\n41.03\n28.19\n4× size, 1× training steps\n85.91\n19.73\n83.86\n78.04\n27.47\n40.71\n28.10\n4× ensembled\n84.77\n20.10\n83.09\n71.74\n28.05\n40.53\n28.57\n4× ensembled, fine-tune only\n84.05\n19.57\n82.36\n71.55\n27.55\n40.22\n28.09"}
{"doc_id": "1910.10683", "para_id": 189, "text": "Table 13: Comparison of different methods of scaling up our baseline model. All methods\nexcept ensembling fine-tuned models use 4× the computation as the baseline.\n“Size” refers to the number of parameters in the model and “training time” refers\nto the number of steps used for both pre-training and fine-tuning."}
{"doc_id": "1910.10683", "para_id": 190, "text": "for 4× as many steps (Shallue et al., 2018). We include an additional experiment where we\ntrain our baseline model with a 4× larger batch size to compare these two cases.\nIt is common practice on many of the benchmarks we consider to eke out additional\nperformance by training and evaluating using an ensemble of models. This provides an\northogonal way of using additional computation. To compare other scaling methods to\nensembling, we also measure the performance of an ensemble of 4 separately pre-trained and\nfine-tuned models. We average the logits across the ensemble before feeding them into the\noutput softmax nonlinearity to obtain an aggregate prediction. Instead of pre-training 4\nseparate models, a cheaper alternative is to take a single pre-trained model and produce 4\nseparate fine-tuned versions. While this does not use our entire 4× computational budget,\nwe also include this method to see if it produces competitive performance to the other scaling\nmethods.\nThe performance achieved after applying these various scaling methods is shown in\nTable 13.\nUnsurprisingly, increasing the training time and/or model size consistently\nimproves the baseline. There was no clear winner between training for 4× as many steps\nor using a 4× larger batch size, though both were beneficial. In general, increasing the\nmodel size resulted in an additional bump in performance compared to solely increasing\nthe training time or batch size. We did not observe a large difference between training a\n2× bigger model for 2× as long and training a 4× bigger model on any of the tasks we\nstudied. This suggests that increasing the training time and increasing the model size can be\ncomplementary means of improving performance. Our results also suggest that ensembling\nprovides an orthogonal and effective means of improving performance through scale. In some\ntasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling 4\ncompletely separately trained models significantly outperformed every other scaling approach.\nEnsembling models that were pre-trained together but fine-tuned separately also gave a\nsubstantial performance increase over the baseline, which suggests a cheaper means of\nimproving performance. The only exception was SuperGLUE, where neither ensembling\napproach significantly improved over the baseline.\nWe note that different scaling methods have different trade-offs that are separate from\ntheir performance. For example, using a larger model can make downstream fine-tuning and"}
{"doc_id": "1910.10683", "para_id": 191, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 192, "text": "inference more expensive. In contrast, the cost of pre-training a small model for longer is\neffectively amortized if it is applied to many downstream tasks. Separately, we note that\nensembling N separate models has a similar cost to using a model that has an N× higher\ncomputational cost. As a result, some consideration for the eventual use of the model is\nimportant when choosing between scaling methods."}
{"doc_id": "1910.10683", "para_id": 193, "text": "We now leverage the insights from our systematic study to determine how far we can push\nperformance on popular NLP benchmarks. We are also interested in exploring the current\nlimits of transfer learning for NLP by training larger models on large amounts of data. We\nstart with our baseline training approach and make the following changes:"}
{"doc_id": "1910.10683", "para_id": 194, "text": "Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption\nobjective described in Section 3.3.4, which was loosely inspired by SpanBERT (Joshi\net al., 2019). Specifically, we use a mean span length of 3 and corrupt 15% of the\noriginal sequence. We found that this objective produced marginally better performance\n(Table 7) while being slightly more computationally efficient due to shorter target\nsequence lengths."}
{"doc_id": "1910.10683", "para_id": 195, "text": "Longer training Our baseline model uses a relatively small amount of pre-training (1⁄4 as\nmuch as BERT (Devlin et al., 2018), 1⁄16 as much as XLNet (Yang et al., 2019), 1⁄64 as\nmuch as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we\ncan train for substantially longer without repeating data (which can be detrimental,\nas shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can\nindeed be helpful, and that both increasing the batch size and increasing the number of\ntraining steps can confer this benefit. We therefore pre-train our models for 1 million\nsteps on a batch size of 211 sequences of length 512, corresponding to a total of about\n1 trillion pre-training tokens (about 32× as many as our baseline). In Section 3.4.1, we\nshowed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC\ndata sets outperformed pre-training on C4 on a few downstream tasks. However, these\ndata set variants are sufficiently small that they would be repeated hundreds of times\nover the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2\nthat this repetition could be harmful, we opted instead to continue using the C4 data\nset."}
{"doc_id": "1910.10683", "para_id": 196, "text": "Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved\nperformance. However, using smaller models can be helpful in settings where limited\ncomputational resources are available for fine-tuning or inference. Based on these\nfactors, we train models with a wide range of sizes:"}
{"doc_id": "1910.10683", "para_id": 197, "text": "• Base.\nThis is our baseline model, whose hyperparameters are described in\nSection 3.1.1. It has roughly 220 million parameters."}
{"doc_id": "1910.10683", "para_id": 198, "text": "• Small. We consider a smaller model, which scales the baseline down by using\ndmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the\nencoder and decoder. This variant has about 60 million parameters."}
{"doc_id": "1910.10683", "para_id": 199, "text": "• Large. Since our baseline uses a BERTBASE-sized encoder and decoder, we\nalso consider a variant where the encoder and decoder are both similar in size\nand structure to BERTLARGE. Specifically, this variant uses dmodel = 1,024,\ndff = 4,096, dkv = 64, 16-headed attention, and 24 layers each in the encoder and\ndecoder, resulting in around 770 million parameters."}
{"doc_id": "1910.10683", "para_id": 200, "text": "• 3B and 11B. To further explore what kind of performance is possible when\nusing larger models, we consider two additional variants. In both cases, we use\ndmodel = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B”\nvariant, we use dff = 16,384 with 32-headed attention, which results in around\n2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention\nproducing a model with about 11 billion parameters. We chose to scale up dff\nspecifically because modern accelerators (such as the TPUs we train our models\non) are most efficient for large dense matrix multiplications like those in the\nTransformer’s feed-forward networks."}
{"doc_id": "1910.10683", "para_id": 201, "text": "Multi-task pre-training In Section 3.5.3, we showed that pre-training on a multi-task\nmixture of unsupervised and supervised tasks before fine-tuning worked as well as\npre-training on the unsupervised task alone. This is the approach advocated by the\n“MT-DNN” (Liu et al., 2015, 2019b). It also has the practical benefit of being able to\nmonitor “downstream” performance for the entire duration of training, rather than\njust during fine-tuning. We therefore used multi-task pre-training in our final set of\nexperiments. We hypothesize that larger models trained for longer might benefit from\na larger proportion of unlabeled data because they are more likely to overfit to smaller\ntraining data sets. However, we also note that the results of Section 3.5.3 suggest that\nfine-tuning after multi-task pre-training can mitigate some of the issues that might\narise from choosing a suboptimal proportion of unlabeled data. Based on these ideas,\nwe substitute the following artificial data set sizes for our unlabeled data before using\nstandard example-proportional mixing (described in Section 3.5.2): 710,000 for Small,\n2,620,000 for Base, 8,660,000 for Large, 33,500,000 for 3B, and 133,000,000 for 11B.\nFor all model variants, we also capped the effective data set size of the WMT English\nto French and WMT English to German data sets to 1M examples during pre-training."}
{"doc_id": "1910.10683", "para_id": 202, "text": "Fine-tuning on individual GLUE and SuperGLUE tasks So far, when fine-tuning\non GLUE and SuperGLUE, we have concatenated all of the data sets in each benchmark\nso that we only fine-tune models once for GLUE and once for SuperGLUE. This\napproach makes our study logistically simpler, but we found that this sacrifices a small\namount of performance on some tasks compared to fine-tuning on the task separately. A\npotential issue with fine-tuning on individual tasks, which would otherwise be mitigated\nby training on all tasks at once, is that we might overfit quickly to low-resource tasks.\nFor example, our large batch size of 211 length-512 sequences would result in the entire\ndata set appearing multiple times in each batch for many of the low-resource GLUE\nand SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences\nduring fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints\nevery 1,000 steps rather than every 5,000 steps to ensure we have access to the model’s\nparameters before it overfits."}
{"doc_id": "1910.10683", "para_id": 203, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 204, "text": "Beam search All of our previous results were reported using greedy decoding. For tasks\nwith long output sequences, we found improved performance from using beam search\n(Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty\nof α = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization\ntasks."}
{"doc_id": "1910.10683", "para_id": 205, "text": "Test set Since this is our final set of experiments, we report results on the test set rather\nthan the validation set. For CNN/Daily Mail, we use the standard test set distributed\nwith the data set. For the WMT tasks, this corresponds to using newstest2014 for\nEnglish-German, newstest2015 for English-French, and newstest2016 for English-\nRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to\ncompute official test set scores.15,16 For SQuAD, evaluating on the test set requires\nrunning inference on a benchmark server. Unfortunately, the computational resources\non this server are insufficient for obtaining predictions from our largest models. As\na result, we instead continue to report performance on the SQuAD validation set.\nFortunately, the model with the highest performance on the SQuAD test set also\nreported results on the validation set, so we can still compare to what is ostensibly\nthe state-of-the-art."}
{"doc_id": "1910.10683", "para_id": 206, "text": "Apart from those changes mentioned above, we use the same training procedure and\nhyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate\nschedule for pre-training, constant learning rate for fine-tuning, dropout regularization,\nvocabulary, etc.). For reference, these details are described in Section 2.\nThe results of this final set of experiments are shown in Table 14. Overall, we achieved\nstate-of-the-art performance on 18 out of the 24 tasks we consider. As expected, our largest\n(11 billion parameter) model performed best among our model size variants across all tasks.\nOur T5-3B model variant did beat the previous state of the art in a few tasks, but scaling\nthe model size to 11 billion parameters was the most important ingredient for achieving our\nbest performance. We now analyze the results for each individual benchmark.\nWe achieved a state-of-the-art average GLUE score of 90.3. Notably, our performance was\nsubstantially better than the previous state-of-the-art for the natural language inference tasks\nMNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine performance\nhas historically lagged behind human performance, which is 93.6 and 95.9 respectively (Wang\net al., 2018). In terms of parameter count, our 11B model variant is the largest model that\nhas been submitted to the GLUE benchmark. However, most of the best-scoring submissions\nuse a large amount of ensembling and computation to produce predictions. For example,\nthe best-performing variant of ALBERT (Lan et al., 2019) uses a model similar in size and\narchitecture to our 3B variant (though it has dramatically fewer parameters due to clever\nparameter sharing). To produce its impressive performance on GLUE, the ALBERT authors\nensembled “from 6 to 17” models depending on the task. This likely results in it being more\ncomputationally expensive to produce predictions with the ALBERT ensemble than it is\nwith T5-11B.\nFor SQuAD, we outperformed the previous state-of-the-art (ALBERT (Lan et al., 2019))\nby over one point on the Exact Match score. SQuAD is a long-standing benchmark that"}
{"doc_id": "1910.10683", "para_id": 207, "text": "15. http://gluebenchmark.com\n16. http://super.gluebenchmark.com"}
{"doc_id": "1910.10683", "para_id": 208, "text": "GLUE\nCoLA\nSST-2\nMRPC\nMRPC\nSTS-B\nSTS-B\nModel\nAverage\nMatthew’s\nAccuracy\nF1\nAccuracy\nPearson\nSpearman"}
{"doc_id": "1910.10683", "para_id": 209, "text": "Previous best\n89.4a\n69.2b\n97.1a\n93.6b\n91.5b\n92.7b\n92.3b"}
{"doc_id": "1910.10683", "para_id": 210, "text": "T5-Small\n77.4\n41.0\n91.8\n89.7\n86.6\n85.6\n85.0\nT5-Base\n82.7\n51.1\n95.2\n90.7\n87.5\n89.4\n88.6\nT5-Large\n86.4\n61.2\n96.3\n92.4\n89.9\n89.9\n89.2\nT5-3B\n88.5\n67.1\n97.4\n92.5\n90.0\n90.6\n89.8\nT5-11B\n90.3\n71.6\n97.5\n92.8\n90.4\n93.1\n92.8"}
{"doc_id": "1910.10683", "para_id": 211, "text": "QQP\nQQP\nMNLI-m\nMNLI-mm\nQNLI\nRTE\nWNLI\nModel\nF1\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 212, "text": "Previous best\n74.8c\n90.7b\n91.3a\n91.0a\n99.2a\n89.2a\n91.8a"}
{"doc_id": "1910.10683", "para_id": 213, "text": "T5-Small\n70.0\n88.0\n82.4\n82.3\n90.3\n69.9\n69.2\nT5-Base\n72.6\n89.4\n87.1\n86.2\n93.7\n80.1\n78.8\nT5-Large\n73.9\n89.9\n89.9\n89.6\n94.8\n87.2\n85.6\nT5-3B\n74.4\n89.7\n91.4\n91.2\n96.3\n91.1\n89.7\nT5-11B\n75.1\n90.6\n92.2\n91.9\n96.9\n92.8\n94.5"}
{"doc_id": "1910.10683", "para_id": 214, "text": "SQuAD\nSQuAD\nSuperGLUE\nBoolQ\nCB\nCB\nCOPA\nModel\nEM\nF1\nAverage\nAccuracy\nF1\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 215, "text": "Previous best\n90.1a\n95.5a\n84.6d\n87.1d\n90.5d\n95.2d\n90.6d"}
{"doc_id": "1910.10683", "para_id": 216, "text": "T5-Small\n79.10\n87.24\n63.3\n76.4\n56.9\n81.6\n46.0\nT5-Base\n85.44\n92.08\n76.2\n81.4\n86.2\n94.0\n71.2\nT5-Large\n86.66\n93.79\n82.3\n85.4\n91.6\n94.8\n83.4\nT5-3B\n88.53\n94.95\n86.4\n89.9\n90.3\n94.4\n92.0\nT5-11B\n91.26\n96.22\n88.9\n91.2\n93.9\n96.8\n94.8"}
{"doc_id": "1910.10683", "para_id": 217, "text": "MultiRC\nMultiRC\nReCoRD\nReCoRD\nRTE\nWiC\nWSC\nModel\nF1a\nEM\nF1\nAccuracy\nAccuracy\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 218, "text": "Previous best\n84.4d\n52.5d\n90.6d\n90.0d\n88.2d\n69.9d\n89.0d"}
{"doc_id": "1910.10683", "para_id": 219, "text": "T5-Small\n69.3\n26.3\n56.3\n55.4\n73.3\n66.9\n70.5\nT5-Base\n79.7\n43.1\n75.0\n74.2\n81.5\n68.3\n80.8\nT5-Large\n83.3\n50.7\n86.8\n85.9\n87.8\n69.3\n86.3\nT5-3B\n86.8\n58.3\n91.2\n90.4\n90.7\n72.1\n90.4\nT5-11B\n88.1\n63.3\n94.1\n93.4\n92.5\n76.9\n93.8"}
{"doc_id": "1910.10683", "para_id": 220, "text": "WMT EnDe\nWMT EnFr\nWMT EnRo\nCNN/DM\nCNN/DM\nCNN/DM\nModel\nBLEU\nBLEU\nBLEU\nROUGE-1\nROUGE-2\nROUGE-L"}
{"doc_id": "1910.10683", "para_id": 221, "text": "Previous best\n33.8e\n43.8e\n38.5f\n43.47g\n20.30g\n40.63g"}
{"doc_id": "1910.10683", "para_id": 222, "text": "T5-Small\n26.7\n36.0\n26.8\n41.12\n19.56\n38.35\nT5-Base\n30.9\n41.2\n28.0\n42.05\n20.34\n39.40\nT5-Large\n32.0\n41.5\n28.1\n42.50\n20.68\n39.75\nT5-3B\n31.8\n42.6\n28.2\n42.72\n21.02\n39.94\nT5-11B\n32.1\n43.4\n28.1\n43.52\n21.55\n40.69"}
{"doc_id": "1910.10683", "para_id": 223, "text": "Table 14: Performance of our T5 variants on every task we study. Small, Base, Large, 3B,\nand 11B refer to model configurations with 60 million, 220 million, 770 million,\n3 billion, and 11 billion parameters, respectively. In the first row of each table,\nwe report the state-of-the-art for the task (as of October 24th, 2019), with the\nsuperscript denoting its source with references listed at the end of this caption. All\nresults are reported on the test set except for SQuAD where we use the validation\nset.\na(Lan et al., 2019) b(Wang et al., 2019c) c(Zhu et al., 2019) d(Liu et al.,\n2019c) e(Edunov et al., 2018) f(Lample and Conneau, 2019) g(Dong et al., 2019)"}
{"doc_id": "1910.10683", "para_id": 224, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 225, "text": "was created over three years ago, and most recent improvements have only increased the\nstate-of-the-art by a fraction of a percentage point. We note that when results are reported\non the test set, they are typically based on an ensemble of models and/or leverage external\ndata sets (e.g. TriviaQA (Joshi et al., 2017) or NewsQA (Trischler et al., 2016)) to augment\nthe small SQuAD training set. Human performance on SQuAD is estimated at 82.30 and\n91.22 for the Exact Match and F1 metric respectively (Rajpurkar et al., 2016), so it is not\nclear if further improvements on this benchmark are meaningful.\nFor SuperGLUE, we improved upon the state-of-the-art by a large margin (from an\naverage score of 84.6 (Liu et al., 2019c) to 88.9). SuperGLUE was designed to include\ntasks that were “beyond the scope of current state-of-the-art systems, but solvable by most\ncollege-educated English speakers” (Wang et al., 2019b). We nearly match the human\nperformance of 89.8 (Wang et al., 2019b). Interestingly, on the reading comprehension tasks\n(MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the\nevaluation metrics used for these tasks may be biased towards machine-made predictions.\nOn the other hand, humans achieve 100% accuracy on both COPA and WSC, which is\nsignificantly better than our model’s performance. This suggests that there remain linguistic\ntasks that are hard for our model to perfect, particularly in the low-resource setting.\nWe did not achieve state-of-the-art performance on any of the WMT translation tasks.\nThis may be in part due to our use of an English-only unlabeled data set. We also note that\nmost of the best results on these tasks use backtranslation (Edunov et al., 2018; Lample and\nConneau, 2019), which is a sophisticated data augmentation scheme. The state of the art on\nthe low-resource English to Romanian benchmark also uses additional forms of cross-lingual\nunsupervised training (Lample and Conneau, 2019). Our results suggest that scale and\nEnglish-language pre-training may be insufficient to match the performance of these more\nsophisticated methods. On a more specific note, the best results on English to German\nnewstest2014 set use the much larger training set from WMT 2018 (Edunov et al., 2018),\nmaking direct comparison to our results difficult.\nFinally, on CNN/Daily Mail we attain state-of-the-art performance, though only by\na significant amount on the ROUGE-2-F score. It has been shown that improvements\nto the ROUGE score do not necessarily correspond to more coherent summaries (Paulus\net al., 2017). Furthermore, while CNN/Daily Mail is posed as an abstractive summarization\nbenchmark, purely extractive approaches have been shown to work well (Liu, 2019). It has\nalso been argued that generative models trained with maximum likelihood are prone to\nproducing repetitive summaries (See et al., 2017). Despite these potential issues, we find\nthat our models do generate coherent and largely correct summaries. We provide some\nnon-cherry-picked validation set examples in Appendix C.\nTo achieve its strong results, T5 combines insights from our experimental study with\nunprecedented scale. Note that in Section 3.6 we found that scaling up the pre-training\namount or size of our baseline model produced substantial gains. Given this, we were\ninterested to measure how much the “non-scaling” changes we introduced into T5 contributed\nto its strong performance. We therefore carried out a final experiment where we compared\nthe following three configurations: First, the standard baseline model, which was pre-trained\non 235 ≈34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e.\nthe same amount of pre-training used for T5), which we refer to as “baseline-1T”; and\nthird, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the"}
{"doc_id": "1910.10683", "para_id": 226, "text": "⋆Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98"}
{"doc_id": "1910.10683", "para_id": 227, "text": "Table 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. “Baseline-1T”\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 ≈34B\ntokens (as was used for the baseline)."}
{"doc_id": "1910.10683", "para_id": 228, "text": "“non-scaling” changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5’s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors."}
{"doc_id": "1910.10683", "para_id": 229, "text": "Having completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field."}
{"doc_id": "1910.10683", "para_id": 230, "text": "Text-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale."}
{"doc_id": "1910.10683", "para_id": 231, "text": "Architectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count."}
{"doc_id": "1910.10683", "para_id": 232, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 233, "text": "Unsupervised objectives Overall, we found that most “denoising” objectives, which train\nthe model to reconstruct randomly corrupted text, performed similarly in the text-to-\ntext setup. As a result, we suggest using objectives that produce short target sequences\nso that unsupervised pre-training is more computationally efficient."}
{"doc_id": "1910.10683", "para_id": 234, "text": "Data sets We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises\nheuristically-cleaned text from the Common Crawl web dump. When comparing C4 to\ndata sets that use additional filtering, we found that training on in-domain unlabeled\ndata could boost performance in a few downstream tasks. However, constraining to\na single domain typically results in a smaller data set. We separately showed that\nperformance can degrade when an unlabeled data set is small enough that it is repeated\nmany times over the course of pre-training. This motivates the use of a large and\ndiverse data set like C4 for generic language understanding tasks."}
{"doc_id": "1910.10683", "para_id": 235, "text": "Training strategies We found that the basic approach of updating all of a pre-trained\nmodel’s parameters during fine-tuning outperformed methods that are designed to\nupdate fewer parameters, although updating all parameters is most expensive. We also\nexperimented with various approaches for training the model on multiple tasks at once,\nwhich in our text-to-text setting simply corresponds to mixing examples from different\ndata sets when constructing batches. The primary concern in multi-task learning is\nsetting the proportion of each task to train on. We ultimately did not find a strategy\nfor setting mixing proportions that matched the performance of the basic approach of\nunsupervised pre-training followed by supervised fine-tuning. However, we found that\nfine-tuning after pre-training on a mixture of tasks produced comparable performance\nto unsupervised pre-training."}
{"doc_id": "1910.10683", "para_id": 236, "text": "Scaling We compared various strategies for taking advantage of additional compute, includ-\ning training the model on more data, training a larger model, and using an ensemble\nof models. We found each approach conferred a significant boost in performance,\nthough training a smaller model on more data was often outperformed by training\na larger model for fewer steps. We also showed an ensemble of models can provide\nsubstantially better results than a single model, which provides an orthogonal means\nof leveraging additional computation. Ensembling models that were fine-tuned from\nthe same base pre-trained model performed worse than pre-training and fine-tuning\nall models completely separately, though fine-tune-only ensembling still substantially\noutperformed a single model."}
{"doc_id": "1910.10683", "para_id": 237, "text": "Pushing the limits We combined our above insights and trained substantially larger\nmodels (up to 11 billion parameters) to achieve state-of-the-art results across many of\nthe benchmarks we considered. For unsupervised training, we extracted text from our\nC4 data set and applied a denoising objective that corrupts contiguous spans of tokens.\nWe pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,\nour models were trained on over 1 trillion tokens. In the interest of facilitating the\nreplication, extension, and application of our results, we release our code, the C4 data\nset, and pre-trained model weights for each T5 variant.1"}
{"doc_id": "1910.10683", "para_id": 238, "text": "The inconvenience of large models An unsurprising but important result from our\nstudy is that larger models tend to perform better. The fact that the hardware used for\nrunning these models is continually getting cheaper and more powerful suggests that\nscaling up may continue to be a promising way to achieve better performance (Sutton,\n2019). However, it will always be the case that there are applications and scenarios\nwhere using a smaller or less expensive model is helpful, for example when performing\nclient-side inference or federated learning (Konečn`y et al., 2015, 2016). Relatedly, one\nbeneficial use of transfer learning is the possibility of attaining good performance on\nlow-resource tasks. Low-resource tasks often occur (by definition) in settings where\none lacks the assets to label more data. It follows that low-resource applications often\nalso have limited access to computational resources which can incur additional costs.\nAs a result, we advocate for research on methods that achieve stronger performance\nwith cheaper models so that transfer learning can be applied where it will have the\nmost impact. Some current work along these lines include distillation (Hinton et al.,\n2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and\nconditional computation (Shazeer et al., 2017)."}
{"doc_id": "1910.10683", "para_id": 239, "text": "More efficient knowledge extraction Recall that one of the goals of pre-training is\n(loosely speaking) to provide the model with general-purpose “knowledge” that improves\nits performance on downstream tasks. The method we use in this work, which is\ncurrently common practice, is to train the model to denoise corrupted spans of text.\nWe suspect that this simplistic technique may not be a very efficient way to teach the\nmodel general-purpose knowledge. More concretely, it would be useful to be able to\nattain good fine-tuning performance without needing to train our models on 1 trillion\ntokens of text first. Some concurrent work along these lines improves efficiency by\npre-training a model to distinguish between real and machine-generated text (Clark\net al., 2020)."}
{"doc_id": "1910.10683", "para_id": 240, "text": "Formalizing the similarity between tasks We observed that pre-training on unlabeled\nin-domain data can improve performance on downstream tasks (Section 3.4). This\nfinding mostly relies on basic observations like the fact that SQuAD was created using\ndata from Wikipedia. It would be useful to formulate a more rigorous notion of the\n“similarity” between the pre-training and downstream tasks, so that we could make\nmore principled choices about what source of unlabeled data to use. There is some\nearly empirical work along these lines in the field of computer vision (Huh et al., 2016;\nKornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could\nalso help choose supervised pre-training tasks, which has been shown to be helpful for\nthe GLUE benchmark (Phang et al., 2018)."}
{"doc_id": "1910.10683", "para_id": 241, "text": "Language-agnostic models We were disappointed to find that English-only pre-training\ndid not achieve state-of-the-art results on the translation tasks we studied. We also\nare interested in avoiding the logistical difficulty of needing to specify which languages\na vocabulary can encode ahead of time. To address these issues, we are interested in\nfurther investigating language-agnostic models, i.e. models that can perform a given\nNLP task with good performance regardless of the text’s language. This is an especially"}
{"doc_id": "1910.10683", "para_id": 242, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 243, "text": "pertinent issue given that English is not the native language for the majority of the\nworld’s population."}
{"doc_id": "1910.10683", "para_id": 244, "text": "The motivation for this paper was the flurry of recent work on transfer learning for\nNLP. Before we began this work, these advances had already enabled breakthroughs in\nsettings where learning-based methods had not yet been shown to be effective. We are\nhappy to be able to continue this trend, for example by nearly matching human-level\nperformance on the SuperGLUE benchmark, a task specifically designed to be difficult\nfor modern transfer-learning pipelines. Our results stem from the combination of a\nstraightforward and unified text-to-text framework, our new C4 data set, and insights\nfrom our systematic study. Additionally, we provided an empirical overview of the\nfield and a perspective on where it stands. We are excited to see continued work using\ntransfer learning towards the goal of general language understanding."}
{"doc_id": "1910.10683", "para_id": 245, "text": "We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito,\nNoah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on\nthis manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for\nhis guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff\nKlingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil\nHoulsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield,\nYi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and\nPierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our\ndownload pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on\nSentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and\nmany other members of the Google Brain team for their discussion and insight."}
{"doc_id": "1910.10683", "para_id": 246, "text": "Colin designed the scope of this project and wrote this paper, ran all the experiments in\nSections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed\nmany of the ideas, including the text-to-text framework, unsupervised objectives, and\ndata set mixing strategies; implemented our base Transformer model and its architectural\nvariants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects\nfor this project, created the C4 data set, implemented our data set pipeline, and added\nvarious benchmark data sets.\nKatherine coordinated experiments, wrote and updated\ndocumentation, ran experiments to help design our baseline, and contributed to many parts\nof our codebase. Sharan contributed some of the required data sets and preprocessors, and\nran assorted preliminary experiments, in addition to co-leading the open-sourcing of our\ncodebase. Michael owned all aspects of the Winograd data sets, ingested many of the data\nsets we used, contributed various improvements and fixes to our infrastructure, and ran some\npreliminary experiments. Yanqi ran experiments and implemented methods to help settle on\na reasonable baseline and helped with the final fine-tuning of the models in Section 3.7. Wei\nalso helped with final fine-tuning and improved some of our preprocessors. Peter prototyped\nan early version of the pre-training data set and resolved issues pertaining to the SQuAD\nand CNN/DM tasks. All authors helped set the scope and research direction we followed in\nthis work."}
{"doc_id": "1910.10683", "para_id": 247, "text": "Appendix B. Converting WNLI to Our Text-to-Text Format"}
{"doc_id": "1910.10683", "para_id": 248, "text": "Note that as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead,\nwhen evaluating on the WNLI test set (for the results in Section 3.7), we convert the WNLI\ntest set to the “referent noun prediction” text-to-text format so that we can evaluate using a\nmodel trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed\nby He et al. (2019). Recall that examples from WNLI consist of a premise, a hypothesis,\nand a label that indicates whether the hypothesis is True or False. Using the example from\nSection 2.4, the hypothesis would be “The city councilmen refused the demonstrators a\npermit because they feared violence.” with the premise “The demonstrators feared violence.”\nand the label False. We first find the location of all pronouns in the premise (“they” in\nour example). Then, we find the maximum number of words that precede or follow each\npronoun that are a substring in the hypothesis (“feared violence” in our example), ignoring\ncase and punctuation. When the premise contains multiple candidate pronouns, we choose\nthe pronoun that is preceded or followed by the largest substring of the hypothesis. We then\nhighlight the pronoun in the premise by surrounding it with asterisks. For the candidate\nnoun (which is compared to our model’s prediction to obtain a True or False label), we\nremove the matching substring from the hypothesis and optionally make it non-possessive\n(resulting in “the demonstrators”)."}
{"doc_id": "1910.10683", "para_id": 249, "text": "To show that our model is generating fluent summaries, we include a few example decodes\nfrom our best model (T5-11B) on the validation set along with the ground-truth summaries.\nThese examples selected at random and were not cherry-picked."}
{"doc_id": "1910.10683", "para_id": 250, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 251, "text": "1. Ground-truth: leopard gave up after spiky creature refused to back down\nin fight in kruger national park, south africa .\nwildlife enthusiast lisl\nmoolman, 41, caught the bizarre battle while out on the road and armed with\nher camera .\nthe leopard and porcupine tumbled out of the bushes and began\nto fight by roadside - watched by ms moolman ."}
{"doc_id": "1910.10683", "para_id": 252, "text": "Prediction: leopard tried to make lunch out of a plucky porcupine in kruger\nnational park, south africa .\nbut the predator was put firmly in its place\nafter the spiky creature refused to back down during a fight .\nwildlife\nenthusiast lisl moolman, 41, caught the bizarre battle while out on the\nroad and armed with her camera ."}
{"doc_id": "1910.10683", "para_id": 253, "text": "2. Ground-truth: researchers say homes are a hotbed of undiscovered species\n.\nstudy of 40 north carolina homes found 8,000 bacterial and archaeal taxa\n.\n11 houses in california revealed a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 254, "text": "Prediction: researchers estimate that the indoor biome makes up about 0.5\npercent of ice-free land, or about 247,000 square miles, almost the size\nof texas .\na study of 40 houses in north carolina used genomic technologies\nto document more than 8,000 bacterial and archaeal taxa .\nanother study\nof 50 houses in the same state described more than 750 types of arthropods,\nincluding insects and spiders .\nan examination of 11 houses in california\nfound a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 255, "text": "3. Ground-truth: executions have been temporarily halted in the state after\nthe drug used for lethal injections appeared faulty on monday night .\nkelly\ngissendaner had her death delayed for a second time when it was found that\nthe lethal pentobarbital dose appeared cloudy .\nstate officials dithered\nover the decision , changing their mind three times before deciding not\nto take the risk that it wouldn’t work .\nit is ’essential that executions\nare carried out in a constitutional manner,’ said georgia attorney general\nsam olens ."}
{"doc_id": "1910.10683", "para_id": 256, "text": "Prediction: the execution of kelly gissendaner, the only woman on georgia’s\ndeath row, was delayed for a second time on monday .\nthe execution team\nfound that the lethal pentobarbital dose appeared cloudy .\nthe cloudy drug\nbolstered death penalty opponents, who have been vocal in their opposition\nafter three botched executions in other parts of the country ."}
{"doc_id": "1910.10683", "para_id": 257, "text": "4. Ground-truth: dani alves was not selected for the brazil squad to face\nfrance and chile .\nbarcelona defender was filmed serving up hot dogs to\npeople on saturday .\nthis week alves released a charity single with former\nteam-mat jose pinto .\nalves looks set to leave barcelona on a free transfer\nthis summer .\nreports in spanish press claim he will sign for paris saint-germain\n."}
{"doc_id": "1910.10683", "para_id": 258, "text": "Prediction: dani alves was not selected for brazil’s friendlies against\nfrance and chile .\nthe barcelona right back has released a charity single\nwith jose pinto .\nalves was filmed serving up snacks from behind the counter"}
{"doc_id": "1910.10683", "para_id": 259, "text": "of a hot dog bar on saturday .\nalves is yet to be offered a new deal at\nthe nou camp .\nclick here for all the latest barcelona news ."}
{"doc_id": "1910.10683", "para_id": 260, "text": "In this section, we provide examples of our preprocessing for each of the data sets we consider."}
{"doc_id": "1910.10683", "para_id": 261, "text": "Processed input: cola sentence: John made Bill master of himself."}
{"doc_id": "1910.10683", "para_id": 262, "text": "Sentence 1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia\n(at the 1991 national census, some 3000 inhabitants of Slovenia declared\nthemselves as ethnic Italians)."}
{"doc_id": "1910.10683", "para_id": 263, "text": "Processed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians\nwere settled in Slovenia (at the 1991 national census, some 3000 inhabitants\nof Slovenia declared themselves as ethnic Italians).\nsentence2: Slovenia\nhas 3,000 inhabitants."}
{"doc_id": "1910.10683", "para_id": 264, "text": "Hypothesis: The St. Louis Cardinals have always won."}
{"doc_id": "1910.10683", "para_id": 265, "text": "Premise: yeah well losing is i mean i’m i’m originally from Saint Louis and\nSaint Louis Cardinals when they were there were uh a mostly a losing team\nbut"}
{"doc_id": "1910.10683", "para_id": 266, "text": "Processed input: mnli hypothesis: The St.\nLouis Cardinals have always won.\npremise:\nyeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis\nCardinals when they were there were uh a mostly a losing team but"}
{"doc_id": "1910.10683", "para_id": 267, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 268, "text": "Sentence 1: We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2: Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" ."}
{"doc_id": "1910.10683", "para_id": 269, "text": "Processed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid .\nsentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" ."}
{"doc_id": "1910.10683", "para_id": 270, "text": "Question: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand."}
{"doc_id": "1910.10683", "para_id": 271, "text": "Processed input: qnli question: Where did Jebe die?\nsentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand."}
{"doc_id": "1910.10683", "para_id": 272, "text": "Question 1: What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?"}
{"doc_id": "1910.10683", "para_id": 273, "text": "Processed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome?\nquestion2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?"}
{"doc_id": "1910.10683", "para_id": 274, "text": "Sentence: it confirms fincher ’s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight ."}
{"doc_id": "1910.10683", "para_id": 275, "text": "Processed input: sst2 sentence: it confirms fincher ’s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n."}
{"doc_id": "1910.10683", "para_id": 276, "text": "Sentence 1: Representatives for Puretunes could not immediately be reached\nfor comment Wednesday."}
{"doc_id": "1910.10683", "para_id": 277, "text": "Sentence 2: Puretunes representatives could not be located Thursday to comment\non the suit."}
{"doc_id": "1910.10683", "para_id": 278, "text": "Processed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday.\nsentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit."}
{"doc_id": "1910.10683", "para_id": 279, "text": "Premise: Valence the void-brain, Valence the virtuous valet.\nWhy couldn’t\nthe figger choose his own portion of titanic anatomy to shaft?\nDid he think\nhe was helping?"}
{"doc_id": "1910.10683", "para_id": 280, "text": "Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet.\nWhy couldn’t the figger choose his own portion\nof titanic anatomy to shaft?\nDid he think he was helping?"}
{"doc_id": "1910.10683", "para_id": 281, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 282, "text": "Premise: Political violence broke out in the nation."}
{"doc_id": "1910.10683", "para_id": 283, "text": "Choice 2: Many citizens took refuge in other territories."}
{"doc_id": "1910.10683", "para_id": 284, "text": "Processed input: copa choice1: Many citizens relocated to the capitol.\nchoice2:\nMany citizens took refuge in other territories.\npremise: Political violence\nbroke out in the nation.\nquestion: effect"}
{"doc_id": "1910.10683", "para_id": 285, "text": "Answer: There was only pie to eat, rather than traditional breakfast foods"}
{"doc_id": "1910.10683", "para_id": 286, "text": "Paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent\n2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent\n3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent\n4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s\npond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food\nbefore they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except\nfor pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),\nor oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went\nto the pond.<br><b>Sent 9: </b>On their way there they saw their friend\nJack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several\nhours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\n</b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went\noff to bed.<br>"}
{"doc_id": "1910.10683", "para_id": 287, "text": "Question: Why was Joey surprised the morning he woke up for breakfast?"}
{"doc_id": "1910.10683", "para_id": 288, "text": "Processed input: multirc question: Why was Joey surprised the morning he woke\nup for breakfast?\nanswer: There was only pie to eat, rather than traditional\nbreakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel\nnamed Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin\nJimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were\nalways laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together"}
{"doc_id": "1910.10683", "para_id": 289, "text": "at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning\nto eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything\nto eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit\n(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and\nJimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their\nfriend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for\nseveral hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The\ntwo squirrels ate some food that Joey’s mom, Jasmine, made and went off to\nbed.<br>"}
{"doc_id": "1910.10683", "para_id": 290, "text": "Sentence 1: It was the deliberation of his act that was insulting ."}
{"doc_id": "1910.10683", "para_id": 291, "text": "Processed input: wic pos: N sentence1: It was the deliberation of his act that\nwas insulting .\nsentence2: The deliberations of the jury .\nword: deliberation"}
{"doc_id": "1910.10683", "para_id": 292, "text": "Text: The stable was very roomy, with four good stalls; a large swinging window\nopened into the yard , which made it pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 293, "text": "Processed input: wsc:\nThe stable was very roomy, with four good stalls; a large\nswinging window opened into the yard , which made *it* pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 294, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 295, "text": "Original input: marouane fellaini and adnan januzaj continue to show the world\nthey are not just teammates but also best mates.\nthe manchester united and\nbelgium duo both posted pictures of themselves out at a restaurant on monday\nnight ahead of their game against newcastle on wednesday .\njanuzaj poses\nin the middle of fellaini and a friend looking like somebody who failed to\nreceive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom\nfrom manchester united’s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame ."}
{"doc_id": "1910.10683", "para_id": 296, "text": "Processed input: summarize: marouane fellaini and adnan januzaj continue to show\nthe world they are not just teammates but also best mates.\nthe manchester\nunited and belgium duo both posted pictures of themselves out at a restaurant\non monday night ahead of their game against newcastle on wednesday .\njanuzaj\nposes in the middle of fellaini and a friend looking like somebody who failed\nto receive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom"}
{"doc_id": "1910.10683", "para_id": 297, "text": "from manchester united’s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame ."}
{"doc_id": "1910.10683", "para_id": 298, "text": "Original target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal’s side currently sit two points clear of liverpool\nin fourth ."}
{"doc_id": "1910.10683", "para_id": 299, "text": "Processed target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal’s side currently sit two points clear of liverpool\nin fourth ."}
{"doc_id": "1910.10683", "para_id": 300, "text": "Question: What does increased oxygen concentrations in the patient’s lungs\ndisplace?"}
{"doc_id": "1910.10683", "para_id": 301, "text": "Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers\nto increase the partial pressure of O 2 around the patient and, when needed,\nthe medical staff.\nCarbon monoxide poisoning, gas gangrene, and decompression\nsickness (the ’bends’) are sometimes treated using these devices.\nIncreased\nO 2 concentration in the lungs helps to displace carbon monoxide from the\nheme group of hemoglobin.\nOxygen gas is poisonous to the anaerobic bacteria\nthat cause gas gangrene, so increasing its partial pressure helps kill them.\nDecompression sickness occurs in divers who decompress too quickly after\na dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming\nin their blood.\nIncreasing the pressure of O 2 as soon as possible is part\nof the treatment."}
{"doc_id": "1910.10683", "para_id": 302, "text": "Processed input: question: What does increased oxygen concentrations in the patient’s\nlungs displace?\ncontext: Hyperbaric (high-pressure) medicine uses special\noxygen chambers to increase the partial pressure of O 2 around the patient\nand, when needed, the medical staff.\nCarbon monoxide poisoning, gas gangrene,\nand decompression sickness (the ’bends’) are sometimes treated using these\ndevices.\nIncreased O 2 concentration in the lungs helps to displace carbon\nmonoxide from the heme group of hemoglobin.\nOxygen gas is poisonous to the\nanaerobic bacteria that cause gas gangrene, so increasing its partial pressure\nhelps kill them.\nDecompression sickness occurs in divers who decompress too\nquickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and\nhelium, forming in their blood.\nIncreasing the pressure of O 2 as soon as\npossible is part of the treatment."}
{"doc_id": "1910.10683", "para_id": 303, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 304, "text": "Original input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 305, "text": "Processed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 306, "text": "Original target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 307, "text": "Processed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 308, "text": "Original input: This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 309, "text": "Processed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 310, "text": "Original target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 311, "text": "Processed target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 312, "text": "Original input: Taco Bell said it plans to add 2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 313, "text": "Processed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 314, "text": "Original target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă\n2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 315, "text": "Processed target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă\n2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 316, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 317, "text": "Appendix E. Scores on Every Task for All Experiments"}
{"doc_id": "1910.10683", "para_id": 318, "text": "The following table lists the scores achieved on every task in the experiments described in\nSections 3.2 to 3.6."}
{"doc_id": "1910.10683", "para_id": 319, "text": "GLUE\nSuperGLUE\nWMT\nScore\nCoLA\nSST-2\nMRPC\nMRPC\nSTSB\nSTSB\nQQP\nQQP\nMNLIm\nMNLImm\nQNLI\nRTE\nCNN/DM\nSQuAD\nScore\nBoolQ\nCB\nCB\nCOPA\nMultiRC\nMultiRC\nReCoRD\nReCoRD\nRTE\nWiC\nWSC\nEnDe\nEnFr\nEnRo\nTable\nExperiment\nAverage\nMCC\nAcc\nF1\nAcc\nPCC\nSCC\nF1\nAcc\nAcc\nAcc\nAcc\nAcc\nR-1-F\nR-2-F\nR-L-F\nEM\nF1\nAverage\nAcc\nF1\nAcc\nAcc\nF1\nEM\nF1\nEM\nAcc\nAcc\nAcc\nBLEU\nBLEU\nBLEU"}
{"doc_id": "1910.10683", "para_id": 320, "text": "1\n⋆Baseline average\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n1\nBaseline standard deviation\n0.235\n1.111\n0.569\n0.729\n1.019\n0.374\n0.418\n0.108\n0.070\n0.291\n0.231\n0.361\n1.393\n0.065\n0.065\n0.058\n0.343\n0.226\n0.416\n0.365\n3.237\n2.560\n2.741\n0.716\n1.011\n0.370\n0.379\n1.228\n0.850\n2.029\n0.112\n0.090\n0.108\n1\nNo pre-training\n66.22\n12.29\n80.62\n81.42\n73.04\n72.58\n72.97\n81.94\n86.62\n68.02\n67.98\n75.69\n58.84\n39.19\n17.60\n36.69\n50.31\n61.97\n53.04\n65.38\n71.61\n76.79\n62.00\n59.10\n0.84\n20.33\n17.95\n54.15\n54.08\n65.38\n25.86\n39.77\n24.04"}
{"doc_id": "1910.10683", "para_id": 321, "text": "2\n⋆Enc/dec, denoising\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n2\nEnc/dec, shared, denoising\n82.81\n55.24\n91.86\n91.58\n88.24\n87.43\n87.58\n88.69\n91.60\n83.88\n84.01\n90.23\n73.65\n41.11\n18.78\n38.48\n80.63\n88.49\n70.73\n77.13\n95.04\n96.43\n65.00\n66.16\n22.98\n68.95\n68.09\n70.76\n68.18\n75.96\n26.72\n39.03\n27.46\n2\nEnc/dec, 6 layers, denoising\n80.88\n46.26\n92.09\n91.51\n87.99\n87.01\n86.76\n87.93\n90.97\n82.20\n82.41\n88.83\n71.48\n40.83\n18.97\n38.31\n77.59\n86.07\n68.42\n73.79\n91.70\n92.86\n67.00\n61.02\n19.62\n61.26\n60.33\n72.20\n65.99\n75.00\n26.38\n38.40\n26.95\n2\nLanguage model, denoising\n74.70\n24.50\n90.60\n86.08\n78.92\n85.22\n85.42\n85.40\n88.99\n76.72\n77.05\n86.02\n64.62\n39.49\n17.93\n36.91\n61.14\n71.37\n55.02\n65.47\n60.08\n71.43\n58.00\n43.03\n2.94\n53.35\n52.31\n53.07\n58.62\n63.46\n25.09\n35.28\n25.86\n2\nPrefix LM, denoising\n81.82\n49.99\n92.43\n91.43\n88.24\n87.20\n86.98\n88.41\n91.39\n82.32\n82.93\n88.71\n74.01\n40.46\n18.61\n37.90\n78.94\n87.31\n68.11\n75.50\n93.37\n91.07\n60.00\n63.43\n21.20\n65.03\n64.11\n71.48\n65.67\n73.08\n26.43\n37.98\n27.39\n2\nEnc/dec, LM\n79.56\n42.03\n91.86\n91.64\n88.24\n87.13\n87.00\n88.21\n91.15\n81.68\n81.66\n88.54\n65.70\n40.67\n18.59\n38.13\n76.02\n84.85\n64.29\n72.23\n85.74\n89.29\n57.00\n60.53\n16.26\n59.28\n58.30\n65.34\n64.89\n70.19\n26.27\n39.17\n26.86\n2\nEnc/dec, shared, LM\n79.60\n44.83\n92.09\n90.20\n85.78\n86.03\n85.87\n87.77\n91.02\n81.74\n82.29\n89.16\n65.34\n40.16\n18.13\n37.59\n76.35\n84.86\n63.50\n70.49\n91.41\n87.50\n55.00\n60.21\n16.89\n57.83\n56.73\n63.54\n63.48\n70.19\n26.62\n39.17\n27.05\n2\nEnc/dec, 6 layers, LM\n78.67\n38.72\n91.40\n90.40\n86.52\n86.82\n86.49\n87.87\n91.03\n80.99\n80.92\n88.05\n65.70\n40.29\n18.26\n37.70\n75.32\n84.06\n64.06\n71.38\n85.25\n89.29\n60.00\n57.56\n16.79\n55.22\n54.30\n66.79\n63.95\n71.15\n26.13\n38.42\n26.89\n2\nLanguage model, LM\n73.78\n28.53\n89.79\n85.23\n78.68\n84.22\n84.00\n84.88\n88.70\n74.94\n75.77\n84.84\n58.84\n38.97\n17.54\n36.37\n53.81\n64.55\n56.51\n64.22\n59.92\n71.43\n64.00\n53.04\n1.05\n46.81\n45.78\n58.84\n56.74\n69.23\n25.23\n34.31\n25.38\n2\nPrefix LM, LM\n79.68\n41.26\n92.09\n90.11\n86.27\n86.82\n86.32\n88.35\n91.35\n81.71\n82.02\n89.04\n68.59\n39.66\n17.84\n37.13\n76.87\n85.39\n64.86\n71.47\n93.37\n91.07\n57.00\n58.67\n16.89\n59.25\n58.16\n64.26\n66.30\n71.15\n26.28\n37.51\n26.76"}
{"doc_id": "1910.10683", "para_id": 322, "text": "4\nLanguage modeling with prefix\n80.69\n44.22\n93.00\n91.68\n88.48\n87.20\n87.18\n88.39\n91.41\n82.66\n83.09\n89.29\n68.95\n40.71\n18.94\n38.15\n77.99\n86.43\n65.27\n73.55\n83.95\n87.50\n55.00\n59.65\n18.89\n61.76\n60.76\n68.59\n65.67\n73.08\n26.86\n39.73\n27.49\n4\nBERT-style (Devlin et al., 2018)\n82.96\n52.49\n92.55\n92.79\n89.95\n87.68\n87.66\n88.47\n91.44\n83.60\n84.05\n90.33\n75.45\n41.27\n19.17\n38.72\n80.65\n88.24\n69.85\n76.48\n94.37\n94.64\n61.00\n63.29\n25.08\n66.76\n65.85\n72.20\n69.12\n75.00\n26.78\n40.03\n27.41\n4\nDeshuffling\n73.17\n22.82\n87.16\n86.88\n81.13\n84.03\n83.82\n86.38\n89.90\n76.30\n76.34\n84.18\n58.84\n40.75\n18.59\n38.10\n67.61\n76.76\n58.47\n69.17\n63.70\n78.57\n56.00\n59.85\n12.70\n45.52\n44.36\n57.04\n64.89\n68.27\n26.11\n39.30\n25.62"}
{"doc_id": "1910.10683", "para_id": 323, "text": "5\nBERT-style (Devlin et al., 2018)\n82.96\n52.49\n92.55\n92.79\n89.95\n87.68\n87.66\n88.47\n91.44\n83.60\n84.05\n90.33\n75.45\n41.27\n19.17\n38.72\n80.65\n88.24\n69.85\n76.48\n94.37\n94.64\n61.00\n63.29\n25.08\n66.76\n65.85\n72.20\n69.12\n75.00\n26.78\n40.03\n27.41\n5\nMASS-style (Song et al., 2019)\n82.32\n47.01\n91.63\n92.53\n89.71\n88.21\n88.18\n88.58\n91.44\n82.96\n83.67\n90.02\n77.26\n41.16\n19.16\n38.55\n80.10\n88.07\n69.28\n75.08\n84.98\n89.29\n63.00\n64.46\n23.50\n66.71\n65.91\n72.20\n67.71\n78.85\n26.79\n39.89\n27.55\n5\n⋆Replace corrupted spans\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n5\nDrop corrupted tokens\n84.44\n60.04\n92.89\n92.79\n89.95\n87.28\n86.85\n88.56\n91.54\n83.94\n83.92\n90.74\n79.42\n41.27\n19.31\n38.70\n80.52\n88.28\n68.67\n75.90\n96.02\n94.64\n56.00\n65.06\n23.92\n65.54\n64.60\n71.12\n67.40\n74.04\n27.07\n39.76\n27.82"}
{"doc_id": "1910.10683", "para_id": 324, "text": "6\nCorruption rate = 10%\n82.82\n52.71\n92.09\n91.55\n88.24\n88.19\n88.15\n88.47\n91.40\n83.50\n84.51\n90.33\n75.45\n41.05\n19.00\n38.53\n80.38\n88.36\n69.55\n74.98\n92.37\n92.86\n62.00\n66.04\n24.66\n67.93\n67.09\n70.76\n67.24\n75.96\n26.87\n39.28\n27.44\n6\n⋆Corruption rate = 15%\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n6\nCorruption rate = 25%\n83.00\n53.47\n93.00\n92.44\n89.46\n87.36\n87.36\n88.68\n91.53\n84.44\n84.15\n90.77\n74.01\n41.69\n19.54\n39.14\n80.96\n88.61\n70.48\n76.39\n93.02\n92.86\n68.00\n65.46\n24.66\n68.20\n67.39\n73.65\n67.87\n72.12\n27.04\n39.83\n27.47\n6\nCorruption rate = 50%\n81.27\n46.26\n91.63\n91.11\n87.99\n87.87\n87.64\n88.70\n91.57\n83.64\n84.10\n90.24\n70.76\n41.51\n19.32\n38.89\n79.80\n87.76\n70.33\n75.02\n93.05\n92.86\n68.00\n62.97\n24.13\n64.94\n64.13\n72.20\n68.50\n77.88\n27.01\n39.90\n27.49"}
{"doc_id": "1910.10683", "para_id": 325, "text": "7\n⋆Baseline (i.i.d.)\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n7\nAverage span length = 2\n83.54\n53.82\n92.20\n93.05\n90.44\n87.85\n87.71\n88.42\n91.40\n84.28\n84.46\n90.88\n77.62\n41.23\n19.39\n38.69\n82.09\n89.69\n72.20\n77.06\n90.43\n91.07\n70.00\n66.28\n26.13\n71.34\n70.61\n75.45\n68.34\n78.85\n26.76\n39.99\n27.63\n7\nAverage span length = 3\n83.49\n53.90\n92.43\n92.25\n89.46\n87.49\n87.53\n88.72\n91.51\n84.85\n84.84\n90.99\n77.26\n41.50\n19.62\n38.94\n81.84\n89.66\n72.53\n76.85\n94.37\n94.64\n70.00\n67.64\n28.75\n70.84\n69.90\n74.73\n67.71\n77.88\n26.86\n39.65\n27.62\n7\nAverage span length = 5\n83.40\n52.12\n93.12\n92.63\n89.71\n88.70\n88.47\n88.84\n91.64\n84.32\n84.29\n90.79\n76.90\n41.39\n19.24\n38.82\n82.05\n89.79\n72.23\n77.06\n83.06\n89.29\n69.00\n68.16\n30.12\n71.36\n70.53\n75.81\n69.91\n79.81\n26.88\n39.40\n27.53\n7\nAverage span length = 10\n82.85\n50.11\n92.09\n91.95\n88.97\n88.45\n88.22\n88.86\n91.63\n84.34\n84.28\n91.07\n76.17\n41.38\n19.33\n38.80\n81.84\n89.39\n70.44\n76.45\n87.40\n89.29\n65.00\n66.87\n29.59\n69.82\n68.94\n72.56\n67.55\n75.96\n26.79\n39.49\n27.69"}
{"doc_id": "1910.10683", "para_id": 326, "text": "8\n⋆C4\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n8\nC4, unfiltered\n81.46\n48.01\n91.63\n92.72\n89.95\n87.79\n87.60\n88.31\n91.27\n82.30\n82.34\n88.71\n72.20\n41.09\n19.14\n38.54\n78.78\n87.04\n68.04\n75.75\n89.17\n91.07\n62.00\n65.52\n25.60\n62.42\n61.58\n69.68\n67.08\n72.12\n26.55\n39.34\n27.21\n8\nRealNews-like\n83.83\n56.55\n92.66\n92.06\n88.97\n87.71\n87.37\n88.51\n91.49\n84.35\n84.46\n90.61\n78.34\n41.38\n19.23\n38.84\n80.39\n88.50\n72.38\n77.00\n93.09\n94.64\n66.00\n65.92\n23.82\n74.56\n73.72\n75.81\n66.61\n80.77\n26.75\n39.90\n27.48\n8\nWebText-like\n84.03\n56.38\n93.12\n92.31\n89.22\n88.69\n88.68\n88.65\n91.56\n84.70\n84.84\n90.83\n77.62\n41.23\n19.31\n38.70\n81.42\n89.15\n71.40\n76.88\n83.08\n89.29\n66.00\n64.10\n24.24\n72.24\n71.36\n75.45\n68.03\n82.69\n26.80\n39.74\n27.59\n8\nWikipedia\n81.85\n45.53\n92.32\n91.67\n88.24\n85.62\n86.40\n88.37\n91.34\n82.61\n83.25\n90.96\n77.26\n41.39\n19.31\n38.81\n81.29\n89.18\n68.01\n76.12\n56.03\n80.36\n67.00\n65.01\n25.92\n69.03\n68.06\n74.73\n67.08\n76.92\n26.94\n39.69\n27.67\n8\nWikipedia + TBC\n83.65\n55.53\n92.78\n92.41\n89.22\n86.67\n86.27\n89.47\n92.29\n84.38\n83.45\n91.94\n76.90\n41.22\n19.28\n38.67\n82.08\n89.70\n73.24\n76.22\n95.40\n92.86\n69.00\n51.59\n50.93\n69.53\n68.51\n77.62\n66.93\n81.73\n26.77\n39.63\n27.57"}
{"doc_id": "1910.10683", "para_id": 327, "text": "9\n⋆Full data set\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n9\n229 (64 repeats)\n82.87\n53.82\n92.78\n91.79\n88.73\n87.56\n87.58\n88.73\n91.54\n84.07\n84.21\n90.59\n73.65\n41.18\n19.19\n38.67\n80.97\n88.90\n72.03\n76.76\n92.96\n92.86\n66.00\n65.11\n26.76\n69.35\n68.49\n75.81\n67.24\n82.69\n26.83\n39.74\n27.63\n9\n227 (256 repeats)\n82.62\n50.60\n92.32\n92.07\n88.73\n87.83\n87.60\n88.65\n91.54\n83.43\n84.37\n90.12\n75.81\n41.24\n19.20\n38.70\n79.78\n87.63\n69.97\n75.29\n93.42\n91.07\n63.00\n61.82\n23.61\n66.27\n65.39\n73.65\n66.30\n80.77\n27.02\n39.71\n27.33\n9\n225 (1,024 repeats)\n79.55\n43.84\n91.28\n89.32\n85.05\n85.92\n85.74\n88.05\n91.09\n81.29\n81.72\n87.90\n69.31\n40.66\n18.57\n38.13\n76.27\n84.58\n64.76\n72.63\n83.97\n82.14\n64.00\n59.39\n17.94\n56.94\n56.04\n64.98\n65.20\n73.08\n26.38\n39.56\n26.80\n9\n223 (4,096 repeats)\n76.34\n32.68\n89.45\n89.84\n86.03\n83.49\n83.42\n87.18\n90.61\n77.80\n78.69\n85.47\n64.62\n40.16\n18.33\n37.66\n70.92\n80.20\n59.29\n69.85\n73.48\n73.21\n56.00\n57.66\n14.38\n46.69\n45.79\n59.57\n65.05\n68.27\n26.37\n38.84\n25.81"}
{"doc_id": "1910.10683", "para_id": 328, "text": "10\n⋆All parameters\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n10\nAdapter layers, d = 32\n80.52\n45.33\n91.63\n90.59\n86.76\n88.38\n88.06\n86.99\n90.26\n83.63\n83.94\n90.72\n67.15\n34.50\n15.08\n32.15\n79.32\n87.70\n60.40\n65.32\n50.87\n73.21\n52.00\n58.61\n19.41\n65.50\n64.58\n62.09\n64.58\n73.08\n13.84\n17.88\n15.54\n10\nAdapter layers, d = 128\n81.51\n45.35\n92.89\n91.49\n88.24\n87.73\n87.65\n87.73\n90.93\n83.64\n84.09\n90.52\n72.56\n36.71\n16.62\n34.37\n79.47\n87.61\n63.03\n69.20\n52.21\n75.00\n56.00\n61.08\n18.05\n67.94\n66.97\n68.59\n66.77\n73.08\n19.83\n27.50\n22.63\n10\nAdapter layers, d = 512\n81.54\n44.25\n93.35\n91.00\n87.25\n88.74\n88.44\n88.02\n91.15\n83.08\n83.80\n89.62\n74.37\n38.63\n17.78\n36.25\n79.18\n87.32\n64.30\n73.18\n59.86\n71.43\n56.00\n62.94\n18.57\n66.56\n65.74\n70.76\n67.87\n74.04\n23.45\n33.98\n25.81\n10\nAdapter layers, d = 2048\n82.62\n49.86\n92.55\n91.30\n87.99\n88.46\n88.35\n88.36\n91.40\n83.63\n83.18\n90.66\n76.53\n39.44\n18.30\n37.06\n79.40\n87.36\n68.61\n74.53\n88.00\n91.07\n58.00\n61.10\n18.89\n66.73\n66.06\n73.29\n71.16\n75.96\n25.64\n36.92\n26.93\n10\nGradual Unfreezing\n82.50\n51.74\n91.97\n92.61\n89.71\n87.27\n86.90\n88.26\n91.35\n83.42\n83.49\n89.71\n75.09\n40.88\n18.95\n38.40\n79.17\n87.30\n70.79\n75.51\n93.09\n94.64\n70.00\n62.03\n21.51\n65.69\n64.79\n72.92\n69.12\n77.89\n26.71\n39.02\n26.93"}
{"doc_id": "1910.10683", "para_id": 329, "text": "11\n⋆Baseline (pre-train/fine-tune)\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n11\nEqual\n76.13\n39.47\n90.94\n82.90\n75.74\n78.83\n78.44\n86.45\n89.71\n82.08\n82.92\n90.13\n59.93\n40.95\n19.02\n38.39\n76.51\n85.61\n63.37\n73.06\n82.37\n83.93\n65.00\n60.89\n17.52\n60.51\n59.70\n61.01\n60.03\n65.38\n23.89\n34.31\n26.78\n11\nExamples-proportional, K = 216\n80.45\n42.07\n91.97\n90.97\n87.50\n85.41\n85.04\n86.89\n90.10\n83.01\n83.66\n90.74\n72.56\n41.16\n19.04\n38.59\n77.25\n85.72\n69.95\n76.67\n86.38\n89.29\n70.00\n65.93\n27.91\n62.78\n61.95\n76.90\n65.83\n73.08\n24.35\n34.99\n27.10\n11\nExamples-proportional, K = 217\n81.56\n47.35\n91.40\n91.55\n88.24\n86.15\n85.93\n86.94\n90.06\n82.76\n84.12\n90.79\n75.09\n41.06\n19.12\n38.47\n77.00\n85.87\n67.91\n77.89\n77.54\n85.71\n57.00\n67.78\n27.07\n61.51\n60.54\n79.06\n65.20\n74.04\n24.36\n35.00\n27.25\n11\nExamples-proportional, K = 218\n81.67\n46.85\n91.63\n91.99\n88.73\n87.68\n87.20\n86.93\n90.35\n83.30\n84.01\n91.47\n73.29\n40.96\n19.07\n38.43\n78.17\n86.74\n67.94\n76.57\n78.88\n87.50\n62.00\n67.70\n30.85\n63.43\n62.54\n76.53\n65.67\n67.31\n24.57\n35.19\n27.39\n11\nExamples-proportional, K = 219\n81.42\n45.94\n91.63\n92.20\n89.22\n88.44\n88.32\n86.84\n90.10\n83.73\n84.29\n91.84\n70.40\n41.26\n19.24\n38.71\n79.78\n88.15\n67.30\n75.66\n75.59\n87.50\n59.00\n68.22\n30.64\n65.32\n64.29\n73.65\n65.05\n69.23\n25.21\n36.30\n27.76\n11\nExamples-proportional, K = 220\n80.80\n42.55\n92.78\n91.27\n87.99\n88.36\n88.10\n86.10\n89.62\n84.15\n84.26\n92.20\n68.95\n41.05\n19.24\n38.46\n80.36\n88.27\n67.38\n73.21\n76.18\n83.93\n62.00\n67.57\n26.86\n66.12\n65.22\n76.90\n64.73\n69.23\n25.66\n36.93\n27.68\n11\nExamples-proportional, K = 221\n79.83\n44.45\n91.28\n89.00\n84.31\n87.54\n87.40\n84.93\n88.53\n82.54\n84.16\n90.85\n67.87\n40.51\n18.79\n37.92\n79.50\n87.48\n65.10\n71.16\n68.88\n85.71\n57.00\n62.75\n23.40\n64.50\n63.65\n72.92\n64.11\n71.15\n25.82\n37.22\n27.13\n11\nTemperature-scaled, T = 2\n81.90\n54.00\n91.74\n90.56\n86.76\n85.11\n84.60\n86.40\n89.74\n83.47\n84.15\n91.51\n72.56\n41.09\n19.28\n38.54\n79.42\n87.77\n69.92\n76.73\n92.37\n92.86\n57.00\n69.80\n31.90\n66.65\n65.74\n72.92\n67.08\n75.96\n25.42\n36.72\n27.20\n11\nTemperature-scaled, T = 4\n80.56\n45.38\n91.97\n89.68\n85.78\n83.13\n82.76\n86.39\n90.00\n82.78\n84.19\n91.16\n73.65\n41.09\n19.22\n38.51\n77.99\n86.81\n69.54\n76.76\n97.36\n96.43\n59.00\n68.10\n31.48\n64.26\n63.27\n74.73\n64.26\n71.15\n25.04\n35.82\n27.45\n11\nTemperature-scaled, T = 8\n77.21\n40.07\n91.06\n88.11\n83.33\n79.20\n79.06\n86.60\n89.90\n83.05\n83.56\n90.21\n59.93\n41.01\n19.10\n38.40\n77.14\n85.99\n66.07\n73.94\n93.70\n94.64\n60.00\n66.36\n26.86\n63.46\n62.60\n62.09\n63.32\n65.38\n24.55\n35.35\n27.17"}
{"doc_id": "1910.10683", "para_id": 330, "text": "12\n⋆Unsupervised pre-training + fine-tuning\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n12\nMulti-task training\n81.42\n45.94\n91.63\n92.20\n89.22\n88.44\n88.32\n86.84\n90.10\n83.73\n84.29\n91.84\n70.40\n41.26\n19.24\n38.71\n79.78\n88.15\n67.30\n75.66\n75.59\n87.50\n59.00\n68.22\n30.64\n65.32\n64.29\n73.65\n65.05\n69.23\n25.21\n36.30\n27.76\n12\nMulti-task pre-training + fine-tuning\n83.11\n51.42\n92.66\n91.73\n88.73\n88.06\n87.70\n88.61\n91.61\n84.09\n84.31\n91.85\n76.53\n41.15\n19.12\n38.59\n80.26\n88.50\n71.03\n79.54\n81.69\n87.50\n65.00\n70.72\n31.48\n65.94\n65.03\n81.23\n68.18\n73.08\n27.08\n39.80\n28.07\n12\nLeave-one-out multi-task training\n81.98\n48.00\n93.23\n91.72\n88.24\n87.76\n87.32\n88.61\n91.44\n84.00\n84.11\n90.79\n72.20\n41.34\n19.05\n38.77\n79.97\n88.10\n71.68\n78.35\n86.76\n89.29\n66.00\n68.09\n29.49\n66.23\n65.27\n79.06\n68.65\n78.85\n26.93\n39.79\n27.87\n12\nSupervised multi-task pre-training\n79.93\n36.60\n92.43\n91.58\n88.24\n87.03\n86.78\n88.15\n91.20\n82.87\n83.16\n90.13\n70.76\n41.12\n18.96\n38.49\n77.38\n85.65\n65.36\n75.66\n68.87\n83.93\n58.00\n64.81\n21.93\n55.37\n54.61\n71.12\n67.40\n75.96\n26.81\n40.13\n28.04"}
{"doc_id": "1910.10683", "para_id": 331, "text": "13\n⋆Baseline\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n13\n1× size, 4× training steps\n85.33\n60.29\n93.81\n94.06\n91.67\n89.42\n89.25\n89.15\n91.87\n86.01\n85.70\n91.63\n78.34\n41.52\n19.33\n38.96\n82.45\n90.19\n74.72\n79.17\n94.75\n92.86\n71.00\n67.34\n29.70\n72.63\n71.59\n78.34\n72.10\n82.69\n27.08\n40.66\n27.93\n13\n1× size, 4× batch size\n84.60\n56.08\n93.12\n92.31\n89.22\n88.85\n88.84\n89.35\n92.07\n85.98\n86.13\n91.07\n80.14\n41.70\n19.42\n39.08\n82.52\n90.21\n74.64\n78.78\n93.69\n94.64\n72.00\n68.09\n30.95\n74.73\n73.90\n76.53\n70.06\n81.73\n27.07\n40.60\n27.84\n13\n2× size, 2× training steps\n86.18\n62.04\n93.69\n93.36\n90.69\n89.18\n89.23\n89.35\n92.05\n87.23\n87.05\n92.68\n81.95\n41.74\n19.66\n39.14\n84.18\n91.29\n77.18\n80.98\n97.36\n96.43\n74.00\n71.34\n35.68\n77.11\n76.34\n80.51\n69.28\n85.58\n27.52\n41.03\n28.19\n13\n4× size, 1× training steps\n85.91\n57.58\n94.38\n92.67\n89.95\n89.60\n89.60\n89.44\n92.14\n87.05\n87.12\n93.12\n83.39\n41.60\n19.73\n39.08\n83.86\n91.32\n78.04\n81.38\n89.09\n94.64\n73.00\n73.74\n40.40\n78.25\n77.40\n81.59\n70.22\n91.35\n27.47\n40.71\n28.10\n13\n4× ensembled\n84.77\n56.14\n93.46\n93.31\n90.67\n89.71\n89.60\n89.62\n92.24\n86.22\n86.53\n91.60\n77.98\n42.10\n20.10\n39.56\n83.09\n90.40\n71.74\n77.58\n89.85\n91.07\n66.00\n69.32\n29.49\n72.67\n71.94\n76.90\n69.12\n72.12\n28.05\n40.53\n28.09\n13\n4× ensembled, fine-tune only\n84.05\n54.78\n92.78\n93.15\n90.44\n88.34\n88.12\n89.27\n91.97\n85.33\n85.88\n90.98\n77.62\n41.66\n19.57\n39.12\n82.36\n89.86\n71.56\n77.43\n90.07\n92.86\n69.00\n67.31\n26.34\n70.47\n69.64\n75.45\n68.18\n74.04\n27.55\n40.22\n28.09"}
{"doc_id": "1910.10683", "para_id": 332, "text": "Table 16: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with ⋆denotes our baseline model (described in Section 3.1)."}
{"doc_id": "1910.10683", "para_id": 333, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 334, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2019."}
{"doc_id": "1910.10683", "para_id": 335, "text": "Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efficient adaptive\noptimization for large-scale learning. arXiv preprint arXiv:1901.11150, 2019."}
{"doc_id": "1910.10683", "para_id": 336, "text": "Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multi-\nlingual neural machine translation in the wild: Findings and challenges. arXiv preprint\narXiv:1907.05019, 2019."}
{"doc_id": "1910.10683", "para_id": 337, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016."}
{"doc_id": "1910.10683", "para_id": 338, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-\ndriven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019."}
{"doc_id": "1910.10683", "para_id": 339, "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In Third International Conference on Learning\nRepresentations, 2015."}
{"doc_id": "1910.10683", "para_id": 340, "text": "Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural\nmachine translation. arXiv preprint arXiv:1909.08478, 2019."}
{"doc_id": "1910.10683", "para_id": 341, "text": "Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific\ntext. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019."}
{"doc_id": "1910.10683", "para_id": 342, "text": "Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo-\nhannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al.\nFindings of the 2014 workshop on statistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation, 2014."}
{"doc_id": "1910.10683", "para_id": 343, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck,\nChris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al.\nFindings of the 2015 workshop on statistical machine translation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation, 2015."}
{"doc_id": "1910.10683", "para_id": 344, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz,\net al. Findings of the 2016 conference on machine translation. In Proceedings of the First\nConference on Machine Translation, 2016."}
{"doc_id": "1910.10683", "para_id": 345, "text": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy\nBengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349,\n2015."}
{"doc_id": "1910.10683", "para_id": 346, "text": "Christian Buck, Kenneth Heafield, and Bas Van Ooyen. N-gram counts and language models\nfrom the common crawl. In LREC, 2014."}
{"doc_id": "1910.10683", "para_id": 347, "text": "Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "1910.10683", "para_id": 348, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.\narXiv preprint arXiv:1708.00055, 2017."}
{"doc_id": "1910.10683", "para_id": 349, "text": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\nmachine reading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1910.10683", "para_id": 350, "text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019."}
{"doc_id": "1910.10683", "para_id": 351, "text": "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning.\nElectra:\nPre-training text encoders as discriminators rather than generators.\narXiv preprint\narXiv:2003.10555, 2020."}
{"doc_id": "1910.10683", "para_id": 352, "text": "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence\nrepresentations. arXiv preprint arXiv:1803.05449, 2018."}
{"doc_id": "1910.10683", "para_id": 353, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Super-\nvised learning of universal sentence representations from natural language inference data.\narXiv preprint arXiv:1705.02364, 2017."}
{"doc_id": "1910.10683", "para_id": 354, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual\nentailment challenge. In Machine Learning Challenges Workshop, 2005."}
{"doc_id": "1910.10683", "para_id": 355, "text": "Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural\ninformation processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 356, "text": "Marie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\nInvestigating projection in naturally occurring discourse. In Sinn und Bedeutung 23, 2019."}
{"doc_id": "1910.10683", "para_id": 357, "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision and\npattern recognition, 2009."}
{"doc_id": "1910.10683", "para_id": 358, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018."}
{"doc_id": "1910.10683", "para_id": 359, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),\n2005."}
{"doc_id": "1910.10683", "para_id": 360, "text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language\nunderstanding and generation. arXiv preprint arXiv:1905.03197, 2019."}
{"doc_id": "1910.10683", "para_id": 361, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 362, "text": "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation\nat scale. arXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "1910.10683", "para_id": 363, "text": "Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893, 2018."}
{"doc_id": "1910.10683", "para_id": 364, "text": "Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013."}
{"doc_id": "1910.10683", "para_id": 365, "text": "Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\nwith free license. In Proceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 914–922, 2016."}
{"doc_id": "1910.10683", "para_id": 366, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016."}
{"doc_id": "1910.10683", "para_id": 367, "text": "Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\npreprint arXiv:1811.08883, 2018."}
{"doc_id": "1910.10683", "para_id": 368, "text": "Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983, 2019."}
{"doc_id": "1910.10683", "para_id": 369, "text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\nAdvances in neural information processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 370, "text": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "1910.10683", "para_id": 371, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016."}
{"doc_id": "1910.10683", "para_id": 372, "text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015."}
{"doc_id": "1910.10683", "para_id": 373, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for NLP. arXiv preprint arXiv:1902.00751, 2019."}
{"doc_id": "1910.10683", "para_id": 374, "text": "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifi-\ncation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1910.10683", "para_id": 375, "text": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Dou-\nglas Eck. Music transformer: Generating music with long-term structure. In Seventh\nInternational Conference on Learning Representations, 2018a."}
{"doc_id": "1910.10683", "para_id": 376, "text": "Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V\nLe, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline\nparallelism. arXiv preprint arXiv:1811.06965, 2018b."}
{"doc_id": "1910.10683", "para_id": 377, "text": "Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet good for\ntransfer learning? arXiv preprint arXiv:1608.08614, 2016."}
{"doc_id": "1910.10683", "para_id": 378, "text": "Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First Quora dataset release: Question\npairs.\nhttps://data.quora.com/First-Quora-Dataset-Release-Question-Pairs,\n2017."}
{"doc_id": "1910.10683", "para_id": 379, "text": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\nSergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\nembedding. In Proceedings of the 22nd ACM international conference on Multimedia,\n2014."}
{"doc_id": "1910.10683", "para_id": 380, "text": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint\narXiv:1909.10351, 2019."}
{"doc_id": "1910.10683", "para_id": 381, "text": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551, 2017."}
{"doc_id": "1910.10683", "para_id": 382, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint\narXiv:1907.10529, 2019."}
{"doc_id": "1910.10683", "para_id": 383, "text": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1910.10683", "para_id": 384, "text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\nfor modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics, 2014."}
{"doc_id": "1910.10683", "para_id": 385, "text": "Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard\nSocher. CTRL: A conditional transformer language model for controllable generation.\narXiv preprint arXiv:1909.05858, 2019a."}
{"doc_id": "1910.10683", "para_id": 386, "text": "Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question\nanswering and text classification via span extraction. arXiv preprint arXiv:1904.09286,\n2019b."}
{"doc_id": "1910.10683", "para_id": 387, "text": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLooking beyond the surface: A challenge set for reading comprehension over multiple\nsentences. In Proceedings of North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018."}
{"doc_id": "1910.10683", "para_id": 388, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 389, "text": "Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information\nprocessing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 390, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. A surprisingly robust trick for Winograd schema challenge. arXiv preprint\narXiv:1905.06290, 2019."}
{"doc_id": "1910.10683", "para_id": 391, "text": "Jakub Konečn`y, Brendan McMahan, and Daniel Ramage. Federated optimization: Dis-\ntributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015."}
{"doc_id": "1910.10683", "para_id": 392, "text": "Jakub Konečn`y, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha\nSuresh, and Dave Bacon. Federated learning: Strategies for improving communication\nefficiency. arXiv preprint arXiv:1610.05492, 2016."}
{"doc_id": "1910.10683", "para_id": 393, "text": "Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better ImageNet models transfer\nbetter? arXiv preprint arXiv:1805.08974, 2018."}
{"doc_id": "1910.10683", "para_id": 394, "text": "Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv\npreprint arXiv:1404.5997, 2014."}
{"doc_id": "1910.10683", "para_id": 395, "text": "Taku Kudo. Subword regularization: Improving neural network translation models with\nmultiple subword candidates. arXiv preprint arXiv:1804.10959, 2018."}
{"doc_id": "1910.10683", "para_id": 396, "text": "Taku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226,\n2018."}
{"doc_id": "1910.10683", "para_id": 397, "text": "Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv\npreprint arXiv:1901.07291, 2019."}
{"doc_id": "1910.10683", "para_id": 398, "text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. ALBERT: A lite BERT for self-supervised learning of language representa-\ntions. arXiv preprint arXiv:1909.11942, 2019."}
{"doc_id": "1910.10683", "para_id": 399, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge.\nIn Thirteenth International Conference on the Principles of Knowledge Representation\nand Reasoning, 2012."}
{"doc_id": "1910.10683", "para_id": 400, "text": "Qi Li. Literature survey: domain adaptation algorithms for natural language processing.\n2012."}
{"doc_id": "1910.10683", "para_id": 401, "text": "Chin-Yew Lin.\nROUGE: A package for automatic evaluation of summaries.\nIn Text\nsummarization branches out, 2004."}
{"doc_id": "1910.10683", "para_id": 402, "text": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018."}
{"doc_id": "1910.10683", "para_id": 403, "text": "Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization\nusing length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998, 2019a."}
{"doc_id": "1910.10683", "para_id": 404, "text": "Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Rep-\nresentation learning using multi-task deep neural networks for semantic classification\nand information retrieval. In Proceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\n2015."}
{"doc_id": "1910.10683", "para_id": 405, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\nnetworks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019b."}
{"doc_id": "1910.10683", "para_id": 406, "text": "Yang Liu. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318,\n2019."}
{"doc_id": "1910.10683", "para_id": 407, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized\nBERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019c."}
{"doc_id": "1910.10683", "para_id": 408, "text": "Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence\nrepresentations. arXiv preprint arXiv:1803.02893, 2018."}
{"doc_id": "1910.10683", "para_id": 409, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In Proceedings of the European Conference on Computer Vision\n(ECCV), 2018."}
{"doc_id": "1910.10683", "para_id": 410, "text": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The nat-\nural language decathlon: Multitask learning as question answering.\narXiv preprint\narXiv:1806.08730, 2018."}
{"doc_id": "1910.10683", "para_id": 411, "text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013a."}
{"doc_id": "1910.10683", "para_id": 412, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in neural\ninformation processing systems, 2013b."}
{"doc_id": "1910.10683", "para_id": 413, "text": "Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing\nXiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond.\narXiv preprint arXiv:1602.06023, 2016."}
{"doc_id": "1910.10683", "para_id": 414, "text": "Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring\nmid-level image representations using convolutional neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014."}
{"doc_id": "1910.10683", "para_id": 415, "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on\nassociation for computational linguistics. Association for Computational Linguistics, 2002."}
{"doc_id": "1910.10683", "para_id": 416, "text": "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1910.10683", "para_id": 417, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 418, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors\nfor word representation. In Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), 2014."}
{"doc_id": "1910.10683", "para_id": 419, "text": "Matthew Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting\npretrained representations to diverse tasks. arXiv preprint arXiv:1903.05987, 2019."}
{"doc_id": "1910.10683", "para_id": 420, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018."}
{"doc_id": "1910.10683", "para_id": 421, "text": "Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Sup-\nplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088,\n2018."}
{"doc_id": "1910.10683", "para_id": 422, "text": "Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for\nevaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "1910.10683", "para_id": 423, "text": "Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771,\n2018."}
{"doc_id": "1910.10683", "para_id": 424, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018."}
{"doc_id": "1910.10683", "para_id": 425, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019."}
{"doc_id": "1910.10683", "para_id": 426, "text": "Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the Winograd\nschema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning. Association\nfor Computational Linguistics, 2012."}
{"doc_id": "1910.10683", "para_id": 427, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016."}
{"doc_id": "1910.10683", "para_id": 428, "text": "Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence\nto sequence learning. arXiv preprint arXiv:1611.02683, 2016."}
{"doc_id": "1910.10683", "para_id": 429, "text": "Alex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré.\nSnorkel MeTaL: Weak supervision for multi-task learning. In Proceedings of the Second\nWorkshop on Data Management for End-To-End Machine Learning, 2018."}
{"doc_id": "1910.10683", "para_id": 430, "text": "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring\nSymposium Series, 2011."}
{"doc_id": "1910.10683", "para_id": 431, "text": "Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017."}
{"doc_id": "1910.10683", "para_id": 432, "text": "Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI\nGalway, 2019."}
{"doc_id": "1910.10683", "para_id": 433, "text": "Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer\nlearning in natural language processing. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Tutorials,\npages 15–18, 2019."}
{"doc_id": "1910.10683", "para_id": 434, "text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale\nvisual recognition challenge. International journal of computer vision, 2015."}
{"doc_id": "1910.10683", "para_id": 435, "text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108,\n2019."}
{"doc_id": "1910.10683", "para_id": 436, "text": "Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017."}
{"doc_id": "1910.10683", "para_id": 437, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1910.10683", "para_id": 438, "text": "Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training.\narXiv preprint arXiv:1811.03600, 2018."}
{"doc_id": "1910.10683", "para_id": 439, "text": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\nrepresentations. arXiv preprint arXiv:1803.02155, 2018."}
{"doc_id": "1910.10683", "para_id": 440, "text": "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. arXiv preprint arXiv:1804.04235, 2018."}
{"doc_id": "1910.10683", "para_id": 441, "text": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1910.10683", "para_id": 442, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan\nSepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In\nAdvances in Neural Information Processing Systems, 2018."}
{"doc_id": "1910.10683", "para_id": 443, "text": "Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-\nBurch, and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In\nProceedings of the 51st Annual Meeting of the Association for Computational Linguistics,\n2013."}
{"doc_id": "1910.10683", "para_id": 444, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing, 2013."}
{"doc_id": "1910.10683", "para_id": 445, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "1910.10683", "para_id": 446, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 447, "text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of\nMachine Learning Research, 2014."}
{"doc_id": "1910.10683", "para_id": 448, "text": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning\ngeneral purpose distributed sentence representations via large scale multi-task learning.\narXiv preprint arXiv:1804.00079, 2018."}
{"doc_id": "1910.10683", "para_id": 449, "text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 450, "text": "Richard S. Sutton.\nThe bitter lesson.\nhttp://www.incompleteideas.net/IncIdeas/\nBitterLesson.html, 2019."}
{"doc_id": "1910.10683", "para_id": 451, "text": "Wilson L. Taylor. “Cloze procedure”: A new tool for measuring readability. Journalism\nBulletin, 1953."}
{"doc_id": "1910.10683", "para_id": 452, "text": "Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847, 2018."}
{"doc_id": "1910.10683", "para_id": 453, "text": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip\nBachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. arXiv\npreprint arXiv:1611.09830, 2016."}
{"doc_id": "1910.10683", "para_id": 454, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, 2017."}
{"doc_id": "1910.10683", "para_id": 455, "text": "Elena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations\nin the transformer: A study with machine translation and language modeling objectives.\narXiv preprint arXiv:1909.01380, 2019."}
{"doc_id": "1910.10683", "para_id": 456, "text": "Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018."}
{"doc_id": "1910.10683", "para_id": 457, "text": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma\nPatel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Can you tell me\nhow to get past Sesame Street? Sentence-level pretraining beyond language modeling. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n2019a."}
{"doc_id": "1910.10683", "para_id": 458, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-\npurpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019b."}
{"doc_id": "1910.10683", "para_id": 459, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT:\nIncorporating language structures into pre-training for deep language understanding.\narXiv preprint arXiv:1908.04577, 2019c."}
{"doc_id": "1910.10683", "para_id": 460, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability\njudgments. arXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1910.10683", "para_id": 461, "text": "Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017."}
{"doc_id": "1910.10683", "para_id": 462, "text": "Ronald J. Williams and David Zipser. A learning algorithm for continually running fully\nrecurrent neural networks. Neural computation, 1989."}
{"doc_id": "1910.10683", "para_id": 463, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144, 2016."}
{"doc_id": "1910.10683", "para_id": 464, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv\npreprint arXiv:1906.08237, 2019."}
{"doc_id": "1910.10683", "para_id": 465, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 466, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad\nNorouzi, and Quoc V. Le. QAnet: Combining local convolution with global self-attention\nfor reading comprehension. arXiv preprint arXiv:1804.09541, 2018."}
{"doc_id": "1910.10683", "para_id": 467, "text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-\nner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616,\n2019."}
{"doc_id": "1910.10683", "para_id": 468, "text": "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. ReCoRD: Bridging the gap between human and machine commonsense\nreading comprehension. arXiv preprint arXiv:1810.12885, 2018."}
{"doc_id": "1910.10683", "para_id": 469, "text": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: En-\nhanced adversarial training for language understanding. arXiv preprint arXiv:1909.11764,\n2019."}
{"doc_id": "1910.10683", "para_id": 470, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In Proceedings of the IEEE international\nconference on computer vision, 2015."}
{"doc_id": "2106.09685", "para_id": 0, "text": "LORA: LOW-RANK ADAPTATION OF LARGE LAN-\nGUAGE MODELS"}
{"doc_id": "2106.09685", "para_id": 1, "text": "Edward Hu∗\nYelong Shen∗\nPhillip Wallis\nZeyuan Allen-Zhu\nYuanzhi Li\nShean Wang\nLu Wang\nWeizhu Chen\nMicrosoft Corporation\n{edwardhu, yeshe, phwallis, zeyuana,\nyuanzhil, swang, luw, wzchen}@microsoft.com\nyuanzhil@andrew.cmu.edu\n(Version 2)"}
{"doc_id": "2106.09685", "para_id": 2, "text": "An important paradigm of natural language processing consists of large-scale pre-\ntraining on general domain data and adaptation to particular tasks or domains. As\nwe pre-train larger models, full ﬁne-tuning, which retrains all model parameters,\nbecomes less feasible. Using GPT-3 175B as an example – deploying indepen-\ndent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively\nexpensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-\ntrained model weights and injects trainable rank decomposition matrices into each\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\nrameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam,\nLoRA can reduce the number of trainable parameters by 10,000 times and the\nGPU memory requirement by 3 times. LoRA performs on-par or better than ﬁne-\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\nno additional inference latency. We also provide an empirical investigation into\nrank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\nmodels and provide our implementations and model checkpoints for RoBERTa,\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."}
{"doc_id": "2106.09685", "para_id": 3, "text": "Many applications in natural language processing rely on adapt-\ning one large-scale, pre-trained language model to multiple down-\nstream applications. Such adaptation is usually done via ﬁne-tuning,\nwhich updates all the parameters of the pre-trained model. The ma-\njor downside of ﬁne-tuning is that the new model contains as many\nparameters as in the original model. As larger models are trained\nevery few months, this changes from a mere “inconvenience” for\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\n175 billion trainable parameters.1"}
{"doc_id": "2106.09685", "para_id": 4, "text": "Many sought to mitigate this by adapting only some parameters or\nlearning external modules for new tasks. This way, we only need\nto store and load a small number of task-speciﬁc parameters in ad-\ndition to the pre-trained model for each task, greatly boosting the\noperational efﬁciency when deployed. However, existing techniques"}
{"doc_id": "2106.09685", "para_id": 5, "text": "Figure 1: Our reparametriza-\ntion. We only train A and B."}
{"doc_id": "2106.09685", "para_id": 6, "text": "∗Equal contribution.\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, ﬁne-tuning boosts its perfor-\nmance signiﬁcantly as shown in Appendix A."}
{"doc_id": "2106.09685", "para_id": 7, "text": "often introduce inference latency (Houlsby et al., 2019; Rebufﬁet al., 2017) by extending model\ndepth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\nmatch the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality."}
{"doc_id": "2106.09685", "para_id": 8, "text": "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\nLow-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) sufﬁces even\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient."}
{"doc_id": "2106.09685", "para_id": 9, "text": "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\nferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\nmatrices A and B in Figure 1, reducing the storage requirement and task-switching over-\nhead signiﬁcantly."}
{"doc_id": "2106.09685", "para_id": 10, "text": "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\nmuch smaller low-rank matrices."}
{"doc_id": "2106.09685", "para_id": 11, "text": "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\nwhen deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\nconstruction."}
{"doc_id": "2106.09685", "para_id": 12, "text": "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\nas preﬁx-tuning. We provide an example in Appendix E."}
{"doc_id": "2106.09685", "para_id": 13, "text": "Terminologies and Conventions\nWe make frequent references to the Transformer architecture\nand use the conventional terminologies for its dimensions.\nWe call the input and output di-\nmension size of a Transformer layer dmodel.\nWe use Wq, Wk, Wv, and Wo to refer to the\nquery/key/value/output projection matrices in the self-attention module. W or W0 refers to a pre-\ntrained weight matrix and ∆W its accumulated gradient update during adaptation. We use r to\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\noptimization and use a Transformer MLP feedforward dimension dffn = 4 × dmodel."}
{"doc_id": "2106.09685", "para_id": 14, "text": "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\ning use case. Below is a brief description of the language modeling problem and, in particular, the\nmaximization of conditional probabilities given a task-speciﬁc prompt."}
{"doc_id": "2106.09685", "para_id": 15, "text": "Suppose we are given a pre-trained autoregressive language model PΦ(y|x) parametrized by Φ.\nFor instance, PΦ(y|x) can be a generic multi-task learner such as GPT (Radford et al., b; Brown\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\nrepresented by a training dataset of context-target pairs: Z = {(xi, yi)}i=1,..,N, where both xi and\nyi are sequences of tokens. For example, in NL2SQL, xi is a natural language query and yi its\ncorresponding SQL command; for summarization, xi is the content of an article and yi its summary."}
{"doc_id": "2106.09685", "para_id": 16, "text": "During full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\nby repeatedly following the gradient to maximize the conditional language modeling objective:"}
{"doc_id": "2106.09685", "para_id": 17, "text": "One of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different\nset of parameters ∆Φ whose dimension |∆Φ| equals |Φ0|. Thus, if the pre-trained model is large\n(such as GPT-3 with |Φ0| ≈175 Billion), storing and deploying many independent instances of\nﬁne-tuned models can be challenging, if at all feasible."}
{"doc_id": "2106.09685", "para_id": 18, "text": "In this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter\nincrement ∆Φ = ∆Φ(Θ) is further encoded by a much smaller-sized set of parameters Θ with\n|Θ| ≪|Φ0|. The task of ﬁnding ∆Φ thus becomes optimizing over Θ:"}
{"doc_id": "2106.09685", "para_id": 19, "text": "In the subsequent sections, we propose to use a low-rank representation to encode ∆Φ that is both\ncompute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train-\nable parameters |Θ| can be as small as 0.01% of |Φ0|."}
{"doc_id": "2106.09685", "para_id": 20, "text": "The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\nof works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec-\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\nare two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\net al., 2019; Rebufﬁet al., 2017; Pfeiffer et al., 2021; R¨uckl´e et al., 2020) or optimizing some forms\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\nlatency-sensitive production scenario."}
{"doc_id": "2106.09685", "para_id": 21, "text": "Adapter Layers Introduce Inference Latency\nThere are many variants of adapters. We focus\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\ning multi-task settings (R¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\na difference in the online inference setting where the batch size is typically as small as one. In a\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\nvery small bottleneck dimension (Table 1)."}
{"doc_id": "2106.09685", "para_id": 22, "text": "This problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\nAllReduce and Broadcast, unless we store the adapter parameters redundantly many times."}
{"doc_id": "2106.09685", "para_id": 23, "text": "Directly Optimizing the Prompt is Hard\nThe other direction, as exempliﬁed by preﬁx tuning (Li\n& Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize\nand that its performance changes non-monotonically in trainable parameters, conﬁrming similar\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\nadaptation necessarily reduces the sequence length available to process a downstream task, which\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\non task performance to Section 5."}
{"doc_id": "2106.09685", "para_id": 24, "text": "Batch Size\n32\n16\n1\nSequence Length\n512\n256\n128\n|Θ|\n0.5M\n11M\n11M"}
{"doc_id": "2106.09685", "para_id": 25, "text": "AdapterL\n1482.0±1.0 (+2.2%)\n354.8±0.5 (+5.0%)\n23.9±2.1 (+20.7%)\nAdapterH\n1492.2±1.0 (+3.0%)\n366.3±0.5 (+8.4%)\n25.8±2.2 (+30.3%)"}
{"doc_id": "2106.09685", "para_id": 26, "text": "Table 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. “|Θ|” denotes the number of trainable\nparameters in adapter layers. AdapterL and AdapterH are two variants of adapter tuning, which we\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an\nonline, short-sequence-length scenario. See the full study in Appendix B."}
{"doc_id": "2106.09685", "para_id": 27, "text": "We describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\nlanguage models in our experiments as the motivating use case."}
{"doc_id": "2106.09685", "para_id": 28, "text": "A neural network contains many dense layers which perform matrix multiplication. The weight\nmatrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\n(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\nlearn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\nsize the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\nweight matrix W0 ∈Rd×k, we constrain its update by representing the latter with a low-rank de-\ncomposition W0 + ∆W = W0 + BA, where B ∈Rd×r, A ∈Rr×k, and the rank r ≪min(d, k).\nDuring training, W0 is frozen and does not receive gradient updates, while A and B contain trainable\nparameters. Note both W0 and ∆W = BA are multiplied with the same input, and their respective\noutput vectors are summed coordinate-wise. For h = W0x, our modiﬁed forward pass yields:"}
{"doc_id": "2106.09685", "para_id": 29, "text": "We illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for A and\nzero for B, so ∆W = BA is zero at the beginning of training. We then scale ∆Wx by α"}
{"doc_id": "2106.09685", "para_id": 30, "text": "r , where α\nis a constant in r. When optimizing with Adam, tuning α is roughly the same as tuning the learning\nrate if we scale the initialization appropriately. As a result, we simply set α to the ﬁrst r we try\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\nr (Yang & Hu, 2021)."}
{"doc_id": "2106.09685", "para_id": 31, "text": "A Generalization of Full Fine-tuning.\nA more general form of ﬁne-tuning allows the training of\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\nness of full ﬁne-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In\nother words, as we increase the number of trainable parameters 3, training LoRA roughly converges\nto training the original model, while adapter-based methods converges to an MLP and preﬁx-based\nmethods to a model that cannot take long input sequences."}
{"doc_id": "2106.09685", "para_id": 32, "text": "No Additional Inference Latency.\nWhen deployed in production, we can explicitly compute and\nstore W = W0 + BA and perform inference as usual. Note that both W0 and BA are in Rd×k.\nWhen we need to switch to another downstream task, we can recover W0 by subtracting BA and\nthen adding a different B′A′, a quick operation with very little memory overhead. Critically, this"}
{"doc_id": "2106.09685", "para_id": 33, "text": "2They represent a negligible number of parameters compared to weights.\n3An inevitability when adapting to hard tasks."}
{"doc_id": "2106.09685", "para_id": 34, "text": "guarantees that we do not introduce any additional latency during inference compared to a ﬁne-tuned\nmodel by construction."}
{"doc_id": "2106.09685", "para_id": 35, "text": "In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\nthe self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv)\nas a single matrix of dimension dmodel ×dmodel, even though the output dimension is usually sliced\ninto attention heads. We limit our study to only adapting the attention weights for downstream\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\nand parameter-efﬁciency.We further study the effect on adapting different types of attention weight\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\nlayers, LayerNorm layers, and biases to a future work."}
{"doc_id": "2106.09685", "para_id": 36, "text": "Practical Beneﬁts and Limitations.\nThe most signiﬁcant beneﬁt comes from the reduction in\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\nusage by up to 2/3 if r ≪dmodel as we do not need to store the optimizer states for the frozen\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\n350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint\nsize is reduced by roughly 10,000× (from 350GB to 35MB)4. This allows us to train with signiﬁ-\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneﬁt is that we can switch between tasks\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\nparameters. This allows for the creation of many customized models that can be swapped in and out\non the ﬂy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\nduring training on GPT-3 175B compared to full ﬁne-tuning5 as we do not need to calculate the\ngradient for the vast majority of the parameters."}
{"doc_id": "2106.09685", "para_id": 37, "text": "LoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\nwith different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical."}
{"doc_id": "2106.09685", "para_id": 38, "text": "We evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\n(NLU) to generation (NLG). Speciﬁcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments."}
{"doc_id": "2106.09685", "para_id": 39, "text": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their\nreported numbers whenever possible. This, however, means that some baselines might only appear\nin certain experiments."}
{"doc_id": "2106.09685", "para_id": 40, "text": "Fine-Tuning (FT) is a common approach for adaptation. During ﬁne-tuning, the model is initialized\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\nvariant is to update only some layers while freezing others. We include one such baseline reported\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers (FTTop2)."}
{"doc_id": "2106.09685", "para_id": 41, "text": "4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\n350GB + 35MB * 100 ≈354GB as opposed to 100 * 350GB ≈35TB.\n5For GPT-3 175B, the training throughput for full ﬁne-tuning is 32.5 tokens/s per V100 GPU; with the same\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA."}
{"doc_id": "2106.09685", "para_id": 42, "text": "Parameters MNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B Avg."}
{"doc_id": "2106.09685", "para_id": 43, "text": "RoBbase (FT)*\n125.0M\n87.6\n94.8\n90.2\n63.6\n92.8\n91.9\n78.7\n91.2\n86.4\nRoBbase (BitFit)*\n0.1M\n84.7\n93.7\n92.7\n62.0\n91.8\n84.0\n81.5\n90.8\n85.2\nRoBbase (AdptD)*\n0.3M 87.1±.0 94.2±.1 88.5±1.1 60.8±.4 93.1±.1 90.2±.0 71.5±2.7 89.7±.3 84.4\nRoBbase (AdptD)*\n0.9M 87.3±.1 94.7±.3 88.4±.1\n62.6±.9 93.0±.2 90.6±.0 75.9±2.2 90.3±.1 85.4\nRoBbase (LoRA)\n0.3M 87.5±.3 95.1±.2 89.7±.7 63.4±1.2 93.3±.3 90.8±.1 86.6±.7\n91.5±.2 87.2"}
{"doc_id": "2106.09685", "para_id": 44, "text": "RoBlarge (FT)*\n355.0M\n90.2\n96.4\n90.9\n68.0\n94.7\n92.2\n86.6\n92.4\n88.9\nRoBlarge (LoRA)\n0.8M 90.6±.2 96.2±.5 90.9±1.2 68.2±1.9 94.9±.3 91.6±.1 87.4±2.5 92.6±.2 89.0"}
{"doc_id": "2106.09685", "para_id": 45, "text": "RoBlarge (AdptP)†\n3.0M 90.2±.3 96.1±.3 90.2±.7 68.3±1.0 94.8±.2 91.9±.1 83.8±2.9 92.1±.7 88.4\nRoBlarge (AdptP)†\n0.8M 90.5±.3 96.6±.2 89.7±1.2 67.8±2.5 94.8±.3 91.7±.2 80.1±2.9 91.9±.4 87.9\nRoBlarge (AdptH)†\n6.0M 89.9±.5 96.2±.3 88.7±2.9 66.5±4.4 94.7±.2 92.1±.1 83.4±1.1 91.0±1.7 87.8\nRoBlarge (AdptH)†\n0.8M 90.3±.3 96.3±.5 87.7±1.7 66.3±2.0 94.7±.2 91.5±.1 72.9±2.9 91.5±.5 86.4\nRoBlarge (LoRA)†\n0.8M 90.6±.2 96.2±.5 90.2±1.0 68.2±1.9 94.8±.3 91.6±.2 85.2±1.1 92.3±.5 88.6"}
{"doc_id": "2106.09685", "para_id": 46, "text": "DeBXXL (FT)*\n1500.0M\n91.8\n97.2\n92.0\n72.0\n96.0\n92.7\n93.9\n92.9\n91.1\nDeBXXL (LoRA)\n4.7M 91.9±.2 96.9±.2 92.6±.6 72.4±1.1 96.0±.1 92.9±.1 94.9±.4\n93.0±.2 91.3"}
{"doc_id": "2106.09685", "para_id": 47, "text": "Table 2: RoBERTabase, RoBERTalarge, and DeBERTaXXL with different adaptation methods on the\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew’s\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\nfor all metrics. * indicates numbers published in prior works. † indicates runs conﬁgured in a setup\nsimilar to Houlsby et al. (2019) for a fair comparison."}
{"doc_id": "2106.09685", "para_id": 48, "text": "Bias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021)."}
{"doc_id": "2106.09685", "para_id": 49, "text": "Preﬁx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\ncial tokens have trainable word embeddings and are generally not in the model’s vocabulary. Where\nto place such tokens can have an impact on performance. We focus on “preﬁxing”, which prepends\nsuch tokens to the prompt, and “inﬁxing”, which appends to the prompt; both are discussed in Li &\nLiang (2021). We use lp (resp. li) denote the number of preﬁx (resp. inﬁx) tokens. The number of\ntrainable parameters is |Θ| = dmodel × (lp + li)."}
{"doc_id": "2106.09685", "para_id": 50, "text": "Preﬁx-layer tuning (PreLayer) is an extension to preﬁx-embedding tuning. Instead of just learning\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\n|Θ| = L × dmodel × (lp + li), where L is the number of Transformer layers."}
{"doc_id": "2106.09685", "para_id": 51, "text": "Adapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\nattention module (and the MLP module) and the subsequent residual connection. There are two\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efﬁcient design with the\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\ninclude another baseline call AdapterDrop (R¨uckl´e et al., 2020) which drops some adapter layers for\ngreater efﬁciency (AdapterD). We cite numbers from prior works whenever possible to maximize\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the ﬁrst column.\nIn all cases, we have |Θ| = ˆLAdpt ×(2×dmodel ×r+r+dmodel)+2× ˆLLN ×dmodel where ˆLAdpt\nis the number of adapter layers and ˆLLN the number of trainable LayerNorms (e.g., in AdapterL)."}
{"doc_id": "2106.09685", "para_id": 52, "text": "LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in Section 4.2, we only apply LoRA to Wq and Wv in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank r and the shape of the original weights:\n|Θ| = 2 × ˆLLoRA × dmodel × r, where ˆLLoRA is the number of weight matrices we apply LoRA to."}
{"doc_id": "2106.09685", "para_id": 53, "text": "Model & Method\n# Trainable\nE2E NLG Challenge\nParameters\nBLEU\nNIST\nMET\nROUGE-L\nCIDEr"}
{"doc_id": "2106.09685", "para_id": 54, "text": "GPT-2 M (FT)*\n354.92M\n68.2\n8.62\n46.2\n71.0\n2.47\nGPT-2 M (AdapterL)*\n0.37M\n66.3\n8.41\n45.0\n69.8\n2.40\nGPT-2 M (AdapterL)*\n11.09M\n68.9\n8.71\n46.1\n71.3\n2.47\nGPT-2 M (AdapterH)\n11.09M\n67.3±.6\n8.50±.07\n46.0±.2\n70.7±.2\n2.44±.01\nGPT-2 M (FTTop2)*\n25.19M\n68.1\n8.59\n46.0\n70.8\n2.41\nGPT-2 M (PreLayer)*\n0.35M\n69.7\n8.81\n46.1\n71.4\n2.49\nGPT-2 M (LoRA)\n0.35M\n70.4±.1\n8.85±.02\n46.8±.2\n71.8±.1\n2.53±.02\nGPT-2 L (FT)*\n774.03M\n68.5\n8.78\n46.0\n69.9\n2.45\nGPT-2 L (AdapterL)\n0.88M\n69.1±.1\n8.68±.03\n46.3±.0\n71.4±.2\n2.49±.0\nGPT-2 L (AdapterL)\n23.00M\n68.9±.3\n8.70±.04\n46.1±.1\n71.3±.2\n2.45±.02\nGPT-2 L (PreLayer)*\n0.77M\n70.3\n8.85\n46.2\n71.7\n2.47\nGPT-2 L (LoRA)\n0.77M\n70.4±.1\n8.89±.02\n46.8±.2\n72.0±.2\n2.47±.02"}
{"doc_id": "2106.09685", "para_id": 55, "text": "Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\nor fewer trainable parameters. Conﬁdence intervals are shown for experiments we ran. * indicates\nnumbers published in prior works."}
{"doc_id": "2106.09685", "para_id": 56, "text": "RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\net al., 2019a) and boosted the latter’s task performance without introducing many more trainable\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\nand evaluate the performance of different efﬁcient adaptation approaches on tasks from the GLUE\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the ﬁne-tuning baseline. Runs\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 57, "text": "DeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\nﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\nSee Section D.2 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 58, "text": "Having shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\ninclude a list of the hyperparameters used in Section D.3."}
{"doc_id": "2106.09685", "para_id": 59, "text": "Model&Method\n# Trainable\nWikiSQL\nMNLI-m\nSAMSum\nParameters\nAcc. (%)\nAcc. (%)\nR1/R2/RL"}
{"doc_id": "2106.09685", "para_id": 60, "text": "GPT-3 (FT)\n175,255.8M\n73.8\n89.5\n52.0/28.0/44.5\nGPT-3 (BitFit)\n14.2M\n71.3\n91.0\n51.3/27.4/43.5\nGPT-3 (PreEmbed)\n3.2M\n63.1\n88.6\n48.3/24.2/40.5\nGPT-3 (PreLayer)\n20.2M\n70.1\n89.5\n50.8/27.3/43.5\nGPT-3 (AdapterH)\n7.1M\n71.9\n89.8\n53.0/28.9/44.8\nGPT-3 (AdapterH)\n40.1M\n73.2\n91.5\n53.2/29.0/45.1"}
{"doc_id": "2106.09685", "para_id": 61, "text": "GPT-3 (LoRA)\n4.7M\n73.4\n91.7\n53.8/29.8/45.9\nGPT-3 (LoRA)\n37.7M\n74.0\n91.6\n53.4/29.2/45.1"}
{"doc_id": "2106.09685", "para_id": 62, "text": "Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\nSAMSum. LoRA performs better than prior approaches, including full ﬁne-tuning. The results\non WikiSQL have a ﬂuctuation around ±0.5%, MNLI-m around ±0.1%, and SAMSum around\n±0.2/±0.2/±0.1 for the three metrics."}
{"doc_id": "2106.09685", "para_id": 63, "text": "As a ﬁnal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 64, "text": "As shown in Table 4, LoRA matches or exceeds the ﬁne-tuning baseline on all three datasets. Note\nthat not all methods beneﬁt monotonically from having more trainable parameters, as shown in Fig-\nure 2. We observe a signiﬁcant performance drop when we use more than 256 special tokens for\npreﬁx-embedding tuning or more than 32 special tokens for preﬁx-layer tuning. This corroborates\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\nperformance of different adaptation approaches in the low-data regime in Section F.3."}
{"doc_id": "2106.09685", "para_id": 65, "text": "Figure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\nSee Section F.2 for more details on the plotted data points."}
{"doc_id": "2106.09685", "para_id": 66, "text": "Transformer Language Models.\nTransformer (Vaswani et al., 2017) is a sequence-to-sequence\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) – both are large Transformer lan-"}
{"doc_id": "2106.09685", "para_id": 67, "text": "guage models trained on a large amount of text – where ﬁne-tuning on task-speciﬁc data after pre-\ntraining on general domain data provides a signiﬁcant performance gain compared to training on\ntask-speciﬁc data directly. Training larger Transformers generally results in better performance and\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\nlanguage model trained to-date with 175B parameters."}
{"doc_id": "2106.09685", "para_id": 68, "text": "Prompt Engineering and Fine-Tuning.\nWhile GPT-3 175B can adapt its behavior with just a\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\nmodel’s performance on a desired task, which is known as prompt engineering or prompt hacking.\nFine-tuning retrains a model pre-trained on general domains to a speciﬁc task Devlin et al. (2019b);\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform ﬁne-tuning in\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\nhas the same memory footprint as pre-training."}
{"doc_id": "2106.09685", "para_id": 69, "text": "Parameter-Efﬁcient Adaptation.\nMany have proposed inserting adapter layers between existing\nlayers in a neural network (Houlsby et al., 2019; Rebufﬁet al., 2017; Lin et al., 2020). Our method\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\nkey functional difference is that our learned weights can be merged with the main weights during\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\nimprove its parameter efﬁciency, which we leave to future work. More recently, many proposed\noptimizing the input word embeddings in lieu of ﬁne-tuning, akin to a continuous and differentiable\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\ntake up available sequence length for task tokens when positional embeddings are learned."}
{"doc_id": "2106.09685", "para_id": 70, "text": "Low-Rank Structures in Deep Learning.\nLow-rank structure is very common in machine learn-\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\nconsiders low-rank update to a frozen model for adaptation to downstream tasks. In theory liter-\nature, it is known that neural networks outperform other classical learning methods, including the\ncorresponding (ﬁnite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\nlow-rank adaptation update is well-motivated by the literature."}
{"doc_id": "2106.09685", "para_id": 71, "text": "Given the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\n(up to 10,000×) without adversely affecting task performances."}
{"doc_id": "2106.09685", "para_id": 72, "text": "We perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt"}
{"doc_id": "2106.09685", "para_id": 73, "text": "to maximize downstream performance? 2) Is the “optimal” adaptation matrix ∆W really rank-\ndeﬁcient? If so, what is a good rank to use in practice? 3) What is the connection between ∆W and\nW? Does ∆W highly correlate with W? How large is ∆W comparing to W?"}
{"doc_id": "2106.09685", "para_id": 74, "text": "We believe that our answers to question (2) and (3) shed light on the fundamental principles of using\npre-trained language models for downstream tasks, which is a critical topic in NLP."}
{"doc_id": "2106.09685", "para_id": 75, "text": "7.1\nWHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?"}
{"doc_id": "2106.09685", "para_id": 76, "text": "Given a limited parameter budget, which types of weights should we adapt with LoRA to obtain\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\nin FP16) on GPT-3 175B, which corresponds to r = 8 if we adapt one type of attention weights or\nr = 4 if we adapt two types, for all 96 layers. The result is presented in Table 5."}
{"doc_id": "2106.09685", "para_id": 77, "text": "Weight Type\nWq\nWk\nWv\nWo\nWq, Wk\nWq, Wv\nWq, Wk, Wv, Wo\nRank r\n8\n8\n8\n8\n4\n4\n2"}
{"doc_id": "2106.09685", "para_id": 78, "text": "WikiSQL (±0.5%)\n70.4\n70.0\n73.0\n73.2\n71.4\n73.7\n73.7\nMultiNLI (±0.1%)\n91.0\n90.8\n91.0\n91.3\n91.3\n91.3\n91.7"}
{"doc_id": "2106.09685", "para_id": 79, "text": "Table 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wq and\nWv gives the best performance overall. We ﬁnd the standard deviation across random seeds to be\nconsistent for a given dataset, which we report in the ﬁrst column."}
{"doc_id": "2106.09685", "para_id": 80, "text": "Note that putting all the parameters in ∆Wq or ∆Wk results in signiﬁcantly lower performance,\nwhile adapting both Wq and Wv yields the best result. This suggests that even a rank of four\ncaptures enough information in ∆W such that it is preferable to adapt more weight matrices than\nadapting a single type of weights with a larger rank."}
{"doc_id": "2106.09685", "para_id": 81, "text": "We turn our attention to the effect of rank r on model performance.\nWe adapt {Wq, Wv},\n{Wq, Wk, Wv, Wc}, and just Wq for a comparison."}
{"doc_id": "2106.09685", "para_id": 82, "text": "WikiSQL(±0.5%)\nWq\n68.8\n69.6\n70.5\n70.4\n70.0\nWq, Wv\n73.4\n73.3\n73.7\n73.8\n73.5\nWq, Wk, Wv, Wo\n74.1\n73.7\n74.0\n74.0\n73.9"}
{"doc_id": "2106.09685", "para_id": 83, "text": "Wq\n90.7\n90.9\n91.1\n90.7\n90.7\nWq, Wv\n91.3\n91.4\n91.3\n91.6\n91.4\nWq, Wk, Wv, Wo\n91.2\n91.7\n91.7\n91.5\n91.4"}
{"doc_id": "2106.09685", "para_id": 84, "text": "Table 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\nrank as small as one sufﬁces for adapting both Wq and Wv on these datasets while training Wq alone\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2."}
{"doc_id": "2106.09685", "para_id": 85, "text": "Table 6 shows that, surprisingly, LoRA already performs competitively with a very small r (more\nso for {Wq, Wv} than just Wq). This suggests the update matrix ∆W could have a very small\n“intrinsic rank”.6 To further support this ﬁnding, we check the overlap of the subspaces learned by\ndifferent choices of r and by different random seeds. We argue that increasing r does not cover a\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufﬁcient."}
{"doc_id": "2106.09685", "para_id": 86, "text": "6However, we do not expect a small r to work for every task or dataset. Consider the following thought\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\nthe entire model (similar to LoRA with r = dmodel) could certainly outperform LoRA with a small r."}
{"doc_id": "2106.09685", "para_id": 87, "text": "Subspace similarity between different r.\nGiven Ar=8 and Ar=64 which are the learned adapta-\ntion matrices with rank r = 8 and 64 using the same pre-trained model, we perform singular value\ndecomposition and obtain the right-singular unitary matrices UAr=8 and UAr=64.7 We hope to an-\nswer: how much of the subspace spanned by the top i singular vectors in UAr=8 (for 1 ≤i ≤8) is\ncontained in the subspace spanned by top j singular vectors of UAr=64 (for 1 ≤j ≤64)? We mea-\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\npendix G for a more formal discussion)"}
{"doc_id": "2106.09685", "para_id": 88, "text": "φ(Ar=8, Ar=64, i, j) = ||U i⊤\nAr=8U j\nAr=64||2\nF\nmin(i, j)\n∈[0, 1]\n(4)"}
{"doc_id": "2106.09685", "para_id": 89, "text": "where U i\nAr=8 represents the columns of UAr=8 corresponding to the top-i singular vectors."}
{"doc_id": "2106.09685", "para_id": 90, "text": "φ(·) has a range of [0, 1], where 1 represents a complete overlap of subspaces and 0 a complete\nseparation. See Figure 3 for how φ changes as we vary i and j. We only look at the 48th layer\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\nin Section H.1."}
{"doc_id": "2106.09685", "para_id": 91, "text": "Figure 3: Subspace similarity between column vectors of Ar=8 and Ar=64 for both ∆Wq and ∆Wv.\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top\ndirections in r = 8 are included in r = 64, and vice versa."}
{"doc_id": "2106.09685", "para_id": 92, "text": "Directions corresponding to the top singular vector overlap signiﬁcantly between\nAr=8 and Ar=64, while others do not. Speciﬁcally, ∆Wv (resp. ∆Wq) of Ar=8\nand ∆Wv (resp. ∆Wq) of Ar=64 share a subspace of dimension 1 with normalized\nsimilarity > 0.5, providing an explanation of why r = 1 performs quite well in our\ndownstream tasks for GPT-3."}
{"doc_id": "2106.09685", "para_id": 93, "text": "Since both Ar=8 and Ar=64 are learned using the same pre-trained model, Figure 3 indicates that\nthe top singular-vector directions of Ar=8 and Ar=64 are the most useful, while other directions\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\ncan indeed have a very low rank."}
{"doc_id": "2106.09685", "para_id": 94, "text": "Subspace similarity between different random seeds.\nWe further conﬁrm this by plotting the\nnormalized subspace similarity between two randomly seeded runs with r = 64, shown in Figure 4.\n∆Wq appears to have a higher “intrinsic rank” than ∆Wv, since more common singular value direc-\ntions are learned by both runs for ∆Wq, which is in line with our empirical observation in Table 6.\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\nsingular value directions with each other."}
{"doc_id": "2106.09685", "para_id": 95, "text": "7.3\nHOW DOES THE ADAPTATION MATRIX ∆W COMPARE TO W ?"}
{"doc_id": "2106.09685", "para_id": 96, "text": "We further investigate the relationship between ∆W and W. In particular, does ∆W highly correlate\nwith W? (Or mathematically, is ∆W mostly contained in the top singular directions of W?) Also,"}
{"doc_id": "2106.09685", "para_id": 97, "text": "7Note that a similar analysis can be carried out with B and the left-singular unitary matrices – we stick with\nA for our experiments."}
{"doc_id": "2106.09685", "para_id": 98, "text": "Figure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\nfrom two random seeds, for both ∆Wq and ∆Wv in the 48-th layer. Right: the same heat-map\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers."}
{"doc_id": "2106.09685", "para_id": 99, "text": "how “large” is ∆W comparing to its corresponding directions in W? This can shed light on the\nunderlying mechanism for adapting pre-trained language models."}
{"doc_id": "2106.09685", "para_id": 100, "text": "To answer these questions, we project W onto the r-dimensional subspace of ∆W by comput-\ning U ⊤WV ⊤, with U/V being the left/right singular-vector matrix of ∆W.\nThen, we com-\npare the Frobenius norm between ∥U ⊤WV ⊤∥F and ∥W∥F . As a comparison, we also compute\n∥U ⊤WV ⊤∥F by replacing U, V with the top r singular vectors of W or a random matrix."}
{"doc_id": "2106.09685", "para_id": 101, "text": "Table 7: The Frobenius norm of U ⊤WqV ⊤where U and V are the left/right top r singular vector\ndirections of either (1) ∆Wq, (2) Wq, or (3) a random matrix. The weight matrices are taken from\nthe 48th layer of GPT-3."}
{"doc_id": "2106.09685", "para_id": 102, "text": "We draw several conclusions from Table 7. First, ∆W has a stronger correlation with W compared\nto a random matrix, indicating that ∆W ampliﬁes some features that are already in W. Second,\ninstead of repeating the top singular directions of W, ∆W only ampliﬁes directions that are not\nemphasized in W. Third, the ampliﬁcation factor is rather huge: 21.5 ≈6.91/0.32 for r = 4.\nSee Section H.4 for why r = 64 has a smaller ampliﬁcation factor. We also provide a visualization\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\nThis suggests that the low-rank adaptation matrix potentially ampliﬁes the important features for\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model."}
{"doc_id": "2106.09685", "para_id": 103, "text": "Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required\nand the storage/switching cost for hosting independent instances for different tasks. We propose\nLoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\non Transformer language models, the proposed principles are generally applicable to any neural\nnetworks with dense layers."}
{"doc_id": "2106.09685", "para_id": 104, "text": "There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-"}
{"doc_id": "2106.09685", "para_id": 105, "text": "tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\nthere more principled ways to do it? 4) Finally, the rank-deﬁciency of ∆W suggests that W could\nbe rank-deﬁcient as well, which can also be a source of inspiration for future works."}
{"doc_id": "2106.09685", "para_id": 106, "text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL\nhttp://arxiv.org/abs/2012.13255."}
{"doc_id": "2106.09685", "para_id": 107, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Kernels? In\nNeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337."}
{"doc_id": "2106.09685", "para_id": 108, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\nlearning. arXiv preprint arXiv:2001.04413, 2020a."}
{"doc_id": "2106.09685", "para_id": 109, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust\ndeep learning. arXiv preprint arXiv:2005.10190, 2020b."}
{"doc_id": "2106.09685", "para_id": 110, "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.\n03962."}
{"doc_id": "2106.09685", "para_id": 111, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."}
{"doc_id": "2106.09685", "para_id": 112, "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165."}
{"doc_id": "2106.09685", "para_id": 113, "text": "Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for\nmatrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010."}
{"doc_id": "2106.09685", "para_id": 114, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001."}
{"doc_id": "2106.09685", "para_id": 115, "text": "Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep\nneural networks with multitask learning. In Proceedings of the 25th international conference\non Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\nhttps://doi.org/10.1145/1390156.1390177."}
{"doc_id": "2106.09685", "para_id": 116, "text": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning, 2014."}
{"doc_id": "2106.09685", "para_id": 117, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019a."}
{"doc_id": "2106.09685", "para_id": 118, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b.\nURL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805."}
{"doc_id": "2106.09685", "para_id": 119, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\nhttps://aclanthology.org/I05-5002."}
{"doc_id": "2106.09685", "para_id": 120, "text": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\nNatural Language Generation, pp. 124–133, 2017."}
{"doc_id": "2106.09685", "para_id": 121, "text": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020."}
{"doc_id": "2106.09685", "para_id": 122, "text": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL\nhttp://arxiv.org/abs/1911.12237."}
{"doc_id": "2106.09685", "para_id": 123, "text": "Lars Grasedyck, Daniel Kressner, and Christine Tobler.\nA literature survey of low-rank tensor\napproximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013."}
{"doc_id": "2106.09685", "para_id": 124, "text": "Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\nlearning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.\n1390204."}
{"doc_id": "2106.09685", "para_id": 125, "text": "Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\nReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/\n2101.00121. arXiv: 2101.00121."}
{"doc_id": "2106.09685", "para_id": 126, "text": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention, 2021."}
{"doc_id": "2106.09685", "para_id": 127, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning\nfor NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.\n00751."}
{"doc_id": "2106.09685", "para_id": 128, "text": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions. arXiv preprint arXiv:1405.3866, 2014."}
{"doc_id": "2106.09685", "para_id": 129, "text": "Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol`o Fusi. Initialization and regularization\nof factorized neural layers, 2021."}
{"doc_id": "2106.09685", "para_id": 130, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."}
{"doc_id": "2106.09685", "para_id": 131, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding, 2020."}
{"doc_id": "2106.09685", "para_id": 132, "text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt\nTuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691.\narXiv: 2104.08691."}
{"doc_id": "2106.09685", "para_id": 133, "text": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\nmension of Objective Landscapes.\narXiv:1804.08838 [cs, stat], April 2018a.\nURL http:\n//arxiv.org/abs/1804.08838. arXiv: 1804.08838."}
{"doc_id": "2106.09685", "para_id": 134, "text": "Xiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation.\narXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190."}
{"doc_id": "2106.09685", "para_id": 135, "text": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\ndescent on structured data. In Advances in Neural Information Processing Systems, 2018."}
{"doc_id": "2106.09685", "para_id": 136, "text": "Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\nproximation via alternating minimization. In International Conference on Machine Learning, pp.\n2358–2367. PMLR, 2016."}
{"doc_id": "2106.09685", "para_id": 137, "text": "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\nory, pp. 2–47. PMLR, 2018b."}
{"doc_id": "2106.09685", "para_id": 138, "text": "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\nvia parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2020.ﬁndings-emnlp.41.\nURL https://aclanthology.\norg/2020.findings-emnlp.41."}
{"doc_id": "2106.09685", "para_id": 139, "text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nUnderstands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/\n2103.10385. arXiv: 2103.10385."}
{"doc_id": "2106.09685", "para_id": 140, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019."}
{"doc_id": "2106.09685", "para_id": 141, "text": "Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017."}
{"doc_id": "2106.09685", "para_id": 142, "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019."}
{"doc_id": "2106.09685", "para_id": 143, "text": "Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank\nhypercomplex adapter layers, 2021."}
{"doc_id": "2106.09685", "para_id": 144, "text": "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\ndata record to text generation. arXiv preprint arXiv:2007.02871, 2020."}
{"doc_id": "2106.09685", "para_id": 145, "text": "Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end-\nto-end generation. arXiv preprint arXiv:1706.09254, 2017."}
{"doc_id": "2106.09685", "para_id": 146, "text": "Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.\nGeneralization guaran-\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\narXiv:1906.05392, 2019."}
{"doc_id": "2106.09685", "para_id": 147, "text": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning, 2021."}
{"doc_id": "2106.09685", "para_id": 148, "text": "Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\nInterspeech, pp. 3743–3747, 2018."}
{"doc_id": "2106.09685", "para_id": 149, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\nstanding by Generative Pre-Training. pp. 12, a."}
{"doc_id": "2106.09685", "para_id": 150, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. pp. 24, b."}
{"doc_id": "2106.09685", "para_id": 151, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822."}
{"doc_id": "2106.09685", "para_id": 152, "text": "Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/\nabs/1705.08045. arXiv: 1705.08045."}
{"doc_id": "2106.09685", "para_id": 153, "text": "Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers, 2020."}
{"doc_id": "2106.09685", "para_id": 154, "text": "Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-dimensional output targets.\nIn 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655–\n6659. IEEE, 2013."}
{"doc_id": "2106.09685", "para_id": 155, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism, 2020."}
{"doc_id": "2106.09685", "para_id": 156, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/D13-1170."}
{"doc_id": "2106.09685", "para_id": 157, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\nternational Conference on Neural Information Processing Systems, pp. 6000–6010, 2017."}
{"doc_id": "2106.09685", "para_id": 158, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019."}
{"doc_id": "2106.09685", "para_id": 159, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems, 2020."}
{"doc_id": "2106.09685", "para_id": 160, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "2106.09685", "para_id": 161, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\norg/anthology/N18-1101."}
{"doc_id": "2106.09685", "para_id": 162, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6."}
{"doc_id": "2106.09685", "para_id": 163, "text": "Greg Yang and Edward J. Hu.\nFeature Learning in Inﬁnite-Width Neural Networks.\narXiv:2011.14522 [cond-mat], May 2021. URL http://arxiv.org/abs/2011.14522.\narXiv: 2011.14522."}
{"doc_id": "2106.09685", "para_id": 164, "text": "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models, 2021."}
{"doc_id": "2106.09685", "para_id": 165, "text": "Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck\nfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pp. 185–189. IEEE, 2014."}
{"doc_id": "2106.09685", "para_id": 166, "text": "Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks.\nIn 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 5005–5009. IEEE, 2016."}
{"doc_id": "2106.09685", "para_id": 167, "text": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL http://\narxiv.org/abs/1709.00103."}
{"doc_id": "2106.09685", "para_id": 168, "text": "A\nLARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES"}
{"doc_id": "2106.09685", "para_id": 169, "text": "Few-shot learning, or prompt engineering, is very advantageous when we only have a handful of\ntraining samples. However, in practice, we can often afford to curate a few thousand or more training\nexamples for performance-sensitive applications. As shown in Table 8, ﬁne-tuning improves the\nmodel performance drastically compared to few-shot learning on datasets large and small. We take\nthe GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched,\nwe use two demonstrations per class and six in-context examples in total."}
{"doc_id": "2106.09685", "para_id": 170, "text": "GPT-3 Few-Shot\n40.6\n69.0\nGPT-3 Fine-Tuned\n89.5\n85.4"}
{"doc_id": "2106.09685", "para_id": 171, "text": "Table 8: Fine-tuning signiﬁcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020)."}
{"doc_id": "2106.09685", "para_id": 172, "text": "Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas\nour proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\nadapter layers must be computed in addition to the base model, inevitably introducing additional\nlatency. While as pointed out in R¨uckl´e et al. (2020), the latency introduced by adapter layers can\nbe mitigated when the model batch size and/or sequence length is large enough to full utilize the\nhardware parallelism. We conﬁrm their observation with a similar latency study on GPT-2 medium\nand point out that there are scenarios, notably online inference where the batch size is small, where\nthe added latency can be signiﬁcant."}
{"doc_id": "2106.09685", "para_id": 173, "text": "We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\nover 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\nr. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\nand a recent, more efﬁcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\nfor more details on the designs. We plot the slow-down in percentage compared to the no-adapter\nbaseline in Figure 5."}
{"doc_id": "2106.09685", "para_id": 174, "text": "Figure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline.\nThe top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and\nsequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\nonline, short-sequence-length scenario. We tweak the colormap for better visibility."}
{"doc_id": "2106.09685", "para_id": 175, "text": "GLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\nMNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\n(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\n(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8 (question-answering), RTE (inference),"}
{"doc_id": "2106.09685", "para_id": 176, "text": "8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs"}
{"doc_id": "2106.09685", "para_id": 177, "text": "and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\nstandard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\nare released under different permissive licenses."}
{"doc_id": "2106.09685", "para_id": 178, "text": "WikiSQL is introduced in Zhong et al. (2017) and contains 56, 355/8, 421 training/validation ex-\namples. The task is to generate SQL queries from natural language questions and table schemata.\nWe encode context as x = {table schema, query} and target as y = {SQL}. The dataset is release\nunder the BSD 3-Clause License."}
{"doc_id": "2106.09685", "para_id": 179, "text": "SAMSum is introduced in Gliwa et al. (2019) and contains 14, 732/819 training/test examples. It\nconsists of staged chat conversations between two people and corresponding abstractive summaries\nwritten by linguists. We encode context as ”\\n” concatenated utterances followed by a ”\\n\\n”,\nand target as y = {summary}. The dataset is released under the non-commercial licence: Creative\nCommons BY-NC-ND 4.0."}
{"doc_id": "2106.09685", "para_id": 180, "text": "E2E NLG Challenge was ﬁrst introduced in Novikova et al. (2017) as a dataset for training end-to-\nend, data-driven natural language generation systems and is commonly used for data-to-text evalua-\ntion. The E2E dataset consists of roughly 42, 000 training, 4, 600 validation, and 4, 600 test exam-\nples from the restaurant domain. Each source table used as input can have multiple references. Each\nsample input (x, y) consists of a sequence of slot-value pairs, along with a corresponding natural\nlanguage reference text. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 181, "text": "DART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\nstructured as sequences of ENTITY — RELATION — ENTITY triples. With 82K examples in\ntotal, DART is a signiﬁcantly larger and more complex data-to-text task compared to E2E. The\ndataset is released under the MIT license."}
{"doc_id": "2106.09685", "para_id": 182, "text": "WebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With\n22K examples in total WebNLG comprises 14 distinct categories, nine of which are seen during\ntraining. Since ﬁve of the 14 total categories are not seen during training, but are represented in\nthe test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U)\nand “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY —\nOBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 183, "text": "We train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\nof training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\nmodules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\nusual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\nrandom seeds; the result for each run is taken from the best epoch. For a fair comparison with the\nsetup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\nand used a ﬁxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\nmodel when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\nThe runs with this restricted setup are marked with †. See the hyperparameters used in our runs\nin Table 9."}
{"doc_id": "2106.09685", "para_id": 184, "text": "We again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\nwe tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\nsequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\nwe initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\nSTS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\nthe median over 5 random seeds; the result for each run is taken from the best epoch. See the\nhyperparameters used in our runs in Table 10."}
{"doc_id": "2106.09685", "para_id": 185, "text": "Method\nDataset\nMNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B"}
{"doc_id": "2106.09685", "para_id": 186, "text": "Optimizer\nAdamW\nWarmup Ratio\n0.06\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 187, "text": "Batch Size\n16\n16\n16\n32\n32\n16\n32\n16\n# Epochs\n30\n60\n30\n80\n25\n25\n80\n40\nLearning Rate\n5E-04\n5E-04\n4E-04\n4E-04\n4E-04\n5E-04\n5E-04\n4E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n8\nMax Seq. Len.\n512"}
{"doc_id": "2106.09685", "para_id": 188, "text": "Batch Size\n4\n4\n4\n4\n4\n4\n8\n8\n# Epochs\n10\n10\n20\n20\n10\n20\n20\n30\nLearning Rate\n3E-04\n4E-04\n3E-04\n2E-04\n2E-04\n3E-04\n4E-04\n2E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n16\nMax Seq. Len.\n128\n128\n512\n128\n512\n512\n512\n512"}
{"doc_id": "2106.09685", "para_id": 189, "text": "Batch Size\n4\n# Epochs\n10\n10\n20\n20\n10\n20\n20\n10\nLearning Rate\n3E-04\n4E-04\n3E-04\n2E-04\n2E-04\n3E-04\n4E-04\n2E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n16\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 190, "text": "Batch Size\n32\n# Epochs\n10\n20\n20\n20\n10\n20\n20\n20\nLearning Rate\n3E-05\n3E-05\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n64\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 191, "text": "Batch Size\n32\n# Epochs\n5\n20\n20\n20\n10\n20\n20\n20\nLearning Rate\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n16\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 192, "text": "Batch Size\n32\n# Epochs\n10\n5\n10\n10\n5\n20\n20\n10\nLearning Rate\n3E-05\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n64\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 193, "text": "Batch Size\n32\n# Epochs\n10\n5\n10\n10\n5\n20\n20\n10\nLearning Rate\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n8\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 194, "text": "Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 195, "text": "We train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\nrate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\nin Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\nmean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\nused for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\n(2021)."}
{"doc_id": "2106.09685", "para_id": 196, "text": "For all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\na batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for"}
{"doc_id": "2106.09685", "para_id": 197, "text": "Method\nDataset\nMNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B"}
{"doc_id": "2106.09685", "para_id": 198, "text": "Optimizer\nAdamW\nWarmup Ratio\n0.1\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 199, "text": "Batch Size\n8\n8\n32\n4\n6\n8\n4\n4\n# Epochs\n5\n16\n30\n10\n8\n11\n11\n10\nLearning Rate\n1E-04\n6E-05\n2E-04\n1E-04\n1E-04\n1E-04\n2E-04\n2E-04\nWeight Decay\n0\n0.01\n0.01\n0\n0.01\n0.01\n0.01\n0.1\nCLS Dropout\n0.15\n0\n0\n0.1\n0.1\n0.2\n0.2\n0.2\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n8\nMax Seq. Len.\n256\n128\n128\n64\n512\n320\n320\n128"}
{"doc_id": "2106.09685", "para_id": 200, "text": "Table 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 201, "text": "Optimizer\nAdamW\nWeight Decay\n0.01\n0.01\n0.0\nDropout Prob\n0.1\n0.1\n0.0\nBatch Size\n8\n# Epoch\n5\nWarmup Steps\n500\nLearning Rate Schedule\nLinear\nLabel Smooth\n0.1\n0.1\n0.0\nLearning Rate\n0.0002\nAdaptation\nrq = rv = 4\nLoRA α\n32"}
{"doc_id": "2106.09685", "para_id": 202, "text": "Beam Size\n10\nLength Penalty\n0.9\n0.8\n0.8\nno repeat ngram size\n4"}
{"doc_id": "2106.09685", "para_id": 203, "text": "Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART."}
{"doc_id": "2106.09685", "para_id": 204, "text": "WikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa\net al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\ndetails on the hyperparameters used. For preﬁx-embedding tuning, we ﬁnd the optimal lp and li\nto be 256 and 8, respectively, totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for\npreﬁx-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We\npresent two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8\nor rq = rk = rv = ro = 2). We report the best validation performance from each run. The training\nhyperparameters used in our GPT-3 experiments are listed in Table 12."}
{"doc_id": "2106.09685", "para_id": 205, "text": "LoRA can be naturally combined with existing preﬁx-based approaches. In this section, we evaluate\ntwo combinations of LoRA and variants of preﬁx-tuning on WikiSQL and MNLI."}
{"doc_id": "2106.09685", "para_id": 206, "text": "LoRA+PreﬁxEmbed (LoRA+PE) combines LoRA with preﬁx-embedding tuning, where we insert\nlp + li special tokens whose embeddings are treated as trainable parameters. For more on preﬁx-\nembedding tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 207, "text": "LoRA+PreﬁxLayer (LoRA+PL) combines LoRA with preﬁx-layer tuning. We also insert lp + li\nspecial tokens; however, instead of letting the hidden representations of these tokens evolve natu-"}
{"doc_id": "2106.09685", "para_id": 208, "text": "Hyperparameters\nFine-Tune\nPreEmbed\nPreLayer\nBitFit\nAdapterH\nLoRA"}
{"doc_id": "2106.09685", "para_id": 209, "text": "Optimizer\nAdamW\nBatch Size\n128\n# Epoch\n2\nWarmup Tokens\n250,000\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 210, "text": "Learning Rate\n5.00E-06\n5.00E-04\n1.00E-04\n1.6E-03\n1.00E-04\n2.00E-04"}
{"doc_id": "2106.09685", "para_id": 211, "text": "Table 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\nsame hyperparameters for all datasets after tuning learning rate."}
{"doc_id": "2106.09685", "para_id": 212, "text": "rally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\nmore on preﬁx-layer tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 213, "text": "In Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\nFirst of all, LoRA+PE signiﬁcantly outperforms both LoRA and preﬁx-embedding tuning on\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preﬁx-embedding tuning. On\nMultiNLI, the combination of LoRA+PE doesn’t perform better than LoRA, possibly because LoRA\non its own already achieves performance comparable to the human baseline. Secondly, we notice\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\ntribute this to the fact that preﬁx-layer tuning is very sensitive to the choice of learning rate and thus\nmakes the optimization of LoRA weights more difﬁcult in LoRA+PL."}
{"doc_id": "2106.09685", "para_id": 214, "text": "We also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\npreﬁx-based approaches given the same number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 215, "text": "Method\n# Trainable\nDART\nParameters\nBLEU↑\nMET↑\nTER↓"}
{"doc_id": "2106.09685", "para_id": 216, "text": "GPT-2 Medium\nFine-Tune\n354M\n46.2\n0.39\n0.46\nAdapterL\n0.37M\n42.4\n0.36\n0.48\nAdapterL\n11M\n45.2\n0.38\n0.46\nFTTop2\n24M\n41.0\n0.34\n0.56\nPrefLayer\n0.35M\n46.4\n0.38\n0.46\nLoRA\n0.35M\n47.1±.2\n0.39\n0.46"}
{"doc_id": "2106.09685", "para_id": 217, "text": "GPT-2 Large\nFine-Tune\n774M\n47.0\n0.39\n0.46\nAdapterL\n0.88M\n45.7±.1\n0.38\n0.46\nAdapterL\n23M\n47.1±.1\n0.39\n0.45\nPrefLayer\n0.77M\n46.7\n0.38\n0.45\nLoRA\n0.77M\n47.5±.1\n0.39\n0.45"}
{"doc_id": "2106.09685", "para_id": 218, "text": "Table 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\nless than 0.01 for all adaption approaches."}
{"doc_id": "2106.09685", "para_id": 219, "text": "GPT-2 Medium\nFine-Tune (354M)\n27.7\n64.2\n46.5\n.30\n.45\n.38\n.76\n.33\n.53\nAdapterL (0.37M)\n45.1\n54.5\n50.2\n.36\n.39\n.38\n.46\n.40\n.43\nAdapterL (11M)\n48.3\n60.4\n54.9\n.38\n.43\n.41\n.45\n.35\n.39\nFTTop2 (24M)\n18.9\n53.6\n36.0\n.23\n.38\n.31\n.99\n.49\n.72\nPreﬁx (0.35M)\n45.6\n62.9\n55.1\n.38\n.44\n.41\n.49\n.35\n.40\nLoRA (0.35M)\n46.7±.4\n62.1±.2\n55.3±.2\n.38\n.44\n.41\n.46\n.33\n.39"}
{"doc_id": "2106.09685", "para_id": 220, "text": "GPT-2 Large\nFine-Tune (774M)\n43.1\n65.3\n55.5\n.38\n.46\n.42\n.53\n.33\n.42\nAdapterL (0.88M)\n49.8±.0\n61.1±.0\n56.0±.0\n.38\n.43\n.41\n.44\n.35\n.39\nAdapterL (23M)\n49.2±.1\n64.7±.2\n57.7±.1\n.39\n.46\n.43\n.46\n.33\n.39\nPreﬁx (0.77M)\n47.7\n63.4\n56.3\n.39\n.45\n.42\n.48\n.34\n.40\nLoRA (0.77M)\n48.4±.3\n64.0±.3\n57.0±.1\n.39\n.45\n.42\n.45\n.32\n.38"}
{"doc_id": "2106.09685", "para_id": 221, "text": "Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\nare less than 0.01 for all the experiments we ran. “U” indicates unseen categories, “S” indicates seen\ncategories, and “A” indicates all categories in the test set of WebNLG."}
{"doc_id": "2106.09685", "para_id": 222, "text": "We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\nidentifying the trade-off between performance and the number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 223, "text": "To evaluate the performance of different adaptation approaches in the low-data regime. we randomly\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\nMNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\nn. To our surprise, PreﬁxEmbed and PreﬁxLayer performs very poorly on MNLI-100 dataset, with\nPreﬁxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreﬁxLayer\nperforms better than PreﬁxEmbed but is still signiﬁcantly worse than Fine-Tune or LoRA on MNLI-\n100. The gap between preﬁx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\ncrease the number of training examples, which might suggest that preﬁx-based approaches are not\nsuitable for low-data tasks in GPT-3. LoRA achieves better performance than ﬁne-tuning on both\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\n(±0.3) variance due to random seeds."}
{"doc_id": "2106.09685", "para_id": 224, "text": "The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\nble 17. We use a smaller learning rate for PreﬁxLayer on the MNLI-100 set, as the training loss does\nnot decrease with a larger learning rate."}
{"doc_id": "2106.09685", "para_id": 225, "text": "In this paper we use the measure φ(A, B, i, j) = ψ(U i\nA, U j\nB) = ∥U i⊤\nA UB∥2\nF\nmin{i,j}\nto measure the subspace"}
{"doc_id": "2106.09685", "para_id": 226, "text": "similarity between two column orthonormal matrices U i\nA ∈Rd×i and U j\nB ∈Rd×j, obtained by\ntaking columns of the left singular matrices of A and B. We point out that this similarity is simply\na reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\n(2008)."}
{"doc_id": "2106.09685", "para_id": 227, "text": "Method\nHyperparameters\n# Trainable Parameters\nWikiSQL\nMNLI-m"}
{"doc_id": "2106.09685", "para_id": 228, "text": "lp = 32, li = 8\n0.4 M\n55.9\n84.9\nlp = 64, li = 8\n0.9 M\n58.7\n88.1\nlp = 128, li = 8\n1.7 M\n60.6\n88.0\nlp = 256, li = 8\n3.2 M\n63.1\n88.6\nlp = 512, li = 8\n6.4 M\n55.9\n85.8"}
{"doc_id": "2106.09685", "para_id": 229, "text": "lp = 2, li = 2\n5.1 M\n68.5\n89.2\nlp = 8, li = 0\n10.1 M\n69.8\n88.2\nlp = 8, li = 8\n20.2 M\n70.1\n89.5\nlp = 32, li = 4\n44.1 M\n66.4\n89.6\nlp = 64, li = 0\n76.1 M\n64.9\n87.9"}
{"doc_id": "2106.09685", "para_id": 230, "text": "r = 1\n7.1 M\n71.9\n89.8\nr = 4\n21.2 M\n73.2\n91.0\nr = 8\n40.1 M\n73.2\n91.5\nr = 16\n77.9 M\n73.2\n91.5\nr = 64\n304.4 M\n72.6\n91.5"}
{"doc_id": "2106.09685", "para_id": 231, "text": "rv = 2\n4.7 M\n73.4\n91.7\nrq = rv = 1\n4.7 M\n73.4\n91.3\nrq = rv = 2\n9.4 M\n73.3\n91.4\nrq = rk = rv = ro = 1\n9.4 M\n74.1\n91.2\nrq = rv = 4\n18.8 M\n73.7\n91.3\nrq = rk = rv = ro = 2\n18.8 M\n73.7\n91.7\nrq = rv = 8\n37.7 M\n73.8\n91.6\nrq = rk = rv = ro = 4\n37.7 M\n74.0\n91.7\nrq = rv = 64\n301.9 M\n73.6\n91.4\nrq = rk = rv = ro = 64\n603.8 M\n73.9\n91.4"}
{"doc_id": "2106.09685", "para_id": 232, "text": "rq = rv = 8, lp = 8, li = 4\n37.8 M\n75.0\n91.4\nrq = rv = 32, lp = 8, li = 4\n151.1 M\n75.9\n91.1\nrq = rv = 64, lp = 8, li = 4\n302.1 M\n76.2\n91.3"}
{"doc_id": "2106.09685", "para_id": 233, "text": "LoRA+PL\nrq = rv = 8, lp = 8, li = 4\n52.8 M\n72.9\n90.2"}
{"doc_id": "2106.09685", "para_id": 234, "text": "Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\npreﬁx-embedding tuning (PreﬁxEmbed) and preﬁx-layer tuning (PreﬁxLayer) perform worse as we\nincrease the number of trainable parameters, while LoRA’s performance stabilizes. Performance is\nmeasured in validation accuracy."}
{"doc_id": "2106.09685", "para_id": 235, "text": "Method\nMNLI(m)-100\nMNLI(m)-1k\nMNLI(m)-10k\nMNLI(m)-392K"}
{"doc_id": "2106.09685", "para_id": 236, "text": "GPT-3 (Fine-Tune)\n60.2\n85.8\n88.9\n89.5\nGPT-3 (PreﬁxEmbed)\n37.6\n75.2\n79.5\n88.6\nGPT-3 (PreﬁxLayer)\n48.3\n82.5\n85.9\n89.6\nGPT-3 (LoRA)\n63.8\n85.6\n89.2\n91.7"}
{"doc_id": "2106.09685", "para_id": 237, "text": "Table 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\nn describes a subset with n training examples. We evaluate with the full validation set. LoRA\nperforms exhibits favorable sample-efﬁciency compared to other methods, including ﬁne-tuning."}
{"doc_id": "2106.09685", "para_id": 238, "text": "To be concrete, let the singular values of U i⊤\nA U j\nB to be σ1, σ2, · · · , σp where p = min{i, j}. We\nknow that the Projection Metric Ham & Lee (2008) is deﬁned as:"}
{"doc_id": "2106.09685", "para_id": 239, "text": "Hyperparameters\nAdaptation\nMNLI-100\nMNLI-1k\nMNLI-10K\nMNLI-392K"}
{"doc_id": "2106.09685", "para_id": 240, "text": "Optimizer\n-\nAdamW\nWarmup Tokens\n-\n250,000\nLR Schedule\n-\nLinear\nBatch Size\n-\n20\n20\n100\n128\n# Epoch\n-\n40\n40\n4\n2"}
{"doc_id": "2106.09685", "para_id": 241, "text": "FineTune\n5.00E-6\nPreﬁxEmbed\n2.00E-04\n2.00E-04\n4.00E-04\n5.00E-04\nPreﬁxLayer\n5.00E-05\n5.00E-05\n5.00E-05\n1.00E-04\nLoRA\n2.00E-4"}
{"doc_id": "2106.09685", "para_id": 242, "text": "PreﬁxEmbed lp\n16\n32\n64\n256\nAdaptation-\nPreﬁxEmbed li\n8\nSpeciﬁc\nPreﬁxTune\nlp = li = 8\nLoRA\nrq = rv = 8"}
{"doc_id": "2106.09685", "para_id": 243, "text": "Table 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n."}
{"doc_id": "2106.09685", "para_id": 244, "text": "φ(A, B, i, j) = ψ(U i\nA, U j\nB) =\nPp\ni=1 σ2\ni\np\n= 1"}
{"doc_id": "2106.09685", "para_id": 245, "text": "This similarity satisﬁes that if U i\nA and U j\nB share the same column span, then φ(A, B, i, j) = 1. If\nthey are completely orthogonal, then φ(A, B, i, j) = 0. Otherwise, φ(A, B, i, j) ∈(0, 1)."}
{"doc_id": "2106.09685", "para_id": 246, "text": "We present additional results from our investigation into the low-rank update matrices."}
{"doc_id": "2106.09685", "para_id": 247, "text": "See Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\nlayers."}
{"doc_id": "2106.09685", "para_id": 248, "text": "We repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge\ndataset as an example, we report the validation loss and test metrics achieved by different choices\nof r after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\nNote that the relationship between model size and the optimal rank for adaptation is still an open\nquestion."}
{"doc_id": "2106.09685", "para_id": 249, "text": "See Figure 8 for the normalized subspace similarity between W and ∆W with varying r."}
{"doc_id": "2106.09685", "para_id": 250, "text": "Note again that ∆W does not contain the top singular directions of W, since the similarity between\nthe top 4 directions in ∆W and the top-10% of those in W barely exceeds 0.2. This gives evidence\nthat ∆W contains those “task-speciﬁc” directions that are otherwise not emphasized in W."}
{"doc_id": "2106.09685", "para_id": 251, "text": "An interesting next question to answer, is how “strong” do we need to amplify those task-speciﬁc\ndirections, in order for the model adaptation to work well?"}
{"doc_id": "2106.09685", "para_id": 252, "text": "Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both\n∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer."}
{"doc_id": "2106.09685", "para_id": 253, "text": "One can naturally consider a feature ampliﬁcation factor as the ratio\n∥∆W ∥F\n∥U ⊤W V ⊤∥F , where U and V\nare the left- and right-singular matrices of the SVD decomposition of ∆W. (Recall UU ⊤WV ⊤V\ngives the “projection” of W onto the subspace spanned by ∆W.)"}
{"doc_id": "2106.09685", "para_id": 254, "text": "Intuitively, when ∆W mostly contains task-speciﬁc directions, this quantity measures how much of\nthem are ampliﬁed by ∆W. As shown in Section 7.3, for r = 4, this ampliﬁcation factor is as large\nas 20. In other words, there are (generally speaking) four feature directions in each layer (out of the\nentire feature space from the pre-trained model W), that need to be ampliﬁed by a very large factor\n20, in order to achieve our reported accuracy for the downstream speciﬁc task. And, one should\nexpect a very different set of feature directions to be ampliﬁed for each different downstream task."}
{"doc_id": "2106.09685", "para_id": 255, "text": "One may notice, however, for r = 64, this ampliﬁcation factor is only around 2, meaning that\nmost directions learned in ∆W with r = 64 are not being ampliﬁed by much. This should not\nbe surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\nthe “task-speciﬁc directions” (thus for model adaptation) is low. In contrast, those directions in the\nrank-4 version of ∆W (corresponding to r = 4) are ampliﬁed by a much larger factor 20."}
{"doc_id": "2106.09685", "para_id": 256, "text": "Figure 7: Normalized subspace similarity between the column vectors of Ar=64 from two randomly\nseeded runs, for both ∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\nformer."}
{"doc_id": "2106.09685", "para_id": 257, "text": "1\n1.23\n68.72\n8.7215\n0.4565\n0.7052\n2.4329\n2\n1.21\n69.17\n8.7413\n0.4590\n0.7052\n2.4639\n4\n1.18\n70.38\n8.8439\n0.4689\n0.7186\n2.5349\n8\n1.17\n69.57\n8.7457\n0.4636\n0.7196\n2.5196\n16\n1.16\n69.61\n8.7483\n0.4629\n0.7177\n2.4985\n32\n1.16\n69.33\n8.7736\n0.4642\n0.7105\n2.5255\n64\n1.16\n69.24\n8.7174\n0.4651\n0.7180\n2.5070\n128\n1.16\n68.73\n8.6718\n0.4628\n0.7127\n2.5030\n256\n1.16\n68.92\n8.6982\n0.4629\n0.7128\n2.5012\n512\n1.16\n68.78\n8.6857\n0.4637\n0.7128\n2.5025\n1024\n1.17\n69.37\n8.7495\n0.4659\n0.7149\n2.5090"}
{"doc_id": "2106.09685", "para_id": 258, "text": "Table 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\ndifferent rank r using GPT-2 Medium. Unlike on GPT-3 where r = 1 sufﬁces for many tasks, here\nthe performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2\nMedium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\nhyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and\nthus might not be optimal for other choices of r."}
{"doc_id": "2106.09685", "para_id": 259, "text": "Figure 8: Normalized subspace similarity between the singular directions of Wq and those of ∆Wq\nwith varying r and a random baseline. ∆Wq ampliﬁes directions that are important but not empha-\nsized in W. ∆W with a larger r tends to pick up more directions that are already emphasized in\nW."}
