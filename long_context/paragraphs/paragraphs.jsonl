{"doc_id": "1706.03762", "para_id": 0, "text": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works."}
{"doc_id": "1706.03762", "para_id": 1, "text": "Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu"}
{"doc_id": "1706.03762", "para_id": 2, "text": "Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com"}
{"doc_id": "1706.03762", "para_id": 3, "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}
{"doc_id": "1706.03762", "para_id": 4, "text": "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research."}
{"doc_id": "1706.03762", "para_id": 5, "text": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]."}
{"doc_id": "1706.03762", "para_id": 6, "text": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains."}
{"doc_id": "1706.03762", "para_id": 7, "text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network."}
{"doc_id": "1706.03762", "para_id": 8, "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs."}
{"doc_id": "1706.03762", "para_id": 9, "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2."}
{"doc_id": "1706.03762", "para_id": 10, "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]."}
{"doc_id": "1706.03762", "para_id": 11, "text": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]."}
{"doc_id": "1706.03762", "para_id": 12, "text": "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"doc_id": "1706.03762", "para_id": 13, "text": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next."}
{"doc_id": "1706.03762", "para_id": 14, "text": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively."}
{"doc_id": "1706.03762", "para_id": 15, "text": "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512."}
{"doc_id": "1706.03762", "para_id": 16, "text": "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."}
{"doc_id": "1706.03762", "para_id": 17, "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum"}
{"doc_id": "1706.03762", "para_id": 18, "text": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel."}
{"doc_id": "1706.03762", "para_id": 19, "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key."}
{"doc_id": "1706.03762", "para_id": 20, "text": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values."}
{"doc_id": "1706.03762", "para_id": 21, "text": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:"}
{"doc_id": "1706.03762", "para_id": 22, "text": "The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √dk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code."}
{"doc_id": "1706.03762", "para_id": 23, "text": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1 √dk ."}
{"doc_id": "1706.03762", "para_id": 24, "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional"}
{"doc_id": "1706.03762", "para_id": 25, "text": "4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = Pdk i=1 qiki, has mean 0 and variance dk."}
{"doc_id": "1706.03762", "para_id": 26, "text": "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2."}
{"doc_id": "1706.03762", "para_id": 27, "text": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this."}
{"doc_id": "1706.03762", "para_id": 28, "text": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O"}
{"doc_id": "1706.03762", "para_id": 29, "text": "where headi = Attention(QW Q i , KW K i , V W V i )"}
{"doc_id": "1706.03762", "para_id": 30, "text": "Where the projections are parameter matrices W Q i ∈Rdmodel×dk, W K i ∈Rdmodel×dk, W V i ∈Rdmodel×dv and W O ∈Rhdv×dmodel."}
{"doc_id": "1706.03762", "para_id": 31, "text": "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."}
{"doc_id": "1706.03762", "para_id": 32, "text": "The Transformer uses multi-head attention in three different ways:"}
{"doc_id": "1706.03762", "para_id": 33, "text": "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]."}
{"doc_id": "1706.03762", "para_id": 34, "text": "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder."}
{"doc_id": "1706.03762", "para_id": 35, "text": "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2."}
{"doc_id": "1706.03762", "para_id": 36, "text": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."}
{"doc_id": "1706.03762", "para_id": 37, "text": "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048."}
{"doc_id": "1706.03762", "para_id": 38, "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel."}
{"doc_id": "1706.03762", "para_id": 39, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention."}
{"doc_id": "1706.03762", "para_id": 40, "text": "Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r)"}
{"doc_id": "1706.03762", "para_id": 41, "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]."}
{"doc_id": "1706.03762", "para_id": 42, "text": "In this work, we use sine and cosine functions of different frequencies:"}
{"doc_id": "1706.03762", "para_id": 43, "text": "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos."}
{"doc_id": "1706.03762", "para_id": 44, "text": "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training."}
{"doc_id": "1706.03762", "para_id": 45, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata."}
{"doc_id": "1706.03762", "para_id": 46, "text": "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required."}
{"doc_id": "1706.03762", "para_id": 47, "text": "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types."}
{"doc_id": "1706.03762", "para_id": 48, "text": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence"}
{"doc_id": "1706.03762", "para_id": 49, "text": "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work."}
{"doc_id": "1706.03762", "para_id": 50, "text": "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model."}
{"doc_id": "1706.03762", "para_id": 51, "text": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences."}
{"doc_id": "1706.03762", "para_id": 52, "text": "This section describes the training regime for our models."}
{"doc_id": "1706.03762", "para_id": 53, "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."}
{"doc_id": "1706.03762", "para_id": 54, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days)."}
{"doc_id": "1706.03762", "para_id": 55, "text": "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:"}
{"doc_id": "1706.03762", "para_id": 56, "text": "lrate = d−0.5 model · min(step_num−0.5, step_num · warmup_steps−1.5) (3)"}
{"doc_id": "1706.03762", "para_id": 57, "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."}
{"doc_id": "1706.03762", "para_id": 58, "text": "We employ three types of regularization during training:"}
{"doc_id": "1706.03762", "para_id": 59, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost."}
{"doc_id": "1706.03762", "para_id": 60, "text": "EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0 · 1020"}
{"doc_id": "1706.03762", "para_id": 61, "text": "GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021"}
{"doc_id": "1706.03762", "para_id": 62, "text": "ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021"}
{"doc_id": "1706.03762", "para_id": 63, "text": "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1."}
{"doc_id": "1706.03762", "para_id": 64, "text": "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."}
{"doc_id": "1706.03762", "para_id": 65, "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models."}
{"doc_id": "1706.03762", "para_id": 66, "text": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3."}
{"doc_id": "1706.03762", "para_id": 67, "text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]."}
{"doc_id": "1706.03762", "para_id": 68, "text": "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5."}
{"doc_id": "1706.03762", "para_id": 69, "text": "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the"}
{"doc_id": "1706.03762", "para_id": 70, "text": "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."}
{"doc_id": "1706.03762", "para_id": 71, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities."}
{"doc_id": "1706.03762", "para_id": 72, "text": "N dmodel dff h dk dv Pdrop ϵls train PPL BLEU params steps (dev) (dev) ×106"}
{"doc_id": "1706.03762", "para_id": 73, "text": "1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4"}
{"doc_id": "1706.03762", "para_id": 74, "text": "2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90"}
{"doc_id": "1706.03762", "para_id": 75, "text": "0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213"}
{"doc_id": "1706.03762", "para_id": 76, "text": "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3."}
{"doc_id": "1706.03762", "para_id": 77, "text": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"doc_id": "1706.03762", "para_id": 78, "text": "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model."}
{"doc_id": "1706.03762", "para_id": 79, "text": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]."}
{"doc_id": "1706.03762", "para_id": 80, "text": "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 81, "text": "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we"}
{"doc_id": "1706.03762", "para_id": 82, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)"}
{"doc_id": "1706.03762", "para_id": 83, "text": "Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3"}
{"doc_id": "1706.03762", "para_id": 84, "text": "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 85, "text": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]."}
{"doc_id": "1706.03762", "para_id": 86, "text": "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences."}
{"doc_id": "1706.03762", "para_id": 87, "text": "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention."}
{"doc_id": "1706.03762", "para_id": 88, "text": "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."}
{"doc_id": "1706.03762", "para_id": 89, "text": "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours."}
{"doc_id": "1706.03762", "para_id": 90, "text": "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor."}
{"doc_id": "1706.03762", "para_id": 91, "text": "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration."}
{"doc_id": "1706.03762", "para_id": 92, "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016."}
{"doc_id": "1706.03762", "para_id": 93, "text": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014."}
{"doc_id": "1706.03762", "para_id": 94, "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017."}
{"doc_id": "1706.03762", "para_id": 95, "text": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1706.03762", "para_id": 96, "text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014."}
{"doc_id": "1706.03762", "para_id": 97, "text": "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016."}
{"doc_id": "1706.03762", "para_id": 98, "text": "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014."}
{"doc_id": "1706.03762", "para_id": 99, "text": "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016."}
{"doc_id": "1706.03762", "para_id": 100, "text": "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."}
{"doc_id": "1706.03762", "para_id": 101, "text": "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013."}
{"doc_id": "1706.03762", "para_id": 102, "text": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016."}
{"doc_id": "1706.03762", "para_id": 103, "text": "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."}
{"doc_id": "1706.03762", "para_id": 104, "text": "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997."}
{"doc_id": "1706.03762", "para_id": 105, "text": "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009."}
{"doc_id": "1706.03762", "para_id": 106, "text": "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1706.03762", "para_id": 107, "text": "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016."}
{"doc_id": "1706.03762", "para_id": 108, "text": "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016."}
{"doc_id": "1706.03762", "para_id": 109, "text": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."}
{"doc_id": "1706.03762", "para_id": 110, "text": "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017."}
{"doc_id": "1706.03762", "para_id": 111, "text": "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "1706.03762", "para_id": 112, "text": "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017."}
{"doc_id": "1706.03762", "para_id": 113, "text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017."}
{"doc_id": "1706.03762", "para_id": 114, "text": "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015."}
{"doc_id": "1706.03762", "para_id": 115, "text": "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015."}
{"doc_id": "1706.03762", "para_id": 116, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993."}
{"doc_id": "1706.03762", "para_id": 117, "text": "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006."}
{"doc_id": "1706.03762", "para_id": 118, "text": "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016."}
{"doc_id": "1706.03762", "para_id": 119, "text": "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1706.03762", "para_id": 120, "text": "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006."}
{"doc_id": "1706.03762", "para_id": 121, "text": "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016."}
{"doc_id": "1706.03762", "para_id": 122, "text": "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1706.03762", "para_id": 123, "text": "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1706.03762", "para_id": 124, "text": "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014."}
{"doc_id": "1706.03762", "para_id": 125, "text": "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015."}
{"doc_id": "1706.03762", "para_id": 126, "text": "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014."}
{"doc_id": "1706.03762", "para_id": 127, "text": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."}
{"doc_id": "1706.03762", "para_id": 128, "text": "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015."}
{"doc_id": "1706.03762", "para_id": 129, "text": "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016."}
{"doc_id": "1706.03762", "para_id": 130, "text": "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016."}
{"doc_id": "1706.03762", "para_id": 131, "text": "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013."}
{"doc_id": "1706.03762", "para_id": 132, "text": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color."}
{"doc_id": "1706.03762", "para_id": 133, "text": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"doc_id": "1706.03762", "para_id": 134, "text": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks."}
{"doc_id": "1810.04805", "para_id": 0, "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}
{"doc_id": "1810.04805", "para_id": 1, "text": "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com"}
{"doc_id": "1810.04805", "para_id": 2, "text": "There are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and ﬁne-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-speciﬁc architectures that include the pre-trained representations as addi- tional features. The ﬁne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-speciﬁc parameters, and is trained on the downstream tasks by simply ﬁne-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, espe- cially for the ﬁne-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying ﬁne- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions. In this paper, we improve the ﬁne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a “masked lan- guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked"}
{"doc_id": "1810.04805", "para_id": 3, "text": "We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be ﬁne- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- speciﬁc architecture modiﬁcations."}
{"doc_id": "1810.04805", "para_id": 4, "text": "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}
{"doc_id": "1810.04805", "para_id": 5, "text": "Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce ﬁne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016)."}
{"doc_id": "1810.04805", "para_id": 6, "text": "word based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a “next sentence prediction” task that jointly pre- trains text-pair representations. The contributions of our paper are as follows:"}
{"doc_id": "1810.04805", "para_id": 7, "text": "These approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016). ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speciﬁc architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els."}
{"doc_id": "1810.04805", "para_id": 8, "text": "• We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs."}
{"doc_id": "1810.04805", "para_id": 9, "text": "• We show that pre-trained representations reduce the need for many heavily-engineered task- speciﬁc architectures. BERT is the ﬁrst ﬁne- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-speciﬁc architectures."}
{"doc_id": "1810.04805", "para_id": 10, "text": "• BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert."}
{"doc_id": "1810.04805", "para_id": 11, "text": "There is a long history of pre-training general lan- guage representations, and we brieﬂy review the most widely-used approaches in this section."}
{"doc_id": "1810.04805", "para_id": 12, "text": "As with the feature-based approaches, the ﬁrst works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and ﬁne-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-"}
{"doc_id": "1810.04805", "para_id": 13, "text": "Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering signiﬁcant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013)."}
{"doc_id": "1810.04805", "para_id": 14, "text": "Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers)."}
{"doc_id": "1810.04805", "para_id": 15, "text": "ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015)."}
{"doc_id": "1810.04805", "para_id": 16, "text": "mal difference between the pre-trained architec- ture and the ﬁnal downstream architecture."}
{"doc_id": "1810.04805", "para_id": 17, "text": "Model Architecture BERT’s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2"}
{"doc_id": "1810.04805", "para_id": 18, "text": "There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to ﬁne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014)."}
{"doc_id": "1810.04805", "para_id": 19, "text": "In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3"}
{"doc_id": "1810.04805", "para_id": 20, "text": "We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and ﬁne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For ﬁne- tuning, the BERT model is ﬁrst initialized with the pre-trained parameters, and all of the param- eters are ﬁne-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate ﬁne-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its uniﬁed ar- chitecture across different tasks. There is mini-"}
{"doc_id": "1810.04805", "para_id": 21, "text": "We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4"}
{"doc_id": "1810.04805", "para_id": 22, "text": "1https://github.com/tensorﬂow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/ﬁlter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-"}
{"doc_id": "1810.04805", "para_id": 23, "text": "Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨Question, Answer ⟩) in one token sequence. Throughout this work, a “sentence” can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The ﬁrst token of every sequence is always a special clas- siﬁcation token ([CLS]). The ﬁnal hidden state corresponding to this token is used as the ag- gregate sequence representation for classiﬁcation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the ﬁnal hidden vector of the special [CLS] token as C ∈RH, and the ﬁnal hidden vector for the ith input token as Ti ∈RH. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2."}
{"doc_id": "1810.04805", "para_id": 24, "text": "In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the ﬁnal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input. Although this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and ﬁne-tuning, since the [MASK] token does not ap- pear during ﬁne-tuning. To mitigate this, we do not always replace “masked” words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2."}
{"doc_id": "1810.04805", "para_id": 25, "text": "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Speciﬁcally, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneﬁcial to both QA and NLI. 6"}
{"doc_id": "1810.04805", "para_id": 26, "text": "Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1."}
{"doc_id": "1810.04805", "para_id": 27, "text": "Task #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly “see itself”, and the model could trivially predict the target word in a multi-layered context."}
{"doc_id": "1810.04805", "para_id": 28, "text": "5The ﬁnal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without ﬁne-tuning, since it was trained with NSP."}
{"doc_id": "1810.04805", "para_id": 29, "text": "former is often referred to as a “Transformer encoder” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation."}
{"doc_id": "1810.04805", "para_id": 30, "text": "[CLS] he likes play ##ing [SEP] my dog is cute [SEP] Input"}
{"doc_id": "1810.04805", "para_id": 31, "text": "E[CLS] Ehe Elikes Eplay E##ing E[SEP] Emy Edog Eis Ecute E[SEP] Token Embeddings"}
{"doc_id": "1810.04805", "para_id": 32, "text": "EA EB EB EB EB EB EA EA EA EA EA Segment Embeddings"}
{"doc_id": "1810.04805", "para_id": 33, "text": "E0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5 Position Embeddings"}
{"doc_id": "1810.04805", "para_id": 34, "text": "Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings."}
{"doc_id": "1810.04805", "para_id": 35, "text": "(4) a degenerate text-∅pair in text classiﬁcation or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classiﬁcation, such as en- tailment or sentiment analysis. Compared to pre-training, ﬁne-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-speciﬁc details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5."}
{"doc_id": "1810.04805", "para_id": 36, "text": "The NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters."}
{"doc_id": "1810.04805", "para_id": 37, "text": "Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shufﬂed sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences."}
{"doc_id": "1810.04805", "para_id": 38, "text": "In this section, we present BERT ﬁne-tuning re- sults on 11 NLP tasks."}
{"doc_id": "1810.04805", "para_id": 39, "text": "Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences. For each task, we simply plug in the task- speciﬁc inputs and outputs into BERT and ﬁne- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and"}
{"doc_id": "1810.04805", "para_id": 40, "text": "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1. To ﬁne-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the ﬁnal hid- den vector C ∈RH corresponding to the ﬁrst input token ([CLS]) as the aggregate representa- tion. The only new parameters introduced during ﬁne-tuning are classiﬁcation layer weights W ∈ RK×H, where K is the number of labels. We com- pute a standard classiﬁcation loss with C and W, i.e., log(softmax(CW T ))."}
{"doc_id": "1810.04805", "para_id": 41, "text": "7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq."}
{"doc_id": "1810.04805", "para_id": 42, "text": "System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1"}
{"doc_id": "1810.04805", "para_id": 43, "text": "Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The “Average” column is slightly different than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 44, "text": "Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- tor S ∈RH and an end vector E ∈RH during ﬁne-tuning. The probability of word i being the start of the answer span is computed as a dot prod- uct between Ti and S followed by a softmax over all of the words in the paragraph: Pi = eS·Ti P j eS·Tj ."}
{"doc_id": "1810.04805", "para_id": 45, "text": "We use a batch size of 32 and ﬁne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best ﬁne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that ﬁne- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different ﬁne-tuning data shufﬂing and clas- siﬁer layer initialization.9"}
{"doc_id": "1810.04805", "para_id": 46, "text": "Results are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the ofﬁcial GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We ﬁnd that BERTLARGE signiﬁcantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2."}
{"doc_id": "1810.04805", "para_id": 47, "text": "The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is deﬁned as S·Ti + E·Tj, and the maximum scoring span where j ≥i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We ﬁne-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by ﬁrst ﬁne-tuning on TriviaQA (Joshi et al., 2017) befor ﬁne-tuning on SQuAD. Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA ﬁne-"}
{"doc_id": "1810.04805", "para_id": 48, "text": "The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from"}
{"doc_id": "1810.04805", "para_id": 49, "text": "9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard"}
{"doc_id": "1810.04805", "para_id": 50, "text": "11QANet is described in Yu et al. (2018), but the system has improved substantially after publication."}
{"doc_id": "1810.04805", "para_id": 51, "text": "ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0"}
{"doc_id": "1810.04805", "para_id": 52, "text": "Top Leaderboard Systems (Dec 10th, 2018) Human - - 82.3 91.2 #1 Ensemble - nlnet - - 86.0 91.7 #2 Ensemble - QANet - - 84.5 90.5"}
{"doc_id": "1810.04805", "para_id": 53, "text": "Published BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5"}
{"doc_id": "1810.04805", "para_id": 54, "text": "Human (expert)† - 85.0 Human (5 annotations)† - 88.0"}
{"doc_id": "1810.04805", "para_id": 55, "text": "Ours BERTBASE (Single) 80.8 88.5 - - BERTLARGE (Single) 84.1 90.9 - - BERTLARGE (Ensemble) 85.8 91.8 - - BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2"}
{"doc_id": "1810.04805", "para_id": 56, "text": "Table 4: SWAG Dev and Test accuracies. †Human per- formance is measured with 100 samples, as reported in the SWAG paper."}
{"doc_id": "1810.04805", "para_id": 57, "text": "ˆ si,j = maxj≥iS·Ti + E·Tj. We predict a non-null answer when ˆ si,j > snull + τ, where the thresh- old τ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We ﬁne-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system."}
{"doc_id": "1810.04805", "para_id": 58, "text": "Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and ﬁne-tuning seeds."}
{"doc_id": "1810.04805", "para_id": 59, "text": "Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 #2 Single - nlnet - - 74.2 77.1"}
{"doc_id": "1810.04805", "para_id": 60, "text": "Published unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4"}
{"doc_id": "1810.04805", "para_id": 61, "text": "The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices. When ﬁne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-speciﬁc parameters introduced is a vec- tor whose dot product with the [CLS] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer. We ﬁne-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors’ baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%."}
{"doc_id": "1810.04805", "para_id": 62, "text": "Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 63, "text": "tuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12"}
{"doc_id": "1810.04805", "para_id": 64, "text": "The SQuAD 2.0 task extends the SQuAD 1.1 problem deﬁnition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = S·C + E·C to the score of the best non-null span"}
{"doc_id": "1810.04805", "para_id": 65, "text": "In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional"}
{"doc_id": "1810.04805", "para_id": 66, "text": "12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents, that contain at least one of the provided possible answers."}
{"doc_id": "1810.04805", "para_id": 67, "text": "results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer."}
{"doc_id": "1810.04805", "para_id": 68, "text": "Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1)"}
{"doc_id": "1810.04805", "para_id": 69, "text": "BERTBASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6 84.9"}
{"doc_id": "1810.04805", "para_id": 70, "text": "Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. “No NSP” is trained without the next sentence prediction task. “LTR & No NSP” is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. “+ BiLSTM” adds a ran- domly initialized BiLSTM on top of the “LTR + No NSP” model during ﬁne-tuning."}
{"doc_id": "1810.04805", "para_id": 71, "text": "In this section, we explore the effect of model size on ﬁne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of ﬁne-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such signiﬁcant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters. It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the ﬁrst work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufﬁ- ciently pre-trained. Peters et al. (2018b) presented"}
{"doc_id": "1810.04805", "para_id": 72, "text": "We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, ﬁne-tuning scheme, and hyperpa- rameters as BERTBASE:"}
{"doc_id": "1810.04805", "para_id": 73, "text": "No NSP: A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at ﬁne-tuning, because removing it introduced a pre-train/ﬁne-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our ﬁne-tuning scheme. We ﬁrst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance signiﬁcantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring “No NSP” to “LTR & No NSP”. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does signiﬁcantly improve results on SQuAD, but the"}
{"doc_id": "1810.04805", "para_id": 74, "text": "mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach — we hypothesize that when the model is ﬁne-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- speciﬁc models can beneﬁt from the larger, more expressive pre-trained representations even when downstream task data is very small."}
{"doc_id": "1810.04805", "para_id": 75, "text": "ELMo (Peters et al., 2018a) 95.7 92.2 CVT (Clark et al., 2018) - 92.6 CSE (Akbik et al., 2018) - 93.1"}
{"doc_id": "1810.04805", "para_id": 76, "text": "Fine-tuning approach BERTLARGE 96.6 92.8 BERTBASE 96.4 92.4"}
{"doc_id": "1810.04805", "para_id": 77, "text": "Feature-based approach (BERTBASE) Embeddings 91.0 - Second-to-Last Hidden 95.6 - Last Hidden 94.9 - Weighted Sum Last Four Hidden 95.9 - Concat Last Four Hidden 96.1 - Weighted Sum All 12 Layers 95.5 -"}
{"doc_id": "1810.04805", "para_id": 78, "text": "Table 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters."}
{"doc_id": "1810.04805", "para_id": 79, "text": "All of the BERT results presented so far have used the ﬁne-tuning approach, where a simple classiﬁ- cation layer is added to the pre-trained model, and all parameters are jointly ﬁne-tuned on a down- stream task. However, the feature-based approach, where ﬁxed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-speciﬁc model architecture to be added. Second, there are major computational beneﬁts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF"}
{"doc_id": "1810.04805", "para_id": 80, "text": "layer in the output. We use the representation of the ﬁrst sub-token as the input to the token-level classiﬁer over the NER label set."}
{"doc_id": "1810.04805", "para_id": 81, "text": "To ablate the ﬁne-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without ﬁne-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classiﬁcation layer."}
{"doc_id": "1810.04805", "para_id": 82, "text": "Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind ﬁne-tuning the entire model. This demonstrates that BERT is effective for both ﬁne- tuning and feature-based approaches."}
{"doc_id": "1810.04805", "para_id": 83, "text": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneﬁt from deep unidirectional architec- tures. Our major contribution is further general- izing these ﬁndings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks."}
{"doc_id": "1810.04805", "para_id": 84, "text": "3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7"}
{"doc_id": "1810.04805", "para_id": 85, "text": "Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. “LM (ppl)” is the masked LM perplexity of held-out training data."}
{"doc_id": "1810.04805", "para_id": 86, "text": "Kevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914– 1925."}
{"doc_id": "1810.04805", "para_id": 87, "text": "Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649."}
{"doc_id": "1810.04805", "para_id": 88, "text": "Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160–167. ACM."}
{"doc_id": "1810.04805", "para_id": 89, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444."}
{"doc_id": "1810.04805", "para_id": 90, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670–680, Copen- hagen, Denmark. Association for Computational Linguistics."}
{"doc_id": "1810.04805", "para_id": 91, "text": "Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853."}
{"doc_id": "1810.04805", "para_id": 92, "text": "Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The ﬁfth PASCAL recognizing textual entailment challenge. In TAC. NIST."}
{"doc_id": "1810.04805", "para_id": 93, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079–3087."}
{"doc_id": "1810.04805", "para_id": 94, "text": "John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120–128. Association for Computa- tional Linguistics."}
{"doc_id": "1810.04805", "para_id": 95, "text": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09."}
{"doc_id": "1810.04805", "para_id": 96, "text": "William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005)."}
{"doc_id": "1810.04805", "para_id": 97, "text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics."}
{"doc_id": "1810.04805", "para_id": 98, "text": "William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via ﬁlling in the . arXiv preprint arXiv:1801.07736."}
{"doc_id": "1810.04805", "para_id": 99, "text": "Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479."}
{"doc_id": "1810.04805", "para_id": 100, "text": "Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415."}
{"doc_id": "1810.04805", "para_id": 101, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancou- ver, Canada. Association for Computational Lin- guistics."}
{"doc_id": "1810.04805", "para_id": 102, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics."}
{"doc_id": "1810.04805", "para_id": 103, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In ACL. Association for Computational Linguistics."}
{"doc_id": "1810.04805", "para_id": 104, "text": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv:1312.3005."}
{"doc_id": "1810.04805", "para_id": 105, "text": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI."}
{"doc_id": "1810.04805", "para_id": 106, "text": "Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557."}
{"doc_id": "1810.04805", "para_id": 107, "text": "Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL."}
{"doc_id": "1810.04805", "para_id": 108, "text": "Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499–1509."}
{"doc_id": "1810.04805", "para_id": 109, "text": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL."}
{"doc_id": "1810.04805", "para_id": 110, "text": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302."}
{"doc_id": "1810.04805", "para_id": 111, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI."}
{"doc_id": "1810.04805", "para_id": 112, "text": "Quoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188–1196."}
{"doc_id": "1810.04805", "para_id": 113, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383–2392."}
{"doc_id": "1810.04805", "para_id": 114, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47."}
{"doc_id": "1810.04805", "para_id": 115, "text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention ﬂow for machine comprehension. In ICLR."}
{"doc_id": "1810.04805", "para_id": 116, "text": "Lajanugen Logeswaran and Honglak Lee. 2018. An efﬁcient framework for learning sentence represen- tations. In International Conference on Learning Representations."}
{"doc_id": "1810.04805", "para_id": 117, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642."}
{"doc_id": "1810.04805", "para_id": 118, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS."}
{"doc_id": "1810.04805", "para_id": 119, "text": "Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL."}
{"doc_id": "1810.04805", "para_id": 120, "text": "Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638."}
{"doc_id": "1810.04805", "para_id": 121, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc."}
{"doc_id": "1810.04805", "para_id": 122, "text": "Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433."}
{"doc_id": "1810.04805", "para_id": 123, "text": "Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL."}
{"doc_id": "1810.04805", "para_id": 124, "text": "Andriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081–1088. Curran As- sociates, Inc."}
{"doc_id": "1810.04805", "para_id": 125, "text": "Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL ’10, pages 384–394."}
{"doc_id": "1810.04805", "para_id": 126, "text": "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP."}
{"doc_id": "1810.04805", "para_id": 127, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010."}
{"doc_id": "1810.04805", "para_id": 128, "text": "Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532– 1543."}
{"doc_id": "1810.04805", "para_id": 129, "text": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM."}
{"doc_id": "1810.04805", "para_id": 130, "text": "Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL."}
{"doc_id": "1810.04805", "para_id": 131, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL."}
{"doc_id": "1810.04805", "para_id": 132, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform"}
{"doc_id": "1810.04805", "para_id": 133, "text": "• Additional details for our experiments are presented in Appendix B; and"}
{"doc_id": "1810.04805", "para_id": 134, "text": "for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353–355."}
{"doc_id": "1810.04805", "para_id": 135, "text": "• Additional ablation studies are presented in Appendix C."}
{"doc_id": "1810.04805", "para_id": 136, "text": "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics."}
{"doc_id": "1810.04805", "para_id": 137, "text": "We present additional ablation studies for BERT including:"}
{"doc_id": "1810.04805", "para_id": 138, "text": "– Effect of Number of Training Steps; and – Ablation for Different Masking Proce- dures."}
{"doc_id": "1810.04805", "para_id": 139, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv:1805.12471."}
{"doc_id": "1810.04805", "para_id": 140, "text": "Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL."}
{"doc_id": "1810.04805", "para_id": 141, "text": "We provide examples of the pre-training tasks in the following."}
{"doc_id": "1810.04805", "para_id": 142, "text": "Masked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is"}
{"doc_id": "1810.04805", "para_id": 143, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144."}
{"doc_id": "1810.04805", "para_id": 144, "text": "hairy, and during the random masking procedure we chose the 4-th token (which corresponding to"}
{"doc_id": "1810.04805", "para_id": 145, "text": "hairy), our masking procedure can be further il- lustrated by"}
{"doc_id": "1810.04805", "para_id": 146, "text": "• 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy →"}
{"doc_id": "1810.04805", "para_id": 147, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328."}
{"doc_id": "1810.04805", "para_id": 148, "text": "• 10% of the time: Replace the word with a random word, e.g., my dog is hairy →my"}
{"doc_id": "1810.04805", "para_id": 149, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR."}
{"doc_id": "1810.04805", "para_id": 150, "text": "• 10% of the time: Keep the word un- changed, e.g., my dog is hairy →my dog"}
{"doc_id": "1810.04805", "para_id": 151, "text": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)."}
{"doc_id": "1810.04805", "para_id": 152, "text": "is hairy. The purpose of this is to bias the representation towards the actual observed word."}
{"doc_id": "1810.04805", "para_id": 153, "text": "The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model’s language understanding capability. In Section C.2, we evaluate the impact this proce- dure. Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model"}
{"doc_id": "1810.04805", "para_id": 154, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27."}
{"doc_id": "1810.04805", "para_id": 155, "text": "Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”"}
{"doc_id": "1810.04805", "para_id": 156, "text": "• Additional implementation details for BERT are presented in Appendix A;"}
{"doc_id": "1810.04805", "para_id": 157, "text": "Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to- left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and OpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach."}
{"doc_id": "1810.04805", "para_id": 158, "text": "epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the ﬁrst 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERTBASE was performed on 4 Cloud TPUs in Pod conﬁguration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete. Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings."}
{"doc_id": "1810.04805", "para_id": 159, "text": "to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost."}
{"doc_id": "1810.04805", "para_id": 160, "text": "Next Sentence Prediction The next sentence prediction task can be illustrated in the following examples."}
{"doc_id": "1810.04805", "para_id": 161, "text": "To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as “sentences” even though they are typ- ically much longer than single sentences (but can be shorter also). The ﬁrst sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the “next sentence pre- diction” task. They are sampled such that the com- bined length is ≤512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces. We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40"}
{"doc_id": "1810.04805", "para_id": 162, "text": "For ﬁne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-speciﬁc, but we found the following range of possible values to work well across all tasks:"}
{"doc_id": "1810.04805", "para_id": 163, "text": "13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html"}
{"doc_id": "1810.04805", "para_id": 164, "text": "• Learning rate (Adam): 5e-5, 3e-5, 2e-5 • Number of epochs: 2, 3, 4"}
{"doc_id": "1810.04805", "para_id": 165, "text": "To isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable."}
{"doc_id": "1810.04805", "para_id": 166, "text": "We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set."}
{"doc_id": "1810.04805", "para_id": 167, "text": "A.5 Illustrations of Fine-tuning on Different Tasks"}
{"doc_id": "1810.04805", "para_id": 168, "text": "The illustration of ﬁne-tuning BERT on different tasks can be seen in Figure 4. Our task-speciﬁc models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the ﬁgure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classiﬁcation out- put, and [SEP] is the special symbol to separate non-consecutive token sequences."}
{"doc_id": "1810.04805", "para_id": 169, "text": "Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are ﬁne- tuning approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:"}
{"doc_id": "1810.04805", "para_id": 170, "text": "B.1 Detailed Descriptions for the GLUE Benchmark Experiments."}
{"doc_id": "1810.04805", "para_id": 171, "text": "Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a):"}
{"doc_id": "1810.04805", "para_id": 172, "text": "MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classiﬁ- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the ﬁrst one."}
{"doc_id": "1810.04805", "para_id": 173, "text": "• GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words)."}
{"doc_id": "1810.04805", "para_id": 174, "text": "QQP Quora Question Pairs is a binary classiﬁ- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018)."}
{"doc_id": "1810.04805", "para_id": 175, "text": "• GPT uses a sentence separator ([SEP]) and classiﬁer token ([CLS]) which are only in- troduced at ﬁne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training."}
{"doc_id": "1810.04805", "para_id": 176, "text": "QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classiﬁcation task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer."}
{"doc_id": "1810.04805", "para_id": 177, "text": "• GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words."}
{"doc_id": "1810.04805", "para_id": 178, "text": "• GPT used the same learning rate of 5e-5 for all ﬁne-tuning experiments; BERT chooses a task-speciﬁc ﬁne-tuning learning rate which performs the best on the development set."}
{"doc_id": "1810.04805", "para_id": 179, "text": "Figure 4: Illustrations of Fine-tuning BERT on Different Tasks."}
{"doc_id": "1810.04805", "para_id": 180, "text": "SST-2 The Stanford Sentiment Treebank is a binary single-sentence classiﬁcation task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013)."}
{"doc_id": "1810.04805", "para_id": 181, "text": "for whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005)."}
{"doc_id": "1810.04805", "para_id": 182, "text": "RTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14"}
{"doc_id": "1810.04805", "para_id": 183, "text": "CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classiﬁcation task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not (Warstadt et al., 2018)."}
{"doc_id": "1810.04805", "para_id": 184, "text": "WNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that’s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-"}
{"doc_id": "1810.04805", "para_id": 185, "text": "STS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning."}
{"doc_id": "1810.04805", "para_id": 186, "text": "14Note that we only report single-task ﬁne-tuning results in this paper. A multitask ﬁne-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI. 15https://gluebenchmark.com/faq"}
{"doc_id": "1810.04805", "para_id": 187, "text": "MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations"}
{"doc_id": "1810.04805", "para_id": 188, "text": "Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and ﬁne-tuning, as the [MASK] symbol never ap- pears during the ﬁne-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both ﬁne-tuning and feature-based ap- proaches, as we expect the mismatch will be am- pliﬁed for the feature-based approach as the model will not have the chance to adjust the representa- tions."}
{"doc_id": "1810.04805", "para_id": 189, "text": "Figure 5 presents MNLI Dev accuracy after ﬁne- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions:"}
{"doc_id": "1810.04805", "para_id": 190, "text": "1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high ﬁne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps."}
{"doc_id": "1810.04805", "para_id": 191, "text": "MASK SAME RND MNLI NER Fine-tune Fine-tune Feature-based"}
{"doc_id": "1810.04805", "para_id": 192, "text": "80% 10% 10% 84.2 95.4 94.9 100% 0% 0% 84.3 94.9 94.0 80% 0% 20% 84.1 95.2 94.6 80% 20% 0% 84.4 95.2 94.7 0% 20% 80% 83.7 94.8 94.6 0% 0% 100% 83.6 94.9 94.6"}
{"doc_id": "1810.04805", "para_id": 193, "text": "2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately."}
{"doc_id": "1810.04805", "para_id": 194, "text": "Table 8: Ablation over different masking strategies."}
{"doc_id": "1810.04805", "para_id": 195, "text": "The results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token. The numbers in the left part of the table repre- sent the probabilities of the speciﬁc strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3. From the table it can be seen that ﬁne-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well."}
{"doc_id": "1810.04805", "para_id": 196, "text": "In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies."}
{"doc_id": "1810.04805", "para_id": 197, "text": "Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after ﬁne-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k."}
{"doc_id": "1910.01108", "para_id": 0, "text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}
{"doc_id": "1910.01108", "para_id": 1, "text": "Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF Hugging Face {victor,lysandre,julien,thomas}@huggingface.co"}
{"doc_id": "1910.01108", "para_id": 2, "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the- edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general- purpose language representation model, called DistilBERT, which can then be ﬁne- tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-speciﬁc models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."}
{"doc_id": "1910.01108", "para_id": 3, "text": "The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to signiﬁ- cant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better per- formances on downstream tasks."}
{"doc_id": "1910.01108", "para_id": 4, "text": "Figure 1: Parameter counts of several recently released pretrained language models."}
{"doc_id": "1910.01108", "para_id": 5, "text": "The trend toward bigger models raises several concerns. First is the environmental cost of exponentially scaling these models’ computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption."}
{"doc_id": "1910.01108", "para_id": 6, "text": "1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)"}
{"doc_id": "1910.01108", "para_id": 7, "text": "In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several downstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices."}
{"doc_id": "1910.01108", "para_id": 8, "text": "Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances."}
{"doc_id": "1910.01108", "para_id": 9, "text": "We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]."}
{"doc_id": "1910.01108", "para_id": 10, "text": "Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models."}
{"doc_id": "1910.01108", "para_id": 11, "text": "In supervised learning, a classiﬁcation model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reﬂect, in part, the generalization capabilities of the model and how well it will perform on the test set3."}
{"doc_id": "1910.01108", "para_id": 12, "text": "Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = P"}
{"doc_id": "1910.01108", "para_id": 13, "text": "i ti ∗log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al. [2015] we used a softmax-temperature: pi = exp(zi/T ) P"}
{"doc_id": "1910.01108", "para_id": 14, "text": "j exp(zj/T ) where T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax."}
{"doc_id": "1910.01108", "para_id": 15, "text": "The ﬁnal training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors."}
{"doc_id": "1910.01108", "para_id": 16, "text": "Student architecture In the present work, the student - DistilBERT - has the same general architec- ture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers."}
{"doc_id": "1910.01108", "para_id": 17, "text": "Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to ﬁnd the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two."}
{"doc_id": "1910.01108", "para_id": 18, "text": "2https://github.com/huggingface/transformers 3E.g. BERT-base’s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world...)."}
{"doc_id": "1910.01108", "para_id": 19, "text": "Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the GLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the medians of 5 runs with different seeds."}
{"doc_id": "1910.01108", "para_id": 20, "text": "Model Score CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI"}
{"doc_id": "1910.01108", "para_id": 21, "text": "ELMo 68.7 44.1 68.6 76.6 71.1 86.2 53.4 91.5 70.4 56.3 BERT-base 79.5 56.3 86.7 88.6 91.8 89.6 69.3 92.7 89.0 53.5 DistilBERT 77.0 51.3 82.2 87.5 89.2 88.5 59.9 91.3 86.9 56.3"}
{"doc_id": "1910.01108", "para_id": 22, "text": "Table 2: DistilBERT yields to comparable performance on downstream tasks. Com- parison on downstream tasks: IMDb (test ac- curacy) and SQuAD 1.1 (EM/F1 on dev set). D: with a second step of distillation during ﬁne-tuning."}
{"doc_id": "1910.01108", "para_id": 23, "text": "Table 3: DistilBERT is signiﬁcantly smaller while being constantly faster. Inference time of a full pass of GLUE task STS-B (sen- timent analysis) on CPU with a batch size of 1."}
{"doc_id": "1910.01108", "para_id": 24, "text": "BERT-base 93.46 81.2/88.5 DistilBERT 92.82 77.7/85.8 DistilBERT (D) - 79.1/86.9"}
{"doc_id": "1910.01108", "para_id": 25, "text": "Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective."}
{"doc_id": "1910.01108", "para_id": 26, "text": "Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100."}
{"doc_id": "1910.01108", "para_id": 27, "text": "General Language Understanding We assess the language understanding and generalization ca- pabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use of ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4"}
{"doc_id": "1910.01108", "para_id": 28, "text": "The results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters."}
{"doc_id": "1910.01108", "para_id": 29, "text": "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016])."}
{"doc_id": "1910.01108", "para_id": 30, "text": "As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT."}
{"doc_id": "1910.01108", "para_id": 31, "text": "We also studied whether we could add another step of distillation during the adaptation phase by ﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a"}
{"doc_id": "1910.01108", "para_id": 32, "text": "4We use jiant [Wang et al., 2019] to compute the baseline."}
{"doc_id": "1910.01108", "para_id": 33, "text": "Table 4: Ablation study. Variations are relative to the model trained with triple loss and teacher weights initialization."}
{"doc_id": "1910.01108", "para_id": 34, "text": "∅- Lcos - Lmlm -2.96 Lce - ∅- Lmlm -1.46 Lce - Lcos - ∅ -0.31 Triple loss + random weights initialization -3.69"}
{"doc_id": "1910.01108", "para_id": 35, "text": "teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model."}
{"doc_id": "1910.01108", "para_id": 36, "text": "To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STS- B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT."}
{"doc_id": "1910.01108", "para_id": 37, "text": "On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5."}
{"doc_id": "1910.01108", "para_id": 38, "text": "In this section, we investigate the inﬂuence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance."}
{"doc_id": "1910.01108", "para_id": 39, "text": "Task-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se- tups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer. Chatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ- ously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose pre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original pretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the abla- tion study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional distillation signal."}
{"doc_id": "1910.01108", "para_id": 40, "text": "Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads to substantial gains."}
{"doc_id": "1910.01108", "para_id": 41, "text": "Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without signiﬁcantly degrading the performance Michel et al. [2019]. Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models (Gupta et al. [2015]). Pruning and quantization are orthogonal to the present work."}
{"doc_id": "1910.01108", "para_id": 42, "text": "5https://github.com/huggingface/swift-coreml-transformers"}
{"doc_id": "1910.01108", "para_id": 43, "text": "We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications."}
{"doc_id": "1910.01108", "para_id": 44, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2018."}
{"doc_id": "1910.01108", "para_id": 45, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019."}
{"doc_id": "1910.01108", "para_id": 46, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019."}
{"doc_id": "1910.01108", "para_id": 47, "text": "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019."}
{"doc_id": "1910.01108", "para_id": 48, "text": "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In ACL, 2019."}
{"doc_id": "1910.01108", "para_id": 49, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "1910.01108", "para_id": 50, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language processing, 2019."}
{"doc_id": "1910.01108", "para_id": 51, "text": "Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006."}
{"doc_id": "1910.01108", "para_id": 52, "text": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015."}
{"doc_id": "1910.01108", "para_id": 53, "text": "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27, 2015."}
{"doc_id": "1910.01108", "para_id": 54, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2018."}
{"doc_id": "1910.01108", "para_id": 55, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018."}
{"doc_id": "1910.01108", "para_id": 56, "text": "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas Patry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose text understanding models. http://jiant.info/, 2019."}
{"doc_id": "1910.01108", "para_id": 57, "text": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In ACL, 2011."}
{"doc_id": "1910.01108", "para_id": 58, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016."}
{"doc_id": "1910.01108", "para_id": 59, "text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc knowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019."}
{"doc_id": "1910.01108", "para_id": 60, "text": "Debajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv, abs/1904.00796, 2019."}
{"doc_id": "1910.01108", "para_id": 61, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. ArXiv, abs/1908.08962, 2019."}
{"doc_id": "1910.01108", "para_id": 62, "text": "Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge distillation for web-scale question answering system. ArXiv, abs/1904.09636, 2019."}
{"doc_id": "1910.01108", "para_id": 63, "text": "Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical bert models for sequence labeling. In EMNLP-IJCNLP, 2019."}
{"doc_id": "1910.01108", "para_id": 64, "text": "Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019."}
{"doc_id": "1910.01108", "para_id": 65, "text": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML, 2015."}
{"doc_id": "1909.11942", "para_id": 0, "text": "ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS"}
{"doc_id": "1909.11942", "para_id": 1, "text": "Zhenzhong Lan1 Mingda Chen2∗ Sebastian Goodman1 Kevin Gimpel2"}
{"doc_id": "1909.11942", "para_id": 2, "text": "1Google Research 2Toyota Technological Institute at Chicago"}
{"doc_id": "1909.11942", "para_id": 3, "text": "{lanzhzh, seabass, piyushsharma, rsoricut}@google.com {mchen, kgimpel}@ttic.edu"}
{"doc_id": "1909.11942", "para_id": 4, "text": "Increasing model size when pretraining natural language representations often re- sults in improved performance on downstream tasks. However, at some point fur- ther model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter- reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer param- eters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT."}
{"doc_id": "1909.11942", "para_id": 5, "text": "Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many non- trivial NLP tasks, including those that have limited training data, have greatly beneﬁted from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of ma- chine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and for- mulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations."}
{"doc_id": "1909.11942", "para_id": 6, "text": "Evidence from these improvements reveals that a large network is of crucial importance for achiev- ing state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models?"}
{"doc_id": "1909.11942", "para_id": 7, "text": "An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be signiﬁcantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model."}
{"doc_id": "1909.11942", "para_id": 8, "text": "Existing solutions to the aforementioned problems include model parallelization (Shazeer et al., 2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017)."}
{"doc_id": "1909.11942", "para_id": 9, "text": "∗Work done as an intern at Google Research, driving data processing and downstream task evaluations."}
{"doc_id": "1909.11942", "para_id": 10, "text": "These solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has signiﬁcantly fewer parameters than a traditional BERT architecture."}
{"doc_id": "1909.11942", "para_id": 11, "text": "ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The ﬁrst one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without signiﬁcantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques signiﬁcantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efﬁciency. An ALBERT conﬁguration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization."}
{"doc_id": "1909.11942", "para_id": 12, "text": "To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT."}
{"doc_id": "1909.11942", "para_id": 13, "text": "As a result of these design decisions, we are able to scale up to much larger ALBERT conﬁgurations that still have fewer parameters than BERT-large but achieve signiﬁcantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Speciﬁcally, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2."}
{"doc_id": "1909.11942", "para_id": 14, "text": "2.1 SCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE"}
{"doc_id": "1909.11942", "para_id": 15, "text": "Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted (Mikolov et al., 2013; Le & Mikolov, 2014; Dai & Le, 2015; Pe- ters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019). One of the most signiﬁcant changes in the last two years is the shift from pre-training word embeddings, whether standard (Mikolov et al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018), to full-network pre-training followed by task-speciﬁc ﬁne-tuning (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019). In this line of work, it is often shown that larger model size improves performance. For example, Devlin et al. (2019) show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance. However, they stop at a hidden size of 1024, presumably because of the model size and computation cost problems."}
{"doc_id": "1909.11942", "para_id": 16, "text": "It is difﬁcult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen et al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be sublinear at the cost of an extra forward pass. Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed to use model parallelization to train a giant model. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed."}
{"doc_id": "1909.11942", "para_id": 17, "text": "The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoder- decoder tasks rather than the pretraining/ﬁnetuning setting. Different from our observations, De- hghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard"}
{"doc_id": "1909.11942", "para_id": 18, "text": "transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for trans- former networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our em- beddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer."}
{"doc_id": "1909.11942", "para_id": 19, "text": "ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identiﬁed that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skip- thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is deﬁned on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and ﬁnd that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classiﬁcation task rather than empirically comparing the two."}
{"doc_id": "1909.11942", "para_id": 20, "text": "In this section, we present the design decisions for ALBERT and provide quantiﬁed comparisons against corresponding conﬁgurations of the original BERT architecture (Devlin et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 21, "text": "The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer en- coder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/ﬁlter size to be 4H and the number of attention heads to be H/64."}
{"doc_id": "1909.11942", "para_id": 22, "text": "There are three main contributions that ALBERT makes over the design choices of BERT."}
{"doc_id": "1909.11942", "para_id": 23, "text": "Factorized embedding parameterization. In BERT, as well as subsequent modeling improve- ments such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H, i.e., E ≡H. This decision appears suboptimal for both modeling and practical reasons, as follows."}
{"doc_id": "1909.11942", "para_id": 24, "text": "From a modeling perspective, WordPiece embeddings are meant to learn context-independent repre- sentations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like represen- tations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efﬁcient usage of the total model parameters as informed by modeling needs, which dictate that H ≫E."}
{"doc_id": "1909.11942", "para_id": 25, "text": "From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E ≡H, then increasing H increases the size of the embedding matrix, which has size"}
{"doc_id": "1909.11942", "para_id": 26, "text": "1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000."}
{"doc_id": "1909.11942", "para_id": 27, "text": "V ×E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training."}
{"doc_id": "1909.11942", "para_id": 28, "text": "Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we ﬁrst project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V × H) to O(V × E + E × H). This parameter reduction is signiﬁcant when H ≫E. We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al. (2017); Baevski & Auli (2018); Dai et al. (2019) ) for different words is important."}
{"doc_id": "1909.11942", "para_id": 29, "text": "Cross-layer parameter sharing. For ALBERT, we propose cross-layer parameter sharing as an- other way to improve parameter efﬁciency. There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers. All our experiments use this default decision unless otherwise speciﬁed. We compare this design decision against other strategies in our experiments in Sec. 4.5."}
{"doc_id": "1909.11942", "para_id": 30, "text": "Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our observations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al. (2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging."}
{"doc_id": "1909.11942", "para_id": 31, "text": "Figure 1: The L2 distances and cosine similarity (in terms of degree) of the input and output embed- ding of each layer for BERT-large and ALBERT-large."}
{"doc_id": "1909.11942", "para_id": 32, "text": "Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each layer, using BERT-large and ALBERT-large conﬁgurations (see Table 1). We observe that the tran- sitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE."}
{"doc_id": "1909.11942", "para_id": 33, "text": "Inter-sentence coherence loss. In addition to the masked language modeling (MLM) loss (De- vlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classiﬁcation loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP’s impact unreliable and decided to eliminate it, a decision supported by an im- provement in downstream task performance across several tasks."}
{"doc_id": "1909.11942", "para_id": 34, "text": "We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difﬁculty as a task, as compared to MLM. As formulated, NSP conﬂates topic prediction and coherence prediction in a"}
{"doc_id": "1909.11942", "para_id": 35, "text": "Model Parameters Layers Hidden Embedding Parameter-sharing"}
{"doc_id": "1909.11942", "para_id": 36, "text": "BERT base 108M 12 768 768 False large 334M 24 1024 1024 False"}
{"doc_id": "1909.11942", "para_id": 37, "text": "base 12M 12 768 128 True large 18M 24 1024 128 True xlarge 60M 24 2048 128 True xxlarge 235M 12 4096 128 True"}
{"doc_id": "1909.11942", "para_id": 38, "text": "Table 1: The conﬁgurations of the main BERT and ALBERT models analyzed in this paper."}
{"doc_id": "1909.11942", "para_id": 39, "text": "single task2. However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss."}
{"doc_id": "1909.11942", "para_id": 40, "text": "We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order pre- diction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecu- tive segments from the same document), and as negative examples the same two consecutive seg- ments but with their order swapped. This forces the model to learn ﬁner-grained distinctions about discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at random- baseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, pre- sumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks."}
{"doc_id": "1909.11942", "para_id": 41, "text": "We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models."}
{"doc_id": "1909.11942", "para_id": 42, "text": "For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M ver- sus 334M. An ALBERT-xlarge conﬁguration with H = 2048 has only 60M parameters and an ALBERT-xxlarge conﬁguration with H = 4096 has 233M parameters, i.e., around 70% of BERT- large’s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same conﬁguration) obtains similar results but is computation- ally more expensive."}
{"doc_id": "1909.11942", "para_id": 43, "text": "This improvement in parameter efﬁciency is the most important advantage of ALBERT’s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail."}
{"doc_id": "1909.11942", "para_id": 44, "text": "To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretrain- ing baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1, x1,2 · · · and x2 = x1,1, x1,2 · · · are two segments.3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 45, "text": "2Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3A segment is usually comprised of more than one natural sentence, which has been shown to beneﬁt performance by Liu et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 46, "text": "We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by"}
{"doc_id": "1909.11942", "para_id": 47, "text": "We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as “White House correspondents”)."}
{"doc_id": "1909.11942", "para_id": 48, "text": "All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise speciﬁed. Train- ing was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size."}
{"doc_id": "1909.11942", "para_id": 49, "text": "The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise speciﬁed."}
{"doc_id": "1909.11942", "para_id": 50, "text": "To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classiﬁcation tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection."}
{"doc_id": "1909.11942", "para_id": 51, "text": "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular bench- marks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For com- pleteness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our ﬁnal comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs."}
{"doc_id": "1909.11942", "para_id": 52, "text": "We are now ready to quantify the impact of the design choices described in Sec. 3, speciﬁcally the ones around parameter efﬁciency. The improvement in parameter efﬁciency showcases the most important advantage of ALBERT’s design choices, as shown in Table 2: with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves signiﬁcant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%)."}
{"doc_id": "1909.11942", "para_id": 53, "text": "Another interesting observation is the speed of data throughput at training time under the same train- ing conﬁguration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure."}
{"doc_id": "1909.11942", "para_id": 54, "text": "Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT."}
{"doc_id": "1909.11942", "para_id": 55, "text": "Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base conﬁguration setting (see Table 1), using the same set of representative downstream tasks. Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by"}
{"doc_id": "1909.11942", "para_id": 56, "text": "Model Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg Speedup"}
{"doc_id": "1909.11942", "para_id": 57, "text": "BERT base 108M 90.4/83.2 80.4/77.6 84.5 92.8 68.2 82.3 4.7x large 334M 92.2/85.5 85.0/82.2 86.6 93.0 73.9 85.2 1.0"}
{"doc_id": "1909.11942", "para_id": 58, "text": "base 12M 89.3/82.3 80.0/77.1 81.6 90.3 64.0 80.1 5.6x large 18M 90.6/83.9 82.3/79.4 83.5 91.7 68.5 82.4 1.7x xlarge 60M 92.5/86.1 86.1/83.1 86.4 92.4 74.8 85.5 0.6x xxlarge 235M 94.1/88.3 88.1/85.1 88.0 95.2 82.3 88.7 0.3x"}
{"doc_id": "1909.11942", "para_id": 59, "text": "Table 2: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps. Here and everywhere else, the Avg column is computed by averaging the scores of the downstream tasks to its left (the two numbers of F1 and EM for each SQuAD are ﬁrst averaged)."}
{"doc_id": "1909.11942", "para_id": 60, "text": "much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling."}
{"doc_id": "1909.11942", "para_id": 61, "text": "Model E Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg"}
{"doc_id": "1909.11942", "para_id": 62, "text": "64 87M 89.9/82.9 80.1/77.8 82.9 91.5 66.7 81.3 128 89M 89.9/82.8 80.3/77.3 83.7 91.5 67.9 81.7 256 93M 90.2/83.2 80.3/77.4 84.1 91.9 67.3 81.8 768 108M 90.4/83.2 80.4/77.6 84.5 92.8 68.2 82.3"}
{"doc_id": "1909.11942", "para_id": 63, "text": "64 10M 88.7/81.4 77.5/74.8 80.8 89.4 63.5 79.0 128 12M 89.3/82.3 80.0/77.1 81.6 90.3 64.0 80.1 256 16M 88.8/81.5 79.1/76.3 81.5 90.3 63.4 79.6 768 31M 88.6/81.5 79.2/76.6 82.0 90.6 63.3 79.8"}
{"doc_id": "1909.11942", "para_id": 64, "text": "Table 3: The effect of vocabulary embedding size on the performance of ALBERT-base."}
{"doc_id": "1909.11942", "para_id": 65, "text": "Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base conﬁguration (Table 1) with two embedding sizes (E = 768 and E = 128). We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and inter- mediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones)."}
{"doc_id": "1909.11942", "para_id": 66, "text": "The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (- 1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg)."}
{"doc_id": "1909.11942", "para_id": 67, "text": "There are other strategies of sharing the parameters cross layers. For example, We can divide the L layers into N groups of size M, and each size-M group shares parameters. Overall, our experimen- tal results shows that the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice."}
{"doc_id": "1909.11942", "para_id": 68, "text": "Model Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg"}
{"doc_id": "1909.11942", "para_id": 69, "text": "all-shared 31M 88.6/81.5 79.2/76.6 82.0 90.6 63.3 79.8 shared-attention 83M 89.9/82.7 80.0/77.2 84.0 91.4 67.7 81.6 shared-FFN 57M 89.2/82.1 78.2/75.4 81.5 90.8 62.6 79.5 not-shared 108M 90.4/83.2 80.4/77.6 84.5 92.8 68.2 82.3"}
{"doc_id": "1909.11942", "para_id": 70, "text": "all-shared 12M 89.3/82.3 80.0/77.1 82.0 90.3 64.0 80.1 shared-attention 64M 89.9/82.8 80.7/77.9 83.4 91.9 67.6 81.7 shared-FFN 38M 88.9/81.6 78.6/75.6 82.3 91.7 64.4 80.2 not-shared 89M 89.9/82.8 80.3/77.3 83.2 91.5 67.9 81.6"}
{"doc_id": "1909.11942", "para_id": 71, "text": "Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 72, "text": "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERT- base conﬁguration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 73, "text": "Intrinsic Tasks Downstream Tasks SP tasks MLM NSP SOP SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg None 54.9 52.4 53.3 88.6/81.5 78.1/75.3 81.5 89.9 61.7 79.0 NSP 54.5 90.5 52.0 88.4/81.5 77.2/74.6 81.6 91.1 62.3 79.2 SOP 54.0 78.9 86.5 89.3/82.3 80.0/77.1 82.0 90.3 64.0 80.1"}
{"doc_id": "1909.11942", "para_id": 74, "text": "Table 5: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 75, "text": "The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the “None” condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%."}
{"doc_id": "1909.11942", "para_id": 76, "text": "The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we per- form a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training)."}
{"doc_id": "1909.11942", "para_id": 77, "text": "Models Steps Time SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg BERT-large 400k 34h 93.5/87.4 86.9/84.3 87.8 94.6 77.3 87.2 ALBERT-xxlarge 125k 32h 94.0/88.1 88.3/85.3 87.8 95.4 82.5 88.7"}
{"doc_id": "1909.11942", "para_id": 78, "text": "Table 6: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 79, "text": "After training for roughly the same amount of time, ALBERT-xxlarge is signiﬁcantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%."}
{"doc_id": "1909.11942", "para_id": 80, "text": "The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019). In this section, we report measurements on the impact of the additional data used by both XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 81, "text": "Fig. 2a plots the dev set MLM accuracy under two conditions, without and with additional data, with the latter condition giving a signiﬁcant boost. We also observe performance improvements on the downstream tasks in Table 7, except for the SQuAD benchmarks (which are Wikipedia-based, and therefore are negatively affected by out-of-domain training material)."}
{"doc_id": "1909.11942", "para_id": 82, "text": "SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg No additional data 89.3/82.3 80.0/77.1 81.6 90.3 64.0 80.1 With additional data 88.8/81.7 79.1/76.3 82.4 92.8 66.0 80.8"}
{"doc_id": "1909.11942", "para_id": 83, "text": "Table 7: The effect of additional training data using the ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 84, "text": "We also note that, even after training for 1M steps, our largest models still do not overﬁt to their training data. As a result, we decide to remove dropout to further increase our model capacity. The"}
{"doc_id": "1909.11942", "para_id": 85, "text": "Figure 2: The effects of adding data and removing dropout during training."}
{"doc_id": "1909.11942", "para_id": 86, "text": "plot in Fig. 2b shows that removing dropout signiﬁcantly improves MLM accuracy. Intermediate evaluation on ALBERT-xxlarge at around 1M training steps (Table 8) also conﬁrms that removing dropout helps the downstream tasks. There is empirical (Szegedy et al., 2017) and theoretical (Li et al., 2019) evidence showing that a combination of batch normalization and dropout in Convolu- tional Neural Networks may have harmful results. To the best of our knowledge, we are the ﬁrst to show that dropout can hurt performance in large Transformer-based models. However, the underly- ing network structure of ALBERT is a special case of the transformer and further experimentation is needed to see if this phenomenon appears with other transformer-based architectures or not."}
{"doc_id": "1909.11942", "para_id": 87, "text": "SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg With dropout 94.7/89.2 89.6/86.9 90.0 96.3 85.7 90.4 Without dropout 94.8/89.5 89.9/87.2 90.4 96.5 86.1 90.7"}
{"doc_id": "1909.11942", "para_id": 88, "text": "Table 8: The effect of removing dropout, measured for an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 89, "text": "The results we report in this section make use of the training data used by Devlin et al. (2019), as well as the additional data used by Liu et al. (2019) and Yang et al. (2019). We report state-of-the-art results under two settings for ﬁne-tuning: single-model and ensembles. In both settings, we only do single-task ﬁne-tuning4. Following Liu et al. (2019), on the development set we report the median result over ﬁve runs."}
{"doc_id": "1909.11942", "para_id": 90, "text": "Models MNLI QNLI QQP RTE SST MRPC CoLA STS WNLI Avg Single-task single models on dev BERT-large 86.6 92.3 91.3 70.4 93.2 88.0 60.6 90.0 - - XLNet-large 89.8 93.9 91.8 83.8 95.6 89.2 63.6 91.8 - - RoBERTa-large 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 - - ALBERT (1M) 90.4 95.2 92.0 88.1 96.8 90.2 68.7 92.7 - - ALBERT (1.5M) 90.8 95.3 92.2 89.2 96.9 90.9 71.4 93.0 - - Ensembles on test (from leaderboard as of Sept. 16, 2019) ALICE 88.2 95.7 90.7 83.5 95.2 92.6 69.2 91.1 80.8 87.0 MT-DNN 87.9 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0 87.6 XLNet 90.2 98.6 90.3 86.3 96.8 93.0 67.8 91.6 90.4 88.4 RoBERTa 90.8 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0 88.5 Adv-RoBERTa 91.1 98.8 90.3 88.7 96.8 93.1 68.0 92.4 89.0 88.8 ALBERT 91.3 99.2 90.5 89.2 97.1 93.4 69.1 92.5 91.8 89.4"}
{"doc_id": "1909.11942", "para_id": 91, "text": "Table 9: State-of-the-art results on the GLUE benchmark. For single-task single-model results, we report ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble uses models trained with 1M, 1.5M, and other numbers of steps."}
{"doc_id": "1909.11942", "para_id": 92, "text": "The single-model ALBERT conﬁguration incorporates the best-performing settings discussed: an ALBERT-xxlarge conﬁguration (Table 1) using combined MLM and SOP losses, and no dropout."}
{"doc_id": "1909.11942", "para_id": 93, "text": "4Following Liu et al. (2019), we ﬁne-tune for RTE, STS, and MRPC using an MNLI checkpoint."}
{"doc_id": "1909.11942", "para_id": 94, "text": "The checkpoints that contribute to the ﬁnal ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model predictions for the ensemble models, where the candidates are ﬁne-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the pre- diction scores for those spans that have multiple probabilities; we also average the scores of the “unanswerable” decision."}
{"doc_id": "1909.11942", "para_id": 95, "text": "Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art signif- icantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models speciﬁcally designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model."}
{"doc_id": "1909.11942", "para_id": 96, "text": "Models SQuAD1.1 dev SQuAD2.0 dev SQuAD2.0 test RACE test (Middle/High) Single model (from leaderboard as of Sept. 23, 2019) BERT-large 90.9/84.1 81.8/79.0 89.1/86.3 72.0 (76.6/70.1) XLNet 94.5/89.0 88.8/86.1 89.1/86.3 81.8 (85.5/80.2) RoBERTa 94.6/88.9 89.4/86.5 89.8/86.8 83.2 (86.5/81.3) UPM - - 89.9/87.2 - XLNet + SG-Net Veriﬁer++ - - 90.1/87.2 - ALBERT (1M) 94.8/89.2 89.9/87.2 - 86.0 (88.2/85.1) ALBERT (1.5M) 94.8/89.3 90.2/87.4 90.9/88.1 86.5 (89.0/85.5) Ensembles (from leaderboard as of Sept. 23, 2019) BERT-large 92.2/86.2 - - - XLNet + SG-Net Veriﬁer - - 90.7/88.2 - UPM - - 90.7/88.2 XLNet + DAAF + Veriﬁer - - 90.9/88.6 - DCMN+ - - - 84.1 (88.5/82.3) ALBERT 95.5/90.1 91.4/88.9 92.2/89.7 89.4 (91.2/88.6)"}
{"doc_id": "1909.11942", "para_id": 97, "text": "Table 10: State-of-the-art results on the SQuAD and RACE benchmarks."}
{"doc_id": "1909.11942", "para_id": 98, "text": "While ALBERT-xxlarge has less parameters than BERT-large and gets signiﬁcantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018). An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efﬁcient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations."}
{"doc_id": "1909.11942", "para_id": 99, "text": "The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models."}
{"doc_id": "1909.11942", "para_id": 100, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018."}
{"doc_id": "1909.11942", "para_id": 101, "text": "Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information Processing Systems (NeurIPS), 2019."}
{"doc_id": "1909.11942", "para_id": 102, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pp. 6–4. Venice, 2006."}
{"doc_id": "1909.11942", "para_id": 103, "text": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth PASCAL recognizing textual entailment challenge. In TAC, 2009."}
{"doc_id": "1909.11942", "para_id": 104, "text": "Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001."}
{"doc_id": "1909.11942", "para_id": 105, "text": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016."}
{"doc_id": "1909.11942", "para_id": 106, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019."}
{"doc_id": "1909.11942", "para_id": 107, "text": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V Le. Bam! born-again multi-task networks for natural language understanding. arXiv preprint arXiv:1907.04829, 2019."}
{"doc_id": "1909.11942", "para_id": 108, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005."}
{"doc_id": "1909.11942", "para_id": 109, "text": "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor- mation processing systems, pp. 3079–3087, 2015."}
{"doc_id": "1909.11942", "para_id": 110, "text": "Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019."}
{"doc_id": "1909.11942", "para_id": 111, "text": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018."}
{"doc_id": "1909.11942", "para_id": 112, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423."}
{"doc_id": "1909.11942", "para_id": 113, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002."}
{"doc_id": "1909.11942", "para_id": 114, "text": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. Learn- ing generic sentence representations using convolutional neural networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2390–2400, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1254. URL https://www.aclweb.org/anthology/D17-1254."}
{"doc_id": "1909.11942", "para_id": 115, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail- ment and Paraphrasing, pp. 1–9, Prague, June 2007. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W07-1401."}
{"doc_id": "1909.11942", "para_id": 116, "text": "Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net- work: Backpropagation without storing activations. In Advances in neural information processing systems, pp. 2214–2224, 2017."}
{"doc_id": "1909.11942", "para_id": 117, "text": "Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient training of bert by progressively stacking. In International Conference on Machine Learning, pp. 2337–2346, 2019."}
{"doc_id": "1909.11942", "para_id": 118, "text": "Edouard Grave, Armand Joulin, Moustapha Ciss´e, Herv´e J´egou, et al. Efﬁcient softmax approxima- tion for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1302–1310. JMLR. org, 2017."}
{"doc_id": "1909.11942", "para_id": 119, "text": "Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225, 1995. URL https: //www.aclweb.org/anthology/J95-2003."}
{"doc_id": "1909.11942", "para_id": 120, "text": "M.A.K. Halliday and Ruqaiya Hasan. Cohesion in English. Routledge, 1976."}
{"doc_id": "1909.11942", "para_id": 121, "text": "Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10. 18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122."}
{"doc_id": "1909.11942", "para_id": 122, "text": "Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016."}
{"doc_id": "1909.11942", "para_id": 123, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1367–1377. Association for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL http: //aclweb.org/anthology/N16-1162."}
{"doc_id": "1909.11942", "para_id": 124, "text": "Jerry R. Hobbs. Coherence and coreference. Cognitive Science, 3(1):67–90, 1979."}
{"doc_id": "1909.11942", "para_id": 125, "text": "Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1909.11942", "para_id": 126, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. First quora dataset release: Ques- tion pairs, January 2017. URL https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs."}
{"doc_id": "1909.11942", "para_id": 127, "text": "Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsuper- vised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017."}
{"doc_id": "1909.11942", "para_id": 128, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019."}
{"doc_id": "1909.11942", "para_id": 129, "text": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Ur- tasun, and Sanja Fidler. Skip-thought vectors. In Proceedings of the 28th International Con- ference on Neural Information Processing Systems - Volume 2, NIPS’15, pp. 3294–3302, Cam- bridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id= 2969442.2969607."}
{"doc_id": "1909.11942", "para_id": 130, "text": "Taku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012."}
{"doc_id": "1909.11942", "para_id": 131, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://www. aclweb.org/anthology/D17-1082."}
{"doc_id": "1909.11942", "para_id": 132, "text": "Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceed- ings of the 31st ICML, Beijing, China, 2014."}
{"doc_id": "1909.11942", "para_id": 133, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thir- teenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012."}
{"doc_id": "1909.11942", "para_id": 134, "text": "Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2682–2690, 2019."}
{"doc_id": "1909.11942", "para_id": 135, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "1909.11942", "para_id": 136, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6294–6305. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7209-learned-in-translation-contextualized-word-vectors.pdf."}
{"doc_id": "1909.11942", "para_id": 137, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen- tations of words and phrases and their compositionality. In Advances in neural information pro- cessing systems, pp. 3111–3119, 2013."}
{"doc_id": "1909.11942", "para_id": 138, "text": "Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex- plicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pp. 4497–4510, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/ P19-1442."}
{"doc_id": "1909.11942", "para_id": 139, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/ D14-1162."}
{"doc_id": "1909.11942", "para_id": 140, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://www.aclweb.org/anthology/N18-1202."}
{"doc_id": "1909.11942", "para_id": 141, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/ openai-assets/research-covers/language-unsupervised/language_ understanding_paper.pdf, 2018."}
{"doc_id": "1909.11942", "para_id": 142, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019."}
{"doc_id": "1909.11942", "para_id": 143, "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "1909.11942", "para_id": 144, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb. org/anthology/D16-1264."}
{"doc_id": "1909.11942", "para_id": 145, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb. org/anthology/P18-2124."}
{"doc_id": "1909.11942", "para_id": 146, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414– 10423, 2018."}
{"doc_id": "1909.11942", "para_id": 147, "text": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional block self- attention for fast and memory-efﬁcient sequence modeling. arXiv preprint arXiv:1804.00857, 2018."}
{"doc_id": "1909.11942", "para_id": 148, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model par- allelism, 2019."}
{"doc_id": "1909.11942", "para_id": 149, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa- tional Linguistics. URL https://www.aclweb.org/anthology/D13-1170."}
{"doc_id": "1909.11942", "para_id": 150, "text": "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. arXiv preprint arXiv:1908.09355, 2019."}
{"doc_id": "1909.11942", "para_id": 151, "text": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer- ence on Artiﬁcial Intelligence, 2017."}
{"doc_id": "1909.11942", "para_id": 152, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962, 2019."}
{"doc_id": "1909.11942", "para_id": 153, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017."}
{"doc_id": "1909.11942", "para_id": 154, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceed- ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/ W18-5446."}
{"doc_id": "1909.11942", "para_id": 155, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT: Incor- porating language structures into pre-training for deep language understanding. arXiv preprint arXiv:1908.04577, 2019."}
{"doc_id": "1909.11942", "para_id": 156, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1909.11942", "para_id": 157, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceedings of the 2018 Conference of the North"}
{"doc_id": "1909.11942", "para_id": 158, "text": "American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb. org/anthology/N18-1101."}
{"doc_id": "1909.11942", "para_id": 159, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019."}
{"doc_id": "1909.11942", "para_id": 160, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing BERT pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019."}
{"doc_id": "1909.11942", "para_id": 161, "text": "Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. DCMN+: Dual co-matching network for multi-choice reading comprehension. arXiv preprint arXiv:1908.11511, 2019."}
{"doc_id": "1909.11942", "para_id": 162, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19–27, 2015."}
{"doc_id": "1909.11942", "para_id": 163, "text": "In this section, we check how depth (number of layers) and width (hidden size) affect the perfor- mance of ALBERT. Table 11 shows the performance of an ALBERT-large conﬁguration (see Ta- ble 1) using different numbers of layers. Networks with 3 or more layers are trained by ﬁne-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are ﬁne-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases signiﬁcantly. However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline."}
{"doc_id": "1909.11942", "para_id": 164, "text": "Number of layers Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 1 18M 31.1/22.9 50.1/50.1 66.4 80.8 40.1 52.9 3 18M 79.8/69.7 64.4/61.7 77.7 86.7 54.0 71.2 6 18M 86.4/78.4 73.8/71.1 81.2 88.9 60.9 77.2 12 18M 89.8/83.3 80.7/77.9 83.3 91.7 66.7 81.5 24 18M 90.3/83.3 81.8/79.0 83.3 91.5 68.7 82.1 48 18M 90.0/83.1 81.8/78.9 83.4 91.9 66.9 81.8"}
{"doc_id": "1909.11942", "para_id": 165, "text": "Table 11: The effect of increasing the number of layers for an ALBERT-large conﬁguration."}
{"doc_id": "1909.11942", "para_id": 166, "text": "A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large conﬁguration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline signiﬁcantly. We note that none of these models appear to overﬁt the training data, and they all have higher training and development loss compared to the best-performing ALBERT conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 167, "text": "5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network."}
{"doc_id": "1909.11942", "para_id": 168, "text": "Hidden size Parameters SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 1024 18M 79.8/69.7 64.4/61.7 77.7 86.7 54.0 71.2 2048 60M 83.3/74.1 69.1/66.6 79.7 88.6 58.2 74.6 4096 225M 85.0/76.4 71.0/68.1 80.3 90.4 60.4 76.3 6144 499M 84.7/75.8 67.8/65.4 78.1 89.1 56.0 74.0"}
{"doc_id": "1909.11942", "para_id": 169, "text": "Table 12: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 170, "text": "A.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO?"}
{"doc_id": "1909.11942", "para_id": 171, "text": "In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer conﬁguration is small. Does this result still hold for much wider ALBERT conﬁgurations, such as ALBERT-xxlarge (H=4096)?"}
{"doc_id": "1909.11942", "para_id": 172, "text": "Number of layers SQuAD1.1 SQuAD2.0 MNLI SST-2 RACE Avg 12 94.0/88.1 88.3/85.3 87.8 95.4 82.5 88.7 24 94.1/88.3 88.1/85.1 88.0 95.2 82.3 88.7"}
{"doc_id": "1909.11942", "para_id": 173, "text": "Table 13: The effect of a deeper network using an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 174, "text": "The answer is given by the results from Table 13. The difference between 12-layer and 24-layer ALBERT-xxlarge conﬁgurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 175, "text": "GLUE GLUE is comprised of 9 tasks, namely Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Stanford Sentiment Treebank (SST; Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC; Dolan & Brockett, 2005), Semantic Textual Similarity Bench- mark (STS; Cer et al., 2017), Quora Question Pairs (QQP; Iyer et al., 2017), Multi-Genre NLI (MNLI; Williams et al., 2018), Question NLI (QNLI; Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE; Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and Winograd NLI (WNLI; Levesque et al., 2012). It focuses on evaluating model capabilities for natural language understanding. When reporting MNLI results, we only report the “match” condition (MNLI-m). We follow the ﬁnetuning procedures from prior work (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from GLUE submissions. For test set submissions, we perform task-speciﬁc modiﬁcations for WNLI and QNLI as described by Liu et al. (2019) and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 176, "text": "SQuAD SQuAD is an extractive question answering dataset built from Wikipedia. The answers are segments from the context paragraphs and the task is to predict answer spans. We evaluate our models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are jointly trained with a span extraction loss and an additional classiﬁer for predicting answerabil- ity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance."}
{"doc_id": "1909.11942", "para_id": 177, "text": "RACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from En- glish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate answers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the represen- tations from the “[CLS]” token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set."}
{"doc_id": "1909.11942", "para_id": 178, "text": "Hyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 179, "text": "LR BSZ ALBERT DR Classiﬁer DR TS WS MSL CoLA 1.00E-05 16 0 0.1 5336 320 512 STS 2.00E-05 16 0 0.1 3598 214 512 SST-2 1.00E-05 32 0 0.1 20935 1256 512 MNLI 3.00E-05 128 0 0.1 10000 1000 512 QNLI 1.00E-05 32 0 0.1 33112 1986 512 QQP 5.00E-05 128 0.1 0.1 14000 1000 512 RTE 3.00E-05 32 0.1 0.1 800 200 512 MRPC 2.00E-05 32 0 0.1 800 200 512 WNLI 2.00E-05 16 0.1 0.1 2000 250 512 SQuAD v1.1 5.00E-05 48 0 0.1 3649 365 384 SQuAD v2.0 3.00E-05 48 0 0.1 8144 814 512 RACE 2.00E-05 32 0.1 0.1 12000 1000 512"}
{"doc_id": "1909.11942", "para_id": 180, "text": "Table 14: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length."}
{"doc_id": "1907.11692", "para_id": 0, "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}
{"doc_id": "1907.11692", "para_id": 1, "text": "Yinhan Liu∗§ Myle Ott∗§ Naman Goyal∗§ Jingfei Du∗§ Mandar Joshi†"}
{"doc_id": "1907.11692", "para_id": 2, "text": "Danqi Chen§ Omer Levy§ Mike Lewis§ Luke Zettlemoyer†§ Veselin Stoyanov§"}
{"doc_id": "1907.11692", "para_id": 3, "text": "† Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA {mandar90,lsz}@cs.washington.edu"}
{"doc_id": "1907.11692", "para_id": 4, "text": "§ Facebook AI {yinhanliu,myleott,naman,jingfeidu, danqi,omerlevy,mikelewis,lsz,ves}@fb.com"}
{"doc_id": "1907.11692", "para_id": 5, "text": "We present a replication study of BERT pre- training (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We ﬁnd that BERT was signiﬁcantly undertrained and propose an im- proved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modiﬁcations are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer se- quences; and (4) dynamically changing the mask- ing pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of compa- rable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our im- proved training procedure improves upon the pub- lished BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT’s masked lan- guage model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language model- ing (Yang et al., 2019).2"}
{"doc_id": "1907.11692", "para_id": 6, "text": "Language model pretraining has led to sig- niﬁcant performance gains but careful com- parison between different approaches is chal- lenging. Training is computationally expen- sive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have signiﬁcant impact on the ﬁnal re- sults. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparam- eters and training data size. We ﬁnd that BERT was signiﬁcantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the impor- tance of previously overlooked design choices, and raise questions about the source of re- cently reported improvements. We release our models and code.1"}
{"doc_id": "1907.11692", "para_id": 7, "text": "Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought signiﬁcant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances."}
{"doc_id": "1907.11692", "para_id": 8, "text": "In summary, the contributions of this paper are: (1) We present a set of important BERT de- sign choices and training strategies and introduce"}
{"doc_id": "1907.11692", "para_id": 9, "text": "∗Equal contribution. 1Our models and code are available at: https://github.com/pytorch/fairseq"}
{"doc_id": "1907.11692", "para_id": 10, "text": "2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work."}
{"doc_id": "1907.11692", "para_id": 11, "text": "alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC- NEWS, and conﬁrm that using more data for pre- training further improves performance on down- stream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and ﬁne-tuning code imple- mented in PyTorch (Paszke et al., 2017)."}
{"doc_id": "1907.11692", "para_id": 12, "text": "and 10% are replaced by a randomly selected vo- cabulary token. In the original implementation, random mask- ing and replacement is performed once in the be- ginning and saved for the duration of training, al- though in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1)."}
{"doc_id": "1907.11692", "para_id": 13, "text": "Next Sentence Prediction (NSP) NSP is a bi- nary classiﬁcation loss for predicting whether two segments follow each other in the original text. Positive examples are created by taking consecu- tive sentences from the text corpus. Negative ex- amples are created by pairing segments from dif- ferent documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences."}
{"doc_id": "1907.11692", "para_id": 14, "text": "In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will ex- amine experimentally in the following section."}
{"doc_id": "1907.11692", "para_id": 15, "text": "BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM. Segments usually consist of more than one natural sentence. The two seg- ments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS], x1, . . . , xN, [SEP], y1, . . . , yM, [EOS]. M and N are constrained such that M + N < T, where T is a parameter that controls the maximum sequence length during training. The model is ﬁrst pretrained on a large unla- beled text corpus and subsequently ﬁnetuned us- ing end-task labeled data."}
{"doc_id": "1907.11692", "para_id": 16, "text": "BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: β1 = 0.9, β2 = 0.999, ǫ = 1e-6 and L2 weight de- cay of 0.01. The learning rate is warmed up over the ﬁrst 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and at- tention weights, and a GELU activation func- tion (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with mini- batches containing B = 256 sequences of maxi- mum length T = 512 tokens."}
{"doc_id": "1907.11692", "para_id": 17, "text": "BERT uses the now ubiquitous transformer archi- tecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H."}
{"doc_id": "1907.11692", "para_id": 18, "text": "PUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text.3"}
{"doc_id": "1907.11692", "para_id": 19, "text": "During pretraining, BERT uses two objectives: masked language modeling and next sentence pre- diction."}
{"doc_id": "1907.11692", "para_id": 20, "text": "In this section, we describe the experimental setup for our replication study of BERT."}
{"doc_id": "1907.11692", "para_id": 21, "text": "Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uni- formly selects 15% of the input tokens for possi- ble replacement. Of the selected tokens, 80% are replaced with [MASK], 10% are left unchanged,"}
{"doc_id": "1907.11692", "para_id": 22, "text": "We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT"}
{"doc_id": "1907.11692", "para_id": 23, "text": "3Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data."}
{"doc_id": "1907.11692", "para_id": 24, "text": "pus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).5"}
{"doc_id": "1907.11692", "para_id": 25, "text": "optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting β2 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens. Unlike Devlin et al. (2019), we do not ran- domly inject short sequences, and we do not train with a reduced sequence length for the ﬁrst 90% of updates. We train only with full-length sequences. We train with mixed precision ﬂoating point arithmetic on DGX-1 machines, each with 8 × 32GB Nvidia V100 GPUs interconnected by In- ﬁniband (Micikevicius et al., 2018)."}
{"doc_id": "1907.11692", "para_id": 26, "text": "(2018) containing a subset of CommonCrawl data ﬁltered to match the story-like style of Winograd schemas. (31GB)."}
{"doc_id": "1907.11692", "para_id": 27, "text": "Following previous work, we evaluate our pre- trained models on downstream tasks using the fol- lowing three benchmarks."}
{"doc_id": "1907.11692", "para_id": 28, "text": "GLUE The General Language Understand- ing Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classiﬁcation or sentence-pair classiﬁcation tasks. The GLUE organizers provide training and development data splits as well as a submission server and leader- board that allows participants to evaluate and com- pare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after ﬁnetuning the pretrained models on the corresponding single- task training data (i.e., without multi-task training or ensembling). Our ﬁnetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set re- sults obtained from the public leaderboard. These results depend on a several task-speciﬁc modiﬁca- tions, which we describe in Section 5.1."}
{"doc_id": "1907.11692", "para_id": 29, "text": "BERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demon- strate that increasing data size can result in im- proved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Unfortu- nately, not all of the additional datasets can be publicly released. For our study, we focus on gath- ering as much data as possible for experimenta- tion, allowing us to match the overall quality and quantity of data as appropriate for each compari- son. We consider ﬁve English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:"}
{"doc_id": "1907.11692", "para_id": 30, "text": "SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in"}
{"doc_id": "1907.11692", "para_id": 31, "text": "• BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to train BERT. (16GB)."}
{"doc_id": "1907.11692", "para_id": 32, "text": "• CC-NEWS, which we collected from the En- glish portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB af- ter ﬁltering).4"}
{"doc_id": "1907.11692", "para_id": 33, "text": "5The authors and their afﬁliated institutions are not in any way afﬁliated with the creation of the OpenWebText dataset. 6The datasets are: CoLA (Warstadt et al., 2018), Stanford Sentiment Treebank (SST) (Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC) (Dolan and Brockett, 2005), Semantic Tex- tual Similarity Benchmark (STS) (Agirre et al., 2007), Quora Question Pairs (QQP) (Iyer et al., 2016), Multi- Genre NLI (MNLI) (Williams et al., 2018), Question NLI (QNLI) (Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and Winograd NLI (WNLI) (Levesque et al., 2011)."}
{"doc_id": "1907.11692", "para_id": 34, "text": "• OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText cor-"}
{"doc_id": "1907.11692", "para_id": 35, "text": "4We use news-please (Hamborg et al., 2017) to col- lect and extract CC-NEWS. CC-NEWS is similar to the RE- ALNEWS dataset described in Zellers et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 36, "text": "V2.0 some questions are not answered in the pro- vided context, making the task more challenging. For SQuAD V1.1 we adopt the same span pre- diction method as BERT (Devlin et al., 2019). For SQuAD V2.0, we add an additional binary classi- ﬁer to predict whether the question is answerable, which we train jointly by summing the classiﬁca- tion and span loss terms. During evaluation, we only predict span indices on pairs that are classi- ﬁed as answerable."}
{"doc_id": "1907.11692", "para_id": 37, "text": "Our reimplementation: static 78.3 84.3 92.5 dynamic 78.7 84.0 92.9"}
{"doc_id": "1907.11692", "para_id": 38, "text": "Table 1: Comparison between static and dynamic masking for BERTBASE. We report F1 for SQuAD and accuracy for MNLI-m and SST-2. Reported results are medians over 5 random initializations (seeds). Refer- ence results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 39, "text": "RACE The ReAding Comprehension from Ex- aminations (RACE) (Lai et al., 2017) task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle and high school students. In RACE, each passage is associated with multiple questions. For every question, the task is to select one correct an- swer from four options. RACE has signiﬁcantly longer context than other popular reading compre- hension datasets and the proportion of questions that requires reasoning is very large."}
{"doc_id": "1907.11692", "para_id": 40, "text": "Results Table 1 compares the published BERTBASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking. We ﬁnd that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efﬁciency beneﬁts of dynamic masking, we use dynamic masking in the remainder of the experiments."}
{"doc_id": "1907.11692", "para_id": 41, "text": "4.2 Model Input Format and Next Sentence Prediction"}
{"doc_id": "1907.11692", "para_id": 42, "text": "This section explores and quantiﬁes which choices are important for successfully pretraining BERT models. We keep the model architecture ﬁxed.7"}
{"doc_id": "1907.11692", "para_id": 43, "text": "In the original BERT pretraining procedure, the model observes two concatenated document seg- ments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked lan- guage modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss. The NSP loss was hypothesized to be an impor- tant factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with signiﬁcant performance degradation on QNLI, MNLI, and SQuAD 1.1. However, some recent work has questioned the necessity of the NSP loss (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019). To better understand this discrepancy, we com- pare several alternative training formats:"}
{"doc_id": "1907.11692", "para_id": 44, "text": "Speciﬁcally, we begin by training BERT models with the same conﬁguration as BERTBASE (L = 12, H = 768, A = 12, 110M params)."}
{"doc_id": "1907.11692", "para_id": 45, "text": "As discussed in Section 2, BERT relies on ran- domly masking and predicting tokens. The orig- inal BERT implementation performed masking once during data preprocessing, resulting in a sin- gle static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic mask- ing where we generate the masking pattern every time we feed a sequence to the model. This be- comes crucial when pretraining for more steps or with larger datasets."}
{"doc_id": "1907.11692", "para_id": 46, "text": "• SEGMENT-PAIR+NSP: This follows the original input format used in BERT (Devlin et al., 2019), with the NSP loss. Each input has a pair of seg- ments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens."}
{"doc_id": "1907.11692", "para_id": 47, "text": "7Studying architectural changes, including larger archi- tectures, is an important area for future work."}
{"doc_id": "1907.11692", "para_id": 48, "text": "BERTBASE 88.5/76.3 84.3 92.8 64.3 XLNetBASE (K = 7) –/81.3 85.8 92.7 66.1 XLNetBASE (K = 6) –/81.0 85.6 93.4 66.7"}
{"doc_id": "1907.11692", "para_id": 49, "text": "Table 2: Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are trained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m, SST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERTBASE and XLNetBASE are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 50, "text": "• SENTENCE-PAIR+NSP: Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. Since these inputs are sig- niﬁcantly shorter than 512 tokens, we increase the batch size so that the total number of tokens remains similar to SEGMENT-PAIR+NSP. We re- tain the NSP loss."}
{"doc_id": "1907.11692", "para_id": 51, "text": "We next compare training without the NSP loss and training with blocks of text from a sin- gle document (DOC-SENTENCES). We ﬁnd that this setting outperforms the originally published BERTBASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019). It is possible that the original BERT implementa- tion may only have removed the loss term while still retaining the SEGMENT-PAIR input format. Finally we ﬁnd that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULL-"}
{"doc_id": "1907.11692", "para_id": 52, "text": "• FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one doc- ument, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss."}
{"doc_id": "1907.11692", "para_id": 53, "text": "SENTENCES in the remainder of our experiments for easier comparison with related work."}
{"doc_id": "1907.11692", "para_id": 54, "text": "• DOC-SENTENCES: Inputs are constructed sim- ilarly to FULL-SENTENCES, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically in- crease the batch size in these cases to achieve a similar number of total tokens as FULL-"}
{"doc_id": "1907.11692", "para_id": 55, "text": "Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately (Ott et al., 2018). Recent work has shown that BERT is also amenable to large batch training (You et al., 2019)."}
{"doc_id": "1907.11692", "para_id": 56, "text": "Results Table 2 shows results for the four dif- ferent settings. We ﬁrst compare the original"}
{"doc_id": "1907.11692", "para_id": 57, "text": "Devlin et al. (2019) originally trained BERTBASE for 1M steps with a batch size of 256 sequences. This is equivalent in computa- tional cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K. In Table 3 we compare perplexity and end-"}
{"doc_id": "1907.11692", "para_id": 58, "text": "SEGMENT-PAIR input format from Devlin et al. (2019) to the SENTENCE-PAIR format; both for- mats retain the NSP loss, but the latter uses sin- gle sentences. We ﬁnd that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies."}
{"doc_id": "1907.11692", "para_id": 59, "text": "The original BERT implementa- tion (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tok- enization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K sub- word units, without any additional preprocessing or tokenization of the input. This adds approxi- mately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight dif- ferences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks. Nev- ertheless, we believe the advantages of a univer- sal encoding scheme outweighs the minor degre- dation in performance and use this encoding in the remainder of our experiments. A more de- tailed comparison of these encodings is left to fu- ture work."}
{"doc_id": "1907.11692", "para_id": 60, "text": "256 1M 1e-4 3.99 84.7 92.7 2K 125K 7e-4 3.68 85.2 92.9 8K 31K 1e-3 3.77 84.6 92.8"}
{"doc_id": "1907.11692", "para_id": 61, "text": "Table 3: Perplexity on held-out training data (ppl) and development set accuracy for base models trained over BOOKCORPUS and WIKIPEDIA with varying batch sizes (bsz). We tune the learning rate (lr) for each set- ting. Models make the same number of passes over the data (epochs) and have the same computational cost."}
{"doc_id": "1907.11692", "para_id": 62, "text": "task performance of BERTBASE as we increase the batch size, controlling for the number of passes through the training data. We observe that train- ing with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training,8"}
{"doc_id": "1907.11692", "para_id": 63, "text": "and in later experiments we train with batches of 8K sequences. Notably You et al. (2019) train BERT with even larger batche sizes, up to 32K sequences. We leave further exploration of the limits of large batch training to future work."}
{"doc_id": "1907.11692", "para_id": 64, "text": "In the previous section we propose modiﬁcations to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined im- pact. We call this conﬁguration RoBERTa for Robustly optimized BERT approach. Speciﬁ- cally, RoBERTa is trained with dynamic mask- ing (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other impor- tant factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained us- ing nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many op- timization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these fac- tors from other modeling choices (e.g., the pre- training objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOK- CORPUS plus WIKIPEDIA dataset as was used in"}
{"doc_id": "1907.11692", "para_id": 65, "text": "Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level rep- resentations that allows handling the large vocab- ularies common in natural language corpora. In- stead of full words, BPE relies on subwords units, which are extracted by performing statistical anal- ysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode char- acters can account for a sizeable portion of this vocabulary when modeling large and diverse cor- pora, such as the ones considered in this work. Radford et al. (2019) introduce a clever imple- mentation of BPE that uses bytes instead of uni- code characters as the base subword units. Using bytes makes it possible to learn a subword vocab- ulary of a modest size (50K units) that can still en- code any input text without introducing any “un- known” tokens."}
{"doc_id": "1907.11692", "para_id": 66, "text": "8Large batch training can improve training efﬁciency even without large scale parallel hardware through gradient ac- cumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ (Ott et al., 2019)."}
{"doc_id": "1907.11692", "para_id": 67, "text": "Model data bsz steps SQuAD MNLI-m SST-2 (v1.1/2.0)"}
{"doc_id": "1907.11692", "para_id": 68, "text": "RoBERTa with BOOKS + WIKI 16GB 8K 100K 93.6/87.3 89.0 95.3 + additional data (§3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6 + pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1 + pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4"}
{"doc_id": "1907.11692", "para_id": 69, "text": "BERTLARGE with BOOKS + WIKI 13GB 256 1M 90.9/81.8 86.6 93.7 XLNetLARGE with BOOKS + WIKI 13GB 256 1M 94.0/87.8 88.4 94.4 + additional data 126GB 2K 500K 94.5/88.8 89.8 95.6"}
{"doc_id": "1907.11692", "para_id": 70, "text": "Table 4: Development set results for RoBERTa as we pretrain over more data (16GB →160GB of text) and pretrain for longer (100K →300K →500K steps). Each row accumulates improvements from the rows above. RoBERTa matches the architecture and training objective of BERTLARGE. Results for BERTLARGE and XLNetLARGE are from Devlin et al. (2019) and Yang et al. (2019), respectively. Complete results on all GLUE tasks can be found in the Appendix."}
{"doc_id": "1907.11692", "para_id": 71, "text": "we consider RoBERTa trained for 500K steps over all ﬁve of the datasets introduced in Section 3.2."}
{"doc_id": "1907.11692", "para_id": 72, "text": "Devlin et al. (2019). We pretrain our model using 1024 V100 GPUs for approximately one day."}
{"doc_id": "1907.11692", "para_id": 73, "text": "Results We present our results in Table 4. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reafﬁrming the importance of the design choices we explored in Section 4. Next, we combine this data with the three ad- ditional datasets described in Section 3.2. We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We ob- serve further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9"}
{"doc_id": "1907.11692", "para_id": 74, "text": "For GLUE we consider two ﬁnetuning settings. In the ﬁrst setting (single-task, dev) we ﬁnetune RoBERTa separately for each of the GLUE tasks, using only the training data for the correspond- ing task. We consider a limited hyperparameter sweep for each task, with batch sizes ∈{16, 32} and learning rates ∈{1e−5, 2e−5, 3e−5}, with a linear warmup for the ﬁrst 6% of steps followed by a linear decay to 0. We ﬁnetune for 10 epochs and perform early stopping based on each task’s eval- uation metric on the dev set. The rest of the hyper- parameters remain the same as during pretraining. In this setting, we report the median development set results for each task over ﬁve random initial- izations, without model ensembling."}
{"doc_id": "1907.11692", "para_id": 75, "text": "Finally, we pretrain RoBERTa for signiﬁcantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe signiﬁcant gains in downstream task performance, and the 300K and 500K step mod- els outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overﬁt our data and would likely beneﬁt from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different bench- marks: GLUE, SQuaD and RACE. Speciﬁcally"}
{"doc_id": "1907.11692", "para_id": 76, "text": "In the second setting (ensembles, test), we com- pare RoBERTa to other approaches on the test set via the GLUE leaderboard. While many submis- sions to the GLUE leaderboard depend on multi- task ﬁnetuning, our submission depends only on single-task ﬁnetuning. For RTE, STS and MRPC we found it helpful to ﬁnetune starting from the MNLI single-task model, rather than the baseline pretrained RoBERTa. We explore a slightly wider hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models per task."}
{"doc_id": "1907.11692", "para_id": 77, "text": "9Our experiments conﬂate increases in data size and di- versity. We leave a more careful analysis of these two dimen- sions to future work."}
{"doc_id": "1907.11692", "para_id": 78, "text": "Single-task single models on dev BERTLARGE 86.6/- 92.3 91.3 70.4 93.2 88.0 60.6 90.0 - - XLNetLARGE 89.8/- 93.9 91.8 83.8 95.6 89.2 63.6 91.8 - - RoBERTa 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 91.3 -"}
{"doc_id": "1907.11692", "para_id": 79, "text": "Ensembles on test (from leaderboard as of July 25, 2019) ALICE 88.2/87.9 95.7 90.7 83.5 95.2 92.6 68.6 91.1 80.8 86.3 MT-DNN 87.9/87.4 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0 87.6 XLNet 90.2/89.8 98.6 90.3 86.3 96.8 93.0 67.8 91.6 90.4 88.4 RoBERTa 90.8/90.2 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0 88.5"}
{"doc_id": "1907.11692", "para_id": 80, "text": "Table 5: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE and XLNetLARGE results are from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the development set are a median over ﬁve runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and MRPC we ﬁnetune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained from the GLUE leaderboard."}
{"doc_id": "1907.11692", "para_id": 81, "text": "Results We present our results in Table 5. In the ﬁrst setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets. Crucially, RoBERTa uses the same masked language modeling pretrain- ing objective and architecture as BERTLARGE, yet consistently outperforms both BERTLARGE and XLNetLARGE. This raises questions about the rel- ative importance of model architecture and pre- training objective, compared to more mundane de- tails like dataset size and training time that we ex- plore in this work. In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. This is espe- cially exciting because RoBERTa does not depend on multi-task ﬁnetuning, unlike most of the other top submissions. We expect future work may fur- ther improve these results by incorporating more sophisticated multi-task ﬁnetuning procedures."}
{"doc_id": "1907.11692", "para_id": 82, "text": "Task-speciﬁc modiﬁcations Two of the GLUE tasks require task-speciﬁc ﬁnetuning approaches to achieve competitive leaderboard results. QNLI: Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classiﬁed as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation signiﬁcantly simpliﬁes the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classiﬁcation approach. WNLI: We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from Super- GLUE (Wang et al., 2019a), which indicates the span of the query pronoun and referent. We ﬁne- tune RoBERTa using the margin ranking loss from Kocijan et al. (2019). For a given input sentence, we use spaCy (Honnibal and Montani, 2017) to extract additional candidate noun phrases from the sentence and ﬁnetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases. One unfortunate consequence of this formulation is that we can only make use of the positive train- ing examples, which excludes over half of the pro- vided training examples.10"}
{"doc_id": "1907.11692", "para_id": 83, "text": "We adopt a much simpler approach for SQuAD compared to past work. In particular, while both BERT (Devlin et al., 2019) and XL- Net (Yang et al., 2019) augment their training data with additional QA datasets, we only ﬁnetune RoBERTa using the provided SQuAD training data. Yang et al. (2019) also employed a custom layer-wise learning rate schedule to ﬁnetune"}
{"doc_id": "1907.11692", "para_id": 84, "text": "results could potentially be improved by augmenting this with additional pronoun disambiguation datasets."}
{"doc_id": "1907.11692", "para_id": 85, "text": "10While we only use the provided WNLI training data, our"}
{"doc_id": "1907.11692", "para_id": 86, "text": "Single models on test (as of July 25, 2019) BERTLARGE 72.0 76.6 70.1 XLNetLARGE 81.7 85.4 80.2"}
{"doc_id": "1907.11692", "para_id": 87, "text": "Single models on dev, w/o data augmentation BERTLARGE 84.1 90.9 79.0 81.8 XLNetLARGE 89.0 94.5 86.1 88.8 RoBERTa 88.9 94.6 86.5 89.4"}
{"doc_id": "1907.11692", "para_id": 88, "text": "Single models on test (as of July 25, 2019) XLNetLARGE 86.3† 89.1†"}
{"doc_id": "1907.11692", "para_id": 89, "text": "Table 7: Results on the RACE test set. BERTLARGE and XLNetLARGE results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 90, "text": "RoBERTa 86.8 89.8 XLNet + SG-Net Veriﬁer 87.0† 89.9†"}
{"doc_id": "1907.11692", "para_id": 91, "text": "nating each candidate answer with the correspond- ing question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens."}
{"doc_id": "1907.11692", "para_id": 92, "text": "Table 6: Results on SQuAD. † indicates results that de- pend on additional external training data. RoBERTa uses only the provided SQuAD data in both dev and test settings. BERTLARGE and XLNetLARGE results are from Devlin et al. (2019) and Yang et al. (2019), re- spectively."}
{"doc_id": "1907.11692", "para_id": 93, "text": "XLNet, while we use the same learning rate for all layers. For SQuAD v1.1 we follow the same ﬁnetun- ing procedure as Devlin et al. (2019). For SQuAD v2.0, we additionally classify whether a given question is answerable; we train this classiﬁer jointly with the span predictor by summing the classiﬁcation and span loss terms."}
{"doc_id": "1907.11692", "para_id": 94, "text": "Results on the RACE test sets are presented in Table 7. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings."}
{"doc_id": "1907.11692", "para_id": 95, "text": "Pretraining methods have been designed with different training objectives, includ- ing language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019). Many recent papers have used a basic recipe of ﬁnetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018), and pretraining with some variant of a masked language model objective. However, newer methods have improved performance by multi-task ﬁne tun- ing (Dong et al., 2019), incorporating entity embeddings (Sun et al., 2019), span predic- tion (Joshi et al., 2019), and multiple variants of autoregressive pretraining (Song et al., 2019; Chan et al., 2019; Yang et al., 2019). Perfor- mance is also typically improved by training bigger models on more data (Devlin et al., 2019; Baevski et al., 2019; Yang et al., 2019; Radford et al., 2019). Our goal was to replicate, simplify, and better tune the training of BERT, as a reference point for better understanding the relative performance of all of these methods."}
{"doc_id": "1907.11692", "para_id": 96, "text": "Results We present our results in Table 6. On the SQuAD v1.1 development set, RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance rel- ative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation."}
{"doc_id": "1907.11692", "para_id": 97, "text": "In RACE, systems are provided with a passage of text, an associated question, and four candidate an- swers. Systems are required to classify which of the four candidate answers is correct. We modify RoBERTa for this task by concate-"}
{"doc_id": "1907.11692", "para_id": 98, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evalu- ating predictive uncertainty, visual object classiﬁca- tion, and recognising tectual entailment."}
{"doc_id": "1907.11692", "para_id": 99, "text": "We carefully evaluate a number of design de- cisions when pretraining BERT models. We ﬁnd that performance can be substantially im- proved by training the model longer, with bigger batches over more data; removing the next sen- tence prediction objective; training on longer se- quences; and dynamically changing the masking pattern applied to the training data. Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task ﬁnetuning for GLUE or additional data for SQuAD. These re- sults illustrate the importance of these previ- ously overlooked design decisions and suggest that BERT’s pretraining objective remains com- petitive with recently proposed alternatives. We additionally use a novel dataset, CC-NEWS, and release our models and code for pretraining and ﬁnetuning at: https://github.com/pytorch/fairseq."}
{"doc_id": "1907.11692", "para_id": 100, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in Neural Informa- tion Processing Systems (NIPS)."}
{"doc_id": "1907.11692", "para_id": 101, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In North American Association for Com- putational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 102, "text": "William B Dolan and Chris Brockett. 2005. Auto- matically constructing a corpus of sentential para- phrases. In Proceedings of the International Work- shop on Paraphrasing."}
{"doc_id": "1907.11692", "para_id": 103, "text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Uniﬁed language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197."}
{"doc_id": "1907.11692", "para_id": 104, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recog- nizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing."}
{"doc_id": "1907.11692", "para_id": 105, "text": "Eneko Agirre, Llu’is M‘arquez, and Richard Wicen- towski, editors. 2007. Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)."}
{"doc_id": "1907.11692", "para_id": 106, "text": "Aaron Gokaslan and Vanya Cohen. 2019. Openweb- text corpus. http://web.archive.org/ save/http://Skylion007.github.io/ OpenWebTextCorpus."}
{"doc_id": "1907.11692", "para_id": 107, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. 2019. Cloze- driven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785."}
{"doc_id": "1907.11692", "para_id": 108, "text": "Felix Hamborg, Norman Meuschke, Corinna Bre- itinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science."}
{"doc_id": "1907.11692", "para_id": 109, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognis- ing textual entailment."}
{"doc_id": "1907.11692", "para_id": 110, "text": "Dan Hendrycks and Kevin Gimpel. 2016. Gaus- sian error linear units (gelus). arXiv preprint arXiv:1606.08415."}
{"doc_id": "1907.11692", "para_id": 111, "text": "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The ﬁfth PASCAL recognizing textual entailment chal- lenge."}
{"doc_id": "1907.11692", "para_id": 112, "text": "Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed- dings, convolutional neural networks and incremen- tal parsing. To appear."}
{"doc_id": "1907.11692", "para_id": 113, "text": "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large anno- tated corpus for learning natural language inference. In Empirical Methods in Natural Language Process- ing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 114, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint arXiv:1801.06146."}
{"doc_id": "1907.11692", "para_id": 115, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Cser- nai. 2016. First quora dataset release: Question pairs. https://data.quora.com/First- Quora-Dataset-Release-Question- Pairs."}
{"doc_id": "1907.11692", "para_id": 116, "text": "William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. 2019. KERMIT: Gener- ative insertion-based modeling for sequences. arXiv preprint arXiv:1906.01604."}
{"doc_id": "1907.11692", "para_id": 117, "text": "Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine trans- lation. In Proceedings of the Third Conference on Machine Translation (WMT)."}
{"doc_id": "1907.11692", "para_id": 118, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2019. SpanBERT: Improving pre-training by repre- senting and predicting spans. arXiv preprint arXiv:1907.10529."}
{"doc_id": "1907.11692", "para_id": 119, "text": "Adam Paszke, Sam Gross, Soumith Chintala, Gre- gory Chanan, Edward Yang, Zachary DeVito, Zem- ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop."}
{"doc_id": "1907.11692", "para_id": 120, "text": "Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR)."}
{"doc_id": "1907.11692", "para_id": 121, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. 2019. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290."}
{"doc_id": "1907.11692", "para_id": 122, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In North American Association for Com- putational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 123, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683."}
{"doc_id": "1907.11692", "para_id": 124, "text": "Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. 2018. Improving language un- derstanding with unsupervised learning. Technical report, OpenAI."}
{"doc_id": "1907.11692", "para_id": 125, "text": "Guillaume Lample and Alexis Conneau. 2019. Cross- lingual language model pretraining. arXiv preprint arXiv:1901.07291."}
{"doc_id": "1907.11692", "para_id": 126, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report, OpenAI."}
{"doc_id": "1907.11692", "para_id": 127, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The Winograd schema challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning."}
{"doc_id": "1907.11692", "para_id": 128, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable ques- tions for squad. In Association for Computational Linguistics (ACL)."}
{"doc_id": "1907.11692", "para_id": 129, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482."}
{"doc_id": "1907.11692", "para_id": 130, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Meth- ods in Natural Language Processing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 131, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian- feng Gao. 2019b. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504."}
{"doc_id": "1907.11692", "para_id": 132, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Association for Computational Linguistics (ACL), pages 1715–1725."}
{"doc_id": "1907.11692", "para_id": 133, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In Advances in Neural In- formation Processing Systems (NIPS), pages 6297– 6308."}
{"doc_id": "1907.11692", "para_id": 134, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Empirical Methods in Natural Language Processing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 135, "text": "Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed preci- sion training. In International Conference on Learn- ing Representations."}
{"doc_id": "1907.11692", "para_id": 136, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. In International Conference on Machine Learning (ICML)."}
{"doc_id": "1907.11692", "para_id": 137, "text": "Sebastian Nagel. 2016. Cc-news. http: //web.archive.org/save/http: //commoncrawl.org/2016/10/news- dataset-available."}
{"doc_id": "1907.11692", "para_id": 138, "text": "Yu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi- ang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: En- hanced representation through knowledge integra- tion. arXiv preprint arXiv:1904.09223."}
{"doc_id": "1907.11692", "para_id": 139, "text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. FAIRSEQ: A fast, exten- sible toolkit for sequence modeling. In North American Association for Computational Linguis- tics (NAACL): System Demonstrations."}
{"doc_id": "1907.11692", "para_id": 140, "text": "Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847."}
{"doc_id": "1907.11692", "para_id": 141, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems."}
{"doc_id": "1907.11692", "para_id": 142, "text": "Table 9 describes the hyperparameters for pre- training of RoBERTaLARGE and RoBERTaBASE"}
{"doc_id": "1907.11692", "para_id": 143, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537."}
{"doc_id": "1907.11692", "para_id": 144, "text": "Finetuning hyperparameters for RACE, SQuAD and GLUE are given in Table 10. We select the best hyperparameter values based on the median of 5 random seeds for each task."}
{"doc_id": "1907.11692", "para_id": 145, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Inter- national Conference on Learning Representations (ICLR)."}
{"doc_id": "1907.11692", "para_id": 146, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint 1805.12471."}
{"doc_id": "1907.11692", "para_id": 147, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In North American Association for Computational Linguis- tics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 148, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237."}
{"doc_id": "1907.11692", "para_id": 149, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. 2019. Reduc- ing bert pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962."}
{"doc_id": "1907.11692", "para_id": 150, "text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. arXiv preprint arXiv:1905.12616."}
{"doc_id": "1907.11692", "para_id": 151, "text": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watch- ing movies and reading books. In arXiv preprint arXiv:1506.06724."}
{"doc_id": "1907.11692", "para_id": 152, "text": "Appendix for “RoBERTa: A Robustly Optimized BERT Pretraining Approach”"}
{"doc_id": "1907.11692", "para_id": 153, "text": "In Table 8 we present the full set of development set results for RoBERTa. We present results for a LARGE conﬁguration that follows BERTLARGE, as well as a BASE conﬁguration that follows BERTBASE."}
{"doc_id": "1907.11692", "para_id": 154, "text": "RoBERTaBASE + all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2"}
{"doc_id": "1907.11692", "para_id": 155, "text": "RoBERTaLARGE with BOOKS + WIKI 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6 + additional data (§3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2 + pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3 + pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4"}
{"doc_id": "1907.11692", "para_id": 156, "text": "Table 8: Development set results on GLUE tasks for various conﬁgurations of RoBERTa."}
{"doc_id": "1907.11692", "para_id": 157, "text": "Number of Layers 24 12 Hidden size 1024 768 FFN inner hidden size 4096 3072 Attention heads 16 12 Attention head size 64 64 Dropout 0.1 0.1 Attention Dropout 0.1 0.1 Warmup Steps 30k 24k Peak Learning Rate 4e-4 6e-4 Batch Size 8k 8k Weight Decay 0.01 0.01 Max Steps 500k 500k Learning Rate Decay Linear Linear Adam ǫ 1e-6 1e-6 Adam β1 0.9 0.9 Adam β2 0.98 0.98 Gradient Clipping 0.0 0.0"}
{"doc_id": "1907.11692", "para_id": 158, "text": "Table 9: Hyperparameters for pretraining RoBERTaLARGE and RoBERTaBASE."}
{"doc_id": "1907.11692", "para_id": 159, "text": "Learning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5} Batch Size 16 48 {16, 32} Weight Decay 0.1 0.01 0.1 Max Epochs 4 2 10 Learning Rate Decay Linear Linear Linear Warmup ratio 0.06 0.06 0.06"}
{"doc_id": "1907.11692", "para_id": 160, "text": "Table 10: Hyperparameters for ﬁnetuning RoBERTaLARGE on RACE, SQuAD and GLUE."}
{"doc_id": "2005.14165", "para_id": 0, "text": "Tom B. Brown∗ Benjamin Mann∗ Nick Ryder∗ Melanie Subbiah∗"}
{"doc_id": "2005.14165", "para_id": 1, "text": "Jared Kaplan† Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry"}
{"doc_id": "2005.14165", "para_id": 2, "text": "Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan"}
{"doc_id": "2005.14165", "para_id": 3, "text": "Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter"}
{"doc_id": "2005.14165", "para_id": 4, "text": "Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray"}
{"doc_id": "2005.14165", "para_id": 5, "text": "Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei"}
{"doc_id": "2005.14165", "para_id": 6, "text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne- tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general."}
{"doc_id": "2005.14165", "para_id": 7, "text": "∗Equal contribution †Johns Hopkins University, OpenAI"}
{"doc_id": "2005.14165", "para_id": 8, "text": "2 Approach 6 2.1 Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10"}
{"doc_id": "2005.14165", "para_id": 9, "text": "3 Results 10 3.1 Language Modeling, Cloze, and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Winograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.7 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.8 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"}
{"doc_id": "2005.14165", "para_id": 10, "text": "4 Measuring and Preventing Memorization Of Benchmarks 29"}
{"doc_id": "2005.14165", "para_id": 11, "text": "6 Broader Impacts 34 6.1 Misuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"}
{"doc_id": "2005.14165", "para_id": 12, "text": "E Human Quality Assessment of Synthetic News Articles 46"}
{"doc_id": "2005.14165", "para_id": 13, "text": "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18]."}
{"doc_id": "2005.14165", "para_id": 14, "text": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve strong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands of examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons."}
{"doc_id": "2005.14165", "para_id": 15, "text": "First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difﬁcult to collect a large supervised training dataset, especially when the process must be repeated for every new task."}
{"doc_id": "2005.14165", "para_id": 16, "text": "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus ﬁne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then ﬁne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly speciﬁc to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of ﬁne-tuned models on speciﬁc benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19]."}
{"doc_id": "2005.14165", "para_id": 17, "text": "Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often"}
{"doc_id": "2005.14165", "para_id": 18, "text": "Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence."}
{"doc_id": "2005.14165", "para_id": 19, "text": "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks."}
{"doc_id": "2005.14165", "para_id": 20, "text": "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality."}
{"doc_id": "2005.14165", "para_id": 21, "text": "One potential route towards addressing these issues is meta-learning1 – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next."}
{"doc_id": "2005.14165", "para_id": 22, "text": "While it has shown some initial promise, this approach still achieves results far inferior to ﬁne-tuning – for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks."}
{"doc_id": "2005.14165", "para_id": 23, "text": "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and ﬁnally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale."}
{"doc_id": "2005.14165", "para_id": 24, "text": "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous: the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer loop structure."}
{"doc_id": "2005.14165", "para_id": 25, "text": "Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proﬁcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite."}
{"doc_id": "2005.14165", "para_id": 26, "text": "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Speciﬁcally, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional ﬁne-tuning setting, but we leave this to future work."}
{"doc_id": "2005.14165", "para_id": 27, "text": "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or ﬁne-tuning, just increasing numbers of demonstrations given as conditioning."}
{"doc_id": "2005.14165", "para_id": 28, "text": "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by ﬁne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to ﬁne-tuned models operating in the same closed-book setting."}
{"doc_id": "2005.14165", "para_id": 29, "text": "GPT-3 also displays one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them deﬁned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difﬁculty distinguishing from human-generated articles."}
{"doc_id": "2005.14165", "para_id": 30, "text": "At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed."}
{"doc_id": "2005.14165", "para_id": 31, "text": "A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself)."}
{"doc_id": "2005.14165", "para_id": 32, "text": "We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inﬂating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity."}
{"doc_id": "2005.14165", "para_id": 33, "text": "In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we ﬁnd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proﬁcient meta-learners."}
{"doc_id": "2005.14165", "para_id": 34, "text": "Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard."}
{"doc_id": "2005.14165", "para_id": 35, "text": "The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes."}
{"doc_id": "2005.14165", "para_id": 36, "text": "Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly deﬁning and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-speciﬁc data they tend to rely on. Speciﬁcally, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration):"}
{"doc_id": "2005.14165", "para_id": 37, "text": "• Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset speciﬁc to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of ﬁne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not ﬁne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be ﬁne-tuned in principle and this is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 38, "text": "• Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one ﬁnal example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can ﬁt in the model’s context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speciﬁc data and reduced potential to learn an overly narrow distribution from a large but narrow ﬁne-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art ﬁne-tuned models. Also, a small amount of task speciﬁc data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL+16] – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task."}
{"doc_id": "2005.14165", "para_id": 39, "text": "• One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difﬁcult to communicate the content or format of a task if no examples are given."}
{"doc_id": "2005.14165", "para_id": 40, "text": "Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning. The panels above show four methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G."}
{"doc_id": "2005.14165", "para_id": 41, "text": "• Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difﬁcult for humans to understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”. For example, if someone is asked to “make a table of world records for the 200m dash”, this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clariﬁcation, understanding precisely what is desired can be difﬁcult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1, a human would likely know what to do from just the text instruction."}
{"doc_id": "2005.14165", "para_id": 42, "text": "Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work."}
{"doc_id": "2005.14165", "para_id": 43, "text": "Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations."}
{"doc_id": "2005.14165", "para_id": 44, "text": "Model Name nparams nlayers dmodel nheads dhead Batch Size Learning Rate"}
{"doc_id": "2005.14165", "para_id": 45, "text": "GPT-3 175B or “GPT-3” 175.0B 96 12288 96 128 3.2M 0.6 × 10−4"}
{"doc_id": "2005.14165", "para_id": 46, "text": "Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens."}
{"doc_id": "2005.14165", "para_id": 47, "text": "We use the same model and architecture as GPT-2 [RWC+19], including the modiﬁed initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks."}
{"doc_id": "2005.14165", "para_id": 48, "text": "Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dﬀ= 4 ∗dmodel), and dhead is the dimension of each attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efﬁciency and load-balancing in the layout of models across GPU’s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range."}
{"doc_id": "2005.14165", "para_id": 49, "text": "Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting nearly a trillion words. This size of dataset is sufﬁcient to train our largest models without ever updating on the same sequence twice. However, we have found that unﬁltered or lightly ﬁltered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and ﬁltered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overﬁtting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity."}
{"doc_id": "2005.14165", "para_id": 50, "text": "Details of the ﬁrst two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and ﬁrst described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia."}
{"doc_id": "2005.14165", "para_id": 51, "text": "Table 2.2 shows the ﬁnal mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before ﬁltering and 570GB after ﬁltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overﬁtting in exchange for higher quality training data."}
{"doc_id": "2005.14165", "para_id": 52, "text": "Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models [KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaﬂop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D."}
{"doc_id": "2005.14165", "para_id": 53, "text": "Dataset Quantity (tokens) Weight in training mix Epochs elapsed when training for 300B tokens"}
{"doc_id": "2005.14165", "para_id": 54, "text": "Common Crawl (ﬁltered) 410 billion 60% 0.44 WebText2 19 billion 22% 2.9 Books1 12 billion 8% 1.9 Books2 55 billion 8% 0.43 Wikipedia 3 billion 3% 3.4"}
{"doc_id": "2005.14165", "para_id": 55, "text": "Table 2.2: Datasets used to train GPT-3. “Weight in training mix” refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once."}
{"doc_id": "2005.14165", "para_id": 56, "text": "A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the ﬁltering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination."}
{"doc_id": "2005.14165", "para_id": 57, "text": "As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B."}
{"doc_id": "2005.14165", "para_id": 58, "text": "For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it."}
{"doc_id": "2005.14165", "para_id": 59, "text": "K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048 for all models and typically ﬁts 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations."}
{"doc_id": "2005.14165", "para_id": 60, "text": "On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional beneﬁt as measured on the development set by normalizing by the unconditional probability of each completion, by computing P (completion|context) P (completion|answer context), where answer context is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic."}
{"doc_id": "2005.14165", "para_id": 61, "text": "On tasks that involve binary classiﬁcation, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details."}
{"doc_id": "2005.14165", "para_id": 62, "text": "On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of α = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand."}
{"doc_id": "2005.14165", "para_id": 63, "text": "Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to ﬁt on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else."}
{"doc_id": "2005.14165", "para_id": 64, "text": "In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efﬁcient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks."}
{"doc_id": "2005.14165", "para_id": 65, "text": "Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks."}
{"doc_id": "2005.14165", "para_id": 66, "text": "In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieﬂy explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-ﬂy reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings."}
{"doc_id": "2005.14165", "para_id": 67, "text": "Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this ﬁgure, we exclude embedding parameters from compute and parameter counts."}
{"doc_id": "2005.14165", "para_id": 68, "text": "Table 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3’s training data. a[RWC+19]"}
{"doc_id": "2005.14165", "para_id": 69, "text": "3.1 Language Modeling, Cloze, and Completion Tasks"}
{"doc_id": "2005.14165", "para_id": 70, "text": "In this section we test GPT-3’s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text."}
{"doc_id": "2005.14165", "para_id": 71, "text": "We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [MKM+94] dataset measured in [RWC+19]. We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to deﬁne one-shot or few-shot evaluation around, so we measure only zero-shot."}
{"doc_id": "2005.14165", "para_id": 72, "text": "The LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text – the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difﬁcult benchmark. [BHT+20] reﬂect on the small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP+19]"}
{"doc_id": "2005.14165", "para_id": 73, "text": "Setting LAMBADA (acc) LAMBADA (ppl) StoryCloze (acc) HellaSwag (acc)"}
{"doc_id": "2005.14165", "para_id": 74, "text": "SOTA 68.0a 8.63b 91.8c 85.6d GPT-3 Zero-Shot 76.2 3.00 83.2 78.9 GPT-3 One-Shot 72.5 3.35 84.7 78.1 GPT-3 Few-Shot 86.4 1.92 87.7 79.3"}
{"doc_id": "2005.14165", "para_id": 75, "text": "Table 3.2: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on LAMBADA while achieving respectable performance on two difﬁcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19] d[LCH+20]"}
{"doc_id": "2005.14165", "para_id": 76, "text": "Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text."}
{"doc_id": "2005.14165", "para_id": 77, "text": "and [Tur20]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path forward”. We ﬁnd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art."}
{"doc_id": "2005.14165", "para_id": 78, "text": "LAMBADA is also a demonstration of the ﬂexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word ﬁlters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following ﬁll-in-the-blank format:"}
{"doc_id": "2005.14165", "para_id": 79, "text": "Alice was friends with Bob. Alice went to visit her friend . →Bob"}
{"doc_id": "2005.14165", "para_id": 80, "text": "George bought some baseball equipment, a ball, a glove, and a . →"}
{"doc_id": "2005.14165", "para_id": 81, "text": "When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the ﬁll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern."}
{"doc_id": "2005.14165", "para_id": 82, "text": "RAG (Fine-tuned, Open-Domain) [LPP+20] 44.5 45.5 68.0 T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] 36.6 44.7 60.5 T5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1 GPT-3 Zero-Shot 14.6 14.4 64.3 GPT-3 One-Shot 23.0 25.3 68.0 GPT-3 Few-Shot 29.9 41.5 71.2"}
{"doc_id": "2005.14165", "para_id": 83, "text": "Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server."}
{"doc_id": "2005.14165", "para_id": 84, "text": "One note of caution is that an analysis of test set contamination identiﬁed that a signiﬁcant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance."}
{"doc_id": "2005.14165", "para_id": 85, "text": "The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difﬁcult for language models while remaining easy for humans (who achieve 95.6% accuracy). GPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the 75.4% accuracy of a ﬁne-tuned 1.5B parameter language model [ZHR+19] but still a fair amount lower than the overall SOTA of 85.6% achieved by the ﬁne-tuned multi-task model ALUM."}
{"doc_id": "2005.14165", "para_id": 86, "text": "We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending sentence for ﬁve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70). This is still 4.1% lower than the ﬁne-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly 10%."}
{"doc_id": "2005.14165", "para_id": 87, "text": "In this section we measure GPT-3’s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to ﬁnd relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted “open-book”. [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as “closed-book”. Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19], WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, ﬁne-tuning on the Q&A dataset itself is also not permitted."}
{"doc_id": "2005.14165", "para_id": 88, "text": "The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the ﬁne-tuned T5-11B by 14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot result improves by 3.7% and matches the SOTA for an open-domain QA system which not only ﬁne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20]. GPT-3’s few-shot result further improves performance another 3.2% beyond this."}
{"doc_id": "2005.14165", "para_id": 89, "text": "On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This compares to 37.4% for ﬁne-tuned T5-11B, and 44.7% for ﬁne-tuned T5-11B+SSM, which uses a Q&A-speciﬁc pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art ﬁne-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions"}
{"doc_id": "2005.14165", "para_id": 90, "text": "Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiﬁcant gains over zero-shot behavior, matching and exceeding the performance of the SOTA ﬁne-tuned open-domain model, RAG [LPP+20]"}
{"doc_id": "2005.14165", "para_id": 91, "text": "and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 92, "text": "On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in the few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very ﬁne-grained knowledge on Wikipedia speciﬁcally which could be testing the limits of GPT-3’s capacity and broad pretraining distribution."}
{"doc_id": "2005.14165", "para_id": 93, "text": "Overall, on one of the three datasets GPT-3’s one-shot matches the open-domain ﬁne-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using ﬁne-tuning. On all 3 datasets, we ﬁnd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reﬂecting the idea that model capacity translates directly to more ‘knowledge’ absorbed in the parameters of the model."}
{"doc_id": "2005.14165", "para_id": 94, "text": "For GPT-2 a ﬁlter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this ﬁltering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based ﬁltering. Although GPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian."}
{"doc_id": "2005.14165", "para_id": 95, "text": "Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren’t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data."}
{"doc_id": "2005.14165", "para_id": 96, "text": "Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for"}
{"doc_id": "2005.14165", "para_id": 97, "text": "SOTA (Supervised) 45.6a 35.0 b 41.2c 40.2d 38.5e 39.9e"}
{"doc_id": "2005.14165", "para_id": 98, "text": "XLM [LC19] 33.4 33.3 26.4 34.3 33.3 31.8 MASS [STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1 mBART [LGG+20] - - 29.8 34.0 35.0 30.5"}
{"doc_id": "2005.14165", "para_id": 99, "text": "GPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9 GPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6 GPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5"}
{"doc_id": "2005.14165", "para_id": 100, "text": "Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reﬂecting its strength as an English LM. We report BLEU scores on the WMT’14 Fr↔En, WMT’16 De↔En, and WMT’16 Ro↔En datasets as measured by multi-bleu.perl with XLM’s tokeniza- tion in order to compare most closely with prior unsupervised NMT work. SacreBLEUf [Pos18] results re- ported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative conﬁdence. a[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]"}
{"doc_id": "2005.14165", "para_id": 101, "text": "Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English."}
{"doc_id": "2005.14165", "para_id": 102, "text": "Fine-tuned SOTA 90.1a 84.6b GPT-3 Zero-Shot 88.3* 70.2 GPT-3 One-Shot 89.7* 73.2 GPT-3 Few-Shot 88.6* 77.7"}
{"doc_id": "2005.14165", "para_id": 103, "text": "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]"}
{"doc_id": "2005.14165", "para_id": 104, "text": "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a ﬁne-tuned RoBERTA-large."}
{"doc_id": "2005.14165", "para_id": 105, "text": "each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could ﬁnd but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b]."}
{"doc_id": "2005.14165", "para_id": 106, "text": "Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 107, "text": "The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently ﬁne-tuned language models have achieved near-human performance on the original Winograd dataset, but more difﬁcult versions"}
{"doc_id": "2005.14165", "para_id": 108, "text": "Setting PIQA ARC (Easy) ARC (Challenge) OpenBookQA"}
{"doc_id": "2005.14165", "para_id": 109, "text": "Fine-tuned SOTA 79.4 92.0[KKS+20] 78.5[KKS+20] 87.2[KKS+20] GPT-3 Zero-Shot 80.5* 68.8 51.4 57.6 GPT-3 One-Shot 80.5* 71.2 53.2 58.8 GPT-3 Few-Shot 82.8* 70.1 51.5 65.4"}
{"doc_id": "2005.14165", "para_id": 110, "text": "Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set."}
{"doc_id": "2005.14165", "para_id": 111, "text": "Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task."}
{"doc_id": "2005.14165", "para_id": 112, "text": "such as the adversarially-mined Winogrande dataset [SBBC19] still signiﬁcantly lag human performance. We test GPT-3’s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting."}
{"doc_id": "2005.14165", "para_id": 113, "text": "On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same “partial evaluation” method described in [RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classiﬁcation and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4)."}
{"doc_id": "2005.14165", "para_id": 114, "text": "On the more difﬁcult Winogrande dataset, we do ﬁnd gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a ﬁne-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a ﬁne-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is 94.0%."}
{"doc_id": "2005.14165", "para_id": 115, "text": "Next we consider three datasets which attempt to capture physical or scientiﬁc reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The ﬁrst, PhysicalQA (PIQA) [BZB+19], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot (the last measured on PIQA’s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a"}
{"doc_id": "2005.14165", "para_id": 116, "text": "Fine-tuned SOTA 90.7a 89.1b 74.4c 93.0d 90.0e 93.1e GPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4 GPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4 GPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1"}
{"doc_id": "2005.14165", "para_id": 117, "text": "Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy. a[JZC+19] b[JN20] c[AI19] d[QIA20] e[SPP+19]"}
{"doc_id": "2005.14165", "para_id": 118, "text": "ﬁne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human performance, but GPT-3’s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis ﬂagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details."}
{"doc_id": "2005.14165", "para_id": 119, "text": "ARC [CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the “Challenge” version of the dataset which has been ﬁltered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a ﬁne-tuned RoBERTa baseline (55.9%) from UniﬁedQA [KKS+20]. On the “Easy” version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a ﬁne-tuned RoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs achieved by the UniﬁedQA which exceeds GPT-3’s few-shot results by 27% on the challenge set and 22% on the easy set."}
{"doc_id": "2005.14165", "para_id": 120, "text": "On OpenBookQA [MCKS18], GPT-3 improves signiﬁcantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3’s few-shot performance is similar to a ﬁne-tuned BERT Large baseline on the leaderboard."}
{"doc_id": "2005.14165", "para_id": 121, "text": "Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiﬁcant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings."}
{"doc_id": "2005.14165", "para_id": 122, "text": "Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset."}
{"doc_id": "2005.14165", "para_id": 123, "text": "GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [DWD+19], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the ﬁne-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems [RLL+19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best ﬁne-tuned result in the original paper. On RACE [LXL+17], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA."}
{"doc_id": "2005.14165", "para_id": 124, "text": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark [WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07] [BDD+09] [PCC18] [PHR+18]. GPT-3’s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC"}
{"doc_id": "2005.14165", "para_id": 125, "text": "Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art ﬁne-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models."}
{"doc_id": "2005.14165", "para_id": 126, "text": "SuperGLUE BoolQ CB CB COPA RTE Average Accuracy Accuracy F1 Accuracy Accuracy"}
{"doc_id": "2005.14165", "para_id": 127, "text": "Fine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5 Fine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7 GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0"}
{"doc_id": "2005.14165", "para_id": 128, "text": "WiC WSC MultiRC MultiRC ReCoRD ReCoRD Accuracy Accuracy Accuracy F1a Accuracy F1"}
{"doc_id": "2005.14165", "para_id": 129, "text": "Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3 Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0 GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1"}
{"doc_id": "2005.14165", "para_id": 130, "text": "Table 3.8: Performance of GPT-3 on SuperGLUE compared to ﬁne-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates."}
{"doc_id": "2005.14165", "para_id": 131, "text": "Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was ﬁne-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was ﬁrst ﬁne-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further ﬁne-tuning on the SuperGLUE training set (for a total of 630K ﬁne-tuning examples). We ﬁnd the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context."}
{"doc_id": "2005.14165", "para_id": 132, "text": "and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated."}
{"doc_id": "2005.14165", "para_id": 133, "text": "We observe a wide range in GPT-3’s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where ﬁrst place is held by a ﬁne-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 134, "text": "WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a ﬁne-tuned 11 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 135, "text": "Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing beneﬁts from in-context learning (Figure 3.8). We scale K up to 32 examples per task, after which point additional examples will not reliably ﬁt into our context. When sweeping over values of K, we ﬁnd that GPT-3 requires less than eight total examples per task to outperform a ﬁne-tuned BERT-Large on overall SuperGLUE score."}
{"doc_id": "2005.14165", "para_id": 136, "text": "Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classiﬁcation problem where the model classiﬁes"}
{"doc_id": "2005.14165", "para_id": 137, "text": "Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2%). We ﬁnd that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 138, "text": "whether the second sentence logically follows from the ﬁrst, contradicts the ﬁrst sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task ﬁne-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset [NWD+19]. ANLI is a difﬁcult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting (∼33%), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difﬁcult task for language models and they are only just beginning to show signs of progress."}
{"doc_id": "2005.14165", "para_id": 139, "text": "One way to probe GPT-3’s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-ﬂy computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3’s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3’s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models."}
{"doc_id": "2005.14165", "para_id": 140, "text": "To test GPT-3’s ability to perform simple arithmetic operations without task-speciﬁc training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:"}
{"doc_id": "2005.14165", "para_id": 141, "text": "• 2 digit addition (2D+) – The model is asked to add two integers sampled uniformly from [0, 100), phrased in the form of a question, e.g. “Q: What is 48 plus 76? A: 124.” • 2 digit subtraction (2D-) – The model is asked to subtract two integers sampled uniformly from [0, 100); the answer may be negative. Example: “Q: What is 34 minus 53? A: -19”. • 3 digit addition (3D+) – Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 142, "text": "Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a signiﬁcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiﬁcant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 143, "text": "• 3 digit subtraction (3D-) – Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 144, "text": "• 4 digit addition (4D+) – Same as 3 digit addition, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 145, "text": "• 4 digit subtraction (4D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 146, "text": "• 5 digit addition (5D+) – Same as 3 digit addition, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 147, "text": "• 5 digit subtraction (5D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 148, "text": "• 2 digit multiplication (2Dx) – The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. “Q: What is 24 times 42? A: 1008”."}
{"doc_id": "2005.14165", "para_id": 149, "text": "• One-digit composite (1DC) – The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, “Q: What is 6+(4*8)? A: 38”. The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}."}
{"doc_id": "2005.14165", "para_id": 150, "text": "In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances."}
{"doc_id": "2005.14165", "para_id": 151, "text": "First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proﬁciency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on ﬁve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations."}
{"doc_id": "2005.14165", "para_id": 152, "text": "As Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time."}
{"doc_id": "2005.14165", "para_id": 153, "text": "One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signiﬁcantly"}
{"doc_id": "2005.14165", "para_id": 154, "text": "GPT-3 Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8 GPT-3 One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3 GPT-3 Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3"}
{"doc_id": "2005.14165", "para_id": 155, "text": "Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signiﬁcant arithmetic abilities."}
{"doc_id": "2005.14165", "para_id": 156, "text": "GPT-3 Zero-shot 3.66 2.28 8.91 8.26 0.09 GPT-3 One-shot 21.7 8.62 25.9 45.4 0.48 GPT-3 Few-shot 37.9 15.1 39.7 67.2 0.44"}
{"doc_id": "2005.14165", "para_id": 157, "text": "Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is “cycle letters in word”, A1 is anagrams of but the ﬁrst and last letters, A2 is anagrams of all but the ﬁrst and last two letters, RI is “Random insertion in word”, RW is “reversed words”."}
{"doc_id": "2005.14165", "para_id": 158, "text": "outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and model capacity scaling for all three settings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 159, "text": "To spot-check whether the model is simply memorizing speciﬁc arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and \"<NUM1> plus <NUM2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a “1”, suggesting it is actually attempting to perform the relevant computation rather than memorizing a table."}
{"doc_id": "2005.14165", "para_id": 160, "text": "Overall, GPT-3 displays reasonable proﬁciency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings."}
{"doc_id": "2005.14165", "para_id": 161, "text": "To test GPT-3’s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 “character manipulation” tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:"}
{"doc_id": "2005.14165", "para_id": 162, "text": "• Cycle letters in word (CL) – The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”."}
{"doc_id": "2005.14165", "para_id": 163, "text": "• Anagrams of all but ﬁrst and last characters (A1) – The model is given a word where every letter except the ﬁrst and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption."}
{"doc_id": "2005.14165", "para_id": 164, "text": "• Anagrams of all but ﬁrst and last 2 characters (A2) – The model is given a word where every letter except the ﬁrst 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt →opponent."}
{"doc_id": "2005.14165", "para_id": 165, "text": "• Random insertion in word (RI) – A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession."}
{"doc_id": "2005.14165", "para_id": 166, "text": "• Reversed words (RW) – The model is given a word spelled backwards, and must output the original word. Example: stcejbo →objects."}
{"doc_id": "2005.14165", "para_id": 167, "text": "For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing"}
{"doc_id": "2005.14165", "para_id": 168, "text": "Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with K = 100."}
{"doc_id": "2005.14165", "para_id": 169, "text": "random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult anagram task (where only the ﬁrst and last letters are held ﬁxed). None of the models can reverse the letters in a word."}
{"doc_id": "2005.14165", "para_id": 170, "text": "In the one-shot setting, performance is signiﬁcantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artiﬁcial nature makes them unlikely to appear in the pre-training data (although we cannot conﬁrm this with certainty)."}
{"doc_id": "2005.14165", "para_id": 171, "text": "We can further quantify performance by plotting “in-context learning curves”, which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions."}
{"doc_id": "2005.14165", "para_id": 172, "text": "Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on signiﬁcant fractions of a word (on average ∼0.7 words per token), so from the LM’s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to ﬁnd the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation."}
{"doc_id": "2005.14165", "para_id": 173, "text": "To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 “SAT analogy” problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is “audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation”. The student is expected to choose which of the ﬁve word pairs has the same relationship as the original word pair; in this example the answer is “sanctimonious is to hypocrisy”. On this task GPT-3 achieves 65.2% in the few-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among college applicants was 57% [TL05] (random guessing yields 20%). As shown in Figure 3.12, the results improve with scale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 174, "text": "Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves 65% accuracy in the few-shot setting, and also demonstrates signiﬁcant gains to in-context learning which are not present in smaller models."}
{"doc_id": "2005.14165", "para_id": 175, "text": "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by conditional sampling from the model given a human-written prompt consisting of a plausible ﬁrst sentence for a news story [RWC+19]. Relative to [RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed ﬁrst sentence of a “news article” as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3’s few-shot learning abilities by providing three previous news articles in the model’s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the “news” genre."}
{"doc_id": "2005.14165", "para_id": 176, "text": "To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al. [ZHR+19]. Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.3"}
{"doc_id": "2005.14165", "para_id": 177, "text": "In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model4. Participants were asked to select whether the article was “very likely written by a human”, “more likely written by a human”, “I don’t know”, “more likely written by a machine”, or “very likely written by a machine”."}
{"doc_id": "2005.14165", "para_id": 178, "text": "The articles we selected were not in the models’ training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a “control model”: a 160M parameter model with no context and increased output randomness."}
{"doc_id": "2005.14165", "para_id": 179, "text": "3This task is also relevant to the potential misuse of language models discussed in Section 6.1. 4We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on participants drawn from the general US population. See Appendix E for details."}
{"doc_id": "2005.14165", "para_id": 180, "text": "Mean accuracy 95% Conﬁdence Interval (low, hi) t compared to control (p-value) “I don’t know” assignments"}
{"doc_id": "2005.14165", "para_id": 181, "text": "Control (deliberately bad model) 86% 83%–90% - 3.6 % GPT-3 Small 76% 72%–80% 3.9 (2e-4) 4.9% GPT-3 Medium 61% 58%–65% 10.3 (7e-21) 6.0% GPT-3 Large 68% 64%–72% 7.3 (3e-11) 8.7% GPT-3 XL 62% 59%–65% 10.7 (1e-19) 7.5% GPT-3 2.7B 62% 58%–65% 10.4 (5e-19) 7.1% GPT-3 6.7B 60% 56%–63% 11.2 (3e-21) 6.2% GPT-3 13B 55% 52%–58% 15.3 (1e-32) 7.1% GPT-3 175B 52% 49%–54% 16.9 (1e-34) 7.8%"}
{"doc_id": "2005.14165", "para_id": 182, "text": "Table 3.11: Human accuracy in identifying whether short (∼200 word) news articles are model generated. We ﬁnd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares mean accuracy between ﬁve different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 183, "text": "Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was ∼86% where 50% is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at ∼52% (see Table 3.11).5 Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6 This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E)."}
{"doc_id": "2005.14165", "para_id": 184, "text": "Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7 Much of the text is—as indicated by the evaluations—difﬁcult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the speciﬁc facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed."}
{"doc_id": "2005.14165", "para_id": 185, "text": "Related work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like G ROV E R [ZHR+19] and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research."}
{"doc_id": "2005.14165", "para_id": 186, "text": "Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model."}
{"doc_id": "2005.14165", "para_id": 187, "text": "We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was ∼88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at ∼52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans ﬁnd difﬁcult to distinguish from human written news articles."}
{"doc_id": "2005.14165", "para_id": 188, "text": "A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it deﬁned only once, or conversely inferring a word’s meaning from only one usage. Here we qualitatively test GPT-3’s ability to do the former. Speciﬁcally, we give GPT-3 the deﬁnition of a nonexistent word, such as “Gigamuru”, and then ask it to use it in a sentence. We provide one to ﬁve previous examples of a (separate)"}
{"doc_id": "2005.14165", "para_id": 189, "text": "5We use a two-sample Student’s T-Test to test for signiﬁcant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value. 6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50%. Indeed, many individual participants scored below 50% on this task. 7Additional non-news samples can be found in Appendix F."}
{"doc_id": "2005.14165", "para_id": 190, "text": "Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately- bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best ﬁt is a power law with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 191, "text": "Mean accuracy 95% Conﬁdence Interval (low, hi) t compared to control (p-value) “I don’t know” assignments"}
{"doc_id": "2005.14165", "para_id": 192, "text": "Control 88% 84%–91% - 2.7% GPT-3 175B 52% 48%–57% 12.7 (3.2e-23) 10.6%"}
{"doc_id": "2005.14165", "para_id": 193, "text": "Table 3.12: People’s ability to identify whether ∼500 word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 194, "text": "Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination. The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church's history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them."}
{"doc_id": "2005.14165", "para_id": 195, "text": "Figure 3.14: The GPT-3 generated news article that humans had the greatest difﬁculty distinguishing from a human written article (accuracy: 12%)."}
{"doc_id": "2005.14165", "para_id": 196, "text": "Title: Star’s Tux Promise Draws Megyn Kelly’s Sarcasm Subtitle: Joaquin Phoenix pledged to not change for each awards event Article: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can't change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that's a really special thing, to not change yourself. I think it's a really special thing to say, `This is what's inside of me, I'm proud of it, and I'm not going to be ashamed because of the way that someone else thinks I should be.'\" Now, it's the Oscars, and Phoenix is at it again. But this time, his publicist is saying he'll be wearing a tux no matter what. Megyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you're saying you're a shape-shifter. I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn't stick. \"I was like, `Okay, I'm going to wear a tuxedo to this thing.' And then I thought, `I don't want to wear a tuxedo to this thing.'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it's too late: \"I'm committed to wearing this.\""}
{"doc_id": "2005.14165", "para_id": 197, "text": "Figure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy: 61%)."}
{"doc_id": "2005.14165", "para_id": 198, "text": "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus."}
{"doc_id": "2005.14165", "para_id": 199, "text": "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles. A \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is: I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there. It was delicious."}
{"doc_id": "2005.14165", "para_id": 200, "text": "A \"Burringo\" is a car with very fast acceleration. An example of a sentence that uses the word Burringo is: In our garage we have a Burringo that my father drives to work every day."}
{"doc_id": "2005.14165", "para_id": 201, "text": "A \"Gigamuru\" is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is: I have a Gigamuru that my uncle gave me as a gift. I love to play it at home."}
{"doc_id": "2005.14165", "para_id": 202, "text": "To \"screeg\" something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeghed at each other for several minutes and then we went outside and ate ice cream."}
{"doc_id": "2005.14165", "para_id": 203, "text": "Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3’s completions, plain text is human prompts. In the ﬁrst example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 other than the conditioning shown here."}
{"doc_id": "2005.14165", "para_id": 204, "text": "nonexistent word being deﬁned and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the speciﬁc word. Table 3.16 shows the 6 examples we generated; all deﬁnitions were human-generated, and the ﬁrst answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the ﬁnal sentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of the word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy sword ﬁght. Overall, GPT-3 appears to be at least proﬁcient at the task of using novel words in a sentence."}
{"doc_id": "2005.14165", "para_id": 205, "text": "Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few- shot setting by giving prompts of the form \"Poor English Input: <sentence>\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17."}
{"doc_id": "2005.14165", "para_id": 206, "text": "4 Measuring and Preventing Memorization Of Benchmarks"}
{"doc_id": "2005.14165", "para_id": 207, "text": "Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to."}
{"doc_id": "2005.14165", "para_id": 208, "text": "This concern is not just hypothetical. One of the ﬁrst papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, ﬁnding that"}
{"doc_id": "2005.14165", "para_id": 209, "text": "Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I'd be more than happy to work with you on another project."}
{"doc_id": "2005.14165", "para_id": 210, "text": "Poor English input: Please provide me with a short brief of the design you’re looking for and that’d be nice if you could share some examples or project you did before. Good English output: Please provide me with a brief description of the design you're looking for and that would be nice if you could share some examples or projects you have done before."}
{"doc_id": "2005.14165", "para_id": 211, "text": "Poor English input: The patient was died. Good English output: The patient died."}
{"doc_id": "2005.14165", "para_id": 212, "text": "Poor English input: We think that Leslie likes ourselves. Good English output: We think that Leslie likes us."}
{"doc_id": "2005.14165", "para_id": 213, "text": "Poor English input: Janet broke Bill on the finger. Good English output: Janet broke Bill's finger."}
{"doc_id": "2005.14165", "para_id": 214, "text": "Poor English input: Mary arranged for, in St. Louis, John to rent a house cheap. Good English output: Mary arranged for John to rent a house in St. Louis."}
{"doc_id": "2005.14165", "para_id": 215, "text": "Poor English input: Today I have went to the store to to buys some many bottle of water. Good English output: Today I went to the store to buy some bottles of water."}
{"doc_id": "2005.14165", "para_id": 216, "text": "Poor English input: I have tried to hit ball with bat, but my swing is has miss. Good English output: I tried to hit the ball with the bat, but my swing missed."}
{"doc_id": "2005.14165", "para_id": 217, "text": "Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3’s completions, plain text is human prompts. In the ﬁrst few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 aside from the ﬁrst few examples as conditioning and the “Poor English input/Good English output” framing. We note that the distinction between ”poor” and ”good” English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what “good” is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word ”cheap” in a way that alters meaning)."}
{"doc_id": "2005.14165", "para_id": 218, "text": "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty rather than overﬁtting."}
{"doc_id": "2005.14165", "para_id": 219, "text": "although models did perform moderately better on data that overlapped between training and testing, this did not signiﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent)."}
{"doc_id": "2005.14165", "para_id": 220, "text": "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared."}
{"doc_id": "2005.14165", "para_id": 221, "text": "We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results."}
{"doc_id": "2005.14165", "para_id": 222, "text": "For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, deﬁned roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively ﬂag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high conﬁdence. The exact procedure is detailed in Appendix C."}
{"doc_id": "2005.14165", "para_id": 223, "text": "We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a signiﬁcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inﬂating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance."}
{"doc_id": "2005.14165", "para_id": 224, "text": "Below, we review in more detail the few speciﬁc cases where either (1) the model performs signiﬁcantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difﬁcult."}
{"doc_id": "2005.14165", "para_id": 225, "text": "Our analysis ﬂagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English"}
{"doc_id": "2005.14165", "para_id": 226, "text": "Figure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high conﬁdence to be clean, and the y-axis shows the difference in performance when evaluating only on the veriﬁed clean subset. Performance on most benchmarks changed negligibly, but some were ﬂagged for further review. On inspection we ﬁnd some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We ﬁnd no evidence that other benchmarks are affected."}
{"doc_id": "2005.14165", "para_id": 227, "text": "translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:"}
{"doc_id": "2005.14165", "para_id": 228, "text": "• Reading Comprehension: Our initial analysis ﬂagged >90% of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difﬁcult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a speciﬁc question. • German translation: We found 25% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the ﬂagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news. • Reversed Words and Anagrams: Recall that these tasks are of the form “alaok = koala”. Due to the short length of these tasks, we used 2-grams for ﬁltering (ignoring punctuation). After inspecting the ﬂagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g “kayak = kayak”. The amount of overlap was small, but removing the trivial tasks lead to an increase in difﬁculty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches. • PIQA: The overlap analysis ﬂagged 29% of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination. • Winograd: The overlap analysis ﬂagged 45% of examples, and found a 2.6% decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk."}
{"doc_id": "2005.14165", "para_id": 229, "text": "• Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark."}
{"doc_id": "2005.14165", "para_id": 230, "text": "We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our ﬁll-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section."}
{"doc_id": "2005.14165", "para_id": 231, "text": "An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inﬂates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing."}
{"doc_id": "2005.14165", "para_id": 232, "text": "Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the ﬁeld in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C."}
{"doc_id": "2005.14165", "para_id": 233, "text": "GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work."}
{"doc_id": "2005.14165", "para_id": 234, "text": "First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufﬁciently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difﬁculty with “common sense physics”, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. Speciﬁcally GPT-3 has difﬁculty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks."}
{"doc_id": "2005.14165", "para_id": 235, "text": "GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved ﬁne-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically beneﬁt from bidirectionality. This may include ﬁll-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at ﬁne-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”."}
{"doc_id": "2005.14165", "para_id": 236, "text": "A more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the"}
{"doc_id": "2005.14165", "para_id": 237, "text": "pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate beneﬁts of customizing prediction to entities of interest. Also, with self-supervised objectives, task speciﬁcation relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW+19a], ﬁne-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY+19]."}
{"doc_id": "2005.14165", "para_id": 238, "text": "Another limitation broadly shared by language models is poor sample efﬁciency during pre-training. While GPT-3 takes a step towards test-time sample efﬁciency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efﬁciency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements."}
{"doc_id": "2005.14165", "para_id": 239, "text": "A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identiﬁes tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a speciﬁc style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deﬁning nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research."}
{"doc_id": "2005.14165", "para_id": 240, "text": "A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large models down to a manageable size for speciﬁc tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a speciﬁc task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size."}
{"doc_id": "2005.14165", "para_id": 241, "text": "Finally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6)."}
{"doc_id": "2005.14165", "para_id": 242, "text": "Language models have a wide range of beneﬁcial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difﬁculty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneﬁcial and harmful applications of language models."}
{"doc_id": "2005.14165", "para_id": 243, "text": "Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brieﬂy discuss issues of energy efﬁciency (Section 6.3)."}
{"doc_id": "2005.14165", "para_id": 244, "text": "Malicious uses of language models can be somewhat difﬁcult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact [Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures."}
{"doc_id": "2005.14165", "para_id": 245, "text": "Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufﬁciently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efﬁcacy."}
{"doc_id": "2005.14165", "para_id": 246, "text": "The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people ﬁnd difﬁcult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard."}
{"doc_id": "2005.14165", "para_id": 247, "text": "Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas [SBC+19]."}
{"doc_id": "2005.14165", "para_id": 248, "text": "To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did ﬁnd signiﬁcant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but signiﬁcant improvements in reliability could change this."}
{"doc_id": "2005.14165", "para_id": 249, "text": "Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing signiﬁcant resources in because there has been no convincing demonstration that current language models are signiﬁcantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage."}
{"doc_id": "2005.14165", "para_id": 250, "text": "Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are inﬂuenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment."}
{"doc_id": "2005.14165", "para_id": 251, "text": "Ease of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to ﬁlter the outputs, which restricts how scalable the operation can be."}
{"doc_id": "2005.14165", "para_id": 252, "text": "Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufﬁciently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers."}
{"doc_id": "2005.14165", "para_id": 253, "text": "Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation. 8"}
{"doc_id": "2005.14165", "para_id": 254, "text": "Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reﬂect all of the model’s biases even within the studied categories."}
{"doc_id": "2005.14165", "para_id": 255, "text": "Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reﬂect stereotypes present in their training data. Below we discuss our preliminary ﬁndings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension."}
{"doc_id": "2005.14165", "para_id": 256, "text": "In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identiﬁer than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identiﬁer by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identiﬁers include midwife, nurse, receptionist, housekeeper etc."}
{"doc_id": "2005.14165", "para_id": 257, "text": "We also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identiﬁer than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as 1 njobs P"}
{"doc_id": "2005.14165", "para_id": 258, "text": "P (male|Context)) ) - was −1.11 for the Neutral Variant, −2.14 for the Competent Variant and −1.15 for the Incompetent Variant."}
{"doc_id": "2005.14165", "para_id": 259, "text": "We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the mod- els ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ‘She’ refers to the\" and found the option with the lowest probability between the two possi- ble options (Choices between Occupation Option: advisor; Participant Option: advisee)."}
{"doc_id": "2005.14165", "para_id": 260, "text": "Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models."}
{"doc_id": "2005.14165", "para_id": 261, "text": "We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre- selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature"}
{"doc_id": "2005.14165", "para_id": 262, "text": "8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, [HZJ+19, NBR20, SCNP19]."}
{"doc_id": "2005.14165", "para_id": 263, "text": "Table 6.1: Most Biased Descriptive Words in 175B Model"}
{"doc_id": "2005.14165", "para_id": 264, "text": "Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts"}
{"doc_id": "2005.14165", "para_id": 265, "text": "Average Number of Co-Occurrences Across All Words: 17.5 Average Number of Co-Occurrences Across All Words: 23.9"}
{"doc_id": "2005.14165", "para_id": 266, "text": "Large (16) Optimistic (12) Mostly (15) Bubbly (12) Lazy (14) Naughty (12) Fantastic (13) Easy-going (12) Eccentric (13) Petite (10) Protect (10) Tight (10) Jolly (10) Pregnant (10) Stable (9) Gorgeous (28) Personable (22) Sucked (8) Survive (7) Beautiful (158)"}
{"doc_id": "2005.14165", "para_id": 267, "text": "of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\"9. We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more often described using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men who were more often described using adjectives that span a greater spectrum."}
{"doc_id": "2005.14165", "para_id": 268, "text": "Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. “Most Favored” here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender."}
{"doc_id": "2005.14165", "para_id": 269, "text": "To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet)."}
{"doc_id": "2005.14165", "para_id": 270, "text": "It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reﬂect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology."}
{"doc_id": "2005.14165", "para_id": 271, "text": "Across the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data."}
{"doc_id": "2005.14165", "para_id": 272, "text": "9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which ‘they’ refers to a singular noun from those where it didn’t, but other forms of gender bias are likely present and could be studied using different approaches."}
{"doc_id": "2005.14165", "para_id": 273, "text": "Religion Most Favored Descriptive Words Atheism ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’"}
{"doc_id": "2005.14165", "para_id": 274, "text": "Buddhism ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En- lightenment’, ‘Non-Violent’"}
{"doc_id": "2005.14165", "para_id": 275, "text": "Christianity ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com- ments’, ‘Ofﬁcially’"}
{"doc_id": "2005.14165", "para_id": 276, "text": "Hinduism ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’"}
{"doc_id": "2005.14165", "para_id": 277, "text": "Islam ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’"}
{"doc_id": "2005.14165", "para_id": 278, "text": "Judaism ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’"}
{"doc_id": "2005.14165", "para_id": 279, "text": "Table 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model."}
{"doc_id": "2005.14165", "para_id": 280, "text": "We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length ≈50 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words."}
{"doc_id": "2005.14165", "para_id": 281, "text": "The following is an example output from the model:"}
{"doc_id": "2005.14165", "para_id": 282, "text": "\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\""}
{"doc_id": "2005.14165", "para_id": 283, "text": "Similar to race, we found that the models make associations with religious terms that indicate some propensity to reﬂect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3."}
{"doc_id": "2005.14165", "para_id": 284, "text": "We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difﬁculties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18]."}
{"doc_id": "2005.14165", "para_id": 285, "text": "Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions speciﬁc to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20]. Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots [GG19, NvNvdG19] but in a holistic manner."}
{"doc_id": "2005.14165", "para_id": 286, "text": "Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaﬂop/s-days of compute during pre-training, compared to tens of petaﬂop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efﬁciency of such models, as advocated by [SDSE19]."}
{"doc_id": "2005.14165", "para_id": 287, "text": "The use of large-scale pre-training also gives another lens through which to view the efﬁciency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and ﬁne-tuned for speciﬁc tasks. Though models like GPT-3 consume signiﬁcant resources during training, they can be surprisingly efﬁcient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efﬁcient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efﬁciency of such models over time, similar to trends observed in image recognition and neural machine translation [HB20]."}
{"doc_id": "2005.14165", "para_id": 288, "text": "Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters [JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters [VSP+17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters [RWC+19], 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and most recently 17 billion parameters [Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models’ capacity to store information without increased computational cost. These approaches rely on the conditional computation framework [BLC13] and speciﬁcally, the mixture-of-experts method [SMM+17] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19], though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time [Gra16] and the universal transformer [DGV+18]. Our work focuses on the ﬁrst approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy."}
{"doc_id": "2005.14165", "para_id": 289, "text": "Several efforts have also systematically studied the effect of scale on language model performance. [KMH+20, RRBS19, LWS+20, HNA+17], ﬁnd a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1), and we also ﬁnd relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling."}
{"doc_id": "2005.14165", "para_id": 290, "text": "Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT [LCG+19] as well as general [HVD15] and"}
{"doc_id": "2005.14165", "para_id": 291, "text": "task-speciﬁc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models."}
{"doc_id": "2005.14165", "para_id": 292, "text": "As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19, IBGC+14, CCE+18, MCKS18], reading comprehension [CHI+18, RCM19], and adversarially constructed datasets designed to be difﬁcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many of these datasets."}
{"doc_id": "2005.14165", "para_id": 293, "text": "Many previous efforts have focused speciﬁcally on question-answering, which constitutes a signiﬁcant fraction of the tasks we tested on. Recent efforts include [RSR+19, RRS20], which ﬁne-tuned an 11 billion parameter language model, and [GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of [GLT+20, LPP+20]."}
{"doc_id": "2005.14165", "para_id": 294, "text": "Metalearning in language models has been utilized in [RWC+19], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks [VBL+16], RL2 [DSC+16], learning to optimize [RL16, ADG+16, LM17] and MAML [FAL17]. Our approach of stufﬁng the model’s context with previous examples is most structurally similar to RL2 and also resembles [HYC01], in that an inner loop of adaptation takes place through computation in the model’s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks deﬁned at inference-time. Few-shot auto-regressive density estimation was explored in [RCP+17] and [GWC+18] studied low-resource NMT as a few-shot learning problem."}
{"doc_id": "2005.14165", "para_id": 295, "text": "While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-ﬁeld with similar goals is semi-supervised learning where approaches such as UDA [XDH+19] also explore methods of ﬁne-tuning when very little labeled data is available."}
{"doc_id": "2005.14165", "para_id": 296, "text": "Giving multi-task models instructions in natural language was ﬁrst formalized in a supervised setting with [MKXS18] and utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for multi-task ﬁne-tuning rather than for in-context learning without weight updates."}
{"doc_id": "2005.14165", "para_id": 297, "text": "Another approach to increasing generality and transfer-learning capability in language models is multi-task learning [Car97], which ﬁne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efﬁciency when updating the weights for a new task. Multi-task learning has shown some promising initial results [LGH+15, LSP+18] and multi-stage ﬁne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed the boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human interaction [ZSW+19b], or active learning [Mac92]."}
{"doc_id": "2005.14165", "para_id": 298, "text": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [DCLT18], preﬁxLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu- tations during training [YDY+19], architectures that improve the efﬁciency of sampling [DYY+19], improvements in data and training procedures [LOG+19], and efﬁciency increases in the embedding parameters [LCG+19]. Many of these techniques provide signiﬁcant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s performance on downstream tasks, especially in the ﬁne-tuning setting, and combining GPT-3’s scale with these algorithmic techniques is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 299, "text": "We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of"}
{"doc_id": "2005.14165", "para_id": 300, "text": "state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems."}
{"doc_id": "2005.14165", "para_id": 301, "text": "The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale."}
{"doc_id": "2005.14165", "para_id": 302, "text": "Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies."}
{"doc_id": "2005.14165", "para_id": 303, "text": "Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments."}
{"doc_id": "2005.14165", "para_id": 304, "text": "Ben Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on the training data."}
{"doc_id": "2005.14165", "para_id": 305, "text": "Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks."}
{"doc_id": "2005.14165", "para_id": 306, "text": "Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research."}
{"doc_id": "2005.14165", "para_id": 307, "text": "Ben Mann implemented sampling without replacement during training."}
{"doc_id": "2005.14165", "para_id": 308, "text": "Alec Radford originally demonstrated few-shot learning occurs in language models."}
{"doc_id": "2005.14165", "para_id": 309, "text": "Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods."}
{"doc_id": "2005.14165", "para_id": 310, "text": "Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training."}
{"doc_id": "2005.14165", "para_id": 311, "text": "Rewon Child and Mark Chen developed an early version of our model-parallel strategy."}
{"doc_id": "2005.14165", "para_id": 312, "text": "Rewon Child and Scott Gray contributed the sparse transformer."}
{"doc_id": "2005.14165", "para_id": 313, "text": "Aditya Ramesh experimented with loss scaling strategies for pretraining."}
{"doc_id": "2005.14165", "para_id": 314, "text": "Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search."}
{"doc_id": "2005.14165", "para_id": 315, "text": "Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature."}
{"doc_id": "2005.14165", "para_id": 316, "text": "Sandhini Agarwal conducted the fairness and representation analysis."}
{"doc_id": "2005.14165", "para_id": 317, "text": "Girish Sastry and Amanda Askell conducted the human evaluations of the model."}
{"doc_id": "2005.14165", "para_id": 318, "text": "Ariel Herbert-Voss conducted the threat analysis of malicious use."}
{"doc_id": "2005.14165", "para_id": 319, "text": "Gretchen Krueger edited and red-teamed the policy sections of the paper."}
{"doc_id": "2005.14165", "para_id": 320, "text": "Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI’s clusters to run the largest models efﬁciently."}
{"doc_id": "2005.14165", "para_id": 321, "text": "Scott Gray developed fast GPU kernels used during training."}
{"doc_id": "2005.14165", "para_id": 322, "text": "Jack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work."}
{"doc_id": "2005.14165", "para_id": 323, "text": "Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper."}
{"doc_id": "2005.14165", "para_id": 324, "text": "Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work."}
{"doc_id": "2005.14165", "para_id": 325, "text": "Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the beneﬁt of weight decay for training."}
{"doc_id": "2005.14165", "para_id": 326, "text": "Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work."}
{"doc_id": "2005.14165", "para_id": 327, "text": "As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) ﬁltering Common Crawl and (2) fuzzy deduplication:"}
{"doc_id": "2005.14165", "para_id": 328, "text": "1. In order to improve the quality of Common Crawl, we developed an automatic ﬁltering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classiﬁer to distinguish these from raw Common Crawl. We then used this classiﬁer to re-sample Common Crawl by prioritizing documents which were predicted by the classiﬁer to be higher quality. The classiﬁer is trained using logistic regression classiﬁer with features from Spark’s standard tokenizer and HashingTF 10. For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unﬁltered Common Crawl. We used this classiﬁer to score Common Crawl documents. We kept each document in our dataset iff"}
{"doc_id": "2005.14165", "para_id": 329, "text": "We chose α = 9 in order to take mostly documents the classiﬁer scored highly, but still include some documents that were out of distribution. α was chosen to match the distribution of scores from our classiﬁer on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples."}
{"doc_id": "2005.14165", "para_id": 330, "text": "2. To further improve model quality and prevent overﬁtting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the same features as were used for classiﬁcation above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%."}
{"doc_id": "2005.14165", "para_id": 331, "text": "After ﬁltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C."}
{"doc_id": "2005.14165", "para_id": 332, "text": "To train all versions of GPT-3, we use Adam with β1 = 0.9, β2 = 0.95, and ϵ = 10−8, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the ﬁrst 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the ﬁrst 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overﬁtting. All models use weight decay of 0.1 to provide a small amount of regularization [LH17]."}
{"doc_id": "2005.14165", "para_id": 333, "text": "During training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efﬁciency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efﬁcient training without need for any special sequence-speciﬁc masking."}
{"doc_id": "2005.14165", "para_id": 334, "text": "In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results."}
{"doc_id": "2005.14165", "para_id": 335, "text": "Initial training set ﬁltering We attempted to remove text occurring in benchmarks from training data by searching for 13−gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding 13−gram as well as a 200 character window around it, splitting the original document into pieces. For ﬁltering purposes we deﬁne a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than 200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and"}
{"doc_id": "2005.14165", "para_id": 336, "text": "10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF"}
{"doc_id": "2005.14165", "para_id": 337, "text": "removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored 13−grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired speciﬁc overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository11."}
{"doc_id": "2005.14165", "para_id": 338, "text": "Overlap methodology For our benchmark overlap analysis in Section 4, we used a variable number of words N to check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data marked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl documents per Section 2.2."}
{"doc_id": "2005.14165", "para_id": 339, "text": "We deﬁne a ‘dirty’ example as one with any N-gram overlap with any training document, and a ‘clean’ example as one with no collision."}
{"doc_id": "2005.14165", "para_id": 340, "text": "Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1"}
{"doc_id": "2005.14165", "para_id": 341, "text": "Overlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we ﬁlter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overﬁt to the examples it has seen. If the clean score is signiﬁcantly better, our ﬁltering scheme may have preferentially marked easier examples as dirty."}
{"doc_id": "2005.14165", "para_id": 342, "text": "This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our ﬁltering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We conﬁrmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after ﬁltering come from a slightly different distribution than the dirty examples."}
{"doc_id": "2005.14165", "para_id": 343, "text": "Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we ﬂagged for further review."}
{"doc_id": "2005.14165", "para_id": 344, "text": "11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md"}
{"doc_id": "2005.14165", "para_id": 345, "text": "Name Split Metric N Acc/F1/BLEU Total Count Dirty Acc/F1/BLEU Dirty Count Clean Acc/F1/BLEU Clean Count Clean Percentage"}
{"doc_id": "2005.14165", "para_id": 346, "text": "Quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20% SQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2% DROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21% Symbol Insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0% CoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1% ReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1% Winograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3% BoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0% MultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1% RACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0% LAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0% LAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1% WSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3% PIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4% RACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3% De→En 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5% En→De 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3% En→Ro 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1% Ro→En 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1% WebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0% ANLI R1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3% ANLI R2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3% TriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0% ANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1% En→Fr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1% Fr→En 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0% WiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0% RTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0% CB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2% Anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7% Reversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26% OpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1% ARC (Easy) test acc 11 70.1 2268 77.5 89 69.8 2179 96% 0% Anagrams 1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8% COPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0% ARC (Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0% HellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0% NQs test acc 11 29.9 3610 32.7 52 29.8 3558 99% 0% Cycled Letters dev acc 2 38.6 10000 20.5 73 38.7 9927 99% 0% SAT Analogies dev acc 9 65.8 374 100.0 2 65.6 372 99% 0% StoryCloze test acc 13 87.7 1871 100.0 2 87.6 1869 100% 0% Winogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0%"}
{"doc_id": "2005.14165", "para_id": 347, "text": "Table C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it has a single N-gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the percent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows the number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we use the metric speciﬁed in “Metric”. These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper."}
{"doc_id": "2005.14165", "para_id": 348, "text": "This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10% of the total compute for the models we are analyzing."}
{"doc_id": "2005.14165", "para_id": 349, "text": "Calculations can be seen in Table D.1 and are explained within the table caption."}
{"doc_id": "2005.14165", "para_id": 350, "text": "Total train compute (ﬂops) Params (M) Training tokens (billions)"}
{"doc_id": "2005.14165", "para_id": 351, "text": "T5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5 T5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5 T5-Large 2.67E+01 2.31E+21 770 1,000 3 3 1 0.5 T5-3B 1.04E+02 9.00E+21 3,000 1,000 3 3 1 0.5 T5-11B 3.82E+02 3.30E+22 11,000 1,000 3 3 1 0.5 BERT-Base 1.89E+00 1.64E+20 109 250 6 3 2 1.0 BERT-Large 6.16E+00 5.33E+20 355 250 6 3 2 1.0 RoBERTa-Base 1.74E+01 1.50E+21 125 2,000 6 3 2 1.0 RoBERTa-Large 4.93E+01 4.26E+21 355 2,000 6 3 2 1.0 GPT-3 Small 2.60E+00 2.25E+20 125 300 6 3 2 1.0 GPT-3 Medium 7.42E+00 6.41E+20 356 300 6 3 2 1.0 GPT-3 Large 1.58E+01 1.37E+21 760 300 6 3 2 1.0 GPT-3 XL 2.75E+01 2.38E+21 1,320 300 6 3 2 1.0 GPT-3 2.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0 GPT-3 6.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0 GPT-3 13B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0 GPT-3 175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0"}
{"doc_id": "2005.14165", "para_id": 352, "text": "Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both ∂params"}
{"doc_id": "2005.14165", "para_id": 353, "text": "∂loss use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total ﬂops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total ﬂops used during training. We report both ﬂops and petaﬂop/s-day (each of which are 8.64e+19 ﬂops)."}
{"doc_id": "2005.14165", "para_id": 354, "text": "E Human Quality Assessment of Synthetic News Articles"}
{"doc_id": "2005.14165", "para_id": 355, "text": "This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We ﬁrst describe the experiments on the ∼200 word news articles, and then describe the preliminary investigation of ∼500 word news articles generated by GPT-3."}
{"doc_id": "2005.14165", "para_id": 356, "text": "Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was ∼38 years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once."}
{"doc_id": "2005.14165", "para_id": 357, "text": "Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants’ judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text."}
{"doc_id": "2005.14165", "para_id": 358, "text": "Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age"}
{"doc_id": "2005.14165", "para_id": 359, "text": "Control 76 7 32:37:0 39 216:216 GPT-3 Small 80 7 41:31:1 40 216:188 GPT-3 Medium 80 7 46:28:2 39 216:202 GPT-3 Large 81 24 46:28:2 37 216:200 GPT-3 XL 79 14 32:32:1 38 216:199 GPT-3 2.7B 80 11 36:33:0 40 216:202 GPT-3 6.7B 76 5 46:28:2 37 216:195 GPT-3 13.0B 81 13 46:28:2 37 216:209 GPT-3 175B 80 9 42:29:0 37 216:216"}
{"doc_id": "2005.14165", "para_id": 360, "text": "Table E.1: Participant details and article lengths for each experiment to evaluate human detection of ∼200 word model generated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 361, "text": "Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best ﬁt is a linear model on a log scale with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 362, "text": "In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shufﬂed for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz."}
{"doc_id": "2005.14165", "para_id": 363, "text": "Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we ﬁt a power law of the form ax−b. The 95% conﬁdence intervals were estimated from the t-distribution of the sample mean."}
{"doc_id": "2005.14165", "para_id": 364, "text": "Duration statistics: In the main text, we discussed the ﬁnding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower"}
{"doc_id": "2005.14165", "para_id": 365, "text": "Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age"}
{"doc_id": "2005.14165", "para_id": 366, "text": "Control 79 17 32:37:0 39 569:464 GPT-3 175B 81 19 32:30:0 40 569:498"}
{"doc_id": "2005.14165", "para_id": 367, "text": "Table E.2: Participant details and article lengths for the experiments investigating human detection of ∼500 word model generated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 368, "text": "accuracy scores despite increased time investment from participants supports the ﬁnding that larger models generate harder-to-distinguish news articles."}
{"doc_id": "2005.14165", "para_id": 369, "text": "Preliminary investigation of ∼500 word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments."}
{"doc_id": "2005.14165", "para_id": 370, "text": "GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We ﬁrst experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with P = 0.9). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary."}
{"doc_id": "2005.14165", "para_id": 371, "text": "Context → The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY [Poem text omitted]"}
{"doc_id": "2005.14165", "para_id": 372, "text": "-------- Generated Poem 1 -------- -------- Generated Poem 3 --------"}
{"doc_id": "2005.14165", "para_id": 373, "text": "I must have shadows on the way The sun was all we had. Now, in the shade If I am to walk I must have All is changed. The mind must dwell on those Each step taken slowly and alone White fields, that to its eyes were always old; To have it ready made Those ancient gleams, convoluting The way they lay among the huge roots, And I must think in lines of grey The great dark tomes of reverie, To have dim thoughts to be my guide The plumed labyrinth of the sea. Must look on blue and green And never let my eye forget When our own breath gonne faint and fade That color is my friend Up the long stairway of the dead And purple must surround me too We shall remember well The blindness of the light, the whiteness The yellow of the sun is no more Of that white land. The footsteps, and Intrusive than the bluish snow The grasses in that light, and in the shade That falls on all of us. I must have The menacing life. Grey thoughts and blue thoughts walk with me If I am to go away at all. We played, too, once, in the warmest rooms. To one content, there is one content -------- Generated Poem 2 -------- For all mankind. The forest glades Are all the more for being in fable, There is a way in the world. And wood and town in legend shadowed- That way a man may live in a small town Once more to return. The one content His lifetime and see no man come, Has in it every happiness, more brave Never the stir of men about a market, For being purposeless; and so Never the bobbing of covered carts, Those island gardens, deep in the mind, Never the creak passed on the stair Make of remoteness familiarity Closing some other life; an old man And of the unspoken name a friend. Can prosper in that town, can prosper When his store is seen to close, his wife -------- Generated Poem 4 -------- Fades in his house, and the world shouts in vain That no one comes to hear him Nobody will come to this place. It is a road that leads nowhere. Where his small bell tolls. The solitude is deep. The mountains are high. In that small town But they are desolate, and they turn the traveler’s face Everything is known. Towards the North. All the sounds of the world are far away. The ways of the young men are known, When the wind rises above the trees, And the ways of the young women The boughs bow to the ground. As they walk to the Spring; Even the birds that inhabit the tangle of weeds The rumors are all known, That is the roadside cover, are silent. One listens, The quarrels, the hurt, the envy But hears no roar of the forest. One is alone. Of the men and the women. One will be taken. Till a day comes One will be taken. That way out of the world There is no utterance, there is no conversation, That leads to a wood. But one is uneasy all the same.... Young, the young man There is a thin blue mist, Walks, a lover now, A darkness rising like smoke, Knows the streets as they are and is of them. And within that darkness A possession of the heart. Comes to the wood. One will be taken.... It was here, and it will be here again- Here, under this sky empty and full of light. There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind."}
{"doc_id": "2005.14165", "para_id": 374, "text": "Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title ‘Shadows on the Way’."}
{"doc_id": "2005.14165", "para_id": 375, "text": "The following ﬁgures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here."}
{"doc_id": "2005.14165", "para_id": 376, "text": "Context → Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic ---- as long as they occur in are respectful and intelligent manner. In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they’re trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. As a general rule,it’s best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one’s salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host’s team."}
{"doc_id": "2005.14165", "para_id": 377, "text": "Q: What shouldn’t you do when talking about sports with colleagues from another country?"}
{"doc_id": "2005.14165", "para_id": 378, "text": "A: Criticizing the sports of your colleagues’ country."}
{"doc_id": "2005.14165", "para_id": 379, "text": "Q: Which is typically a friendly topic in most places according to the author?"}
{"doc_id": "2005.14165", "para_id": 380, "text": "Q: Why are people from Asia more private in their conversation with others?"}
{"doc_id": "2005.14165", "para_id": 381, "text": "A: They don’t want to have their good relationship with others harmed by informal conversation."}
{"doc_id": "2005.14165", "para_id": 382, "text": "Correct Answer → taboo Incorrect Answer → cheerful topics Incorrect Answer → rude topics Incorrect Answer → topics that can never be talked about"}
{"doc_id": "2005.14165", "para_id": 383, "text": "Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 384, "text": "Context → anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada. This locals’ casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile (∼ 1.6km) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither?"}
{"doc_id": "2005.14165", "para_id": 385, "text": "Correct Answer → Neither Incorrect Answer → True Incorrect Answer → False"}
{"doc_id": "2005.14165", "para_id": 386, "text": "Context → Article: Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you’ll be really happy.\""}
{"doc_id": "2005.14165", "para_id": 387, "text": "Q: Which of the following is True according to the passage?"}
{"doc_id": "2005.14165", "para_id": 388, "text": "A: If a kid hated four people,he or she had to carry four potatoes."}
{"doc_id": "2005.14165", "para_id": 389, "text": "Q: The children complained about besides the weight trouble."}
{"doc_id": "2005.14165", "para_id": 390, "text": "Q: Mrs.Smith asked her students to write on the potatoes."}
{"doc_id": "2005.14165", "para_id": 391, "text": "Correct Answer → names Incorrect Answer → numbers Incorrect Answer → time Incorrect Answer → places"}
{"doc_id": "2005.14165", "para_id": 392, "text": "Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 393, "text": "Correct Answer → Using a brush, brush on sealant onto wood until it is fully saturated with the sealant. Incorrect Answer → Using a brush, drip on sealant onto wood until it is fully saturated with the sealant."}
{"doc_id": "2005.14165", "para_id": 394, "text": "Context → My body cast a shadow over the grass because"}
{"doc_id": "2005.14165", "para_id": 395, "text": "Correct Answer → the sun was rising. Incorrect Answer → the grass was cut."}
{"doc_id": "2005.14165", "para_id": 396, "text": "Context → (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to \"Second Amendment people\" in a speech and warned that the words that politicians use can incite violence and undermine democracy. \"Trump’s words are an incitement to the type of political violence that touched me personally,\" Rabin wrote in USAToday. He said that Trump’s appeal to \"Second Amendment people\" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- \"were a new level of ugliness in an ugly campaign season.\""}
{"doc_id": "2005.14165", "para_id": 397, "text": "- The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. - Warns of \"parallels\" between Israel of the 1990s and the U.S. today."}
{"doc_id": "2005.14165", "para_id": 398, "text": "Correct Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump’s aggressive rhetoric. Correct Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin’s aggressive rhetoric."}
{"doc_id": "2005.14165", "para_id": 399, "text": "Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single ”problem” because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script."}
{"doc_id": "2005.14165", "para_id": 400, "text": "Context → anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health & Sport. He also serves on the Justice and Education & Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politican who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither?"}
{"doc_id": "2005.14165", "para_id": 401, "text": "Correct Answer → Neither Incorrect Answer → True Incorrect Answer → False"}
{"doc_id": "2005.14165", "para_id": 402, "text": "Context → Organisms require energy in order to do what?"}
{"doc_id": "2005.14165", "para_id": 403, "text": "Correct Answer → mature and develop. Incorrect Answer → rest soundly. Incorrect Answer → absorb light. Incorrect Answer → take in nutrients."}
{"doc_id": "2005.14165", "para_id": 404, "text": "Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 405, "text": "Context → Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They"}
{"doc_id": "2005.14165", "para_id": 406, "text": "Correct Answer → bake them, then frost and decorate. Incorrect Answer → taste them as they place them on plates. Incorrect Answer → put the frosting on the cake as they pan it. Incorrect Answer → come out and begin decorating the cake as well."}
{"doc_id": "2005.14165", "para_id": 407, "text": "Figure G.9: Formatted dataset example for HellaSwag"}
{"doc_id": "2005.14165", "para_id": 408, "text": "Context → anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither?"}
{"doc_id": "2005.14165", "para_id": 409, "text": "Correct Answer → False Incorrect Answer → True Incorrect Answer → Neither"}
{"doc_id": "2005.14165", "para_id": 410, "text": "Figure G.10: Formatted dataset example for ANLI R3"}
{"doc_id": "2005.14165", "para_id": 411, "text": "Context → Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer:"}
{"doc_id": "2005.14165", "para_id": 412, "text": "Correct Answer → dry palms Incorrect Answer → wet palms Incorrect Answer → palms covered with oil Incorrect Answer → palms covered with lotion"}
{"doc_id": "2005.14165", "para_id": 413, "text": "Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 414, "text": "Correct Answer → cajole is to compliance Incorrect Answer → balk is to fortitude Incorrect Answer → betray is to loyalty Incorrect Answer → hinder is to destination Incorrect Answer → soothe is to passion"}
{"doc_id": "2005.14165", "para_id": 415, "text": "Figure G.12: Formatted dataset example for SAT Analogies"}
{"doc_id": "2005.14165", "para_id": 416, "text": "Correct Context → Grace was happy to trade me her sweater for my jacket. She thinks the sweater Incorrect Context → Grace was happy to trade me her sweater for my jacket. She thinks the jacket"}
{"doc_id": "2005.14165", "para_id": 417, "text": "Figure G.13: Formatted dataset example for Winograd. The ‘partial’ evaluation method we use compares the probability of the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 418, "text": "Correct Context → Johnny likes fruits more than vegetables in his new keto diet because the fruits Incorrect Context → Johnny likes fruits more than vegetables in his new keto diet because the vegetables"}
{"doc_id": "2005.14165", "para_id": 419, "text": "Figure G.14: Formatted dataset example for Winogrande. The ‘partial’ evaluation method we use compares the probability of the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 420, "text": "Context → READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department’s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government’s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President’s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he’d help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued."}
{"doc_id": "2005.14165", "para_id": 421, "text": "Who did The State Department feel should visit both India and Pakistan?"}
{"doc_id": "2005.14165", "para_id": 422, "text": "Correct Answer → - [False] Bin Laden Incorrect Answer → - [True] Bin Laden"}
{"doc_id": "2005.14165", "para_id": 423, "text": "Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use K to refer to the number of questions shown within the context."}
{"doc_id": "2005.14165", "para_id": 424, "text": "Context → Question: Which factor will most likely cause a person to develop a fever? Answer:"}
{"doc_id": "2005.14165", "para_id": 425, "text": "Correct Answer → a bacterial population in the bloodstream Incorrect Answer → a leg muscle relaxing after exercise Incorrect Answer → several viral particles on the skin Incorrect Answer → carbohydrates being digested in the stomach"}
{"doc_id": "2005.14165", "para_id": 426, "text": "Figure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 427, "text": "Context → Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home."}
{"doc_id": "2005.14165", "para_id": 428, "text": "Correct Answer → Bob believed that there were good people in the world. Incorrect Answer → Bob contemplated how unfriendly the world was."}
{"doc_id": "2005.14165", "para_id": 429, "text": "Figure G.17: Formatted dataset example for StoryCloze"}
{"doc_id": "2005.14165", "para_id": 430, "text": "Context → Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities."}
{"doc_id": "2005.14165", "para_id": 431, "text": "The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world’s northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland’s major political, educational, financial, cultural, and research center as well as one of northern Europe’s major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia."}
{"doc_id": "2005.14165", "para_id": 432, "text": "Q: what is the most populous municipality in Finland?"}
{"doc_id": "2005.14165", "para_id": 433, "text": "Q: what percent of the foreign companies that operate in Finland are in Helsinki?"}
{"doc_id": "2005.14165", "para_id": 434, "text": "Q: what towns are a part of the metropolitan area?"}
{"doc_id": "2005.14165", "para_id": 435, "text": "Target Completion → Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns"}
{"doc_id": "2005.14165", "para_id": 436, "text": "Context → Please unscramble the letters into a word, and write that word: asinoc ="}
{"doc_id": "2005.14165", "para_id": 437, "text": "Figure G.19: Formatted dataset example for Cycled Letters"}
{"doc_id": "2005.14165", "para_id": 438, "text": "Context → Passage: Saint Jean de Br´ebeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Br´ebeuf and another missionary were captured when an Iroquois raid took over a Huron village . Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Br´ebeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Question: How many years did Saint Jean de Br´ebeuf stay in New France before he went back to France for a few years? Answer:"}
{"doc_id": "2005.14165", "para_id": 439, "text": "\"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there’s more than a . ->"}
{"doc_id": "2005.14165", "para_id": 440, "text": "Figure G.21: Formatted dataset example for LAMBADA"}
{"doc_id": "2005.14165", "para_id": 441, "text": "Context → Please unscramble the letters into a word, and write that word: skicts ="}
{"doc_id": "2005.14165", "para_id": 442, "text": "Figure G.22: Formatted dataset example for Anagrams 1 (A1)"}
{"doc_id": "2005.14165", "para_id": 443, "text": "Context → Please unscramble the letters into a word, and write that word: volwskagen ="}
{"doc_id": "2005.14165", "para_id": 444, "text": "Figure G.23: Formatted dataset example for Anagrams 2"}
{"doc_id": "2005.14165", "para_id": 445, "text": "Context → Q: Who played tess on touched by an angel?"}
{"doc_id": "2005.14165", "para_id": 446, "text": "Target Completion → Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese"}
{"doc_id": "2005.14165", "para_id": 447, "text": "Figure G.24: Formatted dataset example for Natural Questions"}
{"doc_id": "2005.14165", "para_id": 448, "text": "Context → TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry’s \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents’ goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team’s success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa)."}
{"doc_id": "2005.14165", "para_id": 449, "text": "Context → Please unscramble the letters into a word, and write that word: r e!c.i p r o.c a/l ="}
{"doc_id": "2005.14165", "para_id": 450, "text": "Figure G.26: Formatted dataset example for Symbol Insertion"}
{"doc_id": "2005.14165", "para_id": 451, "text": "Context → Please unscramble the letters into a word, and write that word: taefed ="}
{"doc_id": "2005.14165", "para_id": 452, "text": "Figure G.27: Formatted dataset example for Reversed Words"}
{"doc_id": "2005.14165", "para_id": 453, "text": "Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Ger¨at beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Ger¨at, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger¨at."}
{"doc_id": "2005.14165", "para_id": 454, "text": "Figure G.28: Formatted dataset example for SQuADv2"}
{"doc_id": "2005.14165", "para_id": 455, "text": "Context → Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g (\\displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer:"}
{"doc_id": "2005.14165", "para_id": 456, "text": "Context → The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there’s still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn’t mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer:"}
{"doc_id": "2005.14165", "para_id": 457, "text": "Context → The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer:"}
{"doc_id": "2005.14165", "para_id": 458, "text": "Context → An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word ‘outfitter’ used in the same way in the two sentences above? answer:"}
{"doc_id": "2005.14165", "para_id": 459, "text": "Context → Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ===== Passage: Mr. Moncrieff visited Chester’s luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward’s allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun \"*his*\" refer to? Answer:"}
{"doc_id": "2005.14165", "para_id": 460, "text": "Context → Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist?"}
{"doc_id": "2005.14165", "para_id": 461, "text": "Target Completion → MARCEL DUCHAMP Target Completion → r mutt Target Completion → duchamp Target Completion → marcel duchamp Target Completion → R.Mutt Target Completion → Marcel duChamp Target Completion → Henri-Robert-Marcel Duchamp Target Completion → Marcel du Champ Target Completion → henri robert marcel duchamp Target Completion → Duchampian Target Completion → Duchamp Target Completion → duchampian Target Completion → marcel du champ Target Completion → Marcel Duchamp Target Completion → MARCEL DUCHAMP"}
{"doc_id": "2005.14165", "para_id": 462, "text": "Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions."}
{"doc_id": "2005.14165", "para_id": 463, "text": "Context → Q: What school did burne hogarth establish?"}
{"doc_id": "2005.14165", "para_id": 464, "text": "Context → Keinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden. ="}
{"doc_id": "2005.14165", "para_id": 465, "text": "Target Completion → In no case may they be used for commercial purposes."}
{"doc_id": "2005.14165", "para_id": 466, "text": "Figure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and other langauge tasks, the format for zero-shot learning is “Q: What is the {language} translation of {sentence} A: {translation}.”"}
{"doc_id": "2005.14165", "para_id": 467, "text": "Context → In no case may they be used for commercial purposes. ="}
{"doc_id": "2005.14165", "para_id": 468, "text": "Target Completion → Keinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden."}
{"doc_id": "2005.14165", "para_id": 469, "text": "Context → Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. ="}
{"doc_id": "2005.14165", "para_id": 470, "text": "Target Completion → L’analyse de la distribution de fr´equence des stades larvaires d’I. verticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves m^ales ´etaient `a des stades plus avanc´es que les larves femelles."}
{"doc_id": "2005.14165", "para_id": 471, "text": "Context → L’analyse de la distribution de fr´equence des stades larvaires d’I. verticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves m^ales ´etaient `a des stades plus avanc´es que les larves femelles. ="}
{"doc_id": "2005.14165", "para_id": 472, "text": "Target Completion → Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females."}
{"doc_id": "2005.14165", "para_id": 473, "text": "Context → The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey’s accession to the European Union, despite Turkey’s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. ="}
{"doc_id": "2005.14165", "para_id": 474, "text": "Target Completion → Adev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei europenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea European˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul ¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort."}
{"doc_id": "2005.14165", "para_id": 475, "text": "Context → Adev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei europenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea European˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul ¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort. ="}
{"doc_id": "2005.14165", "para_id": 476, "text": "Target Completion → The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey’s accession to the European Union, despite Turkey’s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill."}
{"doc_id": "2005.14165", "para_id": 477, "text": "Figure G.42: Formatted dataset example for Arithmetic 1DC"}
{"doc_id": "2005.14165", "para_id": 478, "text": "Figure G.43: Formatted dataset example for Arithmetic 2D-"}
{"doc_id": "2005.14165", "para_id": 479, "text": "Figure G.44: Formatted dataset example for Arithmetic 2D+"}
{"doc_id": "2005.14165", "para_id": 480, "text": "Figure G.45: Formatted dataset example for Arithmetic 2Dx"}
{"doc_id": "2005.14165", "para_id": 481, "text": "Figure G.46: Formatted dataset example for Arithmetic 3D-"}
{"doc_id": "2005.14165", "para_id": 482, "text": "Figure G.47: Formatted dataset example for Arithmetic 3D+"}
{"doc_id": "2005.14165", "para_id": 483, "text": "Figure G.48: Formatted dataset example for Arithmetic 4D-"}
{"doc_id": "2005.14165", "para_id": 484, "text": "Figure G.49: Formatted dataset example for Arithmetic 4D+"}
{"doc_id": "2005.14165", "para_id": 485, "text": "Figure G.50: Formatted dataset example for Arithmetic 5D−"}
{"doc_id": "2005.14165", "para_id": 486, "text": "Figure G.51: Formatted dataset example for Arithmetic 5D+"}
{"doc_id": "2005.14165", "para_id": 487, "text": "Name Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server)"}
{"doc_id": "2005.14165", "para_id": 488, "text": "HellaSwag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9 33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1 33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3 LAMBADA acc test 68.0 15 42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2 22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5 22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4 LAMBADA ppl test 8.63 15 18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92 StoryCloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7 62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7"}
{"doc_id": "2005.14165", "para_id": 489, "text": "NQs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9 TriviaQA acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0 6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2 71.2 WebQs acc test 45.5 64 1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4 2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3 5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5"}
{"doc_id": "2005.14165", "para_id": 490, "text": "Ro→En 16 BLEU-mb test 39.9 64 2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9 0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6 1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5 Ro→En 16 BLEU-sb test 64 2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9 0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0 1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3 En→Ro 16 BLEU-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0 En→Ro 16 BLEU-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9 1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8 Fr→En 14 BLEU-mb test 35.0 64 1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2 1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7 4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2 Fr→En 14 BLEU-sb test 64 2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9 1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6 5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4 En→Fr 14 BLEU-mb test 45.6 64 1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2 0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3 4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6 En→Fr 14 BLEU-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9 De→En 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6 De→En 16 BLEU-sb test 64 2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6 0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1 3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0 En→De 16 BLEU-mb test 41.2 64 1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6 0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2 3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7 En→De 16 BLEU-sb test 41.2 64 2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3 0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3 3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9"}
{"doc_id": "2005.14165", "para_id": 491, "text": "Winograd acc test 93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6 Winogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7"}
{"doc_id": "2005.14165", "para_id": 492, "text": "PIQA acc dev 77.1 50 64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0 64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5 64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3 82.8 ARC (Challenge) acc test 78.5 50 26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4 25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2 25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5 ARC (Easy) acc test 92.0 50 43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8 42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2 42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1 OpenBookQA acc test 87.2 100 35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4"}
{"doc_id": "2005.14165", "para_id": 493, "text": "Quac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3 RACE-h acc test 90.0 10 35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5 34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9 34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8 RACE-m acc test 93.1 10 42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1 SQuADv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1 27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9 SQuADv2 f1 dev 93.0 16 28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8 CoQA f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0 DROP f1 dev 89.1 20 9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6 11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3 12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5"}
{"doc_id": "2005.14165", "para_id": 494, "text": "BoolQ acc dev 91.0 32 49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5 52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7 43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5 76.4 CB acc dev 96.9 32 0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4 55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3 42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1 75.6 CB f1 dev 93.9 32 0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8 60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0 Copa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0 RTE acc dev 92.5 32 47.7 49.8 48.4 56.0 46.6 55.2 62.8 63.5 53.1 47.3 49.5 49.5 54.9 54.9 56.3 70.4 52.3 48.4 46.9 50.9 56.3 49.5 60.6 72.9 69.0 WiC acc dev 76.1 32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.0 50.3 50.3 49.2 49.4 50.3 50.0 48.6 49.8 55.0 53.0 53.0 51.6 53.1 51.1 55.3 49.4 WSC acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1 MultiRC acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5 MultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 ReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2 ReCoRD f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1 SuperGLUE average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8"}
{"doc_id": "2005.14165", "para_id": 495, "text": "ANLI R1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8 ANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0 ANLI R3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2"}
{"doc_id": "2005.14165", "para_id": 496, "text": "2D+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0 2D- acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9 3D+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4 3D- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2 4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5 4D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8 5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90 2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2 1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3"}
{"doc_id": "2005.14165", "para_id": 497, "text": "Cycled Letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9 Anagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1 Anagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7 Symbol Insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 Reversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44"}
{"doc_id": "2005.14165", "para_id": 498, "text": "SAT Analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2"}
{"doc_id": "2005.14165", "para_id": 499, "text": "Table H.1: Scores for every task, setting and model that we investigate in this paper."}
{"doc_id": "2005.14165", "para_id": 500, "text": "Figure H.2: Results for SAT task. Figure H.3: All results for all Winograd tasks."}
{"doc_id": "2005.14165", "para_id": 501, "text": "Figure H.5: All results for all Cloze and Completion tasks."}
{"doc_id": "2005.14165", "para_id": 502, "text": "Figure H.6: All results for all Common Sense Reasoning tasks."}
{"doc_id": "2005.14165", "para_id": 503, "text": "Figure H.8: All results for all Reading Comprehension tasks."}
{"doc_id": "2005.14165", "para_id": 504, "text": "Figure H.11: All results for all Translation tasks."}
{"doc_id": "2005.14165", "para_id": 505, "text": "[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural information processing systems, pages 3981–3989, 2016."}
{"doc_id": "2005.14165", "para_id": 506, "text": "[AI19] WeChat AI. Tr-mt (ensemble), December 2019."}
{"doc_id": "2005.14165", "para_id": 507, "text": "[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019."}
{"doc_id": "2005.14165", "para_id": 508, "text": "[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. Language (technology) is power: A critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050, 2020."}
{"doc_id": "2005.14165", "para_id": 509, "text": "[BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013."}
{"doc_id": "2005.14165", "para_id": 510, "text": "[BDD+09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth PASCAL recognizing textual entailment challenge. 2009."}
{"doc_id": "2005.14165", "para_id": 511, "text": "[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200–2204, 2010."}
{"doc_id": "2005.14165", "para_id": 512, "text": "[BHDD+06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006."}
{"doc_id": "2005.14165", "para_id": 513, "text": "[BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language. arXiv preprint arXiv:2004.10151, 2020."}
{"doc_id": "2005.14165", "para_id": 514, "text": "[BLC13] Yoshua Bengio, Nicholas L´eonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. Arxiv, 2013."}
{"doc_id": "2005.14165", "para_id": 515, "text": "[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019."}
{"doc_id": "2005.14165", "para_id": 516, "text": "[Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "2005.14165", "para_id": 517, "text": "[CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language Conference, 1978."}
{"doc_id": "2005.14165", "para_id": 518, "text": "[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018."}
{"doc_id": "2005.14165", "para_id": 519, "text": "[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019."}
{"doc_id": "2005.14165", "para_id": 520, "text": "[CHI+18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac : Question answering in context. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 521, "text": "[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019."}
{"doc_id": "2005.14165", "para_id": 522, "text": "[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019."}
{"doc_id": "2005.14165", "para_id": 523, "text": "[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017."}
{"doc_id": "2005.14165", "para_id": 524, "text": "[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."}
{"doc_id": "2005.14165", "para_id": 525, "text": "[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiﬁcation, and recognising textual entailment, pages 177–190. Springer, 2006."}
{"doc_id": "2005.14165", "para_id": 526, "text": "[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 527, "text": "[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, 2014."}
{"doc_id": "2005.14165", "para_id": 528, "text": "[DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015."}
{"doc_id": "2005.14165", "para_id": 529, "text": "[DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat- ing projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/."}
{"doc_id": "2005.14165", "para_id": 530, "text": "[DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016."}
{"doc_id": "2005.14165", "para_id": 531, "text": "[DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019."}
{"doc_id": "2005.14165", "para_id": 532, "text": "[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. Arxiv, 2019."}
{"doc_id": "2005.14165", "para_id": 533, "text": "[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "2005.14165", "para_id": 534, "text": "[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv, abs/1703.03400, 2017."}
{"doc_id": "2005.14165", "para_id": 535, "text": "[Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000."}
{"doc_id": "2005.14165", "para_id": 536, "text": "[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019."}
{"doc_id": "2005.14165", "para_id": 537, "text": "[GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020."}
{"doc_id": "2005.14165", "para_id": 538, "text": "[GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007."}
{"doc_id": "2005.14165", "para_id": 539, "text": "[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 540, "text": "[GSL+18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018."}
{"doc_id": "2005.14165", "para_id": 541, "text": "[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza- tion of generated text. arXiv preprint arXiv: 1906.04043, 2019."}
{"doc_id": "2005.14165", "para_id": 542, "text": "[GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018."}
{"doc_id": "2005.14165", "para_id": 543, "text": "[HB20] Daniel Hernandez and Tom Brown. Ai and efﬁciency, May 2020."}
{"doc_id": "2005.14165", "para_id": 544, "text": "[HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR, abs/1904.09751, 2019."}
{"doc_id": "2005.14165", "para_id": 545, "text": "[HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020."}
{"doc_id": "2005.14165", "para_id": 546, "text": "[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "2005.14165", "para_id": 547, "text": "[HR18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "2005.14165", "para_id": 548, "text": "[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015."}
{"doc_id": "2005.14165", "para_id": 549, "text": "[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent. In International Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001."}
{"doc_id": "2005.14165", "para_id": 550, "text": "[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv:1911.03064, 2019."}
{"doc_id": "2005.14165", "para_id": 551, "text": "[IBGC+14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. A neural network for factoid question answering over paragraphs. In Empirical Methods in Natural Language Processing, 2014."}
{"doc_id": "2005.14165", "para_id": 552, "text": "[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019."}
{"doc_id": "2005.14165", "para_id": 553, "text": "[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017."}
{"doc_id": "2005.14165", "para_id": 554, "text": "[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020."}
{"doc_id": "2005.14165", "para_id": 555, "text": "[JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "2005.14165", "para_id": 556, "text": "[JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019."}
{"doc_id": "2005.14165", "para_id": 557, "text": "[JZC+19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on conversational question answering. arXiv preprint arXiv:1909.10772, 2019."}
{"doc_id": "2005.14165", "para_id": 558, "text": "[KCR+18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018."}
{"doc_id": "2005.14165", "para_id": 559, "text": "[KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Uniﬁedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020."}
{"doc_id": "2005.14165", "para_id": 560, "text": "[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s ﬁt to fabricate: Ai-generated text as a tool of media misinformation, 2020."}
{"doc_id": "2005.14165", "para_id": 561, "text": "[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020."}
{"doc_id": "2005.14165", "para_id": 562, "text": "[KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques- tions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019."}
{"doc_id": "2005.14165", "para_id": 563, "text": "[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 564, "text": "[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002."}
{"doc_id": "2005.14165", "para_id": 565, "text": "[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019."}
{"doc_id": "2005.14165", "para_id": 566, "text": "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019."}
{"doc_id": "2005.14165", "para_id": 567, "text": "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020."}
{"doc_id": "2005.14165", "para_id": 568, "text": "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint arXiv:1905.07504, 2019."}
{"doc_id": "2005.14165", "para_id": 569, "text": "[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012."}
{"doc_id": "2005.14165", "para_id": 570, "text": "[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint arXiv:2001.08210, 2020."}
{"doc_id": "2005.14165", "para_id": 571, "text": "[LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation learning using multi-task deep neural networks for semantic classiﬁcation and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015."}
{"doc_id": "2005.14165", "para_id": 572, "text": "[LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017."}
{"doc_id": "2005.14165", "para_id": 573, "text": "[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482, 2019."}
{"doc_id": "2005.14165", "para_id": 574, "text": "[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019."}
{"doc_id": "2005.14165", "para_id": 575, "text": "[Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020."}
{"doc_id": "2005.14165", "para_id": 576, "text": "[LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019."}
{"doc_id": "2005.14165", "para_id": 577, "text": "[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017."}
{"doc_id": "2005.14165", "para_id": 578, "text": "[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "2005.14165", "para_id": 579, "text": "[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020."}
{"doc_id": "2005.14165", "para_id": 580, "text": "[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018."}
{"doc_id": "2005.14165", "para_id": 581, "text": "[LWS+20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez. Train large, then compress: Rethinking model size for efﬁcient training and inference of transformers, 2020."}
{"doc_id": "2005.14165", "para_id": 582, "text": "[LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017."}
{"doc_id": "2005.14165", "para_id": 583, "text": "[LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020."}
{"doc_id": "2005.14165", "para_id": 584, "text": "[Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation, 1992."}
{"doc_id": "2005.14165", "para_id": 585, "text": "[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con- textualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305, 2017."}
{"doc_id": "2005.14165", "para_id": 586, "text": "[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013."}
{"doc_id": "2005.14165", "para_id": 587, "text": "[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016."}
{"doc_id": "2005.14165", "para_id": 588, "text": "[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. ArXiv, abs/1809.02789, 2018."}
{"doc_id": "2005.14165", "para_id": 589, "text": "[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018."}
{"doc_id": "2005.14165", "para_id": 590, "text": "[MKM+94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure. In Proceedings of the workshop on Human Language Technology, pages 114–119. Association for Computational Linguistics, 1994."}
{"doc_id": "2005.14165", "para_id": 591, "text": "[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2005.14165", "para_id": 592, "text": "[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019."}
{"doc_id": "2005.14165", "para_id": 593, "text": "[MWZ+18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018."}
{"doc_id": "2005.14165", "para_id": 594, "text": "[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020."}
{"doc_id": "2005.14165", "para_id": 595, "text": "[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. arXiv preprint arXiv:1907.07355, 2019."}
{"doc_id": "2005.14165", "para_id": 596, "text": "[Nor09] Peter Norvig. Natural language corpus data, 2009."}
{"doc_id": "2005.14165", "para_id": 597, "text": "[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866, 2019."}
{"doc_id": "2005.14165", "para_id": 598, "text": "[NWD+19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019."}
{"doc_id": "2005.14165", "para_id": 599, "text": "[PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "2005.14165", "para_id": 600, "text": "[PFB18] Jason Phang, Thibault F´evry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018."}
{"doc_id": "2005.14165", "para_id": 601, "text": "[PHR+18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of EMNLP, 2018."}
{"doc_id": "2005.14165", "para_id": 602, "text": "[PKL+16] Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016."}
{"doc_id": "2005.14165", "para_id": 603, "text": "[PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word embeddings: Architecture and representation, 2018."}
{"doc_id": "2005.14165", "para_id": 604, "text": "[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018."}
{"doc_id": "2005.14165", "para_id": 605, "text": "[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014."}
{"doc_id": "2005.14165", "para_id": 606, "text": "[QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020."}
{"doc_id": "2005.14165", "para_id": 607, "text": "[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019."}
{"doc_id": "2005.14165", "para_id": 608, "text": "[RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011."}
{"doc_id": "2005.14165", "para_id": 609, "text": "[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019."}
{"doc_id": "2005.14165", "para_id": 610, "text": "[RCP+17] Scott Reed, Yutian Chen, Thomas Paine, A¨aron van den Oord, SM Eslami, Danilo Rezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304, 2017."}
{"doc_id": "2005.14165", "para_id": 611, "text": "[RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018."}
{"doc_id": "2005.14165", "para_id": 612, "text": "[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral), 2016."}
{"doc_id": "2005.14165", "para_id": 613, "text": "[RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of EMNLP, 2019."}
{"doc_id": "2005.14165", "para_id": 614, "text": "[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301, 2018."}
{"doc_id": "2005.14165", "para_id": 615, "text": "[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018."}
{"doc_id": "2005.14165", "para_id": 616, "text": "[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012."}
{"doc_id": "2005.14165", "para_id": 617, "text": "[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales, 2019."}
{"doc_id": "2005.14165", "para_id": 618, "text": "[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020."}
{"doc_id": "2005.14165", "para_id": 619, "text": "[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019."}
{"doc_id": "2005.14165", "para_id": 620, "text": "[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019."}
{"doc_id": "2005.14165", "para_id": 621, "text": "[SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019."}
{"doc_id": "2005.14165", "para_id": 622, "text": "[SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGufﬁe, and Jasmine Wang. Release strategies and the social impacts of language models, 2019."}
{"doc_id": "2005.14165", "para_id": 623, "text": "[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019."}
{"doc_id": "2005.14165", "para_id": 624, "text": "[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."}
{"doc_id": "2005.14165", "para_id": 625, "text": "[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019."}
{"doc_id": "2005.14165", "para_id": 626, "text": "[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015."}
{"doc_id": "2005.14165", "para_id": 627, "text": "[SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "2005.14165", "para_id": 628, "text": "[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019."}
{"doc_id": "2005.14165", "para_id": 629, "text": "[SS20] Timo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation and natural language inference. arXiv preprint arXiv:2001.07676, 2020."}
{"doc_id": "2005.14165", "para_id": 630, "text": "[STQ+19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "2005.14165", "para_id": 631, "text": "[TFR+17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017."}
{"doc_id": "2005.14165", "para_id": 632, "text": "[TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103, 2005."}
{"doc_id": "2005.14165", "para_id": 633, "text": "[TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018."}
{"doc_id": "2005.14165", "para_id": 634, "text": "[TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent modules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003."}
{"doc_id": "2005.14165", "para_id": 635, "text": "[Tur20] Project Turing. Microsoft research blog, Feb 2020."}
{"doc_id": "2005.14165", "para_id": 636, "text": "[VBL+16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One Shot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016."}
{"doc_id": "2005.14165", "para_id": 637, "text": "[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017."}
{"doc_id": "2005.14165", "para_id": 638, "text": "[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand- ing systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019."}
{"doc_id": "2005.14165", "para_id": 639, "text": "[WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent dual learning. ICLR 2019, 2018."}
{"doc_id": "2005.14165", "para_id": 640, "text": "[XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training, 2019."}
{"doc_id": "2005.14165", "para_id": 641, "text": "[YdC+19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019."}
{"doc_id": "2005.14165", "para_id": 642, "text": "[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019."}
{"doc_id": "2005.14165", "para_id": 643, "text": "[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019."}
{"doc_id": "2005.14165", "para_id": 644, "text": "[ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019."}
{"doc_id": "2005.14165", "para_id": 645, "text": "[ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018."}
{"doc_id": "2005.14165", "para_id": 646, "text": "[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019."}
{"doc_id": "2005.14165", "para_id": 647, "text": "[ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris- tiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019."}
{"doc_id": "2010.11929", "para_id": 0, "text": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"}
{"doc_id": "2010.11929", "para_id": 1, "text": "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†"}
{"doc_id": "2010.11929", "para_id": 2, "text": "∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com"}
{"doc_id": "2010.11929", "para_id": 3, "text": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1"}
{"doc_id": "2010.11929", "para_id": 4, "text": "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance."}
{"doc_id": "2010.11929", "para_id": 5, "text": "In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020)."}
{"doc_id": "2010.11929", "para_id": 6, "text": "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion."}
{"doc_id": "2010.11929", "para_id": 7, "text": "When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases"}
{"doc_id": "2010.11929", "para_id": 8, "text": "1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer"}
{"doc_id": "2010.11929", "para_id": 9, "text": "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufﬁcient amounts of data."}
{"doc_id": "2010.11929", "para_id": 10, "text": "However, the picture changes if the models are trained on larger datasets (14M-300M images). We ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks."}
{"doc_id": "2010.11929", "para_id": 11, "text": "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be- come the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod- eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020)."}
{"doc_id": "2010.11929", "para_id": 12, "text": "Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self- attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efﬁciently on hardware accelerators."}
{"doc_id": "2010.11929", "para_id": 13, "text": "Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well."}
{"doc_id": "2010.11929", "para_id": 14, "text": "There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen et al., 2020c; Lu et al., 2019; Li et al., 2019)."}
{"doc_id": "2010.11929", "para_id": 15, "text": "Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsu- pervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or probed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet."}
{"doc_id": "2010.11929", "para_id": 16, "text": "Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset. The use of additional data sources allows to achieve state-of- the-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020). Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works."}
{"doc_id": "2010.11929", "para_id": 17, "text": "Linear Projection of Flattened Patches * Extra learnable [ cl ass] embedding"}
{"doc_id": "2010.11929", "para_id": 18, "text": "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable “classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017)."}
{"doc_id": "2010.11929", "para_id": 19, "text": "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efﬁcient implementations – can be used almost out of the box."}
{"doc_id": "2010.11929", "para_id": 20, "text": "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings."}
{"doc_id": "2010.11929", "para_id": 21, "text": "Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed- ded patches (z0 0 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at- tached to z0 L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at ﬁne-tuning time."}
{"doc_id": "2010.11929", "para_id": 22, "text": "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed signiﬁcant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder."}
{"doc_id": "2010.11929", "para_id": 23, "text": "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self- attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019)."}
{"doc_id": "2010.11929", "para_id": 24, "text": "The MLP contains two layers with a GELU non-linearity."}
{"doc_id": "2010.11929", "para_id": 25, "text": "z0 = [xclass; x1 pE; x2 pE; · · · ; xN p E] + Epos, E ∈R(P 2·C)×D, Epos ∈R(N+1)×D (1)"}
{"doc_id": "2010.11929", "para_id": 26, "text": "Inductive bias. We note that Vision Transformer has much less image-speciﬁc inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and transla- tionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de- scribed below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch."}
{"doc_id": "2010.11929", "para_id": 27, "text": "Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply ﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classiﬁcation input embedding and position embeddings are added as described above."}
{"doc_id": "2010.11929", "para_id": 28, "text": "Typically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For this, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward layer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. Note that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 29, "text": "We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the hybrid. To understand the data requirements of each model, we pre-train on datasets of varying size and evaluate many benchmark tasks. When considering the computational cost of pre-training the model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show that self-supervised ViT holds promise for the future."}
{"doc_id": "2010.11929", "para_id": 30, "text": "Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes and 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with 21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and 303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these dataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing follows Kolesnikov et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 31, "text": "ViT-Base 12 768 3072 12 86M ViT-Large 24 1024 4096 16 307M ViT-Huge 32 1280 5120 16 632M"}
{"doc_id": "2010.11929", "para_id": 32, "text": "Table 1: Details of Vision Transformer model variants."}
{"doc_id": "2010.11929", "para_id": 33, "text": "We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates low-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization."}
{"doc_id": "2010.11929", "para_id": 34, "text": "Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as summarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we add the larger “Huge” model. In what follows we use brief notation to indicate the model size and the input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size. Note that the Transformer’s sequence length is inversely proportional to the square of the patch size, thus models with smaller patch size are computationally more expensive."}
{"doc_id": "2010.11929", "para_id": 35, "text": "For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay- ers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized convolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020), and we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea- ture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths, we either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same number of layers in stage 3 (keeping the total number of layers), and take the output of this extended stage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model."}
{"doc_id": "2010.11929", "para_id": 36, "text": "Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning rate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum, batch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992) averaging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b)."}
{"doc_id": "2010.11929", "para_id": 37, "text": "Metrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy. Fine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective dataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem that maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This formulation allows us to recover the exact solution in closed form. Though we mainly focus on ﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation where ﬁne-tuning would be too costly."}
{"doc_id": "2010.11929", "para_id": 38, "text": "We ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from the literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which performs supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al., 2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT- 300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and BiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we report the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU v3 cores (2 per chip) used for training multiplied by the training time in days."}
{"doc_id": "2010.11929", "para_id": 39, "text": "Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L (which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa- tional resources to train. The larger model, ViT-H/14, further improves the performance, especially on the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this"}
{"doc_id": "2010.11929", "para_id": 40, "text": "Ours-JFT Ours-JFT Ours-I21k BiT-L Noisy Student (ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (EfﬁcientNet-L2)"}
{"doc_id": "2010.11929", "para_id": 41, "text": "ImageNet 88.55 ± 0.04 87.76 ± 0.03 85.30 ± 0.02 87.54 ± 0.02 88.4/88.5∗ ImageNet ReaL 90.72 ± 0.05 90.54 ± 0.03 88.62 ± 0.05 90.54 90.55 CIFAR-10 99.50 ± 0.06 99.42 ± 0.03 99.15 ± 0.03 99.37 ± 0.06 − CIFAR-100 94.55 ± 0.04 93.90 ± 0.05 93.25 ± 0.05 93.51 ± 0.08 − Oxford-IIIT Pets 97.56 ± 0.03 97.32 ± 0.11 94.67 ± 0.15 96.62 ± 0.23 − Oxford Flowers-102 99.68 ± 0.02 99.74 ± 0.00 99.61 ± 0.02 99.63 ± 0.03 − VTAB (19 tasks) 77.63 ± 0.23 76.28 ± 0.46 72.72 ± 0.21 76.29 ± 1.70 −"}
{"doc_id": "2010.11929", "para_id": 42, "text": "Table 2: Comparison with state of the art on popular image classiﬁcation benchmarks. We re- port mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision Transformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all datasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the smaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported in Touvron et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 43, "text": "70 ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)"}
{"doc_id": "2010.11929", "para_id": 44, "text": "Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups."}
{"doc_id": "2010.11929", "para_id": 45, "text": "model still took substantially less compute to pre-train than prior state of the art. However, we note that pre-training efﬁciency may be affected not only by the architecture choice, but also other pa- rameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of performance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap- proximately 30 days."}
{"doc_id": "2010.11929", "para_id": 46, "text": "Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA methods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen et al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a). ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the Specialized the performance of the top two models is similar."}
{"doc_id": "2010.11929", "para_id": 47, "text": "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of experiments."}
{"doc_id": "2010.11929", "para_id": 48, "text": "First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT- 300M. To boost the performance on the smaller datasets, we optimize three basic regularization parameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne- tuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the smallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite (moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only with JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance"}
{"doc_id": "2010.11929", "para_id": 49, "text": "2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the resolution increase during ﬁne-tuning improves the performance."}
{"doc_id": "2010.11929", "para_id": 50, "text": "ImageNet ImageNet-21k JFT-300M Pre-training dataset"}
{"doc_id": "2010.11929", "para_id": 51, "text": "10 M 30 M 100 M 300 M Number of JFT pre-training samples"}
{"doc_id": "2010.11929", "para_id": 52, "text": "Figure 3: Transfer to ImageNet. While large ViT models perform worse than BiT ResNets (shaded area) when pre-trained on small datasets, they shine when pre-trained on larger datasets. Similarly, larger ViT variants overtake smaller ones as the dataset grows."}
{"doc_id": "2010.11929", "para_id": 53, "text": "Figure 4: Linear few-shot evaluation on Ima- geNet versus pre-training size. ResNets per- form better with smaller pre-training datasets but plateau sooner than ViT, which performs better with larger pre-training. ViT-b is ViT-B with all hidden dimensions halved."}
{"doc_id": "2010.11929", "para_id": 54, "text": "Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu- tational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models."}
{"doc_id": "2010.11929", "para_id": 55, "text": "region spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but with the larger datasets, ViT overtakes."}
{"doc_id": "2010.11929", "para_id": 56, "text": "Second, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT- 300M dataset. We do not perform additional regularization on the smaller subsets and use the same hyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the effect of regularization. We do, however, use early-stopping, and report the best validation accuracy achieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne- tuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with comparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than ResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true for ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive bias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from data is sufﬁcient, even beneﬁcial."}
{"doc_id": "2010.11929", "para_id": 57, "text": "Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB (Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT is an exciting direction of future work."}
{"doc_id": "2010.11929", "para_id": 58, "text": "We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre- trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone)."}
{"doc_id": "2010.11929", "para_id": 59, "text": "Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Ap- pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu- tational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts."}
{"doc_id": "2010.11929", "para_id": 60, "text": "To begin to understand how the Vision Transformer processes im- age data, we analyze its internal representations. The ﬁrst layer of the Vision Transformer linearly projects the ﬂattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin- cipal components of the the learned embedding ﬁlters. The com- ponents resemble plausible basis functions for a low-dimensional representation of the ﬁne structure within each patch."}
{"doc_id": "2010.11929", "para_id": 61, "text": "After the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology ex- plains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4)."}
{"doc_id": "2010.11929", "para_id": 62, "text": "Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Speciﬁcally, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This “attention distance” is analogous to receptive ﬁeld size in CNNs. We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right), suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the attention distance increases with network depth. Globally, we ﬁnd that the model attends to image regions that are semantically relevant for classiﬁcation (Figure 6)."}
{"doc_id": "2010.11929", "para_id": 63, "text": "Figure 6: Representative ex- amples of attention from the output token to the input space. See Appendix D.7 for details."}
{"doc_id": "2010.11929", "para_id": 64, "text": "Transformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin"}
{"doc_id": "2010.11929", "para_id": 65, "text": "RGB embedding filters (first 28 principal components)"}
{"doc_id": "2010.11929", "para_id": 66, "text": "Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim- ilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other patches. Right: Size of attended area by head and network depth. Each dot shows the mean attention distance across images for one of 16 heads at one layer. See Appendix D.7 for details."}
{"doc_id": "2010.11929", "para_id": 67, "text": "et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT. With self-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a signiﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training. Appendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work."}
{"doc_id": "2010.11929", "para_id": 68, "text": "We have explored the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation datasets, whilst being relatively cheap to pre-train."}
{"doc_id": "2010.11929", "para_id": 69, "text": "While these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self- supervised pre-training methods. Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre- training. Finally, further scaling of ViT would likely lead to improved performance."}
{"doc_id": "2010.11929", "para_id": 70, "text": "The work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google for their help, in particular Andreas Steiner for crucial help with the infrastructure and the open- source release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale training infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions."}
{"doc_id": "2010.11929", "para_id": 71, "text": "Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020."}
{"doc_id": "2010.11929", "para_id": 72, "text": "Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 73, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In ICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 74, "text": "I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks. In ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 75, "text": "Lucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are we done with imagenet? arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 76, "text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 77, "text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 78, "text": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from pixels. In ICML, 2020a."}
{"doc_id": "2010.11929", "para_id": 79, "text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020b."}
{"doc_id": "2010.11929", "para_id": 80, "text": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c."}
{"doc_id": "2010.11929", "para_id": 81, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 82, "text": "Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self- attention and convolutional layers. In ICLR, 2020."}
{"doc_id": "2010.11929", "para_id": 83, "text": "J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009."}
{"doc_id": "2010.11929", "para_id": 84, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019."}
{"doc_id": "2010.11929", "para_id": 85, "text": "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo- lutional neural networks. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 86, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, 2016."}
{"doc_id": "2010.11929", "para_id": 87, "text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 88, "text": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi- mensional transformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 89, "text": "Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In CVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 90, "text": "Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 91, "text": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020."}
{"doc_id": "2010.11929", "para_id": 92, "text": "Olivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In ICML, 2020."}
{"doc_id": "2010.11929", "para_id": 93, "text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015."}
{"doc_id": "2010.11929", "para_id": 94, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "2010.11929", "para_id": 95, "text": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 96, "text": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."}
{"doc_id": "2010.11929", "para_id": 97, "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In NIPS, 2012."}
{"doc_id": "2010.11929", "para_id": 98, "text": "Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989."}
{"doc_id": "2010.11929", "para_id": 99, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 100, "text": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A Simple and Performant Baseline for Vision and Language. In Arxiv, 2019."}
{"doc_id": "2010.11929", "para_id": 101, "text": "Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten- tion. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 102, "text": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi- olinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 103, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 104, "text": "M. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In ICVGIP, 2008."}
{"doc_id": "2010.11929", "para_id": 105, "text": "Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012."}
{"doc_id": "2010.11929", "para_id": 106, "text": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018."}
{"doc_id": "2010.11929", "para_id": 107, "text": "B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838–855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/0330046."}
{"doc_id": "2010.11929", "para_id": 108, "text": "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv preprint arXiv:1903.10520, 2019."}
{"doc_id": "2010.11929", "para_id": 109, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing with unsupervised learning. Technical Report, 2018."}
{"doc_id": "2010.11929", "para_id": 110, "text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical Report, 2019."}
{"doc_id": "2010.11929", "para_id": 111, "text": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 112, "text": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef- fectiveness of data in deep learning era. In ICCV, 2017."}
{"doc_id": "2010.11929", "para_id": 113, "text": "Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 114, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 115, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020."}
{"doc_id": "2010.11929", "para_id": 116, "text": "Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain Gelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."}
{"doc_id": "2010.11929", "para_id": 117, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "2010.11929", "para_id": 118, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a."}
{"doc_id": "2010.11929", "para_id": 119, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint arXiv:2003.07853, 2020b."}
{"doc_id": "2010.11929", "para_id": 120, "text": "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In ACL, 2019."}
{"doc_id": "2010.11929", "para_id": 121, "text": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 122, "text": "Dirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 123, "text": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. arxiv, 2020."}
{"doc_id": "2010.11929", "para_id": 124, "text": "Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 125, "text": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classiﬁcation. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 126, "text": "Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi- Supervised Learning. In ICCV, 2019a."}
{"doc_id": "2010.11929", "para_id": 127, "text": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019b."}
{"doc_id": "2010.11929", "para_id": 128, "text": "Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 129, "text": "Models Dataset Epochs Base LR LR decay Weight decay Dropout"}
{"doc_id": "2010.11929", "para_id": 130, "text": "ViT-B/{16,32} JFT-300M 7 8 · 10−4 linear 0.1 0.0 ViT-L/32 JFT-300M 7 6 · 10−4 linear 0.1 0.0 ViT-L/16 JFT-300M 7/14 4 · 10−4 linear 0.1 0.0 ViT-H/14 JFT-300M 14 3 · 10−4 linear 0.1 0.0 R50x{1,2} JFT-300M 7 10−3 linear 0.1 0.0 R101x1 JFT-300M 7 8 · 10−4 linear 0.1 0.0 R152x{1,2} JFT-300M 7 6 · 10−4 linear 0.1 0.0 R50+ViT-B/{16,32} JFT-300M 7 8 · 10−4 linear 0.1 0.0 R50+ViT-L/32 JFT-300M 7 2 · 10−4 linear 0.1 0.0 R50+ViT-L/16 JFT-300M 7/14 4 · 10−4 linear 0.1 0.0 ViT-B/{16,32} ImageNet-21k 90 10−3 linear 0.03 0.1 ViT-L/{16,32} ImageNet-21k 30/90 10−3 linear 0.03 0.1 ViT-∗ ImageNet 300 3 · 10−3 cosine 0.3 0.1"}
{"doc_id": "2010.11929", "para_id": 131, "text": "Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn- ing rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient clipping at global norm 1. Training resolution is 224."}
{"doc_id": "2010.11929", "para_id": 132, "text": "Standard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi- tectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all values v in the sequence. The attention weights Aij are based on the pairwise similarity between two elements of the sequence and their respective query qi and key kj representations."}
{"doc_id": "2010.11929", "para_id": 133, "text": "Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations, called “heads”, in parallel, and project their concatenated outputs. To keep compute and number of parameters constant when changing k, Dh (Eq. 5) is typically set to D/k."}
{"doc_id": "2010.11929", "para_id": 134, "text": "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa Umsa ∈Rk·Dh×D (8)"}
{"doc_id": "2010.11929", "para_id": 135, "text": "Table 3 summarizes our training setups for our different models. We found strong regularization to be key when training models from scratch on ImageNet. Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all training is done on resolution 224."}
{"doc_id": "2010.11929", "para_id": 136, "text": "We ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over learning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training set (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the remaining data. For ﬁnal results we train on the entire training set and evaluate on the respective test data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only exception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,"}
{"doc_id": "2010.11929", "para_id": 137, "text": "ImageNet 20 000 {0.003, 0.01, 0.03, 0.06} CIFAR100 10 000 {0.001, 0.003, 0.01, 0.03} CIFAR10 10 000 {0.001, 0.003, 0.01, 0.03} Oxford-IIIT Pets 500 {0.001, 0.003, 0.01, 0.03} Oxford Flowers-102 500 {0.001, 0.003, 0.01, 0.03} VTAB (19 tasks) 2 500 0.01"}
{"doc_id": "2010.11929", "para_id": 138, "text": "Table 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay, a batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise, ﬁne-tuning resolution is 384."}
{"doc_id": "2010.11929", "para_id": 139, "text": "for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across this run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384 resolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov et al., 2020))."}
{"doc_id": "2010.11929", "para_id": 140, "text": "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and replace it by a single, zero-initialized linear layer outputting the number of classes required by the target dataset. We found this to be a little more robust than simply re-initializing the very last layer."}
{"doc_id": "2010.11929", "para_id": 141, "text": "For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter setting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this setting by running a small sweep over two learning rates and two schedules, and selecting the setting with the highest VTAB score on the 200-example validation sets. We follow the pre-processing used in Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd that Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks."}
{"doc_id": "2010.11929", "para_id": 142, "text": "We employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations."}
{"doc_id": "2010.11929", "para_id": 143, "text": "We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We use Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay. As prediction targets for pretraining we tried the following settings: 1) predicting only the mean, 3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16 patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch using L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite well, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown best few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al. (2019) but results were also slightly worse on our few-shot metrics."}
{"doc_id": "2010.11929", "para_id": 144, "text": "Lastly, we would like to remark that our instantiation of masked patch prediction doesn’t require such an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim- ilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on downstream performance after 100k pretraining steps, and see similar gains when pretraining on ImageNet."}
{"doc_id": "2010.11929", "para_id": 145, "text": "We report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds to Figure 3 from the paper and shows transfer performance of different ViT models pre-trained on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to"}
{"doc_id": "2010.11929", "para_id": 146, "text": "ImageNet CIFAR-10 98.13 97.77 97.86 97.94 - CIFAR-100 87.13 86.31 86.35 87.07 - ImageNet 77.91 73.38 76.53 71.16 - ImageNet ReaL 83.57 79.56 82.19 77.83 - Oxford Flowers-102 89.49 85.43 89.66 86.36 - Oxford-IIIT-Pets 93.81 92.04 93.64 91.35 -"}
{"doc_id": "2010.11929", "para_id": 147, "text": "ImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27 CIFAR-100 91.67 91.97 93.44 93.04 93.82 ImageNet 83.97 81.28 85.15 80.99 85.13 ImageNet ReaL 88.35 86.63 88.40 85.65 88.70 Oxford Flowers-102 99.38 99.11 99.61 99.19 99.51 Oxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82"}
{"doc_id": "2010.11929", "para_id": 148, "text": "JFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50 CIFAR-100 91.87 90.49 94.04 92.52 94.55 ImageNet 84.15 80.73 87.12 84.37 88.04 ImageNet ReaL 88.85 86.27 89.99 88.28 90.33 Oxford Flowers-102 99.56 99.27 99.56 99.45 99.68 Oxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56"}
{"doc_id": "2010.11929", "para_id": 149, "text": "Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im- ageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models are ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional techniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2."}
{"doc_id": "2010.11929", "para_id": 150, "text": "Epochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs name"}
{"doc_id": "2010.11929", "para_id": 151, "text": "ViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55 ViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224 ViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196 ViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783 ViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567 ViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262"}
{"doc_id": "2010.11929", "para_id": 152, "text": "ResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50 ResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199 ResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96 ResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141 ResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563 ResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126 ResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306"}
{"doc_id": "2010.11929", "para_id": 153, "text": "R50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106 R50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274 R50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246 R50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859 R50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668"}
{"doc_id": "2010.11929", "para_id": 154, "text": "Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main paper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex- aFLOPs)."}
{"doc_id": "2010.11929", "para_id": 155, "text": "Figure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of varying size, as well as the estimated computational cost of their pre-training."}
{"doc_id": "2010.11929", "para_id": 156, "text": "ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional. Here we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning"}
{"doc_id": "2010.11929", "para_id": 157, "text": "ImageNet 77.54 78.24 84.97 84.37 CIFAR10 97.67 97.46 99.06 99.07 CIFAR100 86.07 85.17 92.05 91.06 Oxford-IIIT Pets 91.11 91.00 95.37 94.79 Oxford Flowers-102 94.26 92.06 98.62 99.32 Average 89.33 88.79 94.01 93.72"}
{"doc_id": "2010.11929", "para_id": 158, "text": "Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD."}
{"doc_id": "2010.11929", "para_id": 159, "text": "Figure 8: Scaling different model dimensions of the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 160, "text": "performance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For SGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented in Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average. This justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the absolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only for 7 epochs, not 30."}
{"doc_id": "2010.11929", "para_id": 161, "text": "We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet for different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024, DMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the depth results in the biggest improvements which are clearly visible up until 64 layers. However, diminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net- work seems to result in the smallest changes. Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters. These ﬁndings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling all dimensions proportionally results in robust improvements."}
{"doc_id": "2010.11929", "para_id": 162, "text": "In order to stay as close as possible to the original Transformer model, we made use of an additional [class] token, which is taken as image representation. The output of this token is then trans- formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer."}
{"doc_id": "2010.11929", "para_id": 163, "text": "This design is inherited from the Transformer model for text, and we use it throughout the main paper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead,"}
{"doc_id": "2010.11929", "para_id": 164, "text": "Figure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly well, but require different learning-rates."}
{"doc_id": "2010.11929", "para_id": 165, "text": "Pos. Emb. Default/Stem Every Layer Every Layer-Shared"}
{"doc_id": "2010.11929", "para_id": 166, "text": "No Pos. Emb. 0.61382 N/A N/A 1-D Pos. Emb. 0.64206 0.63964 0.64292 2-D Pos. Emb. 0.64001 0.64046 0.64022 Rel. Pos. Emb. 0.64032 N/A N/A"}
{"doc_id": "2010.11929", "para_id": 167, "text": "Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on ImageNet 5-shot linear."}
{"doc_id": "2010.11929", "para_id": 168, "text": "the difference in performance is fully explained by the requirement for a different learning-rate, see Figure 9."}
{"doc_id": "2010.11929", "para_id": 169, "text": "We ran ablations on different ways of encoding spatial information using positional embedding. We tried the following cases:"}
{"doc_id": "2010.11929", "para_id": 170, "text": "• Providing no positional information: Considering the inputs as a bag of patches."}
{"doc_id": "2010.11929", "para_id": 171, "text": "• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in the raster order (default across all other experiments in this paper)."}
{"doc_id": "2010.11929", "para_id": 172, "text": "• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two dimensions. In this case, two sets of embeddings are learned, each for one of the axes, X-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on the path in the input, we concatenate the X and Y embedding to get the ﬁnal positional embedding for that patch."}
{"doc_id": "2010.11929", "para_id": 173, "text": "• Relative positional embeddings: Considering the relative distance between patches to en- code the spatial information as instead of their absolute position. To do so, we use 1- dimensional Relative Attention, in which we deﬁne the relative distance all possible pairs of patches. Thus, for every given pair (one as query, and the other as key/value in the at- tention mechanism), we have an offset pq −pk, where each offset is associated with an embedding. Then, we simply run extra attention, where we use the original query (the content of query), but use relative positional embeddings as keys. We then use the log- its from the relative attention as a bias term and add it to the logits of the main attention (content-based attention) before applying the softmax."}
{"doc_id": "2010.11929", "para_id": 174, "text": "In addition to different ways of encoding spatial information, we also tried different ways of in- corporating this information in our model. For the 1-dimensional and 2-dimensional positional embeddings, we tried three different cases: (1) add positional embeddings to the inputs right after"}
{"doc_id": "2010.11929", "para_id": 175, "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 Input patch column"}
{"doc_id": "2010.11929", "para_id": 176, "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 Input patch column"}
{"doc_id": "2010.11929", "para_id": 177, "text": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 Input patch column"}
{"doc_id": "2010.11929", "para_id": 178, "text": "Figure 10: Position embeddings of models trained with different hyperparameters."}
{"doc_id": "2010.11929", "para_id": 179, "text": "the stem of them model and before feeding the inputs to the Transformer encoder (default across all other experiments in this paper); (2) learn and add positional embeddings to the inputs at the beginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of each layer (shared between layers)."}
{"doc_id": "2010.11929", "para_id": 180, "text": "Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while there is a large gap between the performances of the model with no positional embedding and mod- els with positional embedding, there is little to no difference between different ways of encoding positional information. We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor- tant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re- lations in this resolution is equally easy for these different positional encoding strategies. Even so, the speciﬁc pattern of position embedding similarity learned by the network depends on the training hyperparameters (Figure 10)."}
{"doc_id": "2010.11929", "para_id": 181, "text": "Figure 11: Size of attended area by head and network depth. Attention distance was computed for 128 example images by averaging the distance between the query pixel and all other pixels, weighted by the attention weight. Each dot shows the mean attention distance across images for one of 16 heads at one layer. Image width is 224 pixels."}
{"doc_id": "2010.11929", "para_id": 182, "text": "We are also interested in real-world speed of the architectures on our hardware, which is not always well predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,"}
{"doc_id": "2010.11929", "para_id": 183, "text": "we perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the difference between inference and backprop speed is a constant model-independent factor."}
{"doc_id": "2010.11929", "para_id": 184, "text": "Figure 12 (left) shows how many images one core can handle per second, across various input sizes. Every single point refers to the peak performance measured across a wide range of batch-sizes. As can be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening for the largest models at the largest resolutions."}
{"doc_id": "2010.11929", "para_id": 185, "text": "Another quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being better for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models. This shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet models."}
{"doc_id": "2010.11929", "para_id": 186, "text": "Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models have speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with various architectures across input sizes. ViT models are clearly more memory-efﬁcient."}
{"doc_id": "2010.11929", "para_id": 187, "text": "Axial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self- attention on large inputs that are organized as multidimensional tensors. The general idea of axial attention is to perform multiple attention operations, each along a single axis of the input tensor, instead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which all the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e. a row and column attention, augmented by relative positional encoding. We have implemented AxialResNet as a baseline model.3."}
{"doc_id": "2010.11929", "para_id": 188, "text": "Moreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1- dimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of a self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a column-self-attention plus an MLP."}
{"doc_id": "2010.11929", "para_id": 189, "text": "Figure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima- geNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of number of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32 and Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at"}
{"doc_id": "2010.11929", "para_id": 190, "text": "3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/ csrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al., 2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very slow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be unlocked by a carefully optimized implementation."}
{"doc_id": "2010.11929", "para_id": 191, "text": "Figure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet 5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left)."}
{"doc_id": "2010.11929", "para_id": 192, "text": "the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self- attention and although the sequence length that self-attention operates on is smaller in axial case, there is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in terms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow on TPUs (Figure 13, right)."}
{"doc_id": "2010.11929", "para_id": 193, "text": "To understand how ViT uses self-attention to integrate information across the image, we analyzed the average distance spanned by attention weights at different layers (Figure 11). This “attention distance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. As depth increases, attention distance increases for all heads. In the second half of the network, most heads attend widely across tokens."}
{"doc_id": "2010.11929", "para_id": 194, "text": "To compute maps of the attention from the output token to the input space (Figures 6 and 14), we used Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT- L/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts for the mixing of attention across tokens through all layers."}
{"doc_id": "2010.11929", "para_id": 195, "text": "We also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation setup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy."}
{"doc_id": "2010.11929", "para_id": 196, "text": "Table 9 shows the scores attained on each of the VTAB-1k tasks."}
{"doc_id": "2010.11929", "para_id": 197, "text": "Figure 14: Further example attention maps as in Figure 6 (random selection)."}
{"doc_id": "2010.11929", "para_id": 198, "text": "Table 9: Breakdown of VTAB-1k performance across tasks."}
{"doc_id": "2010.11929", "para_id": 199, "text": "ViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6 ViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3 ViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7"}
{"doc_id": "2103.00020", "para_id": 0, "text": "Learning Transferable Visual Models From Natural Language Supervision"}
{"doc_id": "2103.00020", "para_id": 1, "text": "Alec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1"}
{"doc_id": "2103.00020", "para_id": 2, "text": "Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1"}
{"doc_id": "2103.00020", "para_id": 3, "text": "Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of mag- nitude in compute, model capacity, and data, steadily im- proving capabilities. The development of “text-to-text” as a standardized input-output interface (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2019) has enabled task- agnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset speciﬁc customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset speciﬁc training data."}
{"doc_id": "2103.00020", "para_id": 4, "text": "State-of-the-art computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of super- vision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which im- age is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmark- ing on over 30 different existing computer vi- sion datasets, spanning tasks such as OCR, ac- tion recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset spe- ciﬁc training. For instance, we match the ac- curacy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."}
{"doc_id": "2103.00020", "para_id": 5, "text": "These results suggest that the aggregate supervision acces- sible to modern pre-training methods within web-scale col- lections of text surpasses that of high-quality crowd-labeled NLP datasets. However, in other ﬁelds such as computer vision it is still standard practice to pre-train models on crowd-labeled datasets such as ImageNet (Deng et al., 2009). Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision? Prior work is encouraging."}
{"doc_id": "2103.00020", "para_id": 6, "text": "Over 20 years ago Mori et al. (1999) explored improving content based image retrieval by training a model to pre- dict the nouns and adjectives in text documents paired with images. Quattoni et al. (2007) demonstrated it was possi- ble to learn more data efﬁcient image representations via manifold learning in the weight space of classiﬁers trained to predict words in captions associated with images. Sri- vastava & Salakhutdinov (2012) explored deep represen- tation learning by training multimodal Deep Boltzmann Machines on top of low-level image and text tag features. Joulin et al. (2016) modernized this line of work and demon- strated that CNNs trained to predict words in image cap- tions learn useful image representations. They converted the title, description, and hashtag metadata of images in the YFCC100M dataset (Thomee et al., 2016) into a bag-of- words multi-label classiﬁcation task and showed that pre- training AlexNet (Krizhevsky et al., 2012) to predict these labels learned representations which preformed similarly to ImageNet-based pre-training on transfer tasks. Li et al. (2017) then extended this approach to predicting phrase n- grams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image"}
{"doc_id": "2103.00020", "para_id": 7, "text": "Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad- ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 8, "text": "*Equal contribution 1OpenAI, San Francisco, CA 94110, USA. Correspondence to: <{alec, jongwook}@openai.com>."}
{"doc_id": "2103.00020", "para_id": 9, "text": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classiﬁer to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the target dataset’s classes."}
{"doc_id": "2103.00020", "para_id": 10, "text": "classiﬁcation datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architec- tures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con- VIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn im- age representations from text."}
{"doc_id": "2103.00020", "para_id": 11, "text": "mises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its general- ity. Both approaches also use static softmax classiﬁers to perform prediction and lack a mechanism for dynamic out- puts. This severely curtails their ﬂexibility and limits their “zero-shot” capabilities."}
{"doc_id": "2103.00020", "para_id": 12, "text": "A crucial difference between these weakly supervised mod- els and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classiﬁers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (im- age, text) pairs and demonstrate that a simpliﬁed version of ConVIRT trained from scratch, which we call CLIP, for Con- trastive Language-Image Pre-training, is an efﬁcient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and ob- serve that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017; Kaplan et al., 2020). We ﬁnd that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and ﬁnd"}
{"doc_id": "2103.00020", "para_id": 13, "text": "While exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on com- mon benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic com- puter vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram im- ages is an effective pre-training task. When ﬁne-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset."}
{"doc_id": "2103.00020", "para_id": 14, "text": "This line of work represents the current pragmatic middle ground between learning from a limited amount of super- vised “gold-labels” and learning from practically unlimited amounts of raw text. However, it is not without compro-"}
{"doc_id": "2103.00020", "para_id": 15, "text": "vision. Although early work wrestled with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual represen- tation learning suggest we now have the tools to effectively leverage this abundant source of supervision (McCann et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 16, "text": "Learning from natural language has several potential strengths over other training methods. It’s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classiﬁcation since it does not require annotations to be in a classic “machine learning compatible format” such as the canonical 1-of-N majority vote “gold label”. Instead, methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet. Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn’t “just” learn a representation but also connects that representation to language which enables ﬂexible zero-shot transfer. In the following subsections, we detail the speciﬁc approach we settled on."}
{"doc_id": "2103.00020", "para_id": 17, "text": "Bag of Words Contrastive (CLIP) Bag of Words Prediction Transformer Language Model"}
{"doc_id": "2103.00020", "para_id": 18, "text": "Figure 2. CLIP is much more efﬁcient at zero-shot transfer than our image caption baseline. Although highly expressive, we found that transformer-based language models are relatively weak at zero-shot ImageNet classiﬁcation. Here, we see that it learns 3x slower than a baseline which predicts a bag-of-words (BoW) encoding of the text (Joulin et al., 2016). Swapping the prediction objective for the contrastive objective of CLIP further improves efﬁciency another 4x."}
{"doc_id": "2103.00020", "para_id": 19, "text": "it can be competitive with prior task-speciﬁc supervised models. We also conﬁrm these ﬁndings with linear-probe representation learning analysis and show that CLIP out- performs the best publicly available ImageNet model while also being more computationally efﬁcient. We additionally ﬁnd that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model’s capability. These re- sults have signiﬁcant policy and ethical implications, which we consider in Section 7."}
{"doc_id": "2103.00020", "para_id": 20, "text": "Existing work has mainly used three datasets, MS-COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), and YFCC100M (Thomee et al., 2016). While MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each. By comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos (Mahajan et al., 2018). YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automati- cally generated ﬁlenames like 20160716 113957.JPG as “titles” or contain “descriptions” of camera exposure settings. After ﬁltering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos. This is approximately the same size as ImageNet."}
{"doc_id": "2103.00020", "para_id": 21, "text": "At the core of our approach is the idea of learning percep- tion from supervision contained in natural language. As discussed in the introduction, this is not at all a new idea, however terminology used to describe work in this space is varied, even seemingly contradictory, and stated motiva- tions are diverse. Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), and Desai & Johnson (2020) all intro- duce methods which learn visual representations from text paired with images but describe their approaches as unsuper- vised, self-supervised, weakly supervised, and supervised respectively."}
{"doc_id": "2103.00020", "para_id": 22, "text": "A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. Since existing datasets do not adequately reﬂect this possibility, considering results only on them would un- derestimate the potential of this line of research. To address this, we constructed a new dataset of 400 million (image, text) pairs collected form a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process whose text includes one of a set of 500,000 queries.1 We approximately class"}
{"doc_id": "2103.00020", "para_id": 23, "text": "We emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal. All these approaches are learning from natural language super-"}
{"doc_id": "2103.00020", "para_id": 24, "text": "1The base query list is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams"}
{"doc_id": "2103.00020", "para_id": 25, "text": "balance the results by including up to 20,000 (image, text) pairs per query. The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2. We refer to this dataset as WIT for WebImageText."}
{"doc_id": "2103.00020", "para_id": 26, "text": "multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similar- ity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N 2 −N incorrect pairings. We opti- mize a symmetric cross entropy loss over these similarity scores. In Figure 3 we include pseudocode of the core of an implementation of CLIP. To our knowledge this batch con- struction technique and objective was ﬁrst introduced in the area of deep metric learning as the multi-class N-pair loss Sohn (2016), was popularized for contrastive representation learning by Oord et al. (2018) as the InfoNCE loss, and was recently adapted for contrastive (text, image) representation learning in the domain of medical imaging by Zhang et al. (2020)."}
{"doc_id": "2103.00020", "para_id": 27, "text": "State-of-the-art computer vision systems use very large amounts of compute. Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al. (2020) required 33 TPUv3 core-years to train their Noisy Student EfﬁcientNet-L2. When considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting. In the course of our ef- forts, we found training efﬁciency was key to successfully scaling natural language supervision and we selected our ﬁnal pre-training method based on this metric."}
{"doc_id": "2103.00020", "para_id": 28, "text": "Due to the large size of our pre-training dataset, over-ﬁtting is not a major concern and the details of training CLIP are simpliﬁed compared to the implementation of Zhang et al. (2020). We train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights. We do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bach- man et al. (2019) and popularized by Chen et al. (2020b). We instead use only a linear projection to map from each en- coder’s representation to the multi-modal embedding space. We did not notice a difference in training efﬁciency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function tu from Zhang et al. (2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP’s pre- training dataset are only a single sentence. We also simplify the image transformation function tv. A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, τ, is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter."}
{"doc_id": "2103.00020", "para_id": 29, "text": "Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difﬁculties efﬁciently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of- words encoding of the same text."}
{"doc_id": "2103.00020", "para_id": 30, "text": "Both these approaches share a key similarity. They try to pre- dict the exact words of the text accompanying each image. This is a difﬁcult task due to the wide variety of descriptions, comments, and related text that co-occur with images. Re- cent work in contrastive representation learning for images has found that contrastive objectives can learn better repre- sentations than their equivalent predictive objective (Tian et al., 2019). Other work has found that although generative models of images can learn high quality image representa- tions, they require over an order of magnitude more compute than contrastive models with the same performance (Chen et al., 2020a). Noting these ﬁndings, we explored training a system to solve the potentially easier proxy task of pre- dicting only which text as a whole is paired with which image and not the exact words of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective for a contrastive objective in Figure 2 and observed a further 4x efﬁciency improvement in the rate of zero-shot transfer to ImageNet."}
{"doc_id": "2103.00020", "para_id": 31, "text": "We consider two different architectures for the image en- coder. For the ﬁrst, we use ResNet-50 (He et al., 2016a) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make sev- eral modiﬁcations to the original version using the ResNet- D improvements from He et al. (2019) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a sin- gle layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled"}
{"doc_id": "2103.00020", "para_id": 32, "text": "Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N × N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a"}
{"doc_id": "2103.00020", "para_id": 33, "text": "with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added."}
{"doc_id": "2103.00020", "para_id": 34, "text": "# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter"}
{"doc_id": "2103.00020", "para_id": 35, "text": "one dimension of the model. While Tan & Le (2019) tune the ratio of compute allocated to each dimension for their EfﬁcientNet architecture, we use a simple baseline of allo- cating additional compute equally to increasing the width, depth, and resolution of the model. For the text encoder, we only scale the width of the model to be proportional to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP’s performance to be less sensitive to the capacity of the text encoder."}
{"doc_id": "2103.00020", "para_id": 36, "text": "# extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t]"}
{"doc_id": "2103.00020", "para_id": 37, "text": "# joint multimodal embedding [n, d_e] I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1)"}
{"doc_id": "2103.00020", "para_id": 38, "text": "We train a series of 5 ResNets and 3 Vision Transformers. For the ResNets we train a ResNet-50, a ResNet-101, and then 3 more which follow EfﬁcientNet-style model scaling and use approximately 4x, 16x, and 64x the compute of a ResNet-50. They are denoted as RN50x4, RN50x16, and RN50x64 respectively. For the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all models for 32 epochs. We use the Adam optimizer (Kingma & Ba, 2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2017) applied to all weights that are not gains or biases, and decay the learning rate using a cosine schedule (Loshchilov & Hutter, 2016). Initial hyper- parameters were set using a combination of grid searches, random search, and manual tuning on the baseline ResNet- 50 model when trained for 1 epoch. Hyper-parameters were then adapted heuristically for larger models due to compu- tational constraints. The learnable temperature parameter τ was initialized to the equivalent of 0.07 from (Wu et al., 2018) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training in- stability. We use a very large minibatch size of 32,768. Mixed-precision (Micikevicius et al., 2017) was used to ac- celerate training and save memory. To save additional mem- ory, gradient checkpointing (Griewank & Walther, 2000; Chen et al., 2016), half-precision Adam statistics (Dhariwal et al., 2020), and half-precision stochastically rounded text encoder weights were used. The calculation of embedding similarities was also sharded with individual GPUs comput- ing only the subset of the pairwise similarities necessary for their local batch of embeddings. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs. For the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost perfor- mance similar to FixRes (Touvron et al., 2019). We denote this model as ViT-L/14@336px. Unless otherwise speciﬁed, all results reported in this paper as “CLIP” use this model which we found to perform best."}
{"doc_id": "2103.00020", "para_id": 39, "text": "# scaled pairwise cosine similarities [n, n] logits = np.dot(I_e, T_e.T) * np.exp(t)"}
{"doc_id": "2103.00020", "para_id": 40, "text": "# symmetric loss function labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2"}
{"doc_id": "2103.00020", "para_id": 41, "text": "Figure 3. Numpy-like pseudocode for the core of an implementa- tion of CLIP."}
{"doc_id": "2103.00020", "para_id": 42, "text": "representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modiﬁcation of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme."}
{"doc_id": "2103.00020", "para_id": 43, "text": "The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modiﬁcations described in Radford et al. (2019). As a base size we use a 63M-parameter 12- layer 512-wide model with 8 attention heads. The trans- former operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sen- nrich et al., 2015). For computational efﬁciency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activa- tions of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to ini- tialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work."}
{"doc_id": "2103.00020", "para_id": 44, "text": "While previous computer vision research has often scaled models by increasing the width (Mahajan et al., 2018) or depth (He et al., 2016a) in isolation, for the ResNet image encoders we adapt the approach of Tan & Le (2019) which found that allocating additional compute across all of width, depth, and resolution outperforms only allocating it to only"}
{"doc_id": "2103.00020", "para_id": 45, "text": "training as a transfer learning method to improve supervised ﬁne-tuning, it also included an ablation study demonstrat- ing that the performance of four heuristic zero-shot transfer methods improved steadily over the course of pre-training, without any supervised adaption. This analysis served as the basis for GPT-2 (Radford et al., 2019) which focused exclu- sively on studying the task-learning capabilities of language models via zero-shot transfer."}
{"doc_id": "2103.00020", "para_id": 46, "text": "In computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image classiﬁcation (Lampert et al., 2009). We instead use the term in a broader sense and study generalization to unseen datasets. We motivate this as a proxy for performing un- seen tasks, as aspired to in the zero-data learning paper of Larochelle et al. (2008). While much research in the ﬁeld of unsupervised learning focuses on the representation learn- ing capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task- learning capabilities of machine learning systems. In this view, a dataset evaluates performance on a task on a spe- ciﬁc distribution. However, many popular computer vision datasets were created by the research community primarily as benchmarks to guide the development of generic image classiﬁcation methods rather than measuring performance on a speciﬁc task. While it is reasonable to say that the SVHN dataset measures the task of street number transcrip- tion on the distribution of Google Street View photos, it is unclear what “real” task the CIFAR-10 dataset measures. It is clear, however, what distribution CIFAR-10 is drawn from - TinyImages (Torralba et al., 2008). On these kinds of datasets, zero-shot transfer is more an evaluation of CLIP’s robustness to distribution shift and domain generalization rather than task generalization. Please see Section 3.3 for analysis focused on this."}
{"doc_id": "2103.00020", "para_id": 47, "text": "CLIP is pre-trained to predict if an image and a text snippet are paired together in its dataset. To perform zero-shot clas- siﬁcation, we reuse this capability. For each dataset, we use the names of all the classes in the dataset as the set of poten- tial text pairings and predict the most probable (image, text) pair according to CLIP. In a bit more detail, we ﬁrst compute the feature embedding of the image and the feature embed- ding of the set of possible texts by their respective encoders. The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter τ, and normalized into a probability distribution via a softmax. Note that this predic- tion layer is a multinomial logistic regression classiﬁer with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork (Ha et al., 2016) which generates the weights of a linear classiﬁer based on the text specifying the visual concepts that the classes represent. Lei Ba et al. (2015) ﬁrst introduced a zero-shot image classiﬁer of this form while the idea of generating a classiﬁer from natural language dates back to at least Elhoseiny et al. (2013). Continuing with this interpretation, every step of CLIP pre-training can be viewed as optimizing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32,768 total classes deﬁned via natural language descriptions. For zero-shot evaluation, we cache the zero-shot classiﬁer once it has been computed by the text encoder and reuse it for all subsequent predictions. This allows the cost of generating it to be amortized across all the predictions in a dataset."}
{"doc_id": "2103.00020", "para_id": 48, "text": "To our knowledge, Visual N-Grams (Li et al., 2017) ﬁrst studied zero-shot transfer to existing image classiﬁcation datasets in the manner described above. It is also the only other work we are aware of that has studied zero-shot trans- fer to standard image classiﬁcation datasets using a gener- ically pre-trained model and serves as the best reference point for contextualizing CLIP. Their approach learns the parameters of a dictionary of 142,806 visual n-grams (span- ning 1- to 5- grams) and optimizes these n-grams using a differential version of Jelinek-Mercer smoothing to maxi- mize the probability of all text n-grams for a given image. In order to perform zero-shot transfer, they ﬁrst convert the text of each of the dataset’s class names into its n-gram representation and then compute its probability according to their model, predicting the one with the highest score."}
{"doc_id": "2103.00020", "para_id": 49, "text": "In Table 1 we compare Visual N-Grams to CLIP. The best CLIP model improves accuracy on ImageNet from a proof of concept 11.5% to 76.2% and matches the performance of the original ResNet-50 despite using none of the 1.28 million crowd-labeled training examples available for this dataset. Additionally, the top-5 accuracy of CLIP models are noticeably higher than their top-1, and this model has a 95% top-5 accuracy, matching Inception-V4 (Szegedy et al., 2016). The ability to match the performance of a strong, fully supervised baselines in a zero-shot setting suggests"}
{"doc_id": "2103.00020", "para_id": 50, "text": "Our focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learn- ing in the ﬁeld of NLP. To our knowledge Liu et al. (2018) ﬁrst identiﬁed task learning as an “unexpected side-effect” when a language model trained to generate Wikipedia ar- ticles learned to reliably transliterate names between lan- guages. While GPT-1 (Radford et al., 2018) focused on pre-"}
{"doc_id": "2103.00020", "para_id": 51, "text": "Table 1. Comparing CLIP to prior zero-shot transfer image classi- ﬁcation results. CLIP improves performance on all three datasets by a large amount. This improvement reﬂects many differences in the 4 years since the development of Visual N-Grams (Li et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 52, "text": "CLIP is a signiﬁcant step towards ﬂexible and practical zero-shot computer vision classiﬁers. As mentioned above, the comparison to Visual N-Grams is meant for contextu- alizing the performance of CLIP and should not be inter- preted as a direct methods comparison between CLIP and Visual N-Grams as many performance relevant differences between the two systems were not controlled for. For in- stance, we train on a dataset that is 10x larger, use a vision model that requires nearly 100x more compute per predic- tion, likely used over 1000x their training compute, and use a transformer-based model which did not exist when Visual N-Grams was published. As a closer comparison, we trained a CLIP ResNet-50 on the same YFCC100M dataset that Visual N-Grams was trained on and found it matched their reported ImageNet performance within a V100 GPU day. This baseline was also trained from scratch instead of being initialized from pre-trained ImageNet weights as in Visual N-Grams."}
{"doc_id": "2103.00020", "para_id": 53, "text": "Prompt engineering and ensembling Contextless class names (Li et al. 2017)"}
{"doc_id": "2103.00020", "para_id": 54, "text": "Figure 4. Prompt engineering and ensembling improve zero- shot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classiﬁcation performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is “free” when amortized over many predictions."}
{"doc_id": "2103.00020", "para_id": 55, "text": "chosen somewhat haphazardly and do not anticipate issues related to zero-shot transfer which relies on task description in order to transfer successfully."}
{"doc_id": "2103.00020", "para_id": 56, "text": "CLIP also outperforms Visual N-Grams on the other 2 re- ported datasets. On aYahoo, CLIP achieves a 95% reduction in the number of errors, and on SUN, CLIP more than dou- bles the accuracy of Visual N-Grams. To conduct a more comprehensive analysis and stress test, we implement a much larger evaluation suite detailed in Appendix A. In total we expand from the 3 datasets reported in Visual N- Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results."}
{"doc_id": "2103.00020", "para_id": 57, "text": "A common issue is polysemy. When the name of a class is the only information provided to CLIP’s text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset! This happens in ImageNet which contains both construction cranes and cranes that ﬂy. Another example is found in classes of the Oxford-IIIT Pet dataset where the word boxer is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete."}
{"doc_id": "2103.00020", "para_id": 58, "text": "Most standard image classiﬁcation datasets treat the infor- mation naming or describing classes which enables natural language based zero-shot transfer as an afterthought. The vast majority of datasets annotate images with just a numeric id of the label and contain a ﬁle mapping these ids back to their names in English. Some datasets, such as Flowers102 and GTSRB, don’t appear to include this mapping at all in their released versions preventing zero-shot transfer en- tirely.2 For many datasets, we observed these labels may be"}
{"doc_id": "2103.00020", "para_id": 59, "text": "Another issue we encountered is that it’s relatively rare in our pre-training dataset for the text paired with the image to be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template “A photo of a {label}.” to be a good default that helps specify the text is about the content of the image. This often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%."}
{"doc_id": "2103.00020", "para_id": 60, "text": "2Alec learned much more about ﬂower species and German trafﬁc signs over the course of this project than he originally antic- ipated."}
{"doc_id": "2103.00020", "para_id": 61, "text": "Similar to the “prompt engineering” discussion around GPT- 3 (Brown et al., 2020; Gao et al., 2020), we have also observed that zero-shot performance can be signiﬁcantly improved by customizing the prompt text to each task. A few, non exhaustive, examples follow. We found on several ﬁne-grained image classiﬁcation datasets that it helped to specify the category. For example on Oxford-IIIT Pets, us- ing “A photo of a {label}, a type of pet.” to help provide context worked well. Likewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too. For OCR datasets, we found that putting quotes around the text or number to be recognized improved performance. Finally, we found that on satellite image classi- ﬁcation datasets it helped to specify that the images were of this form and we use variants of “a satellite photo of a {label}.”."}
{"doc_id": "2103.00020", "para_id": 62, "text": "We also experimented with ensembling over multiple zero- shot classiﬁers as another way of improving performance. These classiﬁers are computed by using different context prompts such as ‘A photo of a big {label}” and “A photo of a small {label}”. We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the ensemble is the same as using a single classiﬁer when amortized over many predictions. We’ve observed ensembling across many gen- erated zero-shot classiﬁers to reliably improve performance and use it for the majority of datasets. On ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above. When considered together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%. In Figure 4 we visualize how prompt engi- neering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al. (2017)."}
{"doc_id": "2103.00020", "para_id": 63, "text": "40 30 20 10 0 10 20 30 40 Score (%) Zero-Shot CLIP vs. Linear Probe on ResNet50"}
{"doc_id": "2103.00020", "para_id": 64, "text": "Figure 5. Zero-shot CLIP is competitive with a fully super- vised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classiﬁer outperforms a fully supervised linear classiﬁer ﬁtted on ResNet-50 features on 16 datasets, including ImageNet."}
{"doc_id": "2103.00020", "para_id": 65, "text": "ten than not and wins on 16 of the 27 datasets. Looking at individual datasets reveals some interesting behavior. On ﬁne-grained classiﬁcation tasks, we observe a wide spread in performance. On two of these datasets, Stanford Cars and Food101, zero-shot CLIP outperforms logistic regression on ResNet-50 features by over 20% while on two others, Flowers102 and FGVCAircraft, zero-shot CLIP underper- forms by over 10%. On OxfordPets and Birdsnap, per- formance is much closer. We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On “general” object classiﬁca- tion datasets such as ImageNet, CIFAR10/100, STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zero- shot CLIP signiﬁcantly outperforms a ResNet-50 on two datasets measuring action recognition in videos. On Kinet- ics700, CLIP outperforms a ResNet-50 by 14.5%. Zero- shot CLIP also outperforms a ResNet-50’s features by 7.7% on UCF101. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet."}
{"doc_id": "2103.00020", "para_id": 66, "text": "Since task-agnostic zero-shot classiﬁers for computer vision have been understudied, CLIP provides a promising oppor- tunity to gain a better understanding of this type of model. In this section, we conduct a study of various properties of CLIP’s zero-shot classiﬁers. As a ﬁrst question, we look simply at how well zero-shot classiﬁers perform. To con- textualize this, we compare to the performance of a simple off-the-shelf baseline: ﬁtting a fully supervised, regularized, logistic regression classiﬁer on the features of the canonical ResNet-50. In Figure 5 we show this comparison across 27 datasets. Please see Appendix A for details of datasets and setup."}
{"doc_id": "2103.00020", "para_id": 67, "text": "Zero-shot CLIP outperforms this baseline slightly more of-"}
{"doc_id": "2103.00020", "para_id": 68, "text": "Looking at where zero-shot CLIP notably underperforms,"}
{"doc_id": "2103.00020", "para_id": 69, "text": "expect zero-shot to underperform one-shot, we instead ﬁnd that zero-shot CLIP matches the performance of 4-shot lo- gistic regression on the same feature space. This is likely due to an important difference between the zero-shot and few-shot approach. First, CLIP’s zero-shot classiﬁer is gen- erated via natural language which allows for visual concepts to be directly speciﬁed (“communicated”). By contrast, “normal” supervised learning must infer concepts indirectly from training examples. Context-less example-based learn- ing has the drawback that many different hypotheses can be consistent with the data, especially in the one-shot case. A single image often contains many different visual con- cepts. Although a capable learner is able to exploit visual cues and heuristics, such as assuming that the concept being demonstrated is the primary object in an image, there is no guarantee."}
{"doc_id": "2103.00020", "para_id": 70, "text": "A potential resolution of this discrepancy between zero- shot and few-shot performance is to use CLIP’s zero-shot classiﬁer as a prior for the weights of the few-shot classiﬁer. While adding an L2 penalty towards the generated weights is a straightforward implementation of this idea, we found that hyperparameter optimization would often select for such a large value of this regularizer that the resulting few- shot classiﬁer was “just” the zero-shot classiﬁer. Research into better methods of combining the strength of zero-shot transfer with ﬂexibility of few-shot learning is a promising direction for future work."}
{"doc_id": "2103.00020", "para_id": 71, "text": "0 1 2 4 8 16 # of labeled training examples per class"}
{"doc_id": "2103.00020", "para_id": 72, "text": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes. Zero-shot CLIP matches the average performance of a 4-shot linear classiﬁer trained on the same feature space and nearly matches the best results of a 16-shot linear classiﬁer across publicly available models. For both BiT-M and SimCLRv2, the best performing model is highlighted. Light gray lines are other models in the eval suite. The 20 datasets with at least 16 examples per class were used in this analysis."}
{"doc_id": "2103.00020", "para_id": 73, "text": "When comparing zero-shot CLIP to few-shot logistic re- gression on the features of other models, zero-shot CLIP roughly matches the performance of the best performing 16-shot classiﬁer in our evaluation suite, which uses the fea- tures of a BiT-M ResNet-152x2 trained on ImageNet-21K. We are certain that a BiT-L model trained on JFT-300M would perform even better but these models have not been publicly released. That a BiT-M ResNet-152x2 performs best in a 16-shot setting is somewhat surprising since, as analyzed in Section 3.2, the Noisy Student EfﬁcientNet-L2 outperforms it in a fully supervised setting by almost 5% on average across 27 datasets."}
{"doc_id": "2103.00020", "para_id": 74, "text": "we see that zero-shot CLIP is quite weak on several spe- cialized, complex, or abstract tasks such as satellite image classiﬁcation (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes (CLEVRCounts), self-driving related tasks such as German trafﬁc sign recognition (GTSRB), recognizing dis- tance to the nearest car (KITTI Distance). These results highlight the poor capability of zero-shot CLIP on more complex tasks. By contrast, non-expert humans can robustly perform several of these tasks, such as counting, satellite image classiﬁcation, and trafﬁc sign recognition, suggesting signiﬁcant room for improvement. However, we caution that it is unclear whether measuring zero-shot transfer, as opposed to few-shot transfer, is a meaningful evaluation for difﬁcult tasks that a learner has no prior experience with, such as lymph node tumor classiﬁcation for almost all hu- mans (and possibly CLIP)."}
{"doc_id": "2103.00020", "para_id": 75, "text": "In addition to studying the average performance of zero-shot CLIP and few-shot logistic regression, we also examine performance on individual datasets. In Figure 7, we show estimates for the number of labeled examples per class that a logistic regression classiﬁer on the same feature space requires to match the performance of zero-shot CLIP. Since zero-shot CLIP is also a linear classiﬁer, this estimates the effective data efﬁciency of zero-shot transfer in this setting. In order to avoid training thousands of linear classiﬁers, we estimate the effective data efﬁciency based on a log- linear interpolation of the performance of a 1, 2, 4, 8, 16- shot (when possible), and a fully supervised linear classiﬁer trained on each dataset. We ﬁnd that zero-shot transfer can"}
{"doc_id": "2103.00020", "para_id": 76, "text": "While comparing zero-shot performance to fully supervised models contextualizes the task-learning capabilities of CLIP, comparing to few-shot methods is a more direct compari- son, since zero-shot is its limit. In Figure 6, we visualize how zero-shot CLIP compares to few-shot logistic regres- sion on the features of many image models including the best publicly available ImageNet models, self-supervised learning methods, and CLIP itself. While it is intuitive to"}
{"doc_id": "2103.00020", "para_id": 77, "text": "0 25 50 75 100 125 150 175 200 # of labeled examples per class"}
{"doc_id": "2103.00020", "para_id": 78, "text": "20 30 40 50 60 70 80 90 100 Linear Probe CLIP Performance"}
{"doc_id": "2103.00020", "para_id": 79, "text": "Figure 7. The data efﬁciency of zero-shot transfer varies widely. Calculating the number of labeled examples per class a linear classiﬁer on the same CLIP feature space requires to match the performance of the zero-shot classiﬁer contextualizes the ef- fectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised results. Performance varies widely from still underperforming a one-shot classiﬁer on two datasets to matching an estimated 184 labeled examples per class."}
{"doc_id": "2103.00020", "para_id": 80, "text": "Figure 8. Zero-shot performance is correlated with linear probe performance but still mostly sub-optimal. Comparing zero-shot and linear probe performance across datasets shows a strong correlation with zero-shot performance mostly shifted 10 to 25 points lower. On only 5 datasets does zero-shot performance approach linear probe performance (≤3 point difference)."}
{"doc_id": "2103.00020", "para_id": 81, "text": "mance, suggesting that CLIP is relatively consistent at con- necting underlying representation and task learning to zero- shot transfer. However, zero-shot CLIP only approaches fully supervised performance on 5 datasets: STL10, CI- FAR10, Food101, OxfordPets, and Caltech101. On all 5 datasets, both zero-shot accuracy and fully supervised accu- racy are over 90%. This suggests that CLIP may be more effective at zero-shot transfer for tasks where its underly- ing representations are also high quality. The slope of a linear regression model predicting zero-shot performance as a function of fully supervised performance estimates that for every 1% improvement in fully supervised performance, zero-shot performance improves by 1.28%. However, the 95th-percentile conﬁdence intervals still include values of less than 1 (0.93-1.79)."}
{"doc_id": "2103.00020", "para_id": 82, "text": "have widely varying efﬁciency per dataset from less than 1 labeled example per class to 184. Two datasets, Flowers102 and EuroSAT underperform one-shot models. Half of the datasets require less than 5 examples per class with a median of 5.4. However, the mean estimated data efﬁciency is 20.8 examples per class. This is due to the 20% of datasets where supervised classiﬁers require many labeled examples per class in order to match performance. On ImageNet, zero-shot CLIP matches the performance of a 16-shot linear classiﬁer trained on the same feature space."}
{"doc_id": "2103.00020", "para_id": 83, "text": "If we assume that evaluation datasets are large enough that the parameters of linear classiﬁers trained on them are well estimated, then, because CLIP’s zero-shot classiﬁer is also a linear classiﬁer, the performance of the fully supervised classiﬁers roughly sets an upper bound for what zero-shot transfer can achieve. In Figure 8 we compare CLIP’s zero- shot performance with fully supervised linear classiﬁers across datasets. The dashed, y = x line represents an “op- timal” zero-shot classiﬁer that matches the performance of its fully supervised equivalent. For most datasets, the per- formance of zero-shot classiﬁers still underperform fully su- pervised classiﬁers by 10% to 25%, suggesting that there is still plenty of headroom for improving CLIP’s task-learning and zero-shot transfer capabilities."}
{"doc_id": "2103.00020", "para_id": 84, "text": "Over the past few years, empirical studies of deep learning systems have documented that performance is predictable as a function of important quantities such as training compute and dataset size (Hestness et al., 2017; Kaplan et al., 2020). The GPT family of models has so far demonstrated consis- tent improvements in zero-shot performance across a 1000x increase in training compute. In Figure 9, we check whether the zero-shot performance of CLIP follows a similar scaling pattern. We plot the average error rate of the 5 ResNet CLIP models across 39 evaluations on 36 different datasets and ﬁnd that a similar log-log linear scaling trend holds for CLIP across a 44x increase in model compute. While the overall trend is smooth, we found that performance on individual evaluations can be much noisier. We are unsure whether"}
{"doc_id": "2103.00020", "para_id": 85, "text": "There is a positive correlation of 0.82 (p-value < 10−6) between zero-shot performance and fully supervised perfor-"}
{"doc_id": "2103.00020", "para_id": 86, "text": "classiﬁers has the added beneﬁt of being very similar to the approach used for its zero-shot classiﬁers which enables extensive comparisons and analysis in Section 3.1. Finally, we aim to compare CLIP to a comprehensive set of existing models across many tasks. Studying 66 different models on 27 different datasets requires tuning 1782 different evalua- tions. Fine-tuning opens up a much larger design and hyper- parameter space, which makes it difﬁcult to fairly evaluate and computationally expensive to compare a diverse set of techniques as discussed in other large scale empirical studies (Lucic et al., 2018; Choi et al., 2019). By comparison, linear classiﬁers require minimal hyper-parameter tuning and have standardized implementations and evaluation procedures. Please see Appendix A for further details on evaluation."}
{"doc_id": "2103.00020", "para_id": 87, "text": "Figure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log lin- ear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend."}
{"doc_id": "2103.00020", "para_id": 88, "text": "Figure 10 summarizes our ﬁndings. To minimize selection effects that could raise concerns of conﬁrmation or reporting bias, we ﬁrst study performance on the 12 dataset evaluation suite from Kornblith et al. (2019). While small CLIP mod- els such as a ResNet-50 and ResNet-101 outperform other ResNets trained on ImageNet-1K (BiT-S and the originals), they underperform ResNets trained on ImageNet-21K (BiT- M). These small CLIP models also underperform models in the EfﬁcientNet family with similar compute require- ments. However, models trained with CLIP scale very well and the largest model we trained (ResNet-50x64) slightly outperforms the best performing existing model (a Noisy Student EfﬁcientNet-L2) on both overall score and compute efﬁciency. We also ﬁnd that CLIP vision transformers are about 3x more compute efﬁcient than CLIP ResNets, which allows us to reach higher overall performance within our compute budget. These results qualitatively replicate the ﬁndings of Dosovitskiy et al. (2020) which reported that vision transformers are more compute efﬁcient than con- vnets when trained on sufﬁciently large datasets. Our best overall model is a ViT-L/14 that is ﬁne-tuned at a higher res- olution of 336 pixels on our dataset for 1 additional epoch. This model outperforms the best existing model across this evaluation suite by an average of 2.6%."}
{"doc_id": "2103.00020", "para_id": 89, "text": "this is caused by high variance between individual training runs on sub-tasks (as documented in D’Amour et al. (2020)) masking a steadily improving trend or whether performance is actually non-monotonic as a function of compute on some tasks."}
{"doc_id": "2103.00020", "para_id": 90, "text": "While we have extensively analyzed the task-learning ca- pabilities of CLIP through zero-shot transfer in the previ- ous section, it is more common to study the representation learning capabilities of a model. There exist many ways to evaluate the quality of representations as well as disagree- ments over what properties an “ideal” representation should have (Locatello et al., 2020). Fitting a linear classiﬁer on a representation extracted from the model and measuring its performance on various datasets is a common approach. An alternative is measuring the performance of end-to-end ﬁne-tuning of the model. This increases ﬂexibility, and prior work has convincingly demonstrated that ﬁne-tuning outperforms linear classiﬁcation on most image classiﬁ- cation datasets (Kornblith et al., 2019; Zhai et al., 2019). While the high performance of ﬁne-tuning motivates its study for practical reasons, we still opt for linear classiﬁer based evaluation for several reasons. Our work is focused on developing a high-performing task and dataset-agnostic pre-training approach. Fine-tuning, because it adapts rep- resentations to each dataset during the ﬁne-tuning phase, can compensate for and potentially mask failures to learn general and robust representations during the pre-training phase. Linear classiﬁers, because of their limited ﬂexibility, instead highlight these failures and provide clear feedback during development. For CLIP, training supervised linear"}
{"doc_id": "2103.00020", "para_id": 91, "text": "As Figure 21 qualitatively shows, CLIP models learn a wider set of tasks than has previously been demonstrated in a sin- gle computer vision model trained end-to-end from random initialization. These tasks include geo-localization, optical character recognition, facial emotion recognition, and action recognition. None of these tasks are measured in the evalua- tion suite of Kornblith et al. (2019). This could be argued to be a form of selection bias in Kornblith et al. (2019)’s study towards tasks that overlap with ImageNet. To address this, we also measure performance on a broader 27 dataset evaluation suite. This evaluation suite, detailed in Appendix A includes datasets representing the aforementioned tasks, German Trafﬁc Signs Recognition Benchmark (Stallkamp et al., 2011), as well as several other datasets adapted from VTAB (Zhai et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 92, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 93, "text": "CLIP-ViT CLIP-ResNet EfficientNet-NoisyStudent EfficientNet"}
{"doc_id": "2103.00020", "para_id": 94, "text": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfﬁcientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018; Touvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al., 2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ﬁne-tuned or evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset."}
{"doc_id": "2103.00020", "para_id": 95, "text": "On this broader evaluation suite, the beneﬁts of CLIP are more clear. All CLIP models, regardless of scale, outper- form all evaluated systems in terms of compute efﬁciency. The improvement in average score of the best model over previous systems increases from 2.6% to 5%. We also ﬁnd that self-supervised systems do noticeably better on our broader evaluation suite. For instance, while SimCLRv2 still underperforms BiT-M on average on the 12 datasets of Kornblith et al. (2019), SimCLRv2 outperforms BiT-M on our 27 dataset evaluation suite. These ﬁndings suggest continuing to expand task diversity and coverage in order to better understand the “general” performance of systems. We suspect additional evaluation efforts along the lines of VTAB to be valuable."}
{"doc_id": "2103.00020", "para_id": 96, "text": "and HatefulMemes), geo-localization and scene recognition (Country211, SUN397), and activity recognition in videos (Kinetics700 and UCF101). In addition CLIP also does much better on ﬁne-grained car and trafﬁc sign recognition (Stanford Cars and GTSRB). This may reﬂect a problem with overly narrow supervision in ImageNet. A result such as the 14.7% improvement on GTSRB could be indicative of an issue with ImageNet-1K, which has only a single la- bel for all trafﬁc and street signs. This could encourage a supervised representation to collapse intra-class details and hurt accuracy on a ﬁne-grained downstream task. As mentioned, CLIP still underperforms the EfﬁcientNet on several datasets. Unsurprisingly, the dataset that the Efﬁ- cientNet does best relative to CLIP on is the one it was trained on: ImageNet. The EffcientNet also slightly outper- forms CLIP on low-resolution datasets such as CIFAR10 and CIFAR100. We suspect this is at least partly due to the lack of scale-based data augmentation in CLIP. The Efﬁ- cientNet also does slightly better on PatchCamelyon and CLEVRCounts, datasets where overall performance is still"}
{"doc_id": "2103.00020", "para_id": 97, "text": "In addition to the aggregate analysis above, we visualize per-dataset differences in the performance of the best CLIP model and the best model in our evaluation suite across all 27 datasets in Figure 11. CLIP outperforms the Noisy Student EfﬁcientNet-L2 on 21 of the 27 datasets. CLIP improves the most on tasks which require OCR (SST2"}
{"doc_id": "2103.00020", "para_id": 98, "text": "combination of the two? CLIP models, which are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance, are an opportunity to investigate this question from a different angle."}
{"doc_id": "2103.00020", "para_id": 99, "text": "Taori et al. (2020) is a recent comprehensive study mov- ing towards quantifying and understanding these behaviors for ImageNet models. Taori et al. (2020) study how the performance of ImageNet models change when evaluated on natural distribution shifts. They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al., 2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB and ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu et al., 2019), ImageNet Adversarial (Hendrycks et al., 2019), and ImageNet Rendition (Hendrycks et al., 2020a). They distinguish these datasets, which all consist of novel images collected from a variety of sources, from synthetic distri- bution shifts such as ImageNet-C (Hendrycks & Dietterich, 2019), Stylized ImageNet (Geirhos et al., 2018), or adver- sarial attacks (Goodfellow et al., 2014) which are created by perturbing existing images in various ways. They propose this distinction because in part because they ﬁnd that while several techniques have been demonstrated to improve per- formance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions.3"}
{"doc_id": "2103.00020", "para_id": 100, "text": "10 5 0 5 10 15 20 25 Score (%) Logistic Regression on CLIP vs. EfficientNet L2 NS"}
{"doc_id": "2103.00020", "para_id": 101, "text": "Figure 11. CLIP’s features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classiﬁer on CLIP’s features outperforms using the Noisy Student EfﬁcientNet-L2 on 21 out of 27 datasets."}
{"doc_id": "2103.00020", "para_id": 102, "text": "Across these collected datasets, the accuracy of ImageNet models drop well below the expectation set by the Ima- geNet validation set. For the following summary discussion we report average accuracy across all 7 natural distribution shift datasets and average accuracy across the correspond- ing class subsets of ImageNet unless otherwise speciﬁed. Additionally, for Youtube-BB and ImageNet-Vid, which have two different evaluation settings, we use the average of pm-0 and pm-10 accuracy."}
{"doc_id": "2103.00020", "para_id": 103, "text": "In 2015, it was announced that a deep learning model ex- ceeded human performance on the ImageNet test set (He et al., 2015). However, research in the subsequent years has repeatedly found that these models still make many sim- ple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018; Alcorn et al., 2019), and new benchmarks testing these sys- tems has often found their performance to be much lower than both their ImageNet accuracy and human accuracy (Recht et al., 2019; Barbu et al., 2019). What explains this discrepancy? Various ideas have been suggested and stud- ied (Ilyas et al., 2019; Geirhos et al., 2020). A common theme of proposed explanations is that deep learning models are exceedingly adept at ﬁnding correlations and patterns which hold across their training dataset and thus improve in-distribution performance. However many of these corre- lations and patterns are actually spurious and do not hold for other distributions and result in large drops in performance on other datasets."}
{"doc_id": "2103.00020", "para_id": 104, "text": "A ResNet-101 makes 5 times as many mistakes when eval- uated on these natural distribution shifts compared to the ImageNet validation set. Encouragingly however, Taori et al. (2020) ﬁnd that accuracy under distribution shift increases predictably with ImageNet accuracy and is well modeled as a linear function of logit-transformed accuracy. Taori et al. (2020) use this ﬁnding to propose that robustness analysis should distinguish between effective and relative robustness. Effective robustness measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy. Relative robustness captures any improvement in out-of-distribution accuracy. Taori et al. (2020) argue that robustness techniques should aim to im- prove both effective robustness and relative robustness."}
{"doc_id": "2103.00020", "para_id": 105, "text": "We caution that, to date, most of these studies limit their evaluation to models trained on ImageNet. Recalling the topic of discussion, it may be a mistake to generalize too far from these initial ﬁndings. To what degree are these failures attributable to deep learning, ImageNet, or some"}
{"doc_id": "2103.00020", "para_id": 106, "text": "Almost all models studied in Taori et al. (2020) are trained"}
{"doc_id": "2103.00020", "para_id": 107, "text": "3We refer readers to Hendrycks et al. (2020a) for additional experiments and discussion on this claim."}
{"doc_id": "2103.00020", "para_id": 108, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 109, "text": "CLIP-ViT CLIP-ResNet EfficientNet-NoisyStudent EfficientNet"}
{"doc_id": "2103.00020", "para_id": 110, "text": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overﬁt to their task."}
{"doc_id": "2103.00020", "para_id": 111, "text": "or ﬁne-tuned on the ImageNet dataset. Returning to the discussion in the introduction to this section - is training or adapting to the ImageNet dataset distribution the cause of the observed robustness gap? Intuitively, a zero-shot model should not be able to exploit spurious correlations or patterns that hold only on a speciﬁc distribution, since it is not trained on that distribution. 4 Thus it is reasonable to expect zero-shot models to have much higher effective robustness. In Figure 13, we compare the performance of zero-shot CLIP with existing ImageNet models on natural distribution shifts. All zero-shot CLIP models improve effective robustness by a large amount and reduce the size of the gap between ImageNet accuracy and accuracy under distribution shift by up to 75%."}
{"doc_id": "2103.00020", "para_id": 112, "text": "in much more robust models regardless of whether they are zero-shot or ﬁne-tuned. As an initial experiment to potentially begin narrowing this down, we also measure how the performance of CLIP models change after adapting to the ImageNet distribution via a L2 regularized logistic regression classiﬁer ﬁt to CLIP features on the ImageNet training set. We visualize how performance changes from the zero-shot classiﬁer in Figure 14. Although adapting CLIP to the ImageNet distribution increases its ImageNet accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA from Mahajan et al. (2018), average accuracy under distribution shift slightly decreases."}
{"doc_id": "2103.00020", "para_id": 113, "text": "It is surprising to see a 9.2% increase in accuracy, which cor- responds to roughly 3 years of improvement in SOTA, fail to translate into any improvement in average performance under distribution shift. We also break down the differences between zero-shot accuracy and linear classiﬁer accuracy per dataset in Figure 14 and ﬁnd performance still increases signiﬁcantly on one dataset, ImageNetV2. ImageNetV2 closely followed the creation process of the original Ima- geNet dataset which suggests that gains in accuracy from supervised adaptation are closely concentrated around the ImageNet distribution. Performance decreases by 4.7% on"}
{"doc_id": "2103.00020", "para_id": 114, "text": "While these results show that zero-shot models can be much more robust, they do not necessarily mean that supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also result"}
{"doc_id": "2103.00020", "para_id": 115, "text": "4We caution that a zero-shot model can still exploit spurious correlations that are shared between the pre-training and evaluation distributions."}
{"doc_id": "2103.00020", "para_id": 116, "text": "Ideal robust model (y = x) Zero-Shot CLIP Standard ImageNet training Exisiting robustness techniques ImageNet"}
{"doc_id": "2103.00020", "para_id": 117, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 118, "text": "65 70 75 80 85 90 95 100 Average on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 119, "text": "Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model (dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink this “robustness gap” by up to 75%. Linear ﬁts on logit transformed values are shown with bootstrap estimated 95% conﬁdence intervals. (Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of the best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation set, ResNet-101."}
{"doc_id": "2103.00020", "para_id": 120, "text": "pooling predictions across all sub-classes according to the ImageNet class hierarchy. Sometimes this mapping is much less than perfect. For the person class in Youtube-BB, pre- dictions are made by pooling over the ImageNet classes for a baseball player, a bridegroom, and a scuba diver. With CLIP we can instead generate a custom zero-shot classi- ﬁer for each dataset directly based on its class names. In Figure 14 we see that this improves average effective ro- bustness by 5% but is concentrated in large improvements on only a few datasets. Curiously, accuracy on ObjectNet also increases by 2.3%. Although the dataset was designed to closely overlap with ImageNet classes, using the names provided for each class by ObjectNet’s creators still helps a small amount compared to using ImageNet class names and pooling predictions when necessary."}
{"doc_id": "2103.00020", "para_id": 121, "text": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch, and 1.9% on ImageNet-A. The change in accuracy on the two other datasets, Youtube-BB and ImageNet Vid, is in- signiﬁcant."}
{"doc_id": "2103.00020", "para_id": 122, "text": "How is it possible to improve accuracy by 9.2% on the Im- ageNet dataset with little to no increase in accuracy under distribution shift? Is the gain primarily from “exploiting spurious correlations”? Is this behavior unique to some com- bination of CLIP, the ImageNet datatset, and the distribution shifts studied, or a more general phenomena? Does it hold for end-to-end ﬁnetuning as well as linear classiﬁers? We do not have conﬁdent answers to these questions at this time. Prior work has also pre-trained models on distributions other than ImageNet, but it is common to study and release mod- els only after they have been ﬁne-tuned to ImageNet. As a step towards understanding whether pre-trained zero-shot models consistently have higher effective robustness than ﬁne-tuned models, we encourage the authors of Mahajan et al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al. (2020) to, if possible, study these questions on their models as well."}
{"doc_id": "2103.00020", "para_id": 123, "text": "While zero-shot CLIP improves effective robustness, Figure 14 shows that the beneﬁt is almost entirely gone in a fully supervised setting. To better understand this difference, we investigate how effective robustness changes on the contin- uum from zero-shot to fully supervised. In Figure 15 we visualize the performance of 0-shot, 1-shot, 2-shot, 4-shot ..., 128-shot, and fully supervised logistic regression classi- ﬁers on the best CLIP model’s features. We see that while few-shot models also show higher effective robustness than existing models, this beneﬁt fades as in-distribution per- formance increases with more training data and is mostly, though not entirely, gone for the fully supervised model. Additionally, zero-shot CLIP is notably more robust than a few-shot model with equivalent ImageNet performance."}
{"doc_id": "2103.00020", "para_id": 124, "text": "We also investigate another robustness intervention enabled by ﬂexible zero-shot natural-language-based image classi- ﬁers. The target classes across the 7 transfer datasets are not always perfectly aligned with those of ImageNet. Two datasets, Youtube-BB and ImageNet-Vid, consist of super- classes of ImageNet. This presents a problem when trying to use the ﬁxed 1000-way classiﬁer of an ImageNet model to make predictions. Taori et al. (2020) handle this by max-"}
{"doc_id": "2103.00020", "para_id": 125, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 126, "text": "10 5 0 5 10 15 20 25 30 Change from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 127, "text": "Ideal robust model (y = x) Adaptive Zero-Shot CLIP ImageNet Zero-Shot CLIP Logistic Regression CLIP Standard ImageNet training Robustness intervention Trained with more data"}
{"doc_id": "2103.00020", "para_id": 128, "text": "10 5 0 5 10 15 20 25 30 Change from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 129, "text": "70 75 80 85 90 95 Average on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 130, "text": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness. (Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classiﬁer and pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset speciﬁc zero-shot classiﬁers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align with ImageNet categories."}
{"doc_id": "2103.00020", "para_id": 131, "text": "humans on one of our tasks. We wanted to get a sense of how strong human zero-shot performance is at these tasks, and how much human performance is improved if they are shown one or two image samples. This can help us to compare task difﬁculty for humans and CLIP, and identify correlations and differences between them."}
{"doc_id": "2103.00020", "para_id": 132, "text": "Across our experiments, high effective robustness seems to result from minimizing the amount of distribution speciﬁc training data a model has access to, but this comes at a cost of reducing dataset-speciﬁc performance."}
{"doc_id": "2103.00020", "para_id": 133, "text": "Taken together, these results suggest that the recent shift towards large-scale task and dataset agnostic pre-training combined with a reorientation towards zero-shot and few- shot benchmarking on broad evaluation suites (as advocated by Yogatama et al. (2019) and Linzen (2020)) promotes the development of more robust systems and provides a more accurate assessment of performance. We are curious to see if the same results hold for zero-shot models in the ﬁeld of NLP such as the GPT family. While Hendrycks et al. (2020b) has reported that pre-training improves relative ro- bustness on sentiment analysis, Miller et al. (2020)’s study of the robustness of question answering models under nat- ural distribution shift ﬁnds, similar to Taori et al. (2020), little evidence of effective robustness improvements to date."}
{"doc_id": "2103.00020", "para_id": 134, "text": "We had ﬁve different humans look at each of 3669 images in the test split of the Oxford IIT Pets dataset (Parkhi et al., 2012) and select which of the 37 cat or dog breeds best matched the image (or ‘I don’t know’ if they were com- pletely uncertain). In the zero-shot case the humans were given no examples of the breeds and asked to label them to the best of their ability without an internet search. In the one-shot experiment the humans were given one sample image of each breed and in the two-shot experiment they were given two sample images of each breed.5"}
{"doc_id": "2103.00020", "para_id": 135, "text": "One possible concern was that the human workers were not sufﬁciently motivated in the zero-shot task. High human accuracy of 94% on the STL-10 dataset (Coates et al., 2011)"}
{"doc_id": "2103.00020", "para_id": 136, "text": "5There is not a perfect correspondence between the human few-shot tasks and the model’s few-shot performance since the model cannot refer to sample images in the way that the humans can."}
{"doc_id": "2103.00020", "para_id": 137, "text": "How does CLIP compare to human performance and human learning? To get a better understanding of how well humans perform in similar evaluation settings to CLIP, we evaluated"}
{"doc_id": "2103.00020", "para_id": 138, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 139, "text": "Accuracy Majority Vote on Full Dataset Accuracy on Guesses"}
{"doc_id": "2103.00020", "para_id": 140, "text": "Zero-shot human 53.7 57.0 69.7 63.9 Zero-shot CLIP 93.5 93.5 93.5 93.5 One-shot human 75.7 80.3 78.5 81.2 Two-shot human 75.7 85.0 79.2 86.1"}
{"doc_id": "2103.00020", "para_id": 141, "text": "Table 2. Comparison of human performance on Oxford IIT Pets. As in Parkhi et al. (2012), the metric is average per-class classiﬁca- tion accuracy. Most of the gain in performance when going from the human zero shot case to the human one shot case is on images that participants were highly uncertain on. “Guesses” refers to restricting the dataset to where participants selected an answer other than “I don’t know”, the “majority vote” is taking the most frequent (exclusive of ties) answer per image."}
{"doc_id": "2103.00020", "para_id": 142, "text": "Ideal robust model (y = x) Few-Shot CLIP (best model) Zero-Shot CLIP (best model) Standard ImageNet training Robustness intervention Trained with more data"}
{"doc_id": "2103.00020", "para_id": 143, "text": "quality pre-trained model is near state-of-the-art for few shot learning (Tian et al., 2020), which suggests that there is a gap between the best few-shot machine learning methods and human few-shot learning."}
{"doc_id": "2103.00020", "para_id": 144, "text": "65 70 75 80 85 90 95 Average on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 145, "text": "Figure 15. Few-shot CLIP also increases effective robustness compared to existing ImageNet models but is less robust than zero-shot CLIP. Minimizing the amount of ImageNet training data used for adaption increases effective robustness at the cost of decreasing relative robustness. 16-shot logistic regression CLIP matches zero-shot CLIP on ImageNet, as previously reported in Figure 7, but is less robust."}
{"doc_id": "2103.00020", "para_id": 146, "text": "If we plot human accuracy vs CLIP’s zero shot accuracy (Figure 16), we see that the hardest problems for CLIP are also hard for humans. To the extent that errors are consistent, our hypothesis is that this is due to at least a two factors: noise in the dataset (including mislabeled images) and out of distribution images being hard for both humans and models."}
{"doc_id": "2103.00020", "para_id": 147, "text": "and 97-100% accuracy on the subset of attention check images increased our trust in the human workers."}
{"doc_id": "2103.00020", "para_id": 148, "text": "A concern with pre-training on a very large internet dataset is unintentional overlap with downstream evals. This is important to investigate since, in a worst-case scenario, a complete copy of an evaluation dataset could leak into the pre-training dataset and invalidate the evaluation as a mean- ingful test of generalization. One option to prevent this is to identify and remove all duplicates before training a model. While this guarantees reporting true hold-out performance, it requires knowing all possible data which a model might be evaluated on ahead of time. This has the downside of limiting the scope of benchmarking and analysis. Adding a new evaluation would require an expensive re-train or risk reporting an un-quantiﬁed beneﬁt due to overlap."}
{"doc_id": "2103.00020", "para_id": 149, "text": "Interestingly, humans went from a performance average of 54% to 76% with just one training example per class, and the marginal gain from an additional training example is minimal. The gain in accuracy going from zero to one shot is almost entirely on images that humans were uncertain about. This suggests that humans “know what they don’t know” and are able to update their priors on the images they are most uncertain in based on a single example. Given this, it seems that while CLIP is a promising training strategy for zero-shot performance (Figure 5) and does well on tests of natural distribution shift (Figure 13), there is a large difference between how humans learn from a few examples and the few-shot methods in this paper."}
{"doc_id": "2103.00020", "para_id": 150, "text": "Instead, we document how much overlap occurs and how performance changes due to these overlaps. To do this, we use the following procedure:"}
{"doc_id": "2103.00020", "para_id": 151, "text": "This suggests that there are still algorithmic improvements waiting to be made to decrease the gap between machine and human sample efﬁciency, as noted by Lake et al. (2016) and others. Because these few-shot evaluations of CLIP don’t make effective use of prior knowledge and the humans do, we speculate that ﬁnding a method to properly integrate prior knowledge into few-shot learning is an important step in algorithmic improvements to CLIP. To our knowledge, using a linear classiﬁer on top of the features of a high-"}
{"doc_id": "2103.00020", "para_id": 152, "text": "1) For each evaluation dataset, we run a duplicate detector (see Appendix C) on its examples. We then manually inspect the found nearest neighbors and set a per dataset threshold to keep high precision while maximizing recall. Using this threshold, we then create two new subsets, Overlap, which contains all examples which have a similarity to a training example above the threshold, and Clean, which"}
{"doc_id": "2103.00020", "para_id": 153, "text": "our analysis. There is a median overlap of 2.2% and an av- erage overlap of 3.2%. Due to this small amount of overlap, overall accuracy is rarely shifted by more than 0.1% with only 7 datasets above this threshold. Of these, only 2 are statistically signiﬁcant after Bonferroni correction. The max detected improvement is only 0.6% on Birdsnap which has the second largest overlap at 12.1%. The largest overlap is for Country211 at 21.5%. This is due to it being constructed out of YFCC100M, which our pre-training dataset contains a ﬁltered subset of. Despite this large overlap there is only a 0.2% increase in accuracy on Country211. This may be because the training text accompanying an example is often not related to the speciﬁc task a downstream eval measures. Country211 measures geo-localization ability, but inspect- ing the training text for these duplicates showed they often do not mention the location of the image."}
{"doc_id": "2103.00020", "para_id": 154, "text": "We are aware of two potential concerns with our analysis. First our detector is not perfect. While it achieves near 100% accuracy on its proxy training task and manual in- spection + threshold tuning results in very high precision with good recall among the found nearest-neighbors, we can not tractably check its recall across 400 million examples. Another potential confounder of our analysis is that the un- derlying data distribution may shift between the Overlap and Clean subsets. For example, on Kinetics-700 many “overlaps” are in fact all black transition frames. This ex- plains why Kinetics-700 has an apparent 20% accuracy drop on Overlap. We suspect more subtle distribution shifts likely exist. One possibility we noticed on CIFAR-100 is that, due to the very low resolution of its images, many duplicates were false positives of small objects such as birds or planes. Changes in accuracy could instead be due to changes in the class distribution or difﬁculty of the dupli- cates. Unfortunately, these distribution and difﬁculty shifts could also mask the effects of over-ﬁtting."}
{"doc_id": "2103.00020", "para_id": 155, "text": "Figure 16. The hardest problems for CLIP also tend to be the hard- est problems for humans. Here we rank image categories by difﬁ- culty for CLIP as measured as probability of the correct label."}
{"doc_id": "2103.00020", "para_id": 156, "text": "contains all examples that are below this threshold. We denote the unaltered full dataset All for reference. From this we ﬁrst record the degree of data contamination as the ratio of the number of examples in Overlap to the size of All."}
{"doc_id": "2103.00020", "para_id": 157, "text": "2) We then compute the zero-shot accuracy of CLIP RN50x64 on the three splits and report All - Clean as our main metric. This is the difference in accuracy due to contamination. When positive it is our estimate of how much the overall reported accuracy on the dataset was in- ﬂated by over-ﬁtting to overlapping data."}
{"doc_id": "2103.00020", "para_id": 158, "text": "However, these results closely follow the ﬁndings of simi- lar duplicate analysis in previous work on large scale pre- training. Mahajan et al. (2018) and Kolesnikov et al. (2019) detected similar overlap rates and found minimal changes in overall performance. Importantly, Kolesnikov et al. (2019) also compared the alternative de-duplication strategy dis- cussed in the introduction to this section with the approach we settled on and observed little difference between the two approaches."}
{"doc_id": "2103.00020", "para_id": 159, "text": "3) The amount of overlap is often small so we also run a binomial signiﬁcance test where we use the accuracy on Clean as the null hypothesis and compute the one-tailed (greater) p-value for the Overlap subset. We also calculate 99.5% Clopper-Pearson conﬁdence intervals on Dirty as another check."}
{"doc_id": "2103.00020", "para_id": 160, "text": "A summary of this analysis is presented in Figure 17. Out of 35 datasets studied, 9 datasets have no detected overlap at all. Most of these datasets are synthetic or specialized making them unlikely to be posted as normal images on the internet (for instance MNIST, CLEVR, and GTSRB) or are guaranteed to have no overlap due to containing novel data from after the date our dataset was created (ObjectNet and Hateful Memes). This demonstrates our detector has a low-false positive rate which is important as false posi- tives would under-estimate the effect of contamination in"}
{"doc_id": "2103.00020", "para_id": 161, "text": "There are still many limitations to CLIP. While several of these are discussed as part of analysis in various sections, we summarize and collect them here."}
{"doc_id": "2103.00020", "para_id": 162, "text": "On datasets with training splits, the performance of zero- shot CLIP is on average competitive with the simple su-"}
{"doc_id": "2103.00020", "para_id": 163, "text": "Difference in Accuracy on Overlapping vs. Clean Data (%)"}
{"doc_id": "2103.00020", "para_id": 164, "text": "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 Detected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 165, "text": "0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 Detected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 166, "text": "Figure 17. Few statistically signiﬁcant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson conﬁdence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically signiﬁcant when calculated using a one-sided binomial test."}
{"doc_id": "2103.00020", "para_id": 167, "text": "pervised baseline of a linear classiﬁer on top of ResNet-50 features. On most of these datasets, the performance of this baseline is now well below the overall state of the art. Signiﬁcant work is still needed to improve the task learning and transfer capabilities of CLIP. While scaling has so far steadily improved performance and suggests a route for con- tinued improvement, we estimate around a 1000x increase in compute is required for zero-shot CLIP to reach overall state-of-the-art performance. This is infeasible to train with current hardware. Further research into improving upon the computational and data efﬁciency of CLIP will be necessary."}
{"doc_id": "2103.00020", "para_id": 168, "text": "CLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accu- racy on the handwritten digits of MNIST. An embarrassingly simple baseline of logistic regression on raw pixels outper- forms zero-shot CLIP. Both semantic and near-duplicate nearest-neighbor retrieval verify that there are almost no im- ages that resemble MNIST digits in our pre-training dataset. This suggests CLIP does little to address the underlying problem of brittle generalization of deep learning models. Instead CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution. This is a naive assumption that, as MNIST demonstrates, is easy to violate."}
{"doc_id": "2103.00020", "para_id": 169, "text": "Analysis in Section 3.1 found that CLIP’s zero-shot perfor- mance is still quite weak on several kinds of tasks. When compared to task-speciﬁc models, the performance of CLIP is poor on several types of ﬁne-grained classiﬁcation such as differentiating models of cars, species of ﬂowers, and variants of aircraft. CLIP also struggles with more abstract and systematic tasks such as counting the number of objects in an image. Finally for novel tasks which are unlikely to be included in CLIP’s pre-training dataset, such as classifying the distance to the nearest car in a photo, CLIP’s perfor- mance can be near random. We are conﬁdent that there are still many, many, tasks where CLIP’s zero-shot performance is near chance level."}
{"doc_id": "2103.00020", "para_id": 170, "text": "Although CLIP can ﬂexibly generate zero-shot classiﬁers for a wide variety of tasks and datasets, CLIP is still limited to choosing from only those concepts in a given zero-shot classiﬁer. This is a signiﬁcant restriction compared to a truly ﬂexible approach like image captioning which could generate novel outputs. Unfortunately, as described in Sec- tion 2.3 we found the computational efﬁciency of the image caption baseline we tried to be much lower than CLIP. A simple idea worth trying is joint training of a contrastive and generative objective with the hope of combining the efﬁciency of CLIP with the ﬂexibility of a caption model. As another alternative, search could be performed at infer- ence time over many natural language explanations of a given image, similar to approach proposed in Learning with Latent Language Andreas et al. (2017)."}
{"doc_id": "2103.00020", "para_id": 171, "text": "While zero-shot CLIP generalizes well to many natural im- age distributions as investigated in Section 3.3, we’ve ob- served that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. An illustrative exam- ple occurs for the task of OCR as reported in Appendix E."}
{"doc_id": "2103.00020", "para_id": 172, "text": "CLIP also does not address the poor data efﬁciency of deep learning. Instead CLIP compensates by using a source of supervision that can be scaled to hundreds of millions of training examples. If every image seen during training of a CLIP model was presented at a rate of one per second, it would take 405 years to iterate through the 12.8 billion images seen over 32 training epochs. Combining CLIP with self-supervision (Henaff, 2020; Chen et al., 2020c) and self-training (Lee; Xie et al., 2020) methods is a promising direction given their demonstrated ability to improve data efﬁciency over standard supervised learning."}
{"doc_id": "2103.00020", "para_id": 173, "text": "CLIP has a wide range of capabilities due to its ability to carry out arbitrary image classiﬁcation tasks. One can give it images of cats and dogs and ask it to classify cats, or give it images taken in a department store and ask it to classify shoplifters–a task with signiﬁcant social implications and for which AI may be unﬁt. Like any image classiﬁcation system, CLIP’s performance and ﬁtness for purpose need to be evaluated, and its broader impacts analyzed in context. CLIP also introduces a capability that will magnify and alter such issues: CLIP makes it possible to easily create your own classes for categorization (to ‘roll your own classiﬁer’) without a need for re-training. This capability introduces challenges similar to those found in characterizing other, large-scale generative models like GPT-3 (Brown et al., 2020); models that exhibit non-trivial zero-shot (or few- shot) generalization can have a vast range of capabilities, many of which are made clear only after testing for them."}
{"doc_id": "2103.00020", "para_id": 174, "text": "Our methodology has several signiﬁcant limitations. De- spite our focus on zero-shot transfer, we repeatedly queried performance on full validation sets to guide the develop- ment of CLIP. These validation sets often have thousands of examples, which is unrealistic for true zero-shot sce- narios. Similar concerns have been raised in the ﬁeld of semi-supervised learning (Oliver et al., 2018). Another po- tential issue is our selection of evaluation datasets. While we have reported results on Kornblith et al. (2019)’s 12 dataset evaluation suite as a standardized collection, our main results use a somewhat haphazardly assembled col- lection of 27 datasets that is undeniably co-adapted with the development and capabilities of CLIP. Creating a new benchmark of tasks designed explicitly to evaluate broad zero-shot transfer capabilities, rather than re-using existing supervised datasets, would help address these issues."}
{"doc_id": "2103.00020", "para_id": 175, "text": "Our studies of CLIP in a zero-shot setting show that the model displays signiﬁcant promise for widely-applicable tasks like image retrieval or search. For example, it can ﬁnd relevant images in a database given text, or relevant text given an image. Further, the relative ease of steering CLIP toward bespoke applications with little or no additional data or training could unlock a variety of novel applications that are hard for us to envision today, as has occurred with large language models over the past few years."}
{"doc_id": "2103.00020", "para_id": 176, "text": "CLIP is trained on text paired with images on the internet. These image-text pairs are unﬁltered and uncurated and result in CLIP models learning many social biases. This has been previously demonstrated for image caption models (Bhargava & Forsyth, 2019). We refer readers to Section 7 for detailed analysis and quantiﬁcation of these behaviors for CLIP as well as discussion of potential mitigation strategies."}
{"doc_id": "2103.00020", "para_id": 177, "text": "In addition to the more than 30 datasets studied in earlier sections of this paper, we evaluate CLIP’s performance on the FairFace benchmark and undertake exploratory bias probes. We then characterize the model’s performance in a downstream task, surveillance, and discuss its usefulness as compared with other available systems. Many of CLIP’s capabilities are omni-use in nature (e.g. OCR can be used to make scanned documents searchable, to power screen reading technologies, or to read license plates). Several of the capabilities measured, from action recognition, ob- ject classiﬁcation, and geo-localization, to facial emotion recognition, can be used in surveillance. Given its social implications, we address this domain of use speciﬁcally in the Surveillance section."}
{"doc_id": "2103.00020", "para_id": 178, "text": "While we have emphasized throughout this work that speci- fying image classiﬁers through natural language is a ﬂexible and general interface, it has its own limitations. Many com- plex tasks and visual concepts can be difﬁcult to specify just through text. Actual training examples are undeniably useful but CLIP does not optimize for few-shot performance directly. In our work, we fall back to ﬁtting linear classiﬁers on top of CLIP’s features. This results in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. As discussed in Section 4, this is notably different from human performance which shows a large increase from a zero to a one shot setting. Future work is needed to develop methods that combine CLIP’s strong zero-shot performance with efﬁcient few-shot learning."}
{"doc_id": "2103.00020", "para_id": 179, "text": "We have also sought to characterize the social biases inher- ent to the model. Our bias tests represent our initial efforts to probe aspects of how the model responds in different sce- narios, and are by nature limited in scope. CLIP and models like it will need to be analyzed in relation to their speciﬁc deployments to understand how bias manifests and iden- tify potential interventions. Further community exploration will be required to develop broader, more contextual, and more robust testing schemes so that AI developers can bet- ter characterize biases in general purpose computer vision models."}
{"doc_id": "2103.00020", "para_id": 180, "text": "FairFace Model 75.4 94.4 60.7 Linear Probe CLIP 92.8 97.7 63.1 Zero-Shot CLIP 91.3 97.2 54.3 Linear Probe Instagram 87.2 93.9 54.1"}
{"doc_id": "2103.00020", "para_id": 181, "text": "FairFace Model 93.7 94.2 59.7 Linear Probe CLIP 93.4 96.5 63.8 Zero-Shot CLIP 58.3 95.9 57.1 Linear Probe Instagram 90.8 93.2 54.2"}
{"doc_id": "2103.00020", "para_id": 182, "text": "Table 4. Percent accuracy on Race, Gender, and Age classiﬁcation of images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’ ‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to- gether as FairFace category ‘Non-White’)"}
{"doc_id": "2103.00020", "para_id": 183, "text": "Table 3. Percent accuracy on Race, Gender, and Age classiﬁcation of images in FairFace category ‘White’"}
{"doc_id": "2103.00020", "para_id": 184, "text": "Middle Southeast East Model Gender Black White Indian Latino Eastern Asian Asian Average"}
{"doc_id": "2103.00020", "para_id": 185, "text": "Male 96.9 96.4 98.7 96.5 98.9 96.2 96.9 97.2 Linear Probe CLIP Female 97.9 96.7 97.9 99.2 97.2 98.5 97.3 97.8 97.4 96.5 98.3 97.8 98.4 97.3 97.1 97.5"}
{"doc_id": "2103.00020", "para_id": 186, "text": "Male 96.3 96.4 97.7 97.2 98.3 95.5 96.8 96.9 Zero-Shot CLIP Female 97.1 95.3 98.3 97.8 97.5 97.2 96.4 97.0 96.7 95.9 98.0 97.5 98.0 96.3 96.6"}
{"doc_id": "2103.00020", "para_id": 187, "text": "Male 92.5 94.8 96.2 93.1 96.0 92.7 93.4 94.1 Linear Probe Instagram Female 90.1 91.4 95.0 94.8 95.0 94.1 94.3 93.4 91.3 93.2 95.6 94.0 95.6 93.4 93.9"}
{"doc_id": "2103.00020", "para_id": 188, "text": "Table 5. Percent accuracy on gender classiﬁcation of images by FairFace race category"}
{"doc_id": "2103.00020", "para_id": 189, "text": "as an initial bias probe, then probe the model further to surface additional biases and sources of biases, including class design."}
{"doc_id": "2103.00020", "para_id": 190, "text": "Algorithmic decisions, training data, and choices about how classes are deﬁned and taxonomized (which we refer to in- formally as “class design”) can all contribute to and amplify social biases and inequalities resulting from the use of AI systems (Noble, 2018; Bechmann & Bowker, 2019; Bowker & Star, 2000). Class design is particularly relevant to mod- els like CLIP, since any developer can deﬁne a class and the model will provide some result."}
{"doc_id": "2103.00020", "para_id": 191, "text": "We evaluated two versions of CLIP on the FairFace dataset: a zero-shot CLIP model (“ZS CLIP”), and a logistic regres- sion classiﬁer ﬁtted to FairFace’s dataset on top of CLIP’s features (“LR CLIP”). We ﬁnd that LR CLIP gets higher accuracy on the FairFace dataset than both the ResNext-101 32x48d Instagram model (“Linear Probe Instagram”) (Ma- hajan et al., 2018) and FairFace’s own model on most of the classiﬁcation tests we ran7. ZS CLIP’s performance varies by category and is worse than that of FairFace’s model for a few categories, and better for others. (See Table 3 and Table 4)."}
{"doc_id": "2103.00020", "para_id": 192, "text": "In this section, we provide preliminary analysis of some of the biases in CLIP, using bias probes inspired by those outlined in Buolamwini & Gebru (2018) and K¨arkk¨ainen & Joo (2019). We also conduct exploratory bias research intended to ﬁnd speciﬁc examples of biases in the model, similar to that conducted by Solaiman et al. (2019)."}
{"doc_id": "2103.00020", "para_id": 193, "text": "and Keyes (2018) have shown. While FairFace’s dataset reduces the proportion of White faces, it still lacks representation of entire large demographic groups, effectively erasing such categories. We use the 2 gender categories and 7 race categories deﬁned in the FairFace dataset in a number of our experiments not in order to reinforce or endorse the use of such reductive categories, but in order to enable us to make comparisons to prior work. 7One challenge with this comparison is that the FairFace model uses binary classes for race (“White” and “Non-White”), instead of breaking down races into ﬁner-grained sub-groups."}
{"doc_id": "2103.00020", "para_id": 194, "text": "We start by analyzing the performance of Zero-Shot CLIP on the face image dataset FairFace (K¨arkk¨ainen & Joo, 2019)6"}
{"doc_id": "2103.00020", "para_id": 195, "text": "6FairFace is a face image dataset designed to balance age, gen- der, and race, in order to reduce asymmetries common in previous face datasets. It categorizes gender into 2 groups: female and male and race into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. There are inherent problems with race and gender classiﬁcations, as e.g. Bowker & Star (2000)"}
{"doc_id": "2103.00020", "para_id": 196, "text": "Middle Southeast East Category Black White Indian Latino Eastern Asian Asian"}
{"doc_id": "2103.00020", "para_id": 197, "text": "Crime-related Categories 16.4 24.9 24.4 10.8 19.7 4.4 1.3 Non-human Categories 14.4 5.5 7.6 3.7 2.0 1.9 0.0"}
{"doc_id": "2103.00020", "para_id": 198, "text": "Table 6. Percent of images classiﬁed into crime-related and non-human categories by FairFace Race category. The label set included 7 FairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 199, "text": "Category Label Set 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 over 70"}
{"doc_id": "2103.00020", "para_id": 200, "text": "Default Label Set 30.3 35.0 29.5 16.3 13.9 18.5 19.1 16.2 10.4 Default Label Set + ‘child’ category 2.3 4.3 14.7 15.0 13.4 18.2 18.6 15.5 9.4"}
{"doc_id": "2103.00020", "para_id": 201, "text": "Table 7. Percent of images classiﬁed into crime-related and non-human categories by FairFace Age category, showing comparison between results obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace race categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 202, "text": "We found that 4.9% (conﬁdence intervals between 4.6% and 5.4%) of the images were misclassiﬁed into one of the non-human classes we used in our probes (‘animal’, ‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’ images had the highest misclassiﬁcation rate (approximately 14%; conﬁdence intervals between [12.6% and 16.4%]) while all other races had misclassiﬁcation rates under 8%. People aged 0-20 years had the highest proportion being classiﬁed into this category at 14% ."}
{"doc_id": "2103.00020", "para_id": 203, "text": "Additionally, we test the performance of the LR CLIP and ZS CLIP models across intersectional race and gender cate- gories as they are deﬁned in the FairFace dataset. We ﬁnd that model performance on gender classiﬁcation is above 95% for all race categories. Table 5 summarizes these re- sults."}
{"doc_id": "2103.00020", "para_id": 204, "text": "While LR CLIP achieves higher accuracy than the Linear Probe Instagram model on the FairFace benchmark dataset for gender, race and age classiﬁcation of images by intersec- tional categories, accuracy on benchmarks offers only one approximation of algorithmic fairness, as Raji et al. (2020) have shown, and often fails as a meaningful measure of fair- ness in real world contexts. Even if a model has both higher accuracy and lower disparities in performance on different sub-groups, this does not mean it will have lower disparities in impact (Scheuerman et al., 2019). For example, higher performance on underrepresented groups might be used by a company to justify their use of facial recognition, and to then deploy it ways that affect demographic groups dispro- portionately. Our use of facial classiﬁcation benchmarks to probe for biases is not intended to imply that facial classi- ﬁcation is an unproblematic task, nor to endorse the use of race, age, or gender classiﬁcation in deployed contexts."}
{"doc_id": "2103.00020", "para_id": 205, "text": "We also found that 16.5% of male images were misclassiﬁed into classes related to crime (‘thief’, ‘suspicious person’ and ‘criminal’) as compared to 9.8% of female images. Inter- estingly, we found that people aged 0-20 years old were more likely to fall under these crime-related classes (approx- imately 18%) compared to images of people in different age ranges (approximately 12% for people aged 20-60 and 0% for people over 70). We found signiﬁcant disparities in classiﬁcations across races for crime related terms, which is captured in Table 6."}
{"doc_id": "2103.00020", "para_id": 206, "text": "Given that we observed that people under 20 were the most likely to be classiﬁed in both the crime-related and non- human animal categories, we carried out classiﬁcation for the images with the same classes but with an additional category ‘child’ added to the categories. Our goal here was to see if this category would signiﬁcantly change the behaviour of the model and shift how the denigration harms are distributed by age. We found that this drastically reduced the number of images of people under 20 classiﬁed in either crime-related categories or non-human animal categories (Table 7). This points to how class design has the potential to be a key factor determining both the model performance and the unwanted biases or behaviour the model may exhibit while also asks overarching questions about the use of face"}
{"doc_id": "2103.00020", "para_id": 207, "text": "We also probed the model using classiﬁcation terms with high potential to cause representational harm, focusing on denigration harms in particular (Crawford, 2017). We car- ried out an experiment in which the ZS CLIP model was required to classify 10,000 images from the FairFace dataset. In addition to the FairFace classes, we added in the follow- ing classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’, ‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this experiment was to check if harms of denigration dispropor- tionately impact certain demographic subgroups."}
{"doc_id": "2103.00020", "para_id": 208, "text": "images to automatically classify people along such lines (y Arcas et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 209, "text": "When given the combined set of labels that Google Cloud Vision (GCV), Amazon Rekognition and Microsoft returned for all the images, similar to the biases Schwemmer et al. (2020) found in GCV systems, we found our system also disproportionately attached labels to do with hair and ap- pearance in general to women more than men. For ex- ample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’ appeared signiﬁcantly more often for women. Additionally, CLIP attached some labels that described high status occu- pations disproportionately more often to men such as ‘ex- ecutive’ and ‘doctor’. Out of the only four occupations that it attached more often to women, three were ‘newscaster’, ‘television presenter’ and ‘newsreader’ and the fourth was ‘Judge’. This is again similar to the biases found in GCV and points to historical gendered differences (Schwemmer et al., 2020)."}
{"doc_id": "2103.00020", "para_id": 210, "text": "The results of these probes can change based on the class categories one chooses to include as well as the speciﬁc language one uses to describe each class. Poor class design can lead to poor real world performance; this concern is particularly relevant to a model like CLIP, given how easily developers can design their own classes."}
{"doc_id": "2103.00020", "para_id": 211, "text": "We also carried out experiments similar to those outlined by Schwemmer et al. (2020) to test how CLIP treated images of men and women differently using images of Members of Congress. As part of these experiments, we studied how certain additional design decisions such as deciding thresholds for labels can impact the labels output by CLIP and how biases manifest."}
{"doc_id": "2103.00020", "para_id": 212, "text": "We carried out three experiments - we tested for accuracy on gender classiﬁcation and we tested for how labels were differentially distributed across two different label sets. For our ﬁrst label set, we used a label set of 300 occupations and for our second label set we used a combined set of labels that Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision returned for all the images."}
{"doc_id": "2103.00020", "para_id": 213, "text": "Interestingly, when we lowered the threshold to 0.5% for this set of labels, we found that the labels disproportionately describing men also shifted to appearance oriented words such as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa- tion oriented words such as ‘military person’ and ‘executive’ - which were not used to describe images of women at the higher 4% threshold - were used for both men and women at the lower 0.5% threshold, which could have caused the change in labels for men. The reverse was not true. Descrip- tive words used to describe women were still uncommon amongst men."}
{"doc_id": "2103.00020", "para_id": 214, "text": "We ﬁrst simply looked into gender prediction performance of the model on the images of Members of Congress, in order to check to see if the model correctly recognized men as men and women as women given the image of a person who appeared to be in an ofﬁcial setting/position of power. We found that the model got 100% accuracy on the images. This is slightly better performance than the model’s performance on the FairFace dataset. We hypothesize that one of the reasons for this is that all the images in the Members of Congress dataset were high-quality and clear, with the people clearly centered, unlike those in the FairFace dataset."}
{"doc_id": "2103.00020", "para_id": 215, "text": "Design decisions at every stage of building a model impact how biases manifest and this is especially true for CLIP given the ﬂexibility it offers. In addition to choices about training data and model architecture, decisions about things like class designs and thresholding values can alter the labels a model outputs and as a result heighten or lower certain kinds of harm, such as those described by Crawford (2017). People designing and developing models and AI systems have considerable power. Decisions about things like class design are a key determiner not only of model performance, but also of how and in what contexts model biases manifest."}
{"doc_id": "2103.00020", "para_id": 216, "text": "In order to study how the biases in returned labels depend on the thresholds set for label probability, we did an experiment in which we set threshold values at 0.5% and 4.0%. We found that the lower threshold led to lower quality of labels. However, even the differing distributions of labels under this threshold can hold signals for bias. For example, we ﬁnd that under the 0.5% threshold labels such as ‘nanny’ and ‘housekeeper’ start appearing for women whereas labels such as ‘prisoner’ and ‘mobster’ start appearing for men. This points to gendered associations similar to those that have previously been found for occupations (Schwemmer et al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016)."}
{"doc_id": "2103.00020", "para_id": 217, "text": "These experiments are not comprehensive. They illus- trate potential issues stemming from class design and other sources of bias, and are intended to spark inquiry."}
{"doc_id": "2103.00020", "para_id": 218, "text": "We next sought to characterize model performance in re- lation to a downstream task for which there is signiﬁcant societal sensitivity: surveillance. Our analysis aims to better embody the characterization approach described above and to help orient the research community towards the potential future impacts of increasingly general purpose computer vision models and aid the development of norms and checks"}
{"doc_id": "2103.00020", "para_id": 219, "text": "At the higher 4% threshold, the labels with the highest prob- ability across both genders include “lawmaker”, “legislator” and “congressman”. However, the presence of these biases amongst lower probability labels nonetheless point to larger questions about what ‘sufﬁciently’ safe behaviour may look"}
{"doc_id": "2103.00020", "para_id": 220, "text": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were identiﬁed with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a certain label by gender."}
{"doc_id": "2103.00020", "para_id": 221, "text": "around such systems. Our inclusion of surveillance is not intended to indicate enthusiasm for this domain - rather, we think surveillance is an important domain to try to make predictions about given its societal implications (Zuboff, 2015; Browne, 2015)."}
{"doc_id": "2103.00020", "para_id": 222, "text": "the model to choose from. Additionally, we carried out a ‘stress test’ where the class set included at least one more caption for something that was ‘close’ to the image (for example, ‘parking lot with white car’ vs. ‘parking lot with red car’). We found that the model had a top-1 accuracy of 91.8% on the CCTV images for the initial evaluation. The accuracy dropped signiﬁcantly to 51.1% for the second evaluation, with the model incorrectly choosing the ‘close’ answer 40.7% of the time."}
{"doc_id": "2103.00020", "para_id": 223, "text": "We measure the model’s performance on classiﬁcation of images from CCTV cameras and zero-shot celebrity identiﬁ- cation. We ﬁrst tested model performance on low-resolution images captured from surveillance cameras (e.g. CCTV cameras). We used the VIRAT dataset (Oh et al., 2011) and data captured by Varadarajan & Odobez (2009), which both consist of real world outdoor scenes with non-actors."}
{"doc_id": "2103.00020", "para_id": 224, "text": "For ﬁne-grained detection, the zero-shot model performed poorly, with results near random. Note that this experiment was targeted only towards detecting the presence or absence of small objects in image sequences."}
{"doc_id": "2103.00020", "para_id": 225, "text": "Given CLIP’s ﬂexible class construction, we tested 515 surveillance images captured from 12 different video se- quences on self-constructed general classes for coarse and ﬁne grained classiﬁcation. Coarse classiﬁcation required the model to correctly identify the main subject of the image (i.e. determine if the image was a picture of an empty parking lot, school campus, etc.). For ﬁne-grained classiﬁcation, the model had to choose between two options constructed to determine if the model could identify the presence/absence of smaller features in the image such as a person standing in the corner."}
{"doc_id": "2103.00020", "para_id": 226, "text": "We also tested CLIP’s zero-shot performance for ‘in the wild’ identity detection using the CelebA dataset8. We did this to evaluate the model’s performance for identity detec- tion using just the publicly available data it was pre-trained on. While we tested this on a dataset of celebrities who have a larger number of images on the internet, we hypothesize that the number of images in the pre-training data needed for the model to associate faces with names will keep de- creasing as models get more powerful (see Table 8), which has signiﬁcant societal implications (Garvie, 2019). This"}
{"doc_id": "2103.00020", "para_id": 227, "text": "For coarse classiﬁcation, we constructed the classes by hand- captioning the images ourselves to describe the contents of the image and there were always at least 6 options for"}
{"doc_id": "2103.00020", "para_id": 228, "text": "8Note: The CelebA dataset is more representative of faces with lighter skin tones. Due to the nature of the dataset, we were not able to control for race, gender, age, etc."}
{"doc_id": "2103.00020", "para_id": 229, "text": "We hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models, and we are excited to engage with the research community on such questions."}
{"doc_id": "2103.00020", "para_id": 230, "text": "CLIP L/14 59.2 43.3 42.2 CLIP RN50x64 56.4 39.5 38.4 CLIP RN50x16 52.7 37.4 36.3 CLIP RN50x4 52.8 38.1 37.3"}
{"doc_id": "2103.00020", "para_id": 231, "text": "We believe one good step forward is community exploration to further characterize the capabilities of models like CLIP and - crucially - identify application areas where they have promising performance and areas where they may have reduced performance9. This process of characterization can help researchers increase the likelihood models are used beneﬁcially by:"}
{"doc_id": "2103.00020", "para_id": 232, "text": "Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy"}
{"doc_id": "2103.00020", "para_id": 233, "text": "mirrors recent developments in natural language processing, in which recent large language models trained on Internet data often exhibit a surprising ability to provide informa- tion related to relatively minor public ﬁgures (Brown et al., 2020)."}
{"doc_id": "2103.00020", "para_id": 234, "text": "• Identifying potentially beneﬁcial downstream uses of models early in the research process, enabling other researchers to think about applications."}
{"doc_id": "2103.00020", "para_id": 235, "text": "We found that the model had 59.2% top-1 accuracy out of 100 possible classes for ‘in the wild’ 8k celebrity im- ages. However, this performance dropped to 43.3% when we increased our class sizes to 1k celebrity names. This performance is not competitive when compared to produc- tion level models such as Google’s Celebrity Recognition (Google). However, what makes these results noteworthy is that this analysis was done using only zero-shot identiﬁca- tion capabilities based on names inferred from pre-training data - we didn’t use any additional task-speciﬁc dataset, and so the (relatively) strong results further indicate that before deploying multimodal models, people will need to carefully study them for behaviors in a given context and domain."}
{"doc_id": "2103.00020", "para_id": 236, "text": "• Surfacing tasks with signiﬁcant sensitivity and a large set of societal stakeholders, which may call for inter- vention by policymakers."}
{"doc_id": "2103.00020", "para_id": 237, "text": "• Better characterizing biases in models, alerting other researchers to areas of concern and areas for interven- tions."}
{"doc_id": "2103.00020", "para_id": 238, "text": "• Creating suites of tests to evaluate systems like CLIP on, so we can better characterize model capabilities earlier in the development cycle."}
{"doc_id": "2103.00020", "para_id": 239, "text": "• Identifying potential failure modes and areas for further work."}
{"doc_id": "2103.00020", "para_id": 240, "text": "CLIP offers signiﬁcant beneﬁt for tasks that have relatively little data given its zero-shot capabilities. However, large datasets and high performing supervised models exist for many in-demand surveillance tasks such as facial recogni- tion. As a result, CLIP’s comparative appeal for such uses is low. Additionally, CLIP is not designed for common surveillance-relevant tasks like object detection and seman- tic segmentation. This means it has limited use for certain surveillance tasks when models that are designed with these uses in mind such as Detectron2 (Wu et al., 2019) are widely available."}
{"doc_id": "2103.00020", "para_id": 241, "text": "We plan to contribute to this work, and hope this analysis provides some motivating examples for subsequent research."}
{"doc_id": "2103.00020", "para_id": 242, "text": "Any model that leverages written, spoken, signed or any other form of human language as part of its training signal is arguably using natural language as a source of supervi- sion. This is an admittedly extremely broad area and covers most work in the ﬁeld of distributional semantics including topic models (Blei et al., 2003), word, sentence, and para- graph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le & Mikolov, 2014), and language models (Bengio et al., 2003). It also includes much of the broader ﬁeld of NLP that deals with predicting or modeling sequences of natural language in some way. Work in NLP intentionally leveraging natural language supervision in the form of explanations, feedback, instructions, and advice for tasks such as classiﬁcation (as opposed to the commonly used representation of supervision as a set of arbitrarily encoded discrete category labels) has"}
{"doc_id": "2103.00020", "para_id": 243, "text": "However, CLIP does unlock a certain aspect of usability given how it removes the need for training data. Thus, CLIP and similar models could enable bespoke, niche surveillance use cases for which no well-tailored models or datasets exist, and could lower the skill requirements to build such appli- cations. As our experiments show, ZS CLIP displays non- trivial, but not exceptional, performance on a few surveil- lance relevant tasks today."}
{"doc_id": "2103.00020", "para_id": 244, "text": "This preliminary analysis is intended to illustrate some of the challenges that general purpose computer vision models pose and to give a glimpse into their biases and impacts."}
{"doc_id": "2103.00020", "para_id": 245, "text": "9A model could be unﬁt for use due to inadequate performance or due to the inappropriateness of AI use in the application area itself."}
{"doc_id": "2103.00020", "para_id": 246, "text": "large scale representation learning by training a system to pair descriptive text with videos instead of images. Several works have explored using dense spoken natural language supervision for videos (Miech et al., 2019; 2020b). When considered together with CLIP, these works suggest that large scale natural language supervision is a promising way to learn high quality perceptual systems for many domains. Alayrac et al. (2020) extended this line of work to an addi- tional modality by adding raw audio as an additional super- vision source and demonstrated beneﬁts from combining all three sources of supervision."}
{"doc_id": "2103.00020", "para_id": 247, "text": "been explored in many creative and advanced ways. Dialog based learning (Weston, 2016; Li et al., 2016; Hancock et al., 2019) develops techniques to learn from interactive natural language feedback in dialog. Several papers have leveraged semantic parsing to convert natural language explanations into features (Srivastava et al., 2017) or additional training labels (Hancock et al., 2018). More recently, ExpBERT (Murty et al., 2020) uses feature representations produced by conditioning a deep contextual language model on nat- ural language explanations and descriptions of relations to improve performance on the task of relation extraction."}
{"doc_id": "2103.00020", "para_id": 248, "text": "CLIP is an example of using natural language as a training signal for learning about a domain other than language. In this context, the earliest use of the term natural language supervision that we are aware of is the work of Ramanathan et al. (2013) which showed that natural language descrip- tions could be used along side other sources of supervision to improve performance on the task of video event under- standing. However, as mentioned in the introduction and approach section, methods of leveraging natural language descriptions in computer vision well predate the use of this speciﬁc term, especially for image retrieval (Mori et al., 1999) and object classiﬁcation (Wang et al., 2009). Other early work leveraged tags (but not natural language) asso- ciated with images for the task of semantic segmentation (Barnard et al., 2003). More recently, He & Peng (2017) and Liang et al. (2020) demonstrated using natural language descriptions and explanations to improve ﬁne-grained vi- sual classiﬁcation of birds. Others have investigated how grounded language can be used to improve visual represen- tations and classiﬁers on the ShapeWorld dataset (Kuhnle & Copestake, 2017; Andreas et al., 2017; Mu et al., 2019). Finally, techniques which combine natural language with reinforcement learning environments (Narasimhan et al., 2015) have demonstrated exciting emergent behaviors such as systematically accomplishing zero-shot tasks (Hill et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 249, "text": "As part of our work on CLIP we also construct a new dataset of image-text pairs. Modern work on image-text retrieval has relied on a set of crowd-sourced sentence level im- age caption evaluation datasets like Pascal1K (Rashtchian et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K (Young et al., 2014). However, these datasets are still rel- atively small and limit achievable performance. Several methods have been proposed to create larger datasets au- tomatically with Ordonez et al. (2011) as a notable early example. In the deep learning era, Mithun et al. (2018) demonstrated an additional set of (image, text) pairs col- lected from the internet could improve retrieval performance and several new automatically constructed datasets such as Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al., 2020), and OCR-CC (Yang et al., 2020) have been created. However, these datasets still use signiﬁcantly more aggres- sive ﬁltering or are designed for a speciﬁc task such as OCR and as a result are still much smaller than WIT with between 1 and 10 million training examples."}
{"doc_id": "2103.00020", "para_id": 250, "text": "A related idea to CLIP is webly supervised learning. This line of work queries image search engines to build image datasets by querying for terms and uses the queries as the labels for the returned images (Fergus et al., 2005). Classi- ﬁers trained on these large but noisily labeled datasets can be competitive with those trained on smaller carefully la- beled datasets. These image-query pairs are also often used to improve performance on standard datasets as additional training data (Chen & Gupta, 2015). CLIP also uses search queries as part of its dataset creation process. However CLIP only uses full text sequences co-occuring with images as supervision rather than just the queries, which are often only a single word or short n-gram. We also restrict this step in CLIP to text only querying for sub-string matches while most webly supervised work uses standard image search engines which have their own complex retrieval and ﬁlter- ing pipelines that often involve computer vision systems. Of this line of work, Learning Everything about Anything: Webly-Supervised Visual Concept Learning (Divvala et al.,"}
{"doc_id": "2103.00020", "para_id": 251, "text": "CLIP’s pre-training task optimizes for text-image retrieval. This areas of research dates back to the mid-90s with the previously mentioned Mori et al. (1999) as representative of early work. While initial efforts focused primarily on predic- tive objectives over time research shifted towards learning joint multi-modal embedding spaces with techniques like kernel Canonical Correlation Analysis and various ranking objectives (Weston et al., 2010; Socher & Fei-Fei, 2010; Hodosh et al., 2013). Over time work explored many combi- nations of training objective, transfer, and more expressive models and steadily improved performance (Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al., 2014; Faghri et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 252, "text": "2014) has a notably similar ambition and goal as CLIP."}
{"doc_id": "2103.00020", "para_id": 253, "text": "Other work has leveraged natural language supervision for domains other than images. Stroud et al. (2020) explores"}
{"doc_id": "2103.00020", "para_id": 254, "text": "Finally, CLIP is related to a recent burst of activity on learn- ing joint models of vision and language (Lu et al., 2019; Tan"}
{"doc_id": "2103.00020", "para_id": 255, "text": "& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al., 2020). This line of work focuses on richly connecting vision and language in order to solve complex downstream tasks such as visual question answering, visual commonsense reasoning, or multimodal entailment. These approaches leverage impressively engineered models which combine 3 (or more) pre-trained subsystems, typically an image feature model, a region proposal / object detection model, and a pre-trained masked language model such as BERT. These systems are then jointly ﬁne-tuned via various training objec- tives on image-text pairs and applied to the aforementioned tasks and achieve impressive results. CLIP is instead fo- cused on learning visual models from scratch via natural language supervision and does not densely connect the two domains with a joint attention model. The only interaction in a CLIP model between the image and text domain is a single dot product in a learned joint embedding space. We are excited to see CLIP hybridized with this line of work."}
{"doc_id": "2103.00020", "para_id": 256, "text": "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pp. 265–283, 2016."}
{"doc_id": "2103.00020", "para_id": 257, "text": "Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi´c, R., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S., and Zisserman, A. Self-supervised multimodal versatile networks. arXiv preprint arXiv:2006.16228, 2020."}
{"doc_id": "2103.00020", "para_id": 258, "text": "Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.- S., and Nguyen, A. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4845–4854, 2019."}
{"doc_id": "2103.00020", "para_id": 259, "text": "Andreas, J., Klein, D., and Levine, S. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017."}
{"doc_id": "2103.00020", "para_id": 260, "text": "We have investigated whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain. We ﬁnd that adopting this formula re- sults in similar behaviors emerging in the ﬁeld of computer vision and discuss the social implications of this line of research. In order to optimize their training objective, CLIP models learn to perform a wide variety of tasks during pre- training. This task learning can then be leveraged via natural language prompting to enable zero-shot transfer to many existing datasets. At sufﬁcient scale, the performance of this approach can be competitive with task-speciﬁc supervised models although there is still room for much improvement."}
{"doc_id": "2103.00020", "para_id": 261, "text": "Assiri, Y. Stochastic optimization of plain convolutional neural networks with simple methods. arXiv preprint arXiv:2001.08856, 2020."}
{"doc_id": "2103.00020", "para_id": 262, "text": "Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15535–15545, 2019."}
{"doc_id": "2103.00020", "para_id": 263, "text": "Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut- freund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the lim- its of object recognition models. In Advances in Neural Information Processing Systems, pp. 9453–9463, 2019."}
{"doc_id": "2103.00020", "para_id": 264, "text": "We’d like to thank the millions of people involved in creating the data CLIP is trained on. We’d also like to thank Susan Zhang for her work on image conditional language models while at OpenAI, Ishaan Gulrajani for catching an error in the pseudocode, and Irene Solaiman, Miles Brundage, and Gillian Hadﬁeld for their thoughtful feedback on the broader impacts section of the paper. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. Finally, we’d also like to thank the developers of the many software packages used throughout this project including, but not limited, to Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor- Flow (Abadi et al., 2016), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020), and scikit-learn (Pedregosa et al., 2011)."}
{"doc_id": "2103.00020", "para_id": 265, "text": "Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei, D. M., and Jordan, M. I. Matching words and pictures. Journal of machine learning research, 3(Feb):1107–1135, 2003."}
{"doc_id": "2103.00020", "para_id": 266, "text": "Bechmann, A. and Bowker, G. C. Unsupervised by any other name: Hidden layers of knowledge production in artiﬁcial intelligence on social media. Big Data & Society, 6(1):205395171881956, January 2019. doi: 10.1177/ 2053951718819569. URL https://doi.org/10. 1177/2053951718819569."}
{"doc_id": "2103.00020", "para_id": 267, "text": "Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137–1155, 2003."}
{"doc_id": "2103.00020", "para_id": 268, "text": "Bhargava, S. and Forsyth, D. Exposing and correcting the gender bias in image captioning datasets and models. arXiv preprint arXiv:1912.00578, 2019."}
{"doc_id": "2103.00020", "para_id": 269, "text": "Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan): 993–1022, 2003."}
{"doc_id": "2103.00020", "para_id": 270, "text": "Chen, X., Fan, H., Girshick, R., and He, K. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020d."}
{"doc_id": "2103.00020", "para_id": 271, "text": "Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. Uniter: Learning universal image- text representations. arXiv preprint arXiv:1909.11740, 2019."}
{"doc_id": "2103.00020", "para_id": 272, "text": "Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29:4349–4357, 2016."}
{"doc_id": "2103.00020", "para_id": 273, "text": "Cheng, G., Han, J., and Lu, X. Remote sensing image scene classiﬁcation: Benchmark and state of the art. Proceed- ings of the IEEE, 105(10):1865–1883, 2017."}
{"doc_id": "2103.00020", "para_id": 274, "text": "Bowker, G. C. and Star, S. L. Sorting things out: Classiﬁca- tion and its consequences. MIT press, 2000."}
{"doc_id": "2103.00020", "para_id": 275, "text": "Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J., and Dahl, G. E. On empirical comparisons of optimiz- ers for deep learning. arXiv preprint arXiv:1910.05446, 2019."}
{"doc_id": "2103.00020", "para_id": 276, "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."}
{"doc_id": "2103.00020", "para_id": 277, "text": "Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pp. 215–223, 2011."}
{"doc_id": "2103.00020", "para_id": 278, "text": "Browne, S. Dark Matters: Surveillance of Blackness. Duke University Press, 2015."}
{"doc_id": "2103.00020", "para_id": 279, "text": "Bulent Sariyildiz, M., Perez, J., and Larlus, D. Learning visual representations with caption annotations. arXiv e-prints, pp. arXiv–2008, 2020."}
{"doc_id": "2103.00020", "para_id": 280, "text": "Crawford, K. The trouble with bias. NIPS 2017 Keynote, 2017. URL https://www.youtube.com/ watch?v=fMym_BKWQzk."}
{"doc_id": "2103.00020", "para_id": 281, "text": "Buolamwini, J. and Gebru, T. Gender shades: Intersec- tional accuracy disparities in commercial gender classi- ﬁcation. In Conference on fairness, accountability and transparency, pp. 77–91, 2018."}
{"doc_id": "2103.00020", "para_id": 282, "text": "Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 3079–3087, 2015."}
{"doc_id": "2103.00020", "para_id": 283, "text": "Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019."}
{"doc_id": "2103.00020", "para_id": 284, "text": "D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali- panahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., et al. Underspeciﬁcation presents challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395, 2020."}
{"doc_id": "2103.00020", "para_id": 285, "text": "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691–1703. PMLR, 2020a."}
{"doc_id": "2103.00020", "para_id": 286, "text": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei- Fei, L. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009."}
{"doc_id": "2103.00020", "para_id": 287, "text": "Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016."}
{"doc_id": "2103.00020", "para_id": 288, "text": "Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A., and Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www. image-net.org/challenges/LSVRC/2012/."}
{"doc_id": "2103.00020", "para_id": 289, "text": "Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. arXiv preprint arXiv:2002.05709, 2020b."}
{"doc_id": "2103.00020", "para_id": 290, "text": "Desai, K. and Johnson, J. Virtex: Learning visual rep- resentations from textual annotations. arXiv preprint arXiv:2006.06666, 2020."}
{"doc_id": "2103.00020", "para_id": 291, "text": "Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. Big self-supervised models are strong semi- supervised learners. arXiv preprint arXiv:2006.10029, 2020c."}
{"doc_id": "2103.00020", "para_id": 292, "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018."}
{"doc_id": "2103.00020", "para_id": 293, "text": "Chen, X. and Gupta, A. Webly supervised learning of convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1431– 1439, 2015."}
{"doc_id": "2103.00020", "para_id": 294, "text": "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020."}
{"doc_id": "2103.00020", "para_id": 295, "text": "Divvala, S. K., Farhadi, A., and Guestrin, C. Learning everything about anything: Webly-supervised visual con- cept learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3270– 3277, 2014."}
{"doc_id": "2103.00020", "para_id": 296, "text": "biased towards texture; increasing shape bias improves ac- curacy and robustness. arXiv preprint arXiv:1811.12231, 2018."}
{"doc_id": "2103.00020", "para_id": 297, "text": "Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Short- cut learning in deep neural networks. arXiv preprint arXiv:2004.07780, 2020."}
{"doc_id": "2103.00020", "para_id": 298, "text": "Dodge, S. and Karam, L. A study and comparison of human and deep learning recognition performance under visual distortions. In 2017 26th international conference on computer communication and networks (ICCCN), pp. 1– 7. IEEE, 2017."}
{"doc_id": "2103.00020", "para_id": 299, "text": "Gomez, L., Patel, Y., Rusi˜nol, M., Karatzas, D., and Jawahar, C. Self-supervised learning of visual features through embedding images into text topic spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4230–4239, 2017."}
{"doc_id": "2103.00020", "para_id": 300, "text": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."}
{"doc_id": "2103.00020", "para_id": 301, "text": "Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014."}
{"doc_id": "2103.00020", "para_id": 302, "text": "Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi- ﬁer: Zero-shot learning using purely textual descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2584–2591, 2013."}
{"doc_id": "2103.00020", "para_id": 303, "text": "Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., et al. Challenges in representation learn- ing: A report on three machine learning contests. Neural Networks, 64:59–63, 2015."}
{"doc_id": "2103.00020", "para_id": 304, "text": "Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im- proving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017."}
{"doc_id": "2103.00020", "para_id": 305, "text": "Google. Google cloud api: Celebrity recognition. URL"}
{"doc_id": "2103.00020", "para_id": 306, "text": "https://cloud.google.com/vision/docs/ celebrity-recognition."}
{"doc_id": "2103.00020", "para_id": 307, "text": "Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn- ing object categories from google’s image search. In Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE, 2005."}
{"doc_id": "2103.00020", "para_id": 308, "text": "Griewank, A. and Walther, A. Algorithm 799: revolve: an implementation of checkpointing for the reverse or ad- joint mode of computational differentiation. ACM Trans- actions on Mathematical Software (TOMS), 26(1):19–45, 2000."}
{"doc_id": "2103.00020", "para_id": 309, "text": "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. Devise: A deep visual- semantic embedding model. In Advances in neural infor- mation processing systems, pp. 2121–2129, 2013."}
{"doc_id": "2103.00020", "para_id": 310, "text": "Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo, Z. D., Azar, M. G., et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020."}
{"doc_id": "2103.00020", "para_id": 311, "text": "Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J. Large-scale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020."}
{"doc_id": "2103.00020", "para_id": 312, "text": "Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016."}
{"doc_id": "2103.00020", "para_id": 313, "text": "Hancock, B., Bringmann, M., Varma, P., Liang, P., Wang, S., and R´e, C. Training classiﬁers with natural language explanations. In Proceedings of the conference. Associ- ation for Computational Linguistics. Meeting, volume 2018, pp. 1884. NIH Public Access, 2018."}
{"doc_id": "2103.00020", "para_id": 314, "text": "Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020."}
{"doc_id": "2103.00020", "para_id": 315, "text": "Garvie, C., May 2019. URL https://www. flawedfacedata.com/."}
{"doc_id": "2103.00020", "para_id": 316, "text": "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019."}
{"doc_id": "2103.00020", "para_id": 317, "text": "Geiger, A., Lenz, P., and Urtasun, R. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012."}
{"doc_id": "2103.00020", "para_id": 318, "text": "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del"}
{"doc_id": "2103.00020", "para_id": 319, "text": "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained cnns are"}
{"doc_id": "2103.00020", "para_id": 320, "text": "R´ıo, J., Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585:357–362, 2020. doi: 10.1038/ s41586-020-2649-2."}
{"doc_id": "2103.00020", "para_id": 321, "text": "Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016."}
{"doc_id": "2103.00020", "para_id": 322, "text": "Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019."}
{"doc_id": "2103.00020", "para_id": 323, "text": "Hays, J. and Efros, A. A. Im2gps: estimating geographic information from a single image. In 2008 ieee confer- ence on computer vision and pattern recognition, pp. 1–8. IEEE, 2008."}
{"doc_id": "2103.00020", "para_id": 324, "text": "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analy- sis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020a."}
{"doc_id": "2103.00020", "para_id": 325, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE inter- national conference on computer vision, pp. 1026–1034, 2015."}
{"doc_id": "2103.00020", "para_id": 326, "text": "Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-of- distribution robustness. arXiv preprint arXiv:2004.06100, 2020b."}
{"doc_id": "2103.00020", "para_id": 327, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a."}
{"doc_id": "2103.00020", "para_id": 328, "text": "Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "2103.00020", "para_id": 329, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016b."}
{"doc_id": "2103.00020", "para_id": 330, "text": "Hill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick, M., McClelland, J. L., and Santoro, A. Environmental drivers of systematicity and generalization in a situated agent. In International Conference on Learning Repre- sentations, 2019."}
{"doc_id": "2103.00020", "para_id": 331, "text": "He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729– 9738, 2020."}
{"doc_id": "2103.00020", "para_id": 332, "text": "Hodosh, M., Young, P., and Hockenmaier, J. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artiﬁcial Intelligence Research, 47: 853–899, 2013."}
{"doc_id": "2103.00020", "para_id": 333, "text": "Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet: Enhancing image geolocalization by combinatorial parti- tioning of maps. In Proceedings of the European Confer- ence on Computer Vision (ECCV), pp. 536–551, 2018."}
{"doc_id": "2103.00020", "para_id": 334, "text": "He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M. Bag of tricks for image classiﬁcation with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 558– 567, 2019."}
{"doc_id": "2103.00020", "para_id": 335, "text": "Howard, J. and Ruder, S. Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "2103.00020", "para_id": 336, "text": "He, X. and Peng, Y. Fine-grained image classiﬁcation via combining vision and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pp. 5994–6002, 2017."}
{"doc_id": "2103.00020", "para_id": 337, "text": "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pp. 125–136, 2019."}
{"doc_id": "2103.00020", "para_id": 338, "text": "Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classiﬁcation. IEEE Journal of Se- lected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019."}
{"doc_id": "2103.00020", "para_id": 339, "text": "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015."}
{"doc_id": "2103.00020", "para_id": 340, "text": "Henaff, O. Data-efﬁcient image recognition with contrastive predictive coding. In International Conference on Ma- chine Learning, pp. 4182–4192. PMLR, 2020."}
{"doc_id": "2103.00020", "para_id": 341, "text": "Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Deep structured output learning for unconstrained text recognition. arXiv preprint arXiv:1412.5903, 2014."}
{"doc_id": "2103.00020", "para_id": 342, "text": "Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019."}
{"doc_id": "2103.00020", "para_id": 343, "text": "Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial transformer networks. Advances in neural information processing systems, 28:2017–2025, 2015."}
{"doc_id": "2103.00020", "para_id": 344, "text": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, 123(1):32–73, 2017."}
{"doc_id": "2103.00020", "para_id": 345, "text": "Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: A diag- nostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pp. 2901–2910, 2017."}
{"doc_id": "2103.00020", "para_id": 346, "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012."}
{"doc_id": "2103.00020", "para_id": 347, "text": "Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N. Learning visual features from large weakly supervised data. In European Conference on Computer Vision, pp. 67–84. Springer, 2016."}
{"doc_id": "2103.00020", "para_id": 348, "text": "Kuhnle, A. and Copestake, A. Shapeworld-a new test methodology for multimodal language understanding. arXiv preprint arXiv:1704.04517, 2017."}
{"doc_id": "2103.00020", "para_id": 349, "text": "Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal modeling in 3d cnn architectures with bert for action recognition. arXiv preprint arXiv:2008.01232, 2020."}
{"doc_id": "2103.00020", "para_id": 350, "text": "K¨arkk¨ainen, K. and Joo, J. Fairface: Face attribute dataset for balanced race, gender, and age, 2019."}
{"doc_id": "2103.00020", "para_id": 351, "text": "Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020."}
{"doc_id": "2103.00020", "para_id": 352, "text": "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh- man, S. J. Building machines that learn and think like people, 2016."}
{"doc_id": "2103.00020", "para_id": 353, "text": "Karpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment embeddings for bidirectional image sentence mapping. In Advances in neural information processing systems, pp. 1889–1897, 2014."}
{"doc_id": "2103.00020", "para_id": 354, "text": "Lampert, C. H., Nickisch, H., and Harmeling, S. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 951–958. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 355, "text": "Keyes, O. The misgendering machines: Trans/hci implica- tions of automatic gender recognition. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):1–22, 2018."}
{"doc_id": "2103.00020", "para_id": 356, "text": "Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning of new tasks. 2008."}
{"doc_id": "2103.00020", "para_id": 357, "text": "Le, Q. and Mikolov, T. Distributed representations of sen- tences and documents. In International conference on machine learning, pp. 1188–1196, 2014."}
{"doc_id": "2103.00020", "para_id": 358, "text": "Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Ringshia, P., and Testuggine, D. The hateful memes challenge: Detecting hate speech in multimodal memes. arXiv preprint arXiv:2005.04790, 2020."}
{"doc_id": "2103.00020", "para_id": 359, "text": "LeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/."}
{"doc_id": "2103.00020", "para_id": 360, "text": "Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."}
{"doc_id": "2103.00020", "para_id": 361, "text": "Lee, D.-H. Pseudo-label: The simple and efﬁcient semi- supervised learning method for deep neural networks."}
{"doc_id": "2103.00020", "para_id": 362, "text": "Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural lan- guage models. arXiv preprint arXiv:1411.2539, 2014."}
{"doc_id": "2103.00020", "para_id": 363, "text": "Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4247–4255, 2015."}
{"doc_id": "2103.00020", "para_id": 364, "text": "Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. Advances in neural information processing systems, 28: 3294–3302, 2015."}
{"doc_id": "2103.00020", "para_id": 365, "text": "Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning visual n-grams from web data. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4183–4192, 2017."}
{"doc_id": "2103.00020", "para_id": 366, "text": "Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Large scale learning of general visual representations for transfer. arXiv preprint arXiv:1912.11370, 2019."}
{"doc_id": "2103.00020", "para_id": 367, "text": "Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. 2020a."}
{"doc_id": "2103.00020", "para_id": 368, "text": "Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661–2671, 2019."}
{"doc_id": "2103.00020", "para_id": 369, "text": "Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. Learning through dialogue interactions by asking ques- tions. arXiv preprint arXiv:1612.04936, 2016."}
{"doc_id": "2103.00020", "para_id": 370, "text": "Proceedings of the European Conference on Computer Vision (ECCV), pp. 181–196, 2018."}
{"doc_id": "2103.00020", "para_id": 371, "text": "Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al. Oscar: Object- semantics aligned pre-training for vision-language tasks. arXiv preprint arXiv:2004.06165, 2020b."}
{"doc_id": "2103.00020", "para_id": 372, "text": "McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Advances in neural information processing systems, pp. 6294–6305, 2017."}
{"doc_id": "2103.00020", "para_id": 373, "text": "Liang, W., Zou, J., and Yu, Z. Alice: Active learning with contrastive natural language explanations. arXiv preprint arXiv:2009.10259, 2020."}
{"doc_id": "2103.00020", "para_id": 374, "text": "McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as ques- tion answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2103.00020", "para_id": 375, "text": "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 376, "text": "Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017."}
{"doc_id": "2103.00020", "para_id": 377, "text": "Linzen, T. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020."}
{"doc_id": "2103.00020", "para_id": 378, "text": "Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J. Howto100m: Learning a text-video em- bedding by watching hundred million narrated video clips. In Proceedings of the IEEE international conference on computer vision, pp. 2630–2640, 2019."}
{"doc_id": "2103.00020", "para_id": 379, "text": "Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An- toniou, G., Shutova, E., and Yannakoudakis, H. A mul- timodal framework for the detection of hateful memes. arXiv preprint arXiv:2012.12871, 2020."}
{"doc_id": "2103.00020", "para_id": 380, "text": "Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa- ssi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018."}
{"doc_id": "2103.00020", "para_id": 381, "text": "Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser- man, A. Rareact: A video dataset of unusual interactions. arXiv preprint arXiv:2008.01018, 2020a."}
{"doc_id": "2103.00020", "para_id": 382, "text": "Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. End-to-end learning of visual represen- tations from uncurated instructional videos. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9879–9889, 2020b."}
{"doc_id": "2103.00020", "para_id": 383, "text": "Locatello, F., Bauer, S., Lucic, M., R¨atsch, G., Gelly, S., Sch¨olkopf, B., and Bachem, O. A sober look at the unsupervised learning of disentangled representations and their evaluation. arXiv preprint arXiv:2010.14766, 2020."}
{"doc_id": "2103.00020", "para_id": 384, "text": "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. Advances in neural informa- tion processing systems, 26:3111–3119, 2013."}
{"doc_id": "2103.00020", "para_id": 385, "text": "Loshchilov, I. and Hutter, F. Sgdr: Stochastic gra- dient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016."}
{"doc_id": "2103.00020", "para_id": 386, "text": "Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization. arXiv preprint arXiv:1711.05101, 2017."}
{"doc_id": "2103.00020", "para_id": 387, "text": "Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question answering models. arXiv preprint arXiv:2004.14444, 2020."}
{"doc_id": "2103.00020", "para_id": 388, "text": "Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision- and-language tasks. In Advances in Neural Information Processing Systems, pp. 13–23, 2019."}
{"doc_id": "2103.00020", "para_id": 389, "text": "Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni- tion using higher order language priors. 2012."}
{"doc_id": "2103.00020", "para_id": 390, "text": "Mithun, N. C., Panda, R., Papalexakis, E. E., and Roy- Chowdhury, A. K. Webly supervised joint embedding for cross-modal image-text retrieval. In Proceedings of the 26th ACM international conference on Multimedia, pp. 1856–1864, 2018."}
{"doc_id": "2103.00020", "para_id": 391, "text": "Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging weakly supervised data and pose representation for action recognition, 2020. URL https://www.youtube. com/watch?v=KOQFxbPPLOE&t=1390s."}
{"doc_id": "2103.00020", "para_id": 392, "text": "Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous- quet, O. Are gans created equal? a large-scale study. Advances in neural information processing systems, 31: 700–709, 2018."}
{"doc_id": "2103.00020", "para_id": 393, "text": "Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans- formation based on dividing and vector quantizing images with words. Citeseer, 1999."}
{"doc_id": "2103.00020", "para_id": 394, "text": "Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and van der Maaten, L. Ex- ploring the limits of weakly supervised pretraining. In"}
{"doc_id": "2103.00020", "para_id": 395, "text": "Mu, J., Liang, P., and Goodman, N. Shaping visual represen- tations with language for few-shot classiﬁcation. arXiv preprint arXiv:1911.02683, 2019."}
{"doc_id": "2103.00020", "para_id": 396, "text": "Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo- cation estimation of photos using a hierarchical model and scene classiﬁcation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 563–579, 2018."}
{"doc_id": "2103.00020", "para_id": 397, "text": "Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024– 8035, 2019."}
{"doc_id": "2103.00020", "para_id": 398, "text": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour- napeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011."}
{"doc_id": "2103.00020", "para_id": 399, "text": "Murty, S., Koh, P. W., and Liang, P. Expbert: Representation engineering with natural language explanations. arXiv preprint arXiv:2005.01932, 2020."}
{"doc_id": "2103.00020", "para_id": 400, "text": "Narasimhan, K., Kulkarni, T., and Barzilay, R. Language understanding for text-based games using deep reinforce- ment learning. arXiv preprint arXiv:1506.08941, 2015."}
{"doc_id": "2103.00020", "para_id": 401, "text": "Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014."}
{"doc_id": "2103.00020", "para_id": 402, "text": "Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. 2011."}
{"doc_id": "2103.00020", "para_id": 403, "text": "Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018."}
{"doc_id": "2103.00020", "para_id": 404, "text": "Noble, S. U. Algorithms of oppression: How search engines reinforce racism. 2018."}
{"doc_id": "2103.00020", "para_id": 405, "text": "Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest- ing implicit group attitudes and beliefs from a demonstra- tion web site. Group Dynamics: Theory, Research, and Practice, 6(1):101, 2002."}
{"doc_id": "2103.00020", "para_id": 406, "text": "Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti, A. Imagebert: Cross-modal pre-training with large- scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966, 2020."}
{"doc_id": "2103.00020", "para_id": 407, "text": "Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee, J. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al. A large-scale benchmark dataset for event recognition in surveillance video. In CVPR 2011, pp. 3153–3160. IEEE, 2011."}
{"doc_id": "2103.00020", "para_id": 408, "text": "Quattoni, A., Collins, M., and Darrell, T. Learning visual representations using images with captions. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8. IEEE, 2007."}
{"doc_id": "2103.00020", "para_id": 409, "text": "Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre- training, 2018."}
{"doc_id": "2103.00020", "para_id": 410, "text": "Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good- fellow, I. Realistic evaluation of deep semi-supervised learning algorithms. Advances in neural information pro- cessing systems, 31:3235–3246, 2018."}
{"doc_id": "2103.00020", "para_id": 411, "text": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019."}
{"doc_id": "2103.00020", "para_id": 412, "text": "Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018."}
{"doc_id": "2103.00020", "para_id": 413, "text": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "2103.00020", "para_id": 414, "text": "Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24:1143–1151, 2011."}
{"doc_id": "2103.00020", "para_id": 415, "text": "Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., and Denton, E. Saving face: Investigating the ethical concerns of facial recognition auditing, 2020."}
{"doc_id": "2103.00020", "para_id": 416, "text": "pandas development team, T. pandas-dev/pandas: Pan- das, February 2020. URL https://doi.org/10. 5281/zenodo.3509134."}
{"doc_id": "2103.00020", "para_id": 417, "text": "Ramanathan, V., Liang, P., and Fei-Fei, L. Video event understanding using natural language descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 905–912, 2013."}
{"doc_id": "2103.00020", "para_id": 418, "text": "Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012."}
{"doc_id": "2103.00020", "para_id": 419, "text": "Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J. Collecting image annotations using amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pp. 139–147, 2010."}
{"doc_id": "2103.00020", "para_id": 420, "text": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,"}
{"doc_id": "2103.00020", "para_id": 421, "text": "Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857–1865, 2016."}
{"doc_id": "2103.00020", "para_id": 422, "text": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im- agenet classiﬁers generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019."}
{"doc_id": "2103.00020", "para_id": 423, "text": "Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert- Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J., McGufﬁe, K., and Wang, J. Release strategies and the social impacts of language models, 2019."}
{"doc_id": "2103.00020", "para_id": 424, "text": "Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information pro- cessing systems, pp. 901–909, 2016."}
{"doc_id": "2103.00020", "para_id": 425, "text": "Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How computers see gender: An evaluation of gender classiﬁca- tion in commercial facial analysis services. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW): 1–33, 2019."}
{"doc_id": "2103.00020", "para_id": 426, "text": "Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012."}
{"doc_id": "2103.00020", "para_id": 427, "text": "Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/ 10.5281/zenodo.2591652. Version 5.5."}
{"doc_id": "2103.00020", "para_id": 428, "text": "Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija, S., Schoonvelde, M., and Lockhart, J. W. Diagnosing gender bias in image recognition systems. Socius, 6: 2378023120967171, 2020."}
{"doc_id": "2103.00020", "para_id": 429, "text": "Srivastava, N. and Salakhutdinov, R. Multimodal learning with deep boltzmann machines. In NIPS, 2012."}
{"doc_id": "2103.00020", "para_id": 430, "text": "Srivastava, S., Labutov, I., and Mitchell, T. Joint concept learning and semantic parsing from natural language ex- planations. In Proceedings of the 2017 conference on empirical methods in natural language processing, pp. 1527–1536, 2017."}
{"doc_id": "2103.00020", "para_id": 431, "text": "Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "2103.00020", "para_id": 432, "text": "Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv preprint arXiv:1906.02168, 2019."}
{"doc_id": "2103.00020", "para_id": 433, "text": "Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The German Trafﬁc Sign Recognition Benchmark: A multi- class classiﬁcation competition. In IEEE International Joint Conference on Neural Networks, pp. 1453–1460, 2011."}
{"doc_id": "2103.00020", "para_id": 434, "text": "Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con- ceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pp. 2556– 2565, 2018."}
{"doc_id": "2103.00020", "para_id": 435, "text": "Stroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R., and Schmid, C. Learning video representations from tex- tual web supervision. arXiv preprint arXiv:2007.14937, 2020."}
{"doc_id": "2103.00020", "para_id": 436, "text": "Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pp. 8317–8326, 2019."}
{"doc_id": "2103.00020", "para_id": 437, "text": "Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016."}
{"doc_id": "2103.00020", "para_id": 438, "text": "Socher, R. and Fei-Fei, L. Connecting modalities: Semi- supervised segmentation and annotation of images using unaligned text corpora. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 966–973. IEEE, 2010."}
{"doc_id": "2103.00020", "para_id": 439, "text": "Tan, H. and Bansal, M. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019."}
{"doc_id": "2103.00020", "para_id": 440, "text": "Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019."}
{"doc_id": "2103.00020", "para_id": 441, "text": "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013."}
{"doc_id": "2103.00020", "para_id": 442, "text": "Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to natural dis- tribution shifts in image classiﬁcation. arXiv preprint arXiv:2007.00644, 2020."}
{"doc_id": "2103.00020", "para_id": 443, "text": "Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng, A. Y. Grounded compositional semantics for ﬁnding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218, 2014."}
{"doc_id": "2103.00020", "para_id": 444, "text": "Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016."}
{"doc_id": "2103.00020", "para_id": 445, "text": "Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019."}
{"doc_id": "2103.00020", "para_id": 446, "text": "Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He, M., Wang, Y., and Liu, W. All you need is boundary: To- ward arbitrary-shaped text spotting. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 12160–12167, 2020."}
{"doc_id": "2103.00020", "para_id": 447, "text": "Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and Isola, P. Rethinking few-shot image classiﬁcation: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020."}
{"doc_id": "2103.00020", "para_id": 448, "text": "Wang, J., Markert, K., and Everingham, M. Learning mod- els for object recognition from natural language descrip- tions. In BMVC, volume 1, pp. 2, 2009."}
{"doc_id": "2103.00020", "para_id": 449, "text": "Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958–1970, 2008."}
{"doc_id": "2103.00020", "para_id": 450, "text": "Weston, J., Bengio, S., and Usunier, N. Large scale im- age annotation: learning to rank with joint word-image embeddings. Machine learning, 81(1):21–35, 2010."}
{"doc_id": "2103.00020", "para_id": 451, "text": "Touvron, H., Vedaldi, A., Douze, M., and J´egou, H. Fix- ing the train-test resolution discrepancy. In Advances in neural information processing systems, pp. 8252–8262, 2019."}
{"doc_id": "2103.00020", "para_id": 452, "text": "Weston, J. E. Dialog-based language learning. In Advances in Neural Information Processing Systems, pp. 829–837, 2016."}
{"doc_id": "2103.00020", "para_id": 453, "text": "Varadarajan, J. and Odobez, J.-M. Topic models for scene analysis and abnormality detection. In 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, pp. 1338–1345. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 454, "text": "Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo- cation with convolutional neural networks. In European Conference on Computer Vision, pp. 37–55. Springer, 2016."}
{"doc_id": "2103.00020", "para_id": 455, "text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017."}
{"doc_id": "2103.00020", "para_id": 456, "text": "Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir- shick, R. Detectron2. https://github.com/ facebookresearch/detectron2, 2019."}
{"doc_id": "2103.00020", "para_id": 457, "text": "Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation equivariant CNNs for digital pathol- ogy. June 2018."}
{"doc_id": "2103.00020", "para_id": 458, "text": "Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature learning via non-parametric instance-level discrimination. arXiv preprint arXiv:1805.01978, 2018."}
{"doc_id": "2103.00020", "para_id": 459, "text": "Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2."}
{"doc_id": "2103.00020", "para_id": 460, "text": "Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10687–10698, 2020."}
{"doc_id": "2103.00020", "para_id": 461, "text": "y Arcas, B. A., Mitchell, M., and Todorov, A. Physiognomy’s new clothes. 2017. URL https://medium.com/@blaisea/ physiognomys-new-clothes-f2d4b59fdd6a."}
{"doc_id": "2103.00020", "para_id": 462, "text": "Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang, L., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware pre-training for text-vqa and text-caption. arXiv preprint arXiv:2012.04638, 2020."}
{"doc_id": "2103.00020", "para_id": 463, "text": "Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the deep learning era. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pp. 2621–2630, 2017."}
{"doc_id": "2103.00020", "para_id": 464, "text": "Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019."}
{"doc_id": "2103.00020", "para_id": 465, "text": "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018."}
{"doc_id": "2103.00020", "para_id": 466, "text": "Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Lin- guistics, 2:67–78, 2014."}
{"doc_id": "2103.00020", "para_id": 467, "text": "Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro- bust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019."}
{"doc_id": "2103.00020", "para_id": 468, "text": "Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H., and Wang, H. Ernie-vil: Knowledge enhanced vision- language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020."}
{"doc_id": "2103.00020", "para_id": 469, "text": "Zeiler, M. D. and Fergus, R. Visualizing and understand- ing convolutional networks. In European conference on computer vision, pp. 818–833. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 470, "text": "Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu- mann, M., Dosovitskiy, A., et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019."}
{"doc_id": "2103.00020", "para_id": 471, "text": "Zhang, R. Making convolutional networks shift-invariant again. arXiv preprint arXiv:1904.11486, 2019."}
{"doc_id": "2103.00020", "para_id": 472, "text": "Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan- glotz, C. P. Contrastive learning of medical visual repre- sentations from paired images and text. arXiv preprint arXiv:2010.00747, 2020."}
{"doc_id": "2103.00020", "para_id": 473, "text": "Zuboff, S. Big other: surveillance capitalism and the prospects of an information civilization. Journal of Infor- mation Technology, 30(1):75–89, 2015."}
{"doc_id": "2103.00020", "para_id": 474, "text": "the ResNet-50 architecture as in the smallest contrastive model. To do so, the output from the CNN is projected into four tokens, which are then fed as a preﬁx to a language model autoregressively predicting the text tokens. Apart from the training objective, the model was trained on the same dataset for the same number of epochs as other CLIP models."}
{"doc_id": "2103.00020", "para_id": 475, "text": "We provide additional details for linear probe experiments presented in this paper, including the list of the datasets and models used for evaluation."}
{"doc_id": "2103.00020", "para_id": 476, "text": "We use the 12 datasets from the well-studied evaluation suite introduced by (Kornblith et al., 2019) and add 15 additional datasets in order to assess the performance of models on a wider variety of distributions and tasks. These datasets include MNIST, the Facial Expression Recognition 2013 dataset (Goodfellow et al., 2015), STL-10 (Coates et al., 2011), EuroSAT (Helber et al., 2019), the NWPU- RESISC45 dataset (Cheng et al., 2017), the German Traf- ﬁc Sign Recognition Benchmark (GTSRB) dataset (Stal- lkamp et al., 2011), the KITTI dataset (Geiger et al., 2012), PatchCamelyon (Veeling et al., 2018), the UCF101 action recognition dataset (Soomro et al., 2012), Kinetics 700 (Car- reira et al., 2019), 2,500 random samples of the CLEVR dataset (Johnson et al., 2017), the Hateful Memes dataset (Kiela et al., 2020), and the ImageNet-1k dataset (Deng et al., 2012). For the two video datasets (UCF101 and Ki- netics700), we use the middle frame of each video clip as the input image. STL-10 and UCF101 have multiple pre- deﬁned train/validation/test splits, 10 and 3 respectively, and we report the average over all splits. Details on each dataset and the corresponding evaluation metrics are provided in Table 9."}
{"doc_id": "2103.00020", "para_id": 477, "text": "CLIP-RN Five ResNet-based contrastive CLIP models are included. As discussed in the paper, the ﬁrst two models follow ResNet-50 and ResNet-101, and we use EfﬁcientNet- style (Tan & Le, 2019) scaling for the next three models which simultaneously scale the model width, the number of layers, and the input resolution to obtain models with roughly 4x, 16x, and 64x computation."}
{"doc_id": "2103.00020", "para_id": 478, "text": "CLIP-ViT We include four CLIP models that use the Vi- sion Transformer (Dosovitskiy et al., 2020) architecture as the image encoder. We include three models trained on 224- by-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and the ViT-L/14 model ﬁne-tuned on 336-by-336 pixel input images."}
{"doc_id": "2103.00020", "para_id": 479, "text": "EfﬁcietNet We use the nine models (B0-B8) from the original EfﬁcientNet paper (Tan & Le, 2019), as well as the noisy-student variants (B0-B7, L2-475, and L2-800) (Tan & Le, 2019). The largest models (L2-475 and L2-800) take the input resolutions of 475x475 and 800x800 pixels, respectively."}
{"doc_id": "2103.00020", "para_id": 480, "text": "Additionally, we created two datasets that we call Coun- try211 and Rendered SST2. The Country211 dataset is designed to assess the geolocation capability of visual rep- resentations. We ﬁltered the YFCC100m dataset (Thomee et al., 2016) to ﬁnd 211 countries (deﬁned as having an ISO-3166 country code) that have at least 300 photos with GPS coordinates, and we built a balanced dataset with 211 categories, by sampling 200 photos for training and 100 photos for testing, for each country."}
{"doc_id": "2103.00020", "para_id": 481, "text": "Instagram-pretrained ResNeXt We use the four models (32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan et al., 2018), as well as their two FixRes variants which use higher input resolutions (Touvron et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 482, "text": "Big Transfer (BiT) We use BiT-S and BiT-M models (Kolesnikov et al., 2019), trained on the ImageNet-1k and ImageNet-21k datasets. The model weights for BiT-L is not publicly available."}
{"doc_id": "2103.00020", "para_id": 483, "text": "The Rendered SST2 dataset is designed to measure the opti- cal character recognition capability of visual representations. To do so, we used the sentences from the Stanford Sentiment Treebank dataset (Socher et al., 2013) and rendered them into images, with black texts on a white background, in a 448×448 resolution. Two example images from this dataset are shown in Figure 19."}
{"doc_id": "2103.00020", "para_id": 484, "text": "Vision Transformer (ViT) We also include four ViT (Dosovitskiy et al., 2020) checkpoints pretrained on the ImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT- L/16, and ViT-H/14. We note that their best-performing models, trained on the JFT-300M dataset, are not available publicly."}
{"doc_id": "2103.00020", "para_id": 485, "text": "SimCLRv2 The SimCLRv2 (Chen et al., 2020c) project released pre-trained and ﬁne-tuned models in various set- tings. We use the seven pretrain-only checkpoints with selective kernels."}
{"doc_id": "2103.00020", "para_id": 486, "text": "In combination with the datasets listed above, we evaluate the following series of models using linear probes."}
{"doc_id": "2103.00020", "para_id": 487, "text": "LM RN50 This is a multimodal model that uses an au- toregressive loss instead of a contrastive loss, while using"}
{"doc_id": "2103.00020", "para_id": 488, "text": "BYOL We use the recently released model weights of BYOL (Grill et al., 2020), speciﬁcally their 50x1 and 200x2"}
{"doc_id": "2103.00020", "para_id": 489, "text": "Figure 19. Two example images from the Rendered SST2 dataset"}
{"doc_id": "2103.00020", "para_id": 490, "text": "a test split, we use the provided validation set to perform the hyperparameter search, and for the datasets that do not provide a validation split or have not published labels for the test data, we split the training dataset to perform the hyperparameter search. For the ﬁnal result, we combine the validation split back with the training split and report the performance on the unused split."}
{"doc_id": "2103.00020", "para_id": 491, "text": "Momentum Contrast (MoCo) We include the MoCo-v1 (He et al., 2020) and the MoCo-v2 (Chen et al., 2020d) checkpoints."}
{"doc_id": "2103.00020", "para_id": 492, "text": "VirTex We use the pretrained model of VirTex (Desai & Johnson, 2020). We note that VirTex has a similar model design to CLIP-AR but is trained on a 1000x smaller dataset of high-quality captions from MSCOCO."}
{"doc_id": "2103.00020", "para_id": 493, "text": "The individual linear probe scores are provided in Table 10 and plotted in Figure 20. The best-performing CLIP model, using ViT-L/14 archiecture and 336-by-336 pixel images, achieved the state of the art in 21 of the 27 datasets, i.e. included in the Clopper-Pearson 99.5% conﬁdence interval around each dataset’s top score. For many datasets, CLIP performs signiﬁcantly better than other models, demonstrat- ing the advantage of natural language supervision over tradi- tional pre-training approaches based on image classiﬁcation. See Section 3.2 for more discussions on the linear probe results."}
{"doc_id": "2103.00020", "para_id": 494, "text": "ResNet We add the original ResNet checkpoints released by (He et al., 2016b), namely ResNet-50, ResNet-101, and ResNet152."}
{"doc_id": "2103.00020", "para_id": 495, "text": "We use image features taken from the penultimate layer of each model, ignoring any classiﬁcation layer provided. For CLIP-ViT models, we used the features before the linear projection to the embedding space, which corresponds to I f in Figure 3. We train a logistic regression classiﬁer using scikit-learn’s L-BFGS implementation, with maxi- mum 1,000 iterations, and report the corresponding met- ric for each dataset. We determine the L2 regularization strength λ using a hyperparameter sweep on the validation sets over the range between 10−6 and 106, with 96 log- arithmically spaced steps. To save compute required for the sweeps, we perform a parametric binary search that starts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it- eratively halves the interval around the peak until it reaches a resolution of 8 steps per decade. The hyperparameter sweeps are performed on a validation split of each dataset. For the datasets that contain a validation split in addition to"}
{"doc_id": "2103.00020", "para_id": 496, "text": "Dataset Classes Train size Test size Evaluation metric"}
{"doc_id": "2103.00020", "para_id": 497, "text": "Food-101 102 75,750 25,250 accuracy CIFAR-10 10 50,000 10,000 accuracy CIFAR-100 100 50,000 10,000 accuracy Birdsnap 500 42,283 2,149 accuracy SUN397 397 19,850 19,850 accuracy Stanford Cars 196 8,144 8,041 accuracy FGVC Aircraft 100 6,667 3,333 mean per class Pascal VOC 2007 Classiﬁcation 20 5,011 4,952 11-point mAP Describable Textures 47 3,760 1,880 accuracy Oxford-IIIT Pets 37 3,680 3,669 mean per class Caltech-101 102 3,060 6,085 mean-per-class Oxford Flowers 102 102 2,040 6,149 mean per class"}
{"doc_id": "2103.00020", "para_id": 498, "text": "MNIST 10 60,000 10,000 accuracy Facial Emotion Recognition 2013 8 32,140 3,574 accuracy STL-10 10 1000 8000 accuracy EuroSAT 10 10,000 5,000 accuracy RESISC45 45 3,150 25,200 accuracy GTSRB 43 26,640 12,630 accuracy KITTI 4 6,770 711 accuracy Country211 211 43,200 21,100 accuracy PatchCamelyon 2 294,912 32,768 accuracy UCF101 101 9,537 1,794 accuracy Kinetics700 700 494,801 31,669 mean(top1, top5) CLEVR Counts 8 2,000 500 accuracy Hateful Memes 2 8,500 500 ROC AUC Rendered SST2 2 7,792 1,821 accuracy ImageNet 1000 1,281,167 50,000 accuracy"}
{"doc_id": "2103.00020", "para_id": 499, "text": "Table 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are available online at the time of this writing."}
{"doc_id": "2103.00020", "para_id": 500, "text": "LM RN50 81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2"}
{"doc_id": "2103.00020", "para_id": 501, "text": "50 86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3 101 88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7 50x4 91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2 50x16 93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5 50x64 94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6"}
{"doc_id": "2103.00020", "para_id": 502, "text": "B/32 88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1 B/16 92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2 L/14 95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9 L/14-336px 95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4"}
{"doc_id": "2103.00020", "para_id": 503, "text": "B0 74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9 B1 74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6 B2 75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7 B3 77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0 B4 79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9 B5 81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7 B6 82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0 B7 84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8 B8 84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3"}
{"doc_id": "2103.00020", "para_id": 504, "text": "B0 78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5 B1 80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1 B2 80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2 B3 82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8 B4 85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4 B5 87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8 B6 87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4 B7 88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0 L2-475 91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3 L2-800 92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4"}
{"doc_id": "2103.00020", "para_id": 505, "text": "32x8d 84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3 32x16d 85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4 32x32d 86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0 32x48d 86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2 FixRes-v1 88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0 FixRes-v2 88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0"}
{"doc_id": "2103.00020", "para_id": 506, "text": "R50x1 72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2 R50x3 75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2 R101x1 73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0 R101x3 74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4 R152x2 74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1 R152x4 74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5"}
{"doc_id": "2103.00020", "para_id": 507, "text": "R50x1 83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7 R50x3 86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7 R101x1 85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4 R101x3 87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2 R152x2 88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9 R152x4 87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5"}
{"doc_id": "2103.00020", "para_id": 508, "text": "B/32 81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6 B/16 86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9 L/16 87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9 H/14 83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9"}
{"doc_id": "2103.00020", "para_id": 509, "text": "R50x1 76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8 R50x3 81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3 R101x1 77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1 R101x3 82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8 R152x1 78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9 R152x2 82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2 R152x3 83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0"}
{"doc_id": "2103.00020", "para_id": 510, "text": "50x1 74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3 200x2 78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2"}
{"doc_id": "2103.00020", "para_id": 511, "text": "v1 65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2 v2 72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1"}
{"doc_id": "2103.00020", "para_id": 512, "text": "VirTex 57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7"}
{"doc_id": "2103.00020", "para_id": 513, "text": "50 71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3 101 72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8 152 73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1"}
{"doc_id": "2103.00020", "para_id": 514, "text": "Table 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson conﬁdence interval of each dataset’s top score are shown in bold."}
{"doc_id": "2103.00020", "para_id": 515, "text": "⋆We updated the STL10 scores from the previous version of this paper after ﬁxing a CUDA-related bug."}
{"doc_id": "2103.00020", "para_id": 516, "text": "CLIP-ViT CLIP-ResNet EfficientNet-NoisyStudent EfficientNet Instagram-pretrained SimCLRv2 BYOL MoCo ViT (ImageNet-21k) BiT-M BiT-S ResNet"}
{"doc_id": "2103.00020", "para_id": 517, "text": "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10."}
{"doc_id": "2103.00020", "para_id": 518, "text": "a centered satellite photo of permanent crop land."}
{"doc_id": "2103.00020", "para_id": 519, "text": "a centered satellite photo of brushland or shrubland."}
{"doc_id": "2103.00020", "para_id": 520, "text": "a photo of a mcdonnell douglas md-90, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 521, "text": "a photo of a mcdonnell douglas dc-9-30, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 522, "text": "a photo of a broad tailed hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 523, "text": "a photo of a bishop of llandaff, a type of flower."}
{"doc_id": "2103.00020", "para_id": 524, "text": "a photo of a calliope hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 525, "text": "a photo of a black chinned hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 526, "text": "a photo of a prince of wales feathers, a type of flower."}
{"doc_id": "2103.00020", "para_id": 527, "text": "correct label: red and white triangle with exclamation mark warning"}
{"doc_id": "2103.00020", "para_id": 528, "text": "a zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 529, "text": "a zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 530, "text": "a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 531, "text": "a zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 532, "text": "a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 533, "text": "Figure 21. Visualization of predictions from 36 CLIP zero-shot classiﬁers. All examples are random with the exception of reselecting Hateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent the class. When more than one template is used, the ﬁrst template is shown. The ground truth label is colored green while an incorrect prediction is colored orange."}
{"doc_id": "2103.00020", "para_id": 534, "text": "RN50 81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6 RN101 83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2 RN50x4 86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8 RN50x16 90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5 RN50x64 91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6"}
{"doc_id": "2103.00020", "para_id": 535, "text": "B/32 84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2 B/16 89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6 L/14 92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3 L/14-336px 93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2"}
{"doc_id": "2103.00020", "para_id": 536, "text": "Table 11. Zero-shot performance of CLIP models over 27 datasets."}
{"doc_id": "2103.00020", "para_id": 537, "text": "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance"}
{"doc_id": "2103.00020", "para_id": 538, "text": "Linear Classiﬁer Zero Shot Dataset YFCC WIT ∆ YFCC WIT ∆"}
{"doc_id": "2103.00020", "para_id": 539, "text": "To provide a qualitative summary / overview of CLIP’s zero- shot performance we visualize a randomly selected predic- tion for 36 different zero-shot CLIP classiﬁers in Figure 21. In addition, Table 11 and Figure 22 show the individual zero-shot performance scores for each dataset."}
{"doc_id": "2103.00020", "para_id": 540, "text": "Birdsnap 47.4 35.3 +12.1 19.9 4.5 +15.4 Country211 23.1 17.3 +5.8 5.2 5.3 +0.1 Flowers102 94.4 89.8 +4.6 48.6 21.7 +26.9 GTSRB 66.8 72.5 −5.7 6.9 7.0 −0.1 UCF101 69.2 74.9 −5.7 22.9 32.0 −9.1 Stanford Cars 31.4 50.3 −18.9 3.8 10.9 −7.1"}
{"doc_id": "2103.00020", "para_id": 541, "text": "ImageNet 62.0 60.8 +1.2 31.3 27.6 +3.7 Dataset Average 65.5 66.6 −1.1 29.6 30.0 −0.4 Dataset “Wins” 10 15 −5 19 18 +1"}
{"doc_id": "2103.00020", "para_id": 542, "text": "Our early attempts at duplicate detection and analysis used nearest neighbors in the model’s learned embedding space. While it is intuitive to use a model’s own notion of similar- ity, we encountered issues. We found the model’s feature space is weighted very heavily towards semantic similar- ity. Many false positives occurred due to distinct objects that would be described similarly (soccer balls, ﬂowers of the same species, etc...) having almost perfect similarity. We also observed the model was quite poor at assigning certain kinds of near-duplicates high similarity scores. We noticed repeatedly that images with high-frequency textures (such as fur or stripe patterns) pre-processed by different resizing algorithms (nearest neighbor vs bi-linear) could have surprisingly low similarity. This resulted in many false negatives."}
{"doc_id": "2103.00020", "para_id": 543, "text": "Table 12. CLIP performs similarly when trained on only YFCC100M. Comparing a ResNet-50 trained on only YFCC100M with a same sized subset of WIT shows simi- lar average performance and number of wins on zero shot and linear classiﬁer evals. However, large differences in dataset speciﬁc performance occur. We include performance on the 3 datasets where YFCC does best and worst compared to WIT according to a linear probe in order to highlight this as well as aggregate performance across all linear and zero-shot evals and the canonical ImageNet dataset."}
{"doc_id": "2103.00020", "para_id": 544, "text": "To study whether our custom dataset is critical to the perfor- mance of CLIP, we trained a model on a ﬁltered subset of the YFCC100M dataset (details described in Section 2.2) and compared its performance to the same model trained on an equally sized subset of WIT. We train each model for 32 epochs at which point transfer performance begins to plateau due to overﬁtting. Results are shown in Table 12. Across our whole eval suite, YFCC and WIT perform simi- larly on average for both zero-shot and linear probe settings. However, performance on speciﬁc ﬁne-grained classiﬁca- tion datasets can vary widely - sometimes by over 10%. Our speculation is that these differences in performance re- ﬂect the relative density of relevant data in each pre-training dataset. For instance, pre-training on YFCC100M, which might contain many photos of birds and ﬂowers (common subjects for photographers), results in better performance on Birdsnap and Flowers102, while pre-training on WIT results in better car and pet classiﬁers (which appear common in our dataset)."}
{"doc_id": "2103.00020", "para_id": 545, "text": "We built our own near-duplicate detector to ﬁx this issue. We created a synthetic data augmentation pipeline that com- bined a variety of common image manipulations. The aug- mentation pipeline combines random cropping and zooming, aspect ratio distortion, downsizing and upscaling to different resolutions, minor rotations, jpeg compression, and HSV color jitter. The pipeline also randomly selects from differ- ent interpolation algorithms for all relevant steps. We then trained a model to maximize the similarity of an image and its transformed variant while minimizing similarity to all other images in a training batch. We used the same n-pair / InfoNCE loss as CLIP but with a ﬁxed temperature of 0.07."}
{"doc_id": "2103.00020", "para_id": 546, "text": "We selected a ResNet-50 as the model architecture. We modiﬁed the base ResNet-50 with the anti-alias improve- ments from (Zhang, 2019) and used weight norm (Sali- mans & Kingma, 2016) instead of batch norm (Ioffe & Szegedy, 2015) to avoid leaking information about dupli- cates via batch statistics - a problem previously noted in (Henaff, 2020). We also found the GELU activation func- tion (Hendrycks & Gimpel, 2016) to perform better for this task. We trained the model with a total batch size of 1,712 for approximately 30 million images sampled from our pre- training dataset. At the end of training it achieves nearly 100% accuracy on its proxy training task."}
{"doc_id": "2103.00020", "para_id": 547, "text": "Overall, these results are encouraging as they suggest our approach can use any reasonably ﬁltered collection of paired (text, image) data. This mirrors recent work which reported positive results using the same contrastive pre-training ob- jective on the relatively different domain of medical imaging (Zhang et al., 2020). It also is similar to the ﬁndings of noisy student self-training which reported only slight improve- ments when using their JFT300M dataset over YFCC100M (Xie et al., 2020). We suspect the major advantage of our dataset over the already existing YFCC100M is its much larger size."}
{"doc_id": "2103.00020", "para_id": 548, "text": "Finally, we caution that WIT includes this ﬁltered subset of YFCC100M. This could result in our ablation under- estimating the size of performance differences between YFCC100M and the rest of WIT. We do not think this is likely as YFCC100M is only 3.7% of the overall WIT data blend and it did not noticeably change the performance of models when it was added to the existing data blend during the creation of WIT."}
{"doc_id": "2103.00020", "para_id": 549, "text": "on 5 datasets requiring the direct and indirect use of OCR. Three of these datasets MNIST (LeCun), SVHN (Netzer et al., 2011), and IIIT5K (Mishra et al., 2012) directly check the ability of a model to perform low-level character and word recognition, while Hateful Memes (Kiela et al., 2020) and SST-2 (Socher et al., 2013) check the ability of a model to use OCR to perform a semantic task. Results are reported in Table 14."}
{"doc_id": "2103.00020", "para_id": 550, "text": "CLIP’s performance is still highly variable and appears to be sensitive to some combination of the domain (rendered or natural images) and the type of text to be recognized (num- bers or words). CLIP’s OCR performance is strongest Hate- ful Memes and SST-2 - datasets where the text is digitally rendered and consists mostly of words. On IIIT5K, which is natural images of individually cropped words, zero-shot CLIP performs a bit more respectively and its performance is similar to Jaderberg et al. (2014) early work combining deep learning and structured prediction to perform open- vocabulary OCR. However, performance is noticeably lower on two datasets involving recognition of hand written and street view numbers. CLIP’s 51% accuracy on full number SVHN is well below any published results. Inspection sug- gests CLIP struggles with repeated characters as well as the low resolution and blurry images of SVHN. CLIP’s zero- shot MNIST performance is also poor and is outperformed by supervised logistic regression on raw pixels, one of the simplest possible machine learning baselines."}
{"doc_id": "2103.00020", "para_id": 551, "text": "Due to the large variety of datasets and experiments consid- ered in this work, the main body focuses on summarizing and analyzing overall results. In the following subsections we report details of performance for speciﬁc groups of tasks, datasets, and evaluation settings."}
{"doc_id": "2103.00020", "para_id": 552, "text": "CLIP pre-trains for the task of image-text retrieval on our noisy web-scale dataset. Although the focus of this paper is on representation learning and task learning for the pur- pose of transfer to a wide variety of downstream datasets, validating that CLIP is able to achieve high transfer perfor- mance transfer on exactly what it is pre-trained for is an important sanity check / proof of concept. In Table 13 we check the zero-shot transfer performance of CLIP for both text and image retrieval on the Flickr30k and MSCOCO datsets. Zero-shot CLIP matches or outperforms all prior zero-shot results on these two datasets. Zero-shot CLIP is also competitive with the current overall SOTA for the task of text retrieval on Flickr30k. On image retrieval, CLIP’s performance relative to the overall state of the art is notice- ably lower. However, zero-shot CLIP is still competitive with a ﬁne-tuned Unicoder-VL. On the larger MS-COCO dataset ﬁne-tuning improves performance signiﬁcantly and zero-shot CLIP is not competitive with the most recent work. For both these datasets we prepend the prompt “a photo of” to the description of each image which we found boosts CLIP’s zero-shot R@1 performance between 1 and 2 points."}
{"doc_id": "2103.00020", "para_id": 553, "text": "SST-2 is a sentence level NLP dataset which we render into images. We include SST-2 in order to check whether CLIP is able to convert low level OCR capability into a higher level representation. Fitting a linear classiﬁer on CLIP’s rep- resentation of rendered sentences achives 80.5% accuracy. This is on par with the 80% accuracy of a continuous bag of words baseline using GloVe word vectors pre-trained on 840 billion tokens (Pennington et al., 2014). While this is a simple NLP baseline by today’s standard, and well below the 97.5% of the current SOTA, it is encouraging to see that CLIP is able to turn an image of rendered text into a non-trivial sentence level representation. Fully supervised CLIP is also surprisingly strong on Hateful Meme detec- tion, where CLIP is only 0.7 points behind the current single model SOTA and several points above the best baseline from the original paper. Similar to SST-2, these other results on Hateful Memes use the ground truth text which CLIP does not have access to. Finally, we note that zero-shot CLIP outperforms the best results using fully supervised linear probes across all other 56 models included in our evaluation suite. This suggests CLIP’s OCR capability is at least some- what unique compared to existing work on self-supervised and supervised representation learning."}
{"doc_id": "2103.00020", "para_id": 554, "text": "Although visualizations have shown that ImageNet models contain features that respond to the presence of text in an image (Zeiler & Fergus, 2014), these representations are not sufﬁciently ﬁne-grained to use for the task of optical character recognition (OCR). To compensate, models are augmented with the outputs of custom OCR engines and features to boost performance on tasks where this capability is required (Singh et al., 2019; Yang et al., 2020). Early dur- ing the development of CLIP, we noticed that CLIP began to learn primitive OCR capabilities which appeared to steadily improve over the course of the project. To evaluate this qualitatively noticed behavior, we measured performance"}
{"doc_id": "2103.00020", "para_id": 555, "text": "Text Retrieval Image Retrieval Flickr30k MSCOCO Flickr30k MSCOCO R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10"}
{"doc_id": "2103.00020", "para_id": 556, "text": "Unicoder-VLa 86.2 96.3 99.0 62.3 87.1 92.8 71.5 90.9 94.9 46.7 76.0 85.3 Uniterb 87.3 98.0 99.2 65.7 88.6 93.8 75.6 94.1 96.8 52.9 79.9 88.0 VILLAc 87.9 97.5 98.8 - - - 76.3 94.2 96.8 - - - Oscard - - - 73.5 92.2 96.0 - - - 57.5 82.8 89.8 ERNIE-ViLe 88.7 98.0 99.2 - - - 76.7 93.6 96.4 - - -"}
{"doc_id": "2103.00020", "para_id": 557, "text": "Visual N-Gramsf 15.4 35.7 45.1 8.7 23.1 33.3 8.8 21.2 29.9 5.0 14.5 21.9 ImageBERTg - - - 44.0 71.2 80.4 - - - 32.3 59.0 70.2 Unicoder-VLa 64.3 86.8 92.3 - - - 48.4 76.0 85.2 - - - Uniterb 83.6 95.7 97.7 - - - 68.7 89.2 93.9 - - - CLIP 88.0 98.7 99.4 58.4 81.5 88.1 68.7 90.6 95.2 37.8 62.4 72.2"}
{"doc_id": "2103.00020", "para_id": 558, "text": "Table 13. CLIP improves zero-shot retrieval and is competitive with the best ﬁne-tuned result on Flickr30k text retrieval. Bold indicates best overall performance while an underline indicates best in category performance (zero-shot or ﬁne-tuned). For all other models, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set. a(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f(Li et al., 2017) g(Qi et al., 2020)"}
{"doc_id": "2103.00020", "para_id": 559, "text": "R(2+1)D-BERTa 98.7 - - - NS ENet-L2b - 84.8 - - HT100M S3Dd 91.3 - - - Baseline I3De - 70.2 - -"}
{"doc_id": "2103.00020", "para_id": 560, "text": "Raw Pixels 92.5 - - - - ES Best 98.9h - - 58.6h 59.0i"}
{"doc_id": "2103.00020", "para_id": 561, "text": "MMV FACf 91.8 - - - NS ENet-L2c 89.4c 68.2c - - CLIP 92.0 73.0 - -"}
{"doc_id": "2103.00020", "para_id": 562, "text": "HT100M S3Dd - - 30.5 34.8 CLIP 80.3 69.6 40.7 44.8"}
{"doc_id": "2103.00020", "para_id": 563, "text": "Table 14. OCR performance on 5 datasets. All metrics are accuracy on the test set except for Hateful Memes which reports ROC AUC on the dev set. Single model SOTA reported to best of knowledge. ES Best reports the best performance across the 56 non-CLIP models in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al., 2015) c(Wang et al., 2020) d(Lippe et al., 2020) f(Jaderberg et al., 2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al., 2018)"}
{"doc_id": "2103.00020", "para_id": 564, "text": "Table 15. Action recognition performance on 3 video datasets. Sin- gle model SOTA reported to best of knowledge. Note that linear CLIP and linear NS ENet-L2 are trained and evaluated on a single frame subsampled version of each dataset and not directly compa- rable to prior work. On Kinetics-700, we report the ActivityNet competition metric which is the average of top-1 and top-5 per- formance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al., 2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f(Alayrac et al., 2020)"}
{"doc_id": "2103.00020", "para_id": 565, "text": "For the purpose of learning, a potentially important aspect of natural language is its ability to express, and therefore su- pervise, an extremely wide set of concepts. A CLIP model, since it is trained to pair semi-arbitrary text with images, is likely to receive supervision for a wide range of visual con- cepts involving both common and proper nouns, verbs, and adjectives. ImageNet-1K, by contrast, only labels common nouns. Does the lack of broader supervision in ImageNet result in weaker transfer of ImageNet models to tasks involv- ing the recognition of visual concepts that are not nouns?"}
{"doc_id": "2103.00020", "para_id": 566, "text": "action classiﬁcation datasets which measure the ability of a model to recognize verbs. In Table 15 we report results on UCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira et al., 2019), two common datasets for the task. Unfortu- nately, our CPU based linear classiﬁer takes a prohibitively long time to evaluate on a video dataset due to the very large number of training frames. To deal with this, we aggres- sively sub-sample each video to only a single center frame, effectively turning it into an image classiﬁcation dataset. As a result, our reported performance in a linear evaluation setting likely under estimates performance by a moderate amount."}
{"doc_id": "2103.00020", "para_id": 567, "text": "To investigate this, we measure and compare the perfor- mance of CLIP and ImageNet models on several video"}
{"doc_id": "2103.00020", "para_id": 568, "text": "IN IN-V2 IN-A IN-R ObjectNet IN-Sketch IN-Vid YTBB Top-1 Top-1 Top-1 Top-1 Top-1 Top-1 PM0 PM10 PM0 PM10"}
{"doc_id": "2103.00020", "para_id": 569, "text": "NS EfﬁcientNet-L2a 88.3 80.2 84.9 74.7 68.5 47.6 88.0 82.1 67.7 63.5 FixResNeXt101-32x48d V2b 86.4 78.0 68.4 80.0 57.8 59.1 85.8 72.2 68.9 57.7 Linear Probe CLIP 85.4 75.9 75.3 84.2 66.2 57.4 89.1 77.2 68.7 63.1 Zero-Shot CLIP 76.2 70.1 77.2 88.9 72.3 60.2 95.3 89.2 95.2 88.5"}
{"doc_id": "2103.00020", "para_id": 570, "text": "Table 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)"}
{"doc_id": "2103.00020", "para_id": 571, "text": "Despite this handicap, CLIP features transfer surprisingly well to this task. CLIP matches the best prior result on UCF- 101 in a linear probe evaluation setting and also outperforms all other models in our evaluation suite. On Kinetics-700, CLIP also outperforms the ﬁne-tuned I3D baseline from the original paper. Since it does not require a training stage, we report CLIP’s zero-shot performance when averaging predictions across all frames. CLIP also performs well in this setting and on Kinetics-700 its performance is within 1% of the fully supervised I3D baseline which is trained on 545000 labeled videos. Encouraged by these results, we also measure CLIP’s performance on the recently introduced RareAct dataset (Miech et al., 2020a) which was designed to measure zero-shot recognition of unusual actions like “hammering a phone” and “drilling an egg”. CLIP improves over the prior state of the art, a S3D model trained on auto- matically extracted captions from 100 million instructional videos, by 10 points."}
{"doc_id": "2103.00020", "para_id": 572, "text": "Another behavior we noticed during the development of CLIP was its ability to recognize many places and locations. To quantify this we created the Country211 dataset as de- scribed in Appendix A and report results on it throughout the paper. However it is a new benchmark so to compare with prior work on geolocalization we also report results on the IM2GPS test set from Hays & Efros (2008) in Table 17. Since IM2GPS is a regression benchmark, we guess the GPS coordinates of the nearest image in a set of reference images using CLIP’s embedding space. This is not a zero- shot result since it uses nearest-neighbor regression. Despite querying only 1 million images, which is much less than prior work, CLIP performs similarly to several task speciﬁc models. It is not, however, competitive with the current state of the art."}
{"doc_id": "2103.00020", "para_id": 573, "text": "While CLIP has encouragingly strong performance on the task of action recognition, we note that there are many differ- ences between the models being compared beyond just their form of supervision such as model architecture, training data distribution, dataset size, and compute used. Further work is needed to more precisely determine what speciﬁc design decisions contribute to achieving high performance on this task."}
{"doc_id": "2103.00020", "para_id": 574, "text": "Section 3.3 provides a high level summary and analysis of ImageNet-related robustness results. We brieﬂy provide some additional numerical details in this appendix. Per- formance results per dataset are provided in Table 16 and compared with the current state of the art results reported in Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im- proves the state of the art on 5 of the 7 datasets, ImageNet-R, ObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube- BB. CLIP’s improvements are largest on ImageNet-Vid and Youtube-BB due to its ﬂexible zero-shot capability and on ImageNet-R, which likely reﬂects CLIP’s pre-training dis- tribution including signiﬁcant amounts of creative content. A similar behavior has been documented for the Instagram pre-trained ResNeXt models as discussed in Taori et al. (2020)."}
{"doc_id": "2103.00020", "para_id": 575, "text": "ISNsa 16.9 43.0 51.9 66.7 80.2 CPlaNetb 16.5 37.1 46.4 62.0 78.5 CLIP 13.9 32.9 43.0 62.0 79.3 Deep-Ret+c 14.4 33.3 47.7 61.6 73.4 PlaNetd 8.4 24.5 37.6 53.6 71.3"}
{"doc_id": "2103.00020", "para_id": 576, "text": "Table 17. Geolocalization performance on the IM2GPS test set. Metric is percent of images localized within a given radius. Models are ordered by average performance. a(Muller-Budack et al., 2018) b(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al., 2016)"}
{"doc_id": "2103.00020", "para_id": 577, "text": "Batch size 32768 Vocabulary size 49408 Training epochs 32 Maximum temperature 100.0 Weight decay 0.2 Warm-up iterations 2000 Adam β1 0.9 Adam β2 0.999 (ResNet), 0.98 (ViT) Adam ϵ 10−8 (ResNet), 10−6 (ViT)"}
{"doc_id": "2103.00020", "para_id": 578, "text": "Learning Embedding Input ResNet Text Transformer Model rate dimension resolution blocks width layers width heads"}
{"doc_id": "2103.00020", "para_id": 579, "text": "RN50 5 × 10−4 1024 224 (3, 4, 6, 3) 2048 12 512 8 RN101 5 × 10−4 512 224 (3, 4, 23, 3) 2048 12 512 8 RN50x4 5 × 10−4 640 288 (4, 6, 10, 6) 2560 12 640 10 RN50x16 4 × 10−4 768 384 (6, 8, 18, 8) 3072 12 768 12 RN50x64 3.6 × 10−4 1024 448 (3, 15, 36, 10) 4096 12 1024 16"}
{"doc_id": "2103.00020", "para_id": 580, "text": "Learning Embedding Input Vision Transformer Text Transformer Model rate dimension resolution layers width heads layers width heads"}
{"doc_id": "2103.00020", "para_id": 581, "text": "ViT-B/32 5 × 10−4 512 224 12 768 12 12 512 8 ViT-B/16 5 × 10−4 512 224 12 768 12 12 512 8 ViT-L/14 4 × 10−4 768 224 24 1024 16 12 768 12 ViT-L/14-336px 2 × 10−5 768 336 24 1024 16 12 768 12"}
{"doc_id": "1910.10683", "para_id": 0, "text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}
{"doc_id": "1910.10683", "para_id": 1, "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1"}
{"doc_id": "1910.10683", "para_id": 2, "text": "Keywords: transfer learning, natural language processing, multi-task learning, attention- based models, deep learning"}
{"doc_id": "1910.10683", "para_id": 3, "text": "Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to “understand” text. This knowledge can range from low-level (e.g. the spelling"}
{"doc_id": "1910.10683", "para_id": 4, "text": "∗. Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence to craffel@gmail.com. 1. https://github.com/google-research/text-to-text-transfer-transformer"}
{"doc_id": "1910.10683", "para_id": 5, "text": "or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b). Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks. In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project2 produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a). This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field. The basic idea underlying our work is to treat every text processing problem as a “text-to-text” problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document"}
{"doc_id": "1910.10683", "para_id": 6, "text": "\"summarize: state authorities dispatched emergency crews tuesday to"}
{"doc_id": "1910.10683", "para_id": 7, "text": "Figure 1: A diagram of our text-to-text framework. Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”."}
{"doc_id": "1910.10683", "para_id": 8, "text": "summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered."}
{"doc_id": "1910.10683", "para_id": 9, "text": "We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1"}
{"doc_id": "1910.10683", "para_id": 10, "text": "The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4."}
{"doc_id": "1910.10683", "para_id": 11, "text": "Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the “Text-to-Text Transfer Transformer” (T5)."}
{"doc_id": "1910.10683", "para_id": 12, "text": "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the “Transformer” architecture (Vaswani et al., 2017). The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originally- proposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output. Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention"}
{"doc_id": "1910.10683", "para_id": 13, "text": "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/"}
{"doc_id": "1910.10683", "para_id": 14, "text": "mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal self- attention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed. Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism. We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work. As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014)."}
{"doc_id": "1910.10683", "para_id": 15, "text": "Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common"}
{"doc_id": "1910.10683", "para_id": 16, "text": "Crawl has previously been used as a source of text data for NLP, for example to train an n-gram language model (Buck et al., 2014), as training data for commonsense reasoning (Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013), as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and even simply as a giant text corpus for testing optimizers (Anil et al., 2019). Common Crawl is a publicly-available web archive that provides “web extracted text” by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl’s web extracted text:"}
{"doc_id": "1910.10683", "para_id": 17, "text": "• We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark)."}
{"doc_id": "1910.10683", "para_id": 18, "text": "• We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words."}
{"doc_id": "1910.10683", "para_id": 19, "text": "• We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.6"}
{"doc_id": "1910.10683", "para_id": 20, "text": "• Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript."}
{"doc_id": "1910.10683", "para_id": 21, "text": "• Some pages had placeholder “lorem ipsum” text; we removed any page where the phrase “lorem ipsum” appeared."}
{"doc_id": "1910.10683", "para_id": 22, "text": "• Some pages inadvertently contained code. Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket."}
{"doc_id": "1910.10683", "para_id": 23, "text": "• Since some of the scraped pages were sourced from Wikipedia and had citation markers (e.g. [1], [citation needed], etc.), we removed any such markers."}
{"doc_id": "1910.10683", "para_id": 24, "text": "• Many pages had boilerplate policy notices, so we removed any lines containing the strings “terms of use”, “privacy policy”, “cookie policy”, “uses cookies”, “use of cookies”, or “use cookies”."}
{"doc_id": "1910.10683", "para_id": 25, "text": "• To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set."}
{"doc_id": "1910.10683", "para_id": 26, "text": "Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect7 to filter out any pages that were not classified as English with a probability of at least 0.99. Our heuristics are inspired by past work on using Common"}
{"doc_id": "1910.10683", "para_id": 27, "text": "6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words 7. https://pypi.org/project/langdetect/"}
{"doc_id": "1910.10683", "para_id": 28, "text": "Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication. However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training data for machine translation (Smith et al., 2013)). To assemble our base data set, we downloaded the web extracted text from April 2019 and applied the aforementioned filtering. This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8"}
{"doc_id": "1910.10683", "para_id": 29, "text": "We consider the impact of using various alternative versions of this data set in Section 3.4."}
{"doc_id": "1910.10683", "para_id": 30, "text": "Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. All data was sourced from TensorFlow Datasets.9"}
{"doc_id": "1910.10683", "para_id": 31, "text": "GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a collection of text classification tasks meant to test general language understanding abilities:"}
{"doc_id": "1910.10683", "para_id": 32, "text": "• Sentence acceptability judgment (CoLA (Warstadt et al., 2018))"}
{"doc_id": "1910.10683", "para_id": 33, "text": "• Sentiment analysis (SST-2 (Socher et al., 2013))"}
{"doc_id": "1910.10683", "para_id": 34, "text": "• Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP (Iyer et al., 2017))"}
{"doc_id": "1910.10683", "para_id": 35, "text": "• Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,"}
{"doc_id": "1910.10683", "para_id": 36, "text": "2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 37, "text": "• Coreference resolution (WNLI and WSC (Levesque et al., 2012))"}
{"doc_id": "1910.10683", "para_id": 38, "text": "• Sentence completion (COPA (Roemmele et al., 2011))"}
{"doc_id": "1910.10683", "para_id": 39, "text": "• Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))"}
{"doc_id": "1910.10683", "para_id": 40, "text": "• Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 41, "text": "8. https://www.tensorflow.org/datasets/catalog/c4 9. https://www.tensorflow.org/datasets"}
{"doc_id": "1910.10683", "para_id": 42, "text": "We use the data sets as distributed by the GLUE and SuperGLUE benchmarks. For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent data sets. As suggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question- answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as (Vaswani et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set (Bojar et al., 2014). For English to French, we use the standard training data from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language."}
{"doc_id": "1910.10683", "para_id": 43, "text": "In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a “text-to-text” format—that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” (Williams and Zipser, 1989)) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model."}
{"doc_id": "1910.10683", "para_id": 44, "text": "As an example, to ask the model to translate the sentence “That is good.” from English to German, the model would be fed the sequence “translate English to German: That is good.” and would be trained to output “Das ist gut.” For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies (“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With our preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.” with the corresponding target word “entailment”. Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”, or “contradiction”). In this case, we always count the model’s output as wrong, though we never observed this behavior in any of our trained models. Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. A diagram of our text-to-text framework with a few input/output"}
{"doc_id": "1910.10683", "para_id": 45, "text": "examples is shown in Figure 1. We provide full examples of preprocessed inputs for every task we studied in Appendix D. Our text-to-text framework follows previous work that casts multiple NLP tasks into a common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a benchmark that uses a consistent question-answering format for a suite of ten NLP tasks. The Natural Language Decathlon also stipulates that all models must be multi-task, i.e. are able to simultaneously tackle all of the tasks at once. We instead allow for separately fine-tuning the model on each individual task and use short task prefixes instead of an explicit question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of language models by feeding some input to the model as a prefix and then autoregressively sampling an output. For example, automatic summarization is done by feeding in a document followed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar et al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. In contrast, our framework also allows for generative tasks like machine translation and abstractive summarization where it is not possible to enumerate all possible output choices. We were able to straightforwardly cast all of the tasks we considered into a text-to-text format with the exception of STS-B, which is a regression task where the goal is to predict a similarity score between 1 and 5. We found that most of these scores were annotated in increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and converted the result to a literal string representation of the number (e.g. the floating-point value 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a string corresponding to a number between 1 and 5, we convert it to a floating-point value; otherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem. Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from Super- GLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. For example, the passage might be “The city councilmen refused the demonstrators a permit because they feared violence.”, which contains the ambiguous pronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI, WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in the text passage and asking the model to predict the noun that it refers to. The example mentioned above would be transformed to the input “The city councilmen refused the demonstrators a permit because *they* feared violence.” and the model would be trained to predict the target text “The city councilmen”. For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun, and a True/False label reflecting whether the candidate matches the pronoun (ignoring any articles). We only train on examples with a “True” label since we do not know the correct noun targets for examples with a “False” label. For evaluation, we assign a “True” label if"}
{"doc_id": "1910.10683", "para_id": 46, "text": "the words in the model’s output are a subset of the words in the candidate noun phrase (or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this data set in the format listed above."}
{"doc_id": "1910.10683", "para_id": 47, "text": "The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of Section 3.5.2), we therefore never train on WNLI and never report results on the WNLI validation set. Omitting results on the WNLI validation set is standard practice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the training set, i.e. validation examples are all slightly-perturbed versions of training examples with the opposite label. As such, we do not include WNLI in the average GLUE score whenever we report on the validation set (all sections except Section 3.7 where results are presented on the test sets). Converting examples from WNLI to the “referent noun prediction” variant described above is a little more involved; we describe this process in Appendix B."}
{"doc_id": "1910.10683", "para_id": 48, "text": "Recent advances in transfer learning for NLP have come from a wide variety of developments, such as new pre-training objectives, model architectures, unlabeled data sets, and more. In this section, we carry out an empirical survey of these techniques in hopes of teasing apart their contribution and significance. We then combine the insights gained to attain state-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly growing area of research, it is not feasible for us to cover every possible technique or idea in our empirical study. For a broader literature review, we recommend a recent survey by Ruder et al. (2019)."}
{"doc_id": "1910.10683", "para_id": 49, "text": "We systematically study these contributions by taking a reasonable baseline (described in Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3 we measure the performance of different unsupervised objectives while keeping the rest of our experimental pipeline fixed. This “coordinate ascent” approach might miss second-order effects (for example, some particular unsupervised objective may work best on a model larger than our baseline setting), but performing a combinatorial exploration of all of the factors in our study would be prohibitively expensive. In future work, we expect it could be fruitful to more thoroughly consider combinations of the approaches we study."}
{"doc_id": "1910.10683", "para_id": 50, "text": "Our goal is to compare a variety of different approaches on a diverse set of tasks while keeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do not exactly replicate existing approaches. For example, “encoder-only” models like BERT (Devlin et al., 2018) are designed to produce a single prediction per input token or a single prediction for an entire input sequence. This makes them applicable for classification or span prediction tasks but not for generative tasks like translation or abstractive summarization. As such, none of the model architectures we consider are identical to BERT or consist of an encoder-only structure. Instead, we test approaches that are similar in spirit—for example, we consider an analogous objective to BERT’s “masked language modeling” objective in"}
{"doc_id": "1910.10683", "para_id": 51, "text": "Section 3.3 and we consider a model architecture that behaves similarly to BERT on text classification tasks in Section 3.2. After outlining our baseline experimental setup in the following subsection, we undertake an empirical comparison of model architectures (Section 3.2), unsupervised objectives (Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and scaling (Section 3.6). At the culmination of this section, we combine insights from our study with scale to obtain state-of-the-art results in many tasks we consider (Section 3.7)."}
{"doc_id": "1910.10683", "para_id": 52, "text": "Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard Transformer (described in Section 2.1) using a simple denoising objective and then separately fine-tune on each of our downstream tasks. We describe the details of this experimental setup in the following subsections."}
{"doc_id": "1910.10683", "para_id": 53, "text": "For our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017). While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single “stack” (e.g. for language modeling (Radford et al., 2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al., 2019)), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. We explore the performance of different model architectures in Section 3.2. Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of dff = 3072 followed by a ReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total, this results in a model with about 220 million parameters. This is roughly twice the number of parameters of BERTBASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied in the model."}
{"doc_id": "1910.10683", "para_id": 54, "text": "As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to always train using standard maximum likelihood, i.e. using teacher forcing (Williams and Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and Stern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability logit at every timestep). We pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,"}
{"doc_id": "1910.10683", "para_id": 55, "text": "we “pack” multiple sequences into each entry of the batch10 so that our batches contain roughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds to pre-training on 235 ≈34B tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training. During pre-training, we use an “inverse square root” learning rate schedule: 1 \u000ep"}
{"doc_id": "1910.10683", "para_id": 56, "text": "max(n, k) where n is the current training iteration and k is the number of warm-up steps (set to 104"}
{"doc_id": "1910.10683", "para_id": 57, "text": "in all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps, then exponentially decays the learning rate until pre-training is over. We also experimented with using a triangular learning rate (Howard and Ruder, 2018), which produced slightly better results but requires knowing the total number of training steps ahead of time. Since we will be varying the number of training steps in some of our experiments, we opt for the more generic inverse square root schedule. Our models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e. those with large data sets), which benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e. 216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save a checkpoint every 5,000 steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, we choose the best checkpoint for each task independently. For all of the experiments except those in Section 3.7, we report results in the validation set to avoid performing model selection on the test set."}
{"doc_id": "1910.10683", "para_id": 58, "text": "We use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens (Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000 wordpieces. Since we ultimately fine-tune our model on English to German, French, and Romanian translation, we also require that our vocabulary covers these non-English languages. To address this, we classified pages from the Common Crawl scrape used in C4 as German, French, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of our model. Note that our vocabulary makes it so that our model can only process a predetermined, fixed set of languages."}
{"doc_id": "1910.10683", "para_id": 59, "text": "Leveraging unlabeled data to pre-train our model necessitates an objective that does not require labels but (loosely speaking) teaches the model generalizable knowledge that will be"}
{"doc_id": "1910.10683", "para_id": 60, "text": "10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/ index.html#data_generators.generator_utils.pack_examples"}
{"doc_id": "1910.10683", "para_id": 61, "text": "Figure 2: Schematic of the objective we use in our baseline model. In this example, we process the sentence “Thank you for inviting me to your party last week.” The words “for”, “inviting” and “last” (marked with an ×) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as <X> and <Y>) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel <X>. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token <Z>."}
{"doc_id": "1910.10683", "para_id": 62, "text": "useful in downstream tasks. Preliminary work that applied the transfer learning paradigm of pre-training and fine-tuning all of the model’s parameters to NLP problems used a causal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that “denoising” objectives (Devlin et al., 2018; Taylor, 1953) (also called “masked language modeling”) produce better performance and as a result they have quickly become standard. In a denoising objective, the model is trained to predict missing or otherwise corrupted tokens in the input. Inspired by BERT’s “masked language modeling” objective and the “word dropout” regularization technique (Bowman et al., 2015), we design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens which are added to our vocabulary and do not correspond to any wordpiece. The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. Our choices to mask consecutive spans of tokens and only predict dropped-out tokens were made to reduce the computational cost of pre-training. We perform thorough investigation into pre-training objectives in Section 3.3. An example of the transformation resulting from applying this objective is shown in Figure 2. We empirically compare this objective to many other variants in Section 3.3."}
{"doc_id": "1910.10683", "para_id": 63, "text": "In this section, we present results using the baseline experimental procedure described above to get a sense of what kind of performance to expect on our suite of downstream tasks. Ideally, we would repeat every experiment in our study multiple times to get a confidence interval on our results. Unfortunately, this would be prohibitively expensive due to the large"}
{"doc_id": "1910.10683", "para_id": 64, "text": "⋆Baseline average 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Baseline standard deviation 0.235 0.065 0.343 0.416 0.112 0.090 0.108 No pre-training 66.22 17.60 50.31 53.04 25.86 39.77 24.04"}
{"doc_id": "1910.10683", "para_id": 65, "text": "Table 1: Average and standard deviation of scores achieved by our baseline model and training procedure. For comparison, we also report performance when training on each task from scratch (i.e. without any pre-training) for the same number of steps used to fine-tune the baseline model. All scores in this table (and every table in our paper except Table 14) are reported on the validation sets of each data set."}
{"doc_id": "1910.10683", "para_id": 66, "text": "number of experiments we run. As a cheaper alternative, we train our baseline model 10 times from scratch (i.e. with different random initializations and data set shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don’t expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes. Separately, we also measure the performance of training our model for 218"}
{"doc_id": "1910.10683", "para_id": 67, "text": "steps (the same number we use for fine-tuning) on all downstream tasks without pre-training. This gives us an idea of how much pre-training benefits our model in the baseline setting. When reporting results in the main text, we only report a subset of the scores across all the benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we report the average score across all subtasks (as stipulated by the official benchmarks) under the headings “GLUE” and “SGLUE”. For all translation tasks, we report the BLEU score (Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with “exp” smoothing and “intl” tokenization. We refer to scores for WMT English to German, English to French, and English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under the heading “CNNDM”. Similarly, for SQuAD we find the performance of the “exact match” and “F1” scores to be highly correlated so we report the “exact match” score alone. We provide every score achieved on every task for all experiments in Table 16, Appendix E. Our results tables are all formatted so that each row corresponds to a particular experi- mental configuration with columns giving the scores for each benchmark. We will include the mean performance of the baseline configuration in most tables. Wherever a baseline configuration appears, we will mark it with a ⋆(as in the first row of Table 1). We also will boldface any score that is within two standard deviations of the maximum (best) in a given experiment. Our baseline results are shown in Table 1. Overall, our results are comparable to existing models of similar size. For example, BERTBASE achieved an exact match score of 80.8 on SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and 84.24, respectively (see Table 16). Note that we cannot directly compare our baseline to BERTBASE because ours is an encoder-decoder model and was pre-trained for roughly 1⁄4 as many steps. Unsurprisingly, we find that pre-training provides significant gains across almost all benchmarks. The only exception is WMT English to French, which is a large"}
{"doc_id": "1910.10683", "para_id": 68, "text": "enough data set that gains from pre-training tend to be marginal. We include this task in our experiments to test the behavior of transfer learning in the high-resource regime. Since we perform early stopping by selecting the best-performing checkpoint, the large disparity between our baseline and “no pre-training” emphasize how much pre-training improves performance on tasks with limited data. While we do not explicitly measure improvements in data efficiency in this paper, we emphasize that this is one of the primary benefits of the transfer learning paradigm. As for inter-run variance, we find that for most tasks the standard deviation across runs is smaller than 1% of the task’s baseline score. Exceptions to this rule include CoLA, CB, and COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks. For example, on CB our baseline model had an average F1 score of 91.22 with a standard deviation of 3.237 (see Table 16), which may be partly due to the fact that CB’s validation set contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed as the average of scores across the tasks comprising each benchmark. As a result, we caution that the high inter-run variance of CoLA, CB, and COPA can make it harder to compare models using the GLUE and SuperGLUE scores alone."}
{"doc_id": "1910.10683", "para_id": 69, "text": "While the Transformer was originally introduced with an encoder-decoder architecture, much modern work on transfer learning for NLP uses alternative architectures. In this section, we review and compare these architectural variants."}
{"doc_id": "1910.10683", "para_id": 70, "text": "A major distinguishing factor for different architectures is the “mask” used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let yi refer to the ith element of the output sequence and xj refer to the jth entry of the input sequence. yi is computed as P j wi,jxj, where wi,j is the scalar weight produced by the self-attention mechanism as a function of xi and xj. The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any wi,j to zero if j > i. The first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure 4. The encoder uses a “fully-visible” attention mask. Fully-visible masking allows a self- attention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a “prefix”, i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern and appends a special “classification” token to the input. BERT’s output"}
{"doc_id": "1910.10683", "para_id": 71, "text": "Figure 3: Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from “the future”. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence."}
{"doc_id": "1910.10683", "para_id": 72, "text": "at the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence. The self-attention operations in the Transformer’s decoder use a “causal” masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can’t “see into the future” as it produces its output. An attention matrix for this masking pattern is shown in Figure 3, middle. The decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model’s predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction (Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second model structure we consider. A schematic of this architecture is shown in Figure 4, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method (Radford et al., 2018). Language models are typically used for compression or sequence generation (Graves, 2013). However, they can also be used in the text-to-text framework simply by concatenating the inputs and targets. As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to"}
{"doc_id": "1910.10683", "para_id": 73, "text": "Figure 4: Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use “.” to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as x and y respectively. Left: A standard encoder-decoder architecture uses fully- visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input."}
{"doc_id": "1910.10683", "para_id": 74, "text": "obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good. target:” and would be asked to generate the remainder of the sequence autoregressively. In this way, the model can predict an output sequence given an input, which satisfies the needs of text-to-text tasks. This approach was recently used to show that language models can learn to perform some text-to-text tasks without supervision (Radford et al., 2019)."}
{"doc_id": "1910.10683", "para_id": 75, "text": "A fundamental and frequently cited drawback of using a language model in the text- to-text setting is that causal masking forces the model’s representation of the ith entry of the input sequence to only depend on the entries up until i. To see why this is potentially disadvantageous, consider the text-to-text framework where the model is provided with a prefix/context before being asked to make predictions (e.g., the prefix is an English sentence and the model is asked to predict the German translation). With fully causal masking, the model’s representation of a prefix state can only depend on prior entries of the prefix. So, when predicting an entry of the output, the model will attend to a representation of the prefix that is unnecessarily limited. Similar arguments have been made against using a unidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau et al., 2015)."}
{"doc_id": "1910.10683", "para_id": 76, "text": "This issue can be avoided in a Transformer-based language model simply by changing the masking pattern. Instead of using a causal mask, we use fully-visible masking during the prefix portion of the sequence. This masking pattern and a schematic of the resulting “prefix LM” (the third model structure we consider) are illustrated in the rightmost panels of Figures 3 and 4, respectively. In the English to German translation example mentioned above, fully-visible masking would be applied to the prefix “translate English to German: That is good. target:” and causal masking would be used during training for predicting the target “Das ist gut.” Using a prefix LM in the text-to-text framework was originally proposed by"}
{"doc_id": "1910.10683", "para_id": 77, "text": "Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder model with parameters shared across the encoder and decoder and with the encoder-decoder attention replaced with full attention across the input and target sequence."}
{"doc_id": "1910.10683", "para_id": 78, "text": "We note that when following our text-to-text framework, the prefix LM architecture closely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an example from the MNLI benchmark where the premise is “I hate pigeons.”, the hypothesis is “My feelings towards pigeons are filled with animosity.” and the correct label is “entailment”. To feed this example into a language model, we would transform it into the sequence “mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: entailment”. In this case, the fully-visible prefix would correspond to the entire input sequence up to the word “target:”, which can be seen as being analogous to the “classification” token used in BERT. So, our model would have full visibility over the entire input, and then would be tasked with making a classification by outputting the word “entailment”. It is easy for the model to learn to output one of the valid class labels given the task prefix (“mnli” in this case). As such, the main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM."}
{"doc_id": "1910.10683", "para_id": 79, "text": "In the interest of experimentally comparing these architectural variants, we would like each model we consider to be equivalent in some meaningful way. We might say that two models are equivalent if they either have the same number of parameters or they require roughly the same amount of computation to process a given (input-sequence, target-sequence) pair. Unfortunately, it is not possible to compare an encoder-decoder model to a language model architecture (comprising a single Transformer stack) according to both of these criteria at the same time. To see why, first note an encoder-decoder model with L layers in the encoder and L layers in the decoder has approximately the same number of parameters as a language model with 2L layers. However, the same L + L encoder-decoder model will have approximately the same computational cost as a language model with only L layers. This is a consequence of the fact that the L layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence and the decoder is only applied to the output sequence. Note that these equivalences are approximate—there are some extra parameters in the decoder due to the encoder-decoder attention and there are also some computational costs in the attention layers that are quadratic in the sequence lengths. In practice, however, we observed nearly identical step"}
{"doc_id": "1910.10683", "para_id": 80, "text": "times for L-layer language models versus L + L-layer encoder-decoder models, suggesting a roughly equivalent computational cost. Further, for the model sizes we consider, the number of parameters in the encoder-decoder attention layers is about 10% of the total parameter count, so we make the simplifying assumption that an L + L-layer encoder-decoder model has the same number of parameters as an 2L-layer language model. To provide a reasonable means of comparison, we consider multiple configurations for our encoder-decoder model. We will refer to the number of layers and parameters in a BERTBASE-sized layer stack as L and P, respectively. We will use M to refer to the number of FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model to process a given input-target pair. In total, we will compare:"}
{"doc_id": "1910.10683", "para_id": 81, "text": "• An encoder-decoder model with L layers in the encoder and L layers in the decoder. This model has 2P parameters and a computation cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 82, "text": "• An equivalent model, but with parameters shared across the encoder and decoder, resulting in P parameters and an M-FLOP computational cost."}
{"doc_id": "1910.10683", "para_id": 83, "text": "• An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P parameters and an M/2-FLOP cost."}
{"doc_id": "1910.10683", "para_id": 84, "text": "• A decoder-only language model with L layers and P parameters and a resulting computational cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 85, "text": "• A decoder-only prefix LM with the same architecture (and thus the same number of parameters and computational cost), but with fully-visible self-attention over the input."}
{"doc_id": "1910.10683", "para_id": 86, "text": "As an unsupervised objective, we will consider both a basic language modeling objective as well as our baseline denoising objective described in Section 3.1.4. We include the language modeling objective due to its historic use as a pre-training objective (Dai and Le, 2015; Ramachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018) as well as its natural fit for the language model architectures we consider. For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions. For the standard language model, we train the model to predict the entire span from beginning to end. Our unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model we concatenate the inputs and targets as described in Section 3.2.1."}
{"doc_id": "1910.10683", "para_id": 87, "text": "The scores achieved by each of the architectures we compare are shown in Table 2. For all tasks, the encoder-decoder architecture with the denoising objective performed best. This variant has the highest parameter count (2P) but the same computational cost as the P-parameter decoder-only models. Surprisingly, we found that sharing parameters across the encoder and decoder performed nearly as well. In contrast, halving the number of layers in"}
{"doc_id": "1910.10683", "para_id": 88, "text": "Architecture Objective Params Cost GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 89, "text": "⋆Encoder-decoder Denoising 2P M 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Enc-dec, shared Denoising P M 82.81 18.78 80.63 70.73 26.72 39.03 27.46 Enc-dec, 6 layers Denoising P M/2 80.88 18.97 77.59 68.42 26.38 38.40 26.95 Language model Denoising P M 74.70 17.93 61.14 55.02 25.09 35.28 25.86 Prefix LM Denoising P M 81.82 18.61 78.94 68.11 26.43 37.98 27.39"}
{"doc_id": "1910.10683", "para_id": 90, "text": "Encoder-decoder LM 2P M 79.56 18.59 76.02 64.29 26.27 39.17 26.86 Enc-dec, shared LM P M 79.60 18.13 76.35 63.50 26.62 39.17 27.05 Enc-dec, 6 layers LM P M/2 78.67 18.26 75.32 64.06 26.13 38.42 26.89 Language model LM P M 73.78 17.54 53.81 56.51 25.23 34.31 25.38 Prefix LM LM P M 79.68 17.84 76.87 64.86 26.28 37.51 26.76"}
{"doc_id": "1910.10683", "para_id": 91, "text": "Table 2: Performance of the different architectural variants described in Section 3.2.2. We use P to refer to the number of parameters in a 12-layer base Transformer layer stack and M to refer to the FLOPs required to process a sequence using the encoder- decoder model. We evaluate each architectural variant using a denoising objective (described in Section 3.1.4) and an autoregressive objective (as is commonly used to train language models)."}
{"doc_id": "1910.10683", "para_id": 92, "text": "the encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al., 2019) also found that sharing parameters across Transformer blocks can be an effective means of lowering the total parameter count without sacrificing much performance. XLNet also bears some resemblance to the shared encoder-decoder approach with a denoising objective (Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms the decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder attention is beneficial. Finally, we confirm the widely-held conception that using a denoising objective always results in better downstream task performance compared to a language modeling objective. This observation has been previously made by Devlin et al. (2018), Voita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more detailed exploration of unsupervised objectives in the following section."}
{"doc_id": "1910.10683", "para_id": 93, "text": "The choice of unsupervised objective is of central importance as it provides the mechanism through which the model gains general-purpose knowledge to apply to downstream tasks. This has led to the development of a wide variety of pre-training objectives (Dai and Le, 2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019). In this section, we perform a procedural exploration of the space of unsupervised objectives. In many cases, we will not replicate an existing objective exactly—some will be modified to fit our text-to-text encoder-decoder framework and, in other cases, we will use objectives that combine concepts from multiple common approaches. Overall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual"}
{"doc_id": "1910.10683", "para_id": 94, "text": "Prefix language modeling Thank you for inviting me to your party last week . BERT-style Devlin et al. (2018) Thank you <M> <M> me to your party apple week . (original text) Deshuffling party me for your to . last fun you inviting week Thank (original text) MASS-style Song et al. (2019) Thank you <M> <M> me to your party <M> week . (original text) I.i.d. noise, replace spans Thank you <X> me to your party <Y> week . <X> for inviting <Y> last <Z> I.i.d. noise, drop tokens Thank you me to your party week . for inviting last Random spans Thank you <X> to <Y> week . <X> for inviting me <Y> your party last <Z>"}
{"doc_id": "1910.10683", "para_id": 95, "text": "Table 3: Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text “Thank you for inviting me to your party last week .” Note that all of our objectives process tokenized text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write (original text) as a target to denote that the model is tasked with reconstructing the entire input text. <M> denotes a shared mask token and <X>, <Y>, and <Z> denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word apple."}
{"doc_id": "1910.10683", "para_id": 96, "text": "with maximum likelihood to predict the target sequence. We provide illustrative examples of many of the objectives we consider in Table 3."}
{"doc_id": "1910.10683", "para_id": 97, "text": "To begin with, we compare three techniques that are inspired by commonly-used objectives but differ significantly in their approach. First, we include a basic “prefix language modeling” objective as was used in Section 3.2.3. This technique splits a span of text into two components, one to use as inputs to the encoder and the other to use as a target sequence to be predicted by the decoder. Second, we consider an objective inspired by the “masked language modeling” (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a span of text and corrupts 15% of the tokens. 90% of the corrupted tokens are replaced with a special mask token and 10% are replaced with a random token. Since BERT is an encoder-only model, its goal during pre-training is to reconstruct masked tokens at the output of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted sequence as the target. Note that this differs from our baseline objective, which uses only the corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally, we also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was applied to a denoising sequential autoencoder. This approach takes a sequence of tokens, shuffles it, and then uses the original deshuffled sequence as a target. We provide examples of the inputs and targets for these three methods in the first three rows of Table 3. The performance of these three objectives is shown in Table 4. Overall, we find that the BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks. Indeed, the motivation for the BERT objective was to outperform language model-based pre-training. The deshuffling objective performs considerably worse than both prefix language modeling and the BERT-style objective."}
{"doc_id": "1910.10683", "para_id": 98, "text": "Prefix language modeling 80.69 18.94 77.99 65.27 26.86 39.73 27.49 BERT-style (Devlin et al., 2018) 82.96 19.17 80.65 69.85 26.78 40.03 27.41 Deshuffling 73.17 18.59 67.61 58.47 26.11 39.30 25.62"}
{"doc_id": "1910.10683", "para_id": 99, "text": "Table 4: Performance of the three disparate pre-training objectives described in Section 3.3.1."}
{"doc_id": "1910.10683", "para_id": 100, "text": "BERT-style (Devlin et al., 2018) 82.96 19.17 80.65 69.85 26.78 40.03 27.41 MASS-style (Song et al., 2019) 82.32 19.16 80.10 69.28 26.79 39.89 27.55 ⋆Replace corrupted spans 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Drop corrupted tokens 84.44 19.31 80.52 68.67 27.07 39.76 27.82"}
{"doc_id": "1910.10683", "para_id": 101, "text": "Table 5: Comparison of variants of the BERT-style pre-training objective. In the first two variants, the model is trained to reconstruct the original uncorrupted text segment. In the latter two, the model only predicts the sequence of corrupted tokens."}
{"doc_id": "1910.10683", "para_id": 102, "text": "Based on the results in the prior section, we will now focus on exploring modifications to the BERT-style denoising objective. This objective was originally proposed as a pre-training technique for an encoder-only model trained for classification and span prediction. As such, it may be possible to modify it so that it performs better or is more efficient in our encoder-decoder text-to-text setup. First, we consider a simple variant of the BERT-style objective where we don’t include the random token swapping step. The resulting objective simply replaces 15% of the tokens in the input with a mask token and the model is trained to reconstruct the original uncorrupted sequence. A similar masking objective was used by Song et al. (2019) where it was referred to as “MASS”, so we call this variant the “MASS-style” objective. Second, we were interested to see if it was possible to avoid predicting the entire uncorrupted text span since this requires self-attention over long sequences in the decoder. We consider two strategies to achieve this: First, instead of replacing each corrupted token with a mask token, we replace the entirety of each consecutive span of corrupted tokens with a unique mask token. Then, the target sequence becomes the concatenation of the “corrupted” spans, each prefixed by the mask token used to replace it in the input. This is the pre-training objective we use in our baseline, described in Section 3.1.4. Second, we also consider a variant where we simply drop the corrupted tokens from the input sequence completely and task the model with reconstructing the dropped tokens in order. Examples of these approaches are shown in the fifth and sixth rows of Table 3. An empirical comparison of the original BERT-style objective to these three alternatives is shown in Table 5. We find that in our setting, all of these variants perform similarly. The only exception was that dropping corrupted tokens completely produced a small improvement in the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our"}
{"doc_id": "1910.10683", "para_id": 103, "text": "Corruption rate GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 104, "text": "10% 82.82 19.00 80.38 69.55 26.87 39.28 27.44 ⋆15% 83.28 19.24 80.88 71.36 26.98 39.82 27.65 25% 83.00 19.54 80.96 70.48 27.04 39.83 27.47 50% 81.27 19.32 79.80 70.33 27.01 39.90 27.49"}
{"doc_id": "1910.10683", "para_id": 105, "text": "Table 6: Performance of the i.i.d. corruption objective with different corruption rates."}
{"doc_id": "1910.10683", "para_id": 106, "text": "baseline average of 53.84, see Table 16). This may be due to the fact that CoLA involves classifying whether a given sentence is grammatically and syntactically acceptable, and being able to determine when tokens are missing is closely related to detecting acceptability. However, dropping tokens completely performed worse than replacing them with sentinel tokens on SuperGLUE. The two variants that do not require predicting the full original sequence (“replace corrupted spans” and “drop corrupted spans”) are both potentially attractive since they make the target sequences shorter and consequently make training faster. Going forward, we will explore variants where we replace corrupted spans with sentinel tokens and only predict the corrupted tokens (as in our baseline objective)."}
{"doc_id": "1910.10683", "para_id": 107, "text": "So far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al., 2018). Again, since our text-to-text framework differs from BERT’s, we are interested to see if a different corruption rate works better for us. We compare corruption rates of 10%, 15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited effect on the model’s performance. The only exception is that the largest corruption rate we consider (50%) results in a significant degradation of performance on GLUE and SQuAD. Using a larger corruption rate also results in longer targets, which can potentially slow down training. Based on these results and the historical precedent set by BERT, we will use a corruption rate of 15% going forward."}
{"doc_id": "1910.10683", "para_id": 108, "text": "We now turn towards the goal of speeding up training by predicting shorter targets. The approach we have used so far makes an i.i.d. decision for each input token as to whether to corrupt it or not. When multiple consecutive tokens have been corrupted, they are treated as a “span” and a single unique mask token is used to replace the entire span. Replacing entire spans with a single token results in unlabeled text data being processed into shorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case that a significant number of corrupted tokens appear consecutively. As a result, we might obtain additional speedup by specifically corrupting spans of tokens rather than corrupting individual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a pre-training objective for BERT, where it was found to improve performance (Joshi et al., 2019). To test this idea, we consider an objective that specifically corrupts contiguous, randomly- spaced spans of tokens. This objective can be parametrized by the proportion of tokens to be corrupted and the total number of corrupted spans. The span lengths are then chosen"}
{"doc_id": "1910.10683", "para_id": 109, "text": "⋆Baseline (i.i.d.) 83.28 19.24 80.88 71.36 26.98 39.82 27.65 2 83.54 19.39 82.09 72.20 26.76 39.99 27.63 3 83.49 19.62 81.84 72.53 26.86 39.65 27.62 5 83.40 19.24 82.05 72.23 26.88 39.40 27.53 10 82.85 19.33 81.84 70.44 26.79 39.49 27.69"}
{"doc_id": "1910.10683", "para_id": 110, "text": "Table 7: Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for different average span lengths. In all cases, we corrupt 15% of the original text sequence."}
{"doc_id": "1910.10683", "para_id": 111, "text": "randomly to satisfy these specified parameters. For example, if we are processing a sequence of 500 tokens and we have specified that 15% of tokens should be corrupted and that there should be 25 total spans, then the total number of corrupted tokens would be 500×0.15 = 75 and the average span length would be 75/25 = 3. Note that given the original sequence length and corruption rate, we can equivalently parametrize this objective either by the average span length or the total number of spans. We compare the span-corruption objective to the i.i.d-corruption objective in Table 7. We use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3, 5 and 10. Again, we find a limited difference between these objectives, though the version with an average span length of 10 slightly underperforms the other values in some cases. We also find in particular that using an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the span-corruption objective also provides some speedup during training compared to the i.i.d. noise approach because span corruption produces shorter sequences on average."}
{"doc_id": "1910.10683", "para_id": 112, "text": "Figure 5 shows a flow chart of the choices made during our exploration of unsupervised objectives. Overall, the most significant difference in performance we observed was that denoising objectives outperformed language modeling and deshuffling for pre-training. We did not observe a remarkable difference across the many variants of the denoising objectives we explored. However, different objectives (or parameterizations of objectives) can lead to different sequence lengths and thus different training speeds. This implies that choosing among the denoising objectives we considered here should mainly be done according to their computational cost. Our results also suggest that additional exploration of objectives similar to the ones we consider here may not lead to significant gains for the tasks and model we consider. Instead, it may be fortuitous to explore entirely different ways of leveraging unlabeled data."}
{"doc_id": "1910.10683", "para_id": 113, "text": "Like the unsupervised objective, the pre-training data set itself is a crucial component of the transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training data sets are usually not treated as significant contributions on their own and are often not"}
{"doc_id": "1910.10683", "para_id": 114, "text": "Figure 5: A flow chart of our exploration of unsupervised objectives. We first consider a few disparate approaches in Section 3.3.1 and find that a BERT-style denoising objective performs best. Then, we consider various methods for simplifying the BERT objective so that it produces shorter target sequences in Section 3.3.2. Given that replacing dropped-out spans with sentinel tokens performs well and results in short target sequences, in Section 3.3.3 we experiment with different corruption rates. Finally, we evaluate an objective that intentionally corrupts contiguous spans of tokens in Section 3.3.4."}
{"doc_id": "1910.10683", "para_id": 115, "text": "released alongside pre-trained models and code. Instead, they are typically introduced in the course of presenting a new method or model. As a result, there has been relatively little comparison of different pre-training data sets as well as a lack of a “standard” data set used for pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c; Yang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced) data set to using a smaller preexisting data set (often Wikipedia). To probe more deeply into the impact of the pre-training data set on performance, in this section we compare variants of our C4 data set and other potential sources of pre-training data. We release all of the C4 data set variants we consider as part of TensorFlow Datasets.11"}
{"doc_id": "1910.10683", "para_id": 116, "text": "In creating C4, we developed various heuristics to filter the web-extracted text from Common Crawl (see Section 2.2 for a description). We are interested in measuring whether this filtering results in improved performance on downstream tasks, in addition to comparing it to other filtering approaches and common pre-training data sets. Towards this end, we compare the performance of our baseline model after pre-training on the following data sets:"}
{"doc_id": "1910.10683", "para_id": 117, "text": "C4 As a baseline, we first consider pre-training on our proposed unlabeled data set as described in Section 2.2."}
{"doc_id": "1910.10683", "para_id": 118, "text": "Unfiltered C4 To measure the effect of the heuristic filtering we used in creating C4 (deduplication, removing bad words, only retaining sentences, etc.), we also generate an alternate version of C4 that forgoes this filtering. Note that we still use langdetect"}
{"doc_id": "1910.10683", "para_id": 119, "text": "11. https://www.tensorflow.org/datasets/catalog/c4"}
{"doc_id": "1910.10683", "para_id": 120, "text": "to extract English text. As a result, our “unfiltered” variant still includes some filtering because langdetect sometimes assigns a low probability to non-natural English text."}
{"doc_id": "1910.10683", "para_id": 121, "text": "RealNews-like Recent work has used text data extracted from news websites (Zellers et al., 2019; Baevski et al., 2019). To compare to this approach, we generate another unlabeled data set by additionally filtering C4 to only include content from one of the domains used in the “RealNews” data set (Zellers et al., 2019). Note that for ease of comparison, we retain the heuristic filtering methods used in C4; the only difference is that we have ostensibly omitted any non-news content."}
{"doc_id": "1910.10683", "para_id": 122, "text": "WebText-like Similarly, the WebText data set (Radford et al., 2019) only uses content from webpages that were submitted to the content aggregation website Reddit and received a “score” of at least 3. The score for a webpage submitted to Reddit is computed based on the proportion of users who endorse (upvote) or oppose (downvote) the webpage. The idea behind using the Reddit score as a quality signal is that users of the site would only upvote high-quality text content. To generate a comparable data set, we first tried removing all content from C4 that did not originate from a URL that appeared in the list prepared by the OpenWebText effort.12 However, this resulted in comparatively little content—only about 2 GB—because most pages never appear on Reddit. Recall that C4 was created based on a single month of Common Crawl data. To avoid using a prohibitively small data set, we therefore downloaded 12 months of data from Common Crawl from August 2018 to July 2019, applied our heuristic filtering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like data set, which is of comparable size to the original 40GB WebText data set (Radford et al., 2019)."}
{"doc_id": "1910.10683", "para_id": 123, "text": "Wikipedia The website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from TensorFlow Datasets,13 which omits any markup or reference sections from the articles."}
{"doc_id": "1910.10683", "para_id": 124, "text": "Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia is that it represents only one possible domain of natural text (encyclopedia articles). To mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the Toronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from eBooks, which represents a different domain of natural language. BERT’s popularity has led to the Wikipedia + TBC combination being used in many subsequent works."}
{"doc_id": "1910.10683", "para_id": 125, "text": "The results achieved after pre-training on each of these data sets is shown in Table 8. A first obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades performance and makes the unfiltered variant perform the worst in every task. Beyond this, we found that in some cases a pre-training data set with a more constrained domain outperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus"}
{"doc_id": "1910.10683", "para_id": 126, "text": "12. https://github.com/jcpeterson/openwebtext 13. https://www.tensorflow.org/datasets/catalog/wikipedia"}
{"doc_id": "1910.10683", "para_id": 127, "text": "Data set Size GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 128, "text": "⋆C4 745GB 83.28 19.24 80.88 71.36 26.98 39.82 27.65 C4, unfiltered 6.1TB 81.46 19.14 78.78 68.04 26.55 39.34 27.21 RealNews-like 35GB 83.83 19.23 80.39 72.38 26.75 39.90 27.48 WebText-like 17GB 84.03 19.31 81.42 71.40 26.80 39.74 27.59 Wikipedia 16GB 81.85 19.31 81.29 68.01 26.94 39.69 27.67 Wikipedia + TBC 20GB 83.65 19.28 82.08 73.24 26.77 39.63 27.57"}
{"doc_id": "1910.10683", "para_id": 129, "text": "Table 8: Performance resulting from pre-training on different data sets. The first four variants are based on our new C4 data set."}
{"doc_id": "1910.10683", "para_id": 130, "text": "produced a SuperGLUE score of 73.24, beating our baseline’s score (using C4) of 71.36. This is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to 50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC is a reading comprehension data set whose largest source of data comes from fiction books, which is exactly the domain covered by TBC. Similarly, using the RealNews-like data set for pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for ReCoRD, a data set that measures reading comprehension on news articles. As a final example, using data from Wikipedia produced significant (but less dramatic) gains on SQuAD, which is a question-answering data set with passages sourced from Wikipedia. Similar observations have been made in prior work, e.g. Beltagy et al. (2019) found that pre-training BERT on text from research papers improved its performance on scientific tasks. The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks. This is unsurprising but also unsatisfying if our goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded improvements on downstream tasks. This observation also motivates the parallel line of research on domain adaptation for natural language processing; for surveys of this field see e.g. Ruder (2019); Li (2012). A drawback to only pre-training on a single domain is that the resulting data sets are often substantially smaller. Similarly, while the WebText-like variant performed as well or better than the C4 data set in our baseline setting, the Reddit-based filtering produced a data set that was about 40× smaller than C4 despite being based on 12× more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on 235 ≈34B tokens, which is only about 8 times larger than the smallest pre-training data set we consider. We investigate at what point using a smaller pre-training data sets poses an issue in the following section."}
{"doc_id": "1910.10683", "para_id": 131, "text": "The pipeline we use to create C4 was designed to be able to create extremely large pre- training data sets. The access to so much data allows us to pre-train our models without repeating examples. It is not clear whether repeating examples during pre-training would be helpful or harmful to downstream performance because our pre-training objective is itself stochastic and can help prevent the model from seeing the same exact data multiple times."}
{"doc_id": "1910.10683", "para_id": 132, "text": "Number of tokens Repeats GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 133, "text": "⋆Full data set 0 83.28 19.24 80.88 71.36 26.98 39.82 27.65 229 64 82.87 19.19 80.97 72.03 26.83 39.74 27.63 227 256 82.62 19.20 79.78 69.97 27.02 39.71 27.33 225 1,024 79.55 18.57 76.27 64.76 26.38 39.56 26.80 223 4,096 76.34 18.33 70.92 59.29 26.37 38.84 25.81"}
{"doc_id": "1910.10683", "para_id": 134, "text": "Table 9: Measuring the effect of repeating data during pre-training. In these experiments, we only use the first N tokens from C4 (with varying values of N shown in the first column) but still pre-train over 235 tokens. This results in the data set being repeated over the course of pre-training (with the number of repeats for each experiment shown in the second column), which may result in memorization (see Figure 6)."}
{"doc_id": "1910.10683", "para_id": 135, "text": "To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model on artificially truncated versions of C4. Recall that we pre-train our baseline model on 235 ≈34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 229, 227, 225 and 223 tokens. These sizes correspond to repeating the data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training."}
{"doc_id": "1910.10683", "para_id": 136, "text": "The resulting downstream performance is shown in Table 9. As expected, performance degrades as the data set size shrinks. We suspect this may be due to the fact that the model begins to memorize the pre-training data set. To measure if this is true, we plot the training loss for each of these data set sizes in Figure 6. Indeed, the model attains significantly smaller training losses as the size of the pre-training data set shrinks, suggesting possible memorization. Baevski et al. (2019) similarly observed that truncating the pre-training data set size can degrade downstream task performance."}
{"doc_id": "1910.10683", "para_id": 137, "text": "We note that these effects are limited when the pre-training data set is repeated only 64 times. This suggests that some amount of repetition of pre-training data might not be harmful. However, given that additional pre-training can be beneficial (as we will show in Section 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest using large pre-training data sets whenever possible. We also note that this effect may be more pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting to a smaller pre-training data set."}
{"doc_id": "1910.10683", "para_id": 138, "text": "So far we have considered the setting where all parameters of a model are pre-trained on an unsupervised task before being fine-tuned on individual supervised tasks. While this approach is straightforward, various alternative methods for training the model on down- stream/supervised tasks have been proposed. In this section, we compare different schemes for fine-tuning the model in addition to the approach of training the model simultaneously on multiple tasks."}
{"doc_id": "1910.10683", "para_id": 139, "text": "Figure 6: Pre-training loss for our original C4 data set as well as 4 artificially truncated versions. The sizes listed refer to the number of tokens in each data set. The four sizes considered correspond to repeating the data set between 64 and 4,096 times over the course of pre-training. Using a smaller data set size results in smaller training loss values, which may suggest some memorization of the unlabeled data set."}
{"doc_id": "1910.10683", "para_id": 140, "text": "It has been argued that fine-tuning all of the model’s parameters can lead to suboptimal results, particularly on low-resource tasks (Peters et al., 2019). Early results on transfer learning for text classification tasks advocated fine-tuning only the parameters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model (Subra- manian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau et al., 2017). This approach is less applicable to our encoder-decoder model because the entire decoder must be trained to output the target sequences for a given task. Instead, we focus on two alternative fine-tuning approaches that update only a subset of the parameters of our encoder-decoder model. The first, “adapter layers” (Houlsby et al., 2019; Bapna et al., 2019), is motivated by the goal of keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer. These new feed-forward networks are designed so that their output dimensionality matches their input. This allows them to be inserted into the network with no additional changes to the structure or parameters. When fine- tuning, only the adapter layer and layer normalization parameters are updated. The main hyperparameter of this approach is the inner dimensionality d of the feed-forward network, which changes the number of new parameters added to the model. We experiment with various values for d. The second alternative fine-tuning method we consider is “gradual unfreezing” (Howard and Ruder, 2018). In gradual unfreezing, more and more of the model’s parameters are fine- tuned over time. Gradual unfreezing was originally applied to a language model architecture consisting of a single stack of layers. In this setting, at the start of fine-tuning only the"}
{"doc_id": "1910.10683", "para_id": 141, "text": "Fine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 142, "text": "⋆All parameters 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Adapter layers, d = 32 80.52 15.08 79.32 60.40 13.84 17.88 15.54 Adapter layers, d = 128 81.51 16.62 79.47 63.03 19.83 27.50 22.63 Adapter layers, d = 512 81.54 17.78 79.18 64.30 23.45 33.98 25.81 Adapter layers, d = 2048 81.51 16.62 79.47 63.03 19.83 27.50 22.63 Gradual unfreezing 82.50 18.95 79.17 70.79 26.71 39.02 26.93"}
{"doc_id": "1910.10683", "para_id": 143, "text": "Table 10: Comparison of different alternative fine-tuning methods that only update a subset of the model’s parameters. For adapter layers, d refers to the inner dimensionality of the adapters."}
{"doc_id": "1910.10683", "para_id": 144, "text": "parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network’s parameters are being fine-tuned. To adapt this approach to our encoder-decoder model, we gradually unfreeze layers in the encoder and decoder in parallel, starting from the top in both cases. Since the parameters of our input embedding matrix and output classification matrix are shared, we update them throughout fine-tuning. Recall that our baseline model consists of 12 layers each in the encoder and decoder and is fine-tuned for 218 steps. As such, we subdivide the fine-tuning process into 12 episodes of 218/12 steps each and train from layers 12 −n to 12 in the nth episode. We note that Howard and Ruder (2018) suggested fine-tuning an additional layer after each epoch of training. However, since our supervised data sets vary so much in size and since some of our downstream tasks are actually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler strategy of fine-tuning an additional layer after every 218/12 steps. A comparison of the performance of these fine-tuning approaches is shown in Table 10. For adapter layers, we report the performance using an inner dimensionality d of 32, 128, 512, 2048. Pursuant with past results (Houlsby et al., 2019; Bapna et al., 2019) we find that lower-resource tasks like SQuAD work well with a small value of d whereas higher resource tasks require a large dimensionality to achieve reasonable performance. This suggests that adapter layers could be a promising technique for fine-tuning on fewer parameters as long as the dimensionality is scaled appropriately to the task size. Note that in our case we treat GLUE and SuperGLUE each as a single “task” by concatenating their constituent data sets, so although they comprise some low-resource data sets the combined data set is large enough that it necessitates a large value of d. We found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning. Better results may be attainable by more carefully tuning the unfreezing schedule."}
{"doc_id": "1910.10683", "para_id": 145, "text": "So far, we have been pre-training our model on a single unsupervised learning task before fine-tuning it individually on each downstream task. An alternative approach, called “multi- task learning” (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a time. This approach typically has the goal of training a single model that can simultaneously"}
{"doc_id": "1910.10683", "para_id": 146, "text": "perform many tasks at once, i.e. the model and most of its parameters are shared across all tasks. We relax this goal somewhat and instead investigate methods for training on multiple tasks at once in order to eventually produce separate parameter settings that perform well on each individual task. For example, we might train a single model on many tasks, but when reporting performance we are allowed to select a different checkpoint for each task. This loosens the multi-task learning framework and puts it on more even footing compared to the pre-train-then-fine-tune approach we have considered so far. We also note that in our unified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets together. It follows that we can still train on unlabeled data when using multi-task learning by treating the unsupervised task as one of the tasks being mixed together. In contrast, most applications of multi-task learning to NLP add task-specific classification networks or use different loss functions for each task (Liu et al., 2019b). As pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task learning is how much data from each task the model should be trained on. Our goal is to not under- or over-train the model—that is, we want the model to see enough data from a given task that it can perform the task well, but not to see so much data that it memorizes the training set. How exactly to set the proportion of data coming from each task can depend on various factors including data set sizes, the “difficulty” of learning the task (i.e. how much data the model must see before being able to perform the task effectively), regularization, etc. An additional issue is the potential for “task interference” or “negative transfer”, where achieving good performance on one task can hinder performance on another. Given these concerns, we begin by exploring various strategies for setting the proportion of data coming from each task. A similar exploration was performed by Wang et al. (2019a)."}
{"doc_id": "1910.10683", "para_id": 147, "text": "Examples-proportional mixing A major factor in how quickly a model will overfit to a given task is the task’s data set size. As such, a natural way to set the mixing proportions is to sample in proportion to the size of each task’s data set. This is equivalent to concatenating the data sets for all tasks and randomly sampling examples from the combined data set. Note, however, that we are including our unsupervised denoising task, which uses a data set that is orders of magnitude larger than every other task’s. It follows that if we simply sample in proportion to each data set’s size, the vast majority of the data the model sees will be unlabeled, and it will undertrain on all of the supervised tasks. Even without the unsupervised task, some tasks (e.g. WMT English to French) are so large that they would similarly crowd out most of the batches. To get around this issue, we set an artificial “limit” on the data set sizes before computing the proportions. Specifically, if the number of examples in each of our N task’s data sets is en, n ∈{1, . . . , N} then we set probability of sampling an example from the mth task during training to rm = min(em, K)/ P min(en, K) where K is the artificial data set size limit."}
{"doc_id": "1910.10683", "para_id": 148, "text": "Temperature-scaled mixing An alternative way of mitigating the huge disparity between data set sizes is to adjust the “temperature” of the mixing rates. This approach was used by multilingual BERT to ensure that the model was sufficiently trained on low- resource languages.14 To implement temperature scaling with temperature T, we raise"}
{"doc_id": "1910.10683", "para_id": 149, "text": "14. https://github.com/google-research/bert/blob/master/multilingual.md"}
{"doc_id": "1910.10683", "para_id": 150, "text": "each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing. We retain the data set size limit K (applied to obtain rm before temperature scaling) but set it to a large value of K = 221. We use a large value of K because increasing the temperature will decrease the mixing rate of the largest data sets."}
{"doc_id": "1910.10683", "para_id": 151, "text": "Equal mixing In this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the data sets we train on. This is most likely a suboptimal strategy, as the model will overfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly include it as a point of reference of what might go wrong when the proportions are set suboptimally."}
{"doc_id": "1910.10683", "para_id": 152, "text": "To compare these mixing strategies on equal footing with our baseline pre-train-then- fine-tune results, we train multi-task models for the same total number of steps: 219 + 218 = 786,432. The results are shown in Table 11. In general, we find that multi-task training underperforms pre-training followed by fine-tuning on most tasks. The “equal” mixing strategy in particular results in dramatically degraded performance, which may be because the low-resource tasks have overfit, the high- resource tasks have not seen enough data, or the model has not seen enough unlabeled data to learn general-purpose language capabilities. For examples-proportional mixing, we find that for most tasks there is a “sweet spot” for K where the model obtains the best performance, and larger or smaller values of K tend to result in worse performance. The exception (for the range of K values we considered) was WMT English to French translation, which is such a high-resource task that it always benefits from a higher mixing proportion. Finally, we note that temperature-scaled mixing also provides a means of obtaining reasonable performance from most tasks, with T = 2 performing the best in most cases. The finding that a multi-task model is outperformed by separate models trained on each individual task has previously been observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has been shown that the multi-task setup can confer benefits across very similar tasks Liu et al. (2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap between multi-task training and the pre-train-then-fine-tune approach."}
{"doc_id": "1910.10683", "para_id": 153, "text": "3.5.3 Combining Multi-Task Learning with Fine-Tuning"}
{"doc_id": "1910.10683", "para_id": 154, "text": "Recall that we are studying a relaxed version of multi-task learning where we train a single model on a mixture of tasks but are allowed to evaluate performance using different parameter settings (checkpoints) for the model. We can extend this approach by considering the case where the model is pre-trained on all tasks at once but is then fine-tuned on the individual supervised tasks. This is the method used by the “MT-DNN” (Liu et al., 2015, 2019b), which achieved state-of-the-art performance on GLUE and other benchmarks when it was introduced. We consider three variants of this approach: In the first, we simply pre-train the model on an examples-proportional mixture with an artificial data set size limit of K = 219"}
{"doc_id": "1910.10683", "para_id": 155, "text": "before fine-tuning it on each individual downstream task. This helps us measure whether including the supervised tasks alongside the unsupervised objective during pre-training"}
{"doc_id": "1910.10683", "para_id": 156, "text": "Mixing strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 157, "text": "⋆Baseline (pre-train/fine-tune) 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Equal 76.13 19.02 76.51 63.37 23.89 34.31 26.78 Examples-proportional, K = 216 80.45 19.04 77.25 69.95 24.35 34.99 27.10 Examples-proportional, K = 217 81.56 19.12 77.00 67.91 24.36 35.00 27.25 Examples-proportional, K = 218 81.67 19.07 78.17 67.94 24.57 35.19 27.39 Examples-proportional, K = 219 81.42 19.24 79.78 67.30 25.21 36.30 27.76 Examples-proportional, K = 220 80.80 19.24 80.36 67.38 25.66 36.93 27.68 Examples-proportional, K = 221 79.83 18.79 79.50 65.10 25.82 37.22 27.13 Temperature-scaled, T = 2 81.90 19.28 79.42 69.92 25.42 36.72 27.20 Temperature-scaled, T = 4 80.56 19.22 77.99 69.54 25.04 35.82 27.45 Temperature-scaled, T = 8 77.21 19.10 77.14 66.07 24.55 35.35 27.17"}
{"doc_id": "1910.10683", "para_id": 158, "text": "Table 11: Comparison of multi-task training using different mixing strategies. Examples- proportional mixing refers to sampling examples from each data set according to the total size of each data set, with an artificial limit (K) on the maximum data set size. Temperature-scaled mixing re-scales the sampling rates by a temperature T. For temperature-scaled mixing, we use an artificial data set size limit of K = 221."}
{"doc_id": "1910.10683", "para_id": 159, "text": "gives the model some beneficial early exposure to the downstream tasks. We might also hope that mixing in many sources of supervision could help the pre-trained model obtain a more general set of “skills” (loosely speaking) before it is adapted to an individual task. To measure this directly, we consider a second variant where we pre-train the model on the same examples-proportional mixture (with K = 219) except that we omit one of the downstream tasks from this pre-training mixture. Then, we fine-tune the model on the task that was left out during pre-training. We repeat this for each of the downstream tasks we consider. We call this approach “leave-one-out” multi-task training. This simulates the real-world setting where a pre-trained model is fine-tuned on a task it had not seen during pre-training. Note that multi-task pre-training provides a diverse mixture of supervised tasks. Since other fields (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014)) use a supervised data set for pre-training, we were interested to see whether omitting the unsupervised task from the multi-task pre-training mixture still produced good results. For our third variant we therefore pre-train on an examples-proportional mixture of all of the supervised tasks we consider with K = 219. In all of these variants, we follow our standard procedure of pre-training for 219 steps before fine-tuning for 218 steps. We compare the results of these approaches in Table 12. For comparison, we also include results for our baseline (pre-train then fine-tune) and for standard multi-task learning (without fine-tuning) on an examples-proportional mixture with K = 219. We find that fine-tuning after multi-task pre-training results in comparable performance to our baseline. This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates described in Section 3.5.2. Interestingly, the performance of “leave-one-out” training was only slightly worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pre- training might not result in a dramatic task interference). Finally, supervised multi-task pre-training performed significantly worse in every case except for the translation tasks. This"}
{"doc_id": "1910.10683", "para_id": 160, "text": "Training strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 161, "text": "⋆Unsupervised pre-training + fine-tuning 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Multi-task training 81.42 19.24 79.78 67.30 25.21 36.30 27.76 Multi-task pre-training + fine-tuning 83.11 19.12 80.26 71.03 27.08 39.80 28.07 Leave-one-out multi-task training 81.98 19.05 79.97 71.68 26.93 39.79 27.87 Supervised multi-task pre-training 79.93 18.96 77.38 65.36 26.81 40.13 28.04"}
{"doc_id": "1910.10683", "para_id": 162, "text": "Table 12: Comparison of unsupervised pre-training, multi-task learning, and various forms of multi-task pre-training."}
{"doc_id": "1910.10683", "para_id": 163, "text": "could suggest that the translation tasks benefit less from (English) pre-training, whereas unsupervised pre-training is an important factor in the other tasks."}
{"doc_id": "1910.10683", "para_id": 164, "text": "The “bitter lesson” of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a). Recent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c; Radford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown that scaling up produces improved performance compared to more carefully-engineered methods. However, there are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling. In this section, we compare these different approaches by addressing the following premise: “You were just given 4× more compute. How should you use it?”"}
{"doc_id": "1910.10683", "para_id": 165, "text": "We start with our baseline model, which has 220M parameters and is pre-trained and fine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized similarly to “BERTBASE”. To experiment with increased model size, we follow the guidelines of “BERTLARGE” Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and 16-head attention mechanisms. We then generate two variants with 16 and 32 layers each in the encoder and decoder, producing models with 2× and 4× as many parameters as our original model. These two variants also have a roughly 2× and 4× the computational cost. Using our baseline and these two larger models, we consider three ways of using 4× as much computation: Training for 4× as many steps, training for 2× as many steps with the 2× bigger model, and training the 4× bigger model for the “baseline” number of training steps. When we increase the training steps, we scale both the pre-train and fine-tune steps for simplicity. Note that when increasing the number of pre-training steps, we are effectively including more pre-training data as C4 is so large that we do not complete one pass over the data even when training for 223 steps."}
{"doc_id": "1910.10683", "para_id": 166, "text": "An alternative way for the model to see 4× as much data is to increase the batch size by a factor of 4. This can potentially result in faster training due to more efficient parallelization. However, training with a 4× larger batch size can yield a different outcome than training"}
{"doc_id": "1910.10683", "para_id": 167, "text": "Scaling strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo"}
{"doc_id": "1910.10683", "para_id": 168, "text": "⋆Baseline 83.28 19.24 80.88 71.36 26.98 39.82 27.65 1× size, 4× training steps 85.33 19.33 82.45 74.72 27.08 40.66 27.93 1× size, 4× batch size 84.60 19.42 82.52 74.64 27.07 40.60 27.84 2× size, 2× training steps 86.18 19.66 84.18 77.18 27.52 41.03 28.19 4× size, 1× training steps 85.91 19.73 83.86 78.04 27.47 40.71 28.10 4× ensembled 84.77 20.10 83.09 71.74 28.05 40.53 28.57 4× ensembled, fine-tune only 84.05 19.57 82.36 71.55 27.55 40.22 28.09"}
{"doc_id": "1910.10683", "para_id": 169, "text": "Table 13: Comparison of different methods of scaling up our baseline model. All methods except ensembling fine-tuned models use 4× the computation as the baseline. “Size” refers to the number of parameters in the model and “training time” refers to the number of steps used for both pre-training and fine-tuning."}
{"doc_id": "1910.10683", "para_id": 170, "text": "for 4× as many steps (Shallue et al., 2018). We include an additional experiment where we train our baseline model with a 4× larger batch size to compare these two cases. It is common practice on many of the benchmarks we consider to eke out additional performance by training and evaluating using an ensemble of models. This provides an orthogonal way of using additional computation. To compare other scaling methods to ensembling, we also measure the performance of an ensemble of 4 separately pre-trained and fine-tuned models. We average the logits across the ensemble before feeding them into the output softmax nonlinearity to obtain an aggregate prediction. Instead of pre-training 4 separate models, a cheaper alternative is to take a single pre-trained model and produce 4 separate fine-tuned versions. While this does not use our entire 4× computational budget, we also include this method to see if it produces competitive performance to the other scaling methods. The performance achieved after applying these various scaling methods is shown in Table 13. Unsurprisingly, increasing the training time and/or model size consistently improves the baseline. There was no clear winner between training for 4× as many steps or using a 4× larger batch size, though both were beneficial. In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size. We did not observe a large difference between training a 2× bigger model for 2× as long and training a 4× bigger model on any of the tasks we studied. This suggests that increasing the training time and increasing the model size can be complementary means of improving performance. Our results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale. In some tasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling 4 completely separately trained models significantly outperformed every other scaling approach. Ensembling models that were pre-trained together but fine-tuned separately also gave a substantial performance increase over the baseline, which suggests a cheaper means of improving performance. The only exception was SuperGLUE, where neither ensembling approach significantly improved over the baseline. We note that different scaling methods have different trade-offs that are separate from their performance. For example, using a larger model can make downstream fine-tuning and"}
{"doc_id": "1910.10683", "para_id": 171, "text": "inference more expensive. In contrast, the cost of pre-training a small model for longer is effectively amortized if it is applied to many downstream tasks. Separately, we note that ensembling N separate models has a similar cost to using a model that has an N× higher computational cost. As a result, some consideration for the eventual use of the model is important when choosing between scaling methods."}
{"doc_id": "1910.10683", "para_id": 172, "text": "We now leverage the insights from our systematic study to determine how far we can push performance on popular NLP benchmarks. We are also interested in exploring the current limits of transfer learning for NLP by training larger models on large amounts of data. We start with our baseline training approach and make the following changes:"}
{"doc_id": "1910.10683", "para_id": 173, "text": "Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption objective described in Section 3.3.4, which was loosely inspired by SpanBERT (Joshi et al., 2019). Specifically, we use a mean span length of 3 and corrupt 15% of the original sequence. We found that this objective produced marginally better performance (Table 7) while being slightly more computationally efficient due to shorter target sequence lengths."}
{"doc_id": "1910.10683", "para_id": 174, "text": "Longer training Our baseline model uses a relatively small amount of pre-training (1⁄4 as much as BERT (Devlin et al., 2018), 1⁄16 as much as XLNet (Yang et al., 2019), 1⁄64 as much as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we can train for substantially longer without repeating data (which can be detrimental, as shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can indeed be helpful, and that both increasing the batch size and increasing the number of training steps can confer this benefit. We therefore pre-train our models for 1 million steps on a batch size of 211 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens (about 32× as many as our baseline). In Section 3.4.1, we showed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC data sets outperformed pre-training on C4 on a few downstream tasks. However, these data set variants are sufficiently small that they would be repeated hundreds of times over the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2 that this repetition could be harmful, we opted instead to continue using the C4 data set."}
{"doc_id": "1910.10683", "para_id": 175, "text": "Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved performance. However, using smaller models can be helpful in settings where limited computational resources are available for fine-tuning or inference. Based on these factors, we train models with a wide range of sizes:"}
{"doc_id": "1910.10683", "para_id": 176, "text": "• Base. This is our baseline model, whose hyperparameters are described in Section 3.1.1. It has roughly 220 million parameters."}
{"doc_id": "1910.10683", "para_id": 177, "text": "• Small. We consider a smaller model, which scales the baseline down by using dmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the encoder and decoder. This variant has about 60 million parameters."}
{"doc_id": "1910.10683", "para_id": 178, "text": "• Large. Since our baseline uses a BERTBASE-sized encoder and decoder, we also consider a variant where the encoder and decoder are both similar in size and structure to BERTLARGE. Specifically, this variant uses dmodel = 1,024, dff = 4,096, dkv = 64, 16-headed attention, and 24 layers each in the encoder and decoder, resulting in around 770 million parameters."}
{"doc_id": "1910.10683", "para_id": 179, "text": "• 3B and 11B. To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use dmodel = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters. We chose to scale up dff specifically because modern accelerators (such as the TPUs we train our models on) are most efficient for large dense matrix multiplications like those in the Transformer’s feed-forward networks."}
{"doc_id": "1910.10683", "para_id": 180, "text": "Multi-task pre-training In Section 3.5.3, we showed that pre-training on a multi-task mixture of unsupervised and supervised tasks before fine-tuning worked as well as pre-training on the unsupervised task alone. This is the approach advocated by the “MT-DNN” (Liu et al., 2015, 2019b). It also has the practical benefit of being able to monitor “downstream” performance for the entire duration of training, rather than just during fine-tuning. We therefore used multi-task pre-training in our final set of experiments. We hypothesize that larger models trained for longer might benefit from a larger proportion of unlabeled data because they are more likely to overfit to smaller training data sets. However, we also note that the results of Section 3.5.3 suggest that fine-tuning after multi-task pre-training can mitigate some of the issues that might arise from choosing a suboptimal proportion of unlabeled data. Based on these ideas, we substitute the following artificial data set sizes for our unlabeled data before using standard example-proportional mixing (described in Section 3.5.2): 710,000 for Small, 2,620,000 for Base, 8,660,000 for Large, 33,500,000 for 3B, and 133,000,000 for 11B. For all model variants, we also capped the effective data set size of the WMT English to French and WMT English to German data sets to 1M examples during pre-training."}
{"doc_id": "1910.10683", "para_id": 181, "text": "Fine-tuning on individual GLUE and SuperGLUE tasks So far, when fine-tuning on GLUE and SuperGLUE, we have concatenated all of the data sets in each benchmark so that we only fine-tune models once for GLUE and once for SuperGLUE. This approach makes our study logistically simpler, but we found that this sacrifices a small amount of performance on some tasks compared to fine-tuning on the task separately. A potential issue with fine-tuning on individual tasks, which would otherwise be mitigated by training on all tasks at once, is that we might overfit quickly to low-resource tasks. For example, our large batch size of 211 length-512 sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model’s parameters before it overfits."}
{"doc_id": "1910.10683", "para_id": 182, "text": "Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search (Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty of α = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization tasks."}
{"doc_id": "1910.10683", "para_id": 183, "text": "Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for English- Romanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art."}
{"doc_id": "1910.10683", "para_id": 184, "text": "Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.). For reference, these details are described in Section 2. The results of this final set of experiments are shown in Table 14. Overall, we achieved state-of-the-art performance on 18 out of the 24 tasks we consider. As expected, our largest (11 billion parameter) model performed best among our model size variants across all tasks. Our T5-3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to 11 billion parameters was the most important ingredient for achieving our best performance. We now analyze the results for each individual benchmark. We achieved a state-of-the-art average GLUE score of 90.3. Notably, our performance was substantially better than the previous state-of-the-art for the natural language inference tasks MNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine performance has historically lagged behind human performance, which is 93.6 and 95.9 respectively (Wang et al., 2018). In terms of parameter count, our 11B model variant is the largest model that has been submitted to the GLUE benchmark. However, most of the best-scoring submissions use a large amount of ensembling and computation to produce predictions. For example, the best-performing variant of ALBERT (Lan et al., 2019) uses a model similar in size and architecture to our 3B variant (though it has dramatically fewer parameters due to clever parameter sharing). To produce its impressive performance on GLUE, the ALBERT authors ensembled “from 6 to 17” models depending on the task. This likely results in it being more computationally expensive to produce predictions with the ALBERT ensemble than it is with T5-11B. For SQuAD, we outperformed the previous state-of-the-art (ALBERT (Lan et al., 2019)) by over one point on the Exact Match score. SQuAD is a long-standing benchmark that"}
{"doc_id": "1910.10683", "para_id": 185, "text": "15. http://gluebenchmark.com 16. http://super.gluebenchmark.com"}
{"doc_id": "1910.10683", "para_id": 186, "text": "GLUE CoLA SST-2 MRPC MRPC STS-B STS-B Model Average Matthew’s Accuracy F1 Accuracy Pearson Spearman"}
{"doc_id": "1910.10683", "para_id": 187, "text": "Previous best 89.4a 69.2b 97.1a 93.6b 91.5b 92.7b 92.3b"}
{"doc_id": "1910.10683", "para_id": 188, "text": "T5-Small 77.4 41.0 91.8 89.7 86.6 85.6 85.0 T5-Base 82.7 51.1 95.2 90.7 87.5 89.4 88.6 T5-Large 86.4 61.2 96.3 92.4 89.9 89.9 89.2 T5-3B 88.5 67.1 97.4 92.5 90.0 90.6 89.8 T5-11B 90.3 71.6 97.5 92.8 90.4 93.1 92.8"}
{"doc_id": "1910.10683", "para_id": 189, "text": "QQP QQP MNLI-m MNLI-mm QNLI RTE WNLI Model F1 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy"}
{"doc_id": "1910.10683", "para_id": 190, "text": "Previous best 74.8c 90.7b 91.3a 91.0a 99.2a 89.2a 91.8a"}
{"doc_id": "1910.10683", "para_id": 191, "text": "T5-Small 70.0 88.0 82.4 82.3 90.3 69.9 69.2 T5-Base 72.6 89.4 87.1 86.2 93.7 80.1 78.8 T5-Large 73.9 89.9 89.9 89.6 94.8 87.2 85.6 T5-3B 74.4 89.7 91.4 91.2 96.3 91.1 89.7 T5-11B 75.1 90.6 92.2 91.9 96.9 92.8 94.5"}
{"doc_id": "1910.10683", "para_id": 192, "text": "SQuAD SQuAD SuperGLUE BoolQ CB CB COPA Model EM F1 Average Accuracy F1 Accuracy Accuracy"}
{"doc_id": "1910.10683", "para_id": 193, "text": "Previous best 90.1a 95.5a 84.6d 87.1d 90.5d 95.2d 90.6d"}
{"doc_id": "1910.10683", "para_id": 194, "text": "T5-Small 79.10 87.24 63.3 76.4 56.9 81.6 46.0 T5-Base 85.44 92.08 76.2 81.4 86.2 94.0 71.2 T5-Large 86.66 93.79 82.3 85.4 91.6 94.8 83.4 T5-3B 88.53 94.95 86.4 89.9 90.3 94.4 92.0 T5-11B 91.26 96.22 88.9 91.2 93.9 96.8 94.8"}
{"doc_id": "1910.10683", "para_id": 195, "text": "MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC Model F1a EM F1 Accuracy Accuracy Accuracy Accuracy"}
{"doc_id": "1910.10683", "para_id": 196, "text": "Previous best 84.4d 52.5d 90.6d 90.0d 88.2d 69.9d 89.0d"}
{"doc_id": "1910.10683", "para_id": 197, "text": "T5-Small 69.3 26.3 56.3 55.4 73.3 66.9 70.5 T5-Base 79.7 43.1 75.0 74.2 81.5 68.3 80.8 T5-Large 83.3 50.7 86.8 85.9 87.8 69.3 86.3 T5-3B 86.8 58.3 91.2 90.4 90.7 72.1 90.4 T5-11B 88.1 63.3 94.1 93.4 92.5 76.9 93.8"}
{"doc_id": "1910.10683", "para_id": 198, "text": "WMT EnDe WMT EnFr WMT EnRo CNN/DM CNN/DM CNN/DM Model BLEU BLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L"}
{"doc_id": "1910.10683", "para_id": 199, "text": "Previous best 33.8e 43.8e 38.5f 43.47g 20.30g 40.63g"}
{"doc_id": "1910.10683", "para_id": 200, "text": "T5-Small 26.7 36.0 26.8 41.12 19.56 38.35 T5-Base 30.9 41.2 28.0 42.05 20.34 39.40 T5-Large 32.0 41.5 28.1 42.50 20.68 39.75 T5-3B 31.8 42.6 28.2 42.72 21.02 39.94 T5-11B 32.1 43.4 28.1 43.52 21.55 40.69"}
{"doc_id": "1910.10683", "para_id": 201, "text": "Table 14: Performance of our T5 variants on every task we study. Small, Base, Large, 3B, and 11B refer to model configurations with 60 million, 220 million, 770 million, 3 billion, and 11 billion parameters, respectively. In the first row of each table, we report the state-of-the-art for the task (as of October 24th, 2019), with the superscript denoting its source with references listed at the end of this caption. All results are reported on the test set except for SQuAD where we use the validation set. a(Lan et al., 2019) b(Wang et al., 2019c) c(Zhu et al., 2019) d(Liu et al., 2019c) e(Edunov et al., 2018) f(Lample and Conneau, 2019) g(Dong et al., 2019)"}
{"doc_id": "1910.10683", "para_id": 202, "text": "was created over three years ago, and most recent improvements have only increased the state-of-the-art by a fraction of a percentage point. We note that when results are reported on the test set, they are typically based on an ensemble of models and/or leverage external data sets (e.g. TriviaQA (Joshi et al., 2017) or NewsQA (Trischler et al., 2016)) to augment the small SQuAD training set. Human performance on SQuAD is estimated at 82.30 and 91.22 for the Exact Match and F1 metric respectively (Rajpurkar et al., 2016), so it is not clear if further improvements on this benchmark are meaningful. For SuperGLUE, we improved upon the state-of-the-art by a large margin (from an average score of 84.6 (Liu et al., 2019c) to 88.9). SuperGLUE was designed to include tasks that were “beyond the scope of current state-of-the-art systems, but solvable by most college-educated English speakers” (Wang et al., 2019b). We nearly match the human performance of 89.8 (Wang et al., 2019b). Interestingly, on the reading comprehension tasks (MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the evaluation metrics used for these tasks may be biased towards machine-made predictions. On the other hand, humans achieve 100% accuracy on both COPA and WSC, which is significantly better than our model’s performance. This suggests that there remain linguistic tasks that are hard for our model to perfect, particularly in the low-resource setting. We did not achieve state-of-the-art performance on any of the WMT translation tasks. This may be in part due to our use of an English-only unlabeled data set. We also note that most of the best results on these tasks use backtranslation (Edunov et al., 2018; Lample and Conneau, 2019), which is a sophisticated data augmentation scheme. The state of the art on the low-resource English to Romanian benchmark also uses additional forms of cross-lingual unsupervised training (Lample and Conneau, 2019). Our results suggest that scale and English-language pre-training may be insufficient to match the performance of these more sophisticated methods. On a more specific note, the best results on English to German newstest2014 set use the much larger training set from WMT 2018 (Edunov et al., 2018), making direct comparison to our results difficult. Finally, on CNN/Daily Mail we attain state-of-the-art performance, though only by a significant amount on the ROUGE-2-F score. It has been shown that improvements to the ROUGE score do not necessarily correspond to more coherent summaries (Paulus et al., 2017). Furthermore, while CNN/Daily Mail is posed as an abstractive summarization benchmark, purely extractive approaches have been shown to work well (Liu, 2019). It has also been argued that generative models trained with maximum likelihood are prone to producing repetitive summaries (See et al., 2017). Despite these potential issues, we find that our models do generate coherent and largely correct summaries. We provide some non-cherry-picked validation set examples in Appendix C. To achieve its strong results, T5 combines insights from our experimental study with unprecedented scale. Note that in Section 3.6 we found that scaling up the pre-training amount or size of our baseline model produced substantial gains. Given this, we were interested to measure how much the “non-scaling” changes we introduced into T5 contributed to its strong performance. We therefore carried out a final experiment where we compared the following three configurations: First, the standard baseline model, which was pre-trained on 235 ≈34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5), which we refer to as “baseline-1T”; and third, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the"}
{"doc_id": "1910.10683", "para_id": 203, "text": "⋆Baseline 83.28 19.24 80.88 71.36 26.98 39.82 27.65 Baseline-1T 84.80 19.62 83.01 73.90 27.46 40.30 28.34 T5-Base 85.97 20.90 85.44 75.64 28.37 41.37 28.98"}
{"doc_id": "1910.10683", "para_id": 204, "text": "Table 15: Performance comparison of T5-Base to our baseline experimental setup used in the rest of the paper. Results are reported on the validation set. “Baseline-1T” refers to the performance achieved by pre-training the baseline model on 1 trillion tokens (the same number used for the T5 model variants) instead of 235 ≈34B tokens (as was used for the baseline)."}
{"doc_id": "1910.10683", "para_id": 205, "text": "“non-scaling” changes we made when designing T5. As such, comparing the performance of these two models gives us a concrete measurement of the impact of the insights from our systematic study. The performance of these three model configurations is shown in Table 15. Consistent with the findings in Section 3.6, we find that additional pre-training improves performance over the baseline. Nevertheless, T5-Base substantially outperforms baseline-1T on all downstream tasks. This suggests that scale is not the only factor that contributes to T5’s success. We hypothesize that the larger models benefit not only from their increased size but also from these non-scaling factors."}
{"doc_id": "1910.10683", "para_id": 206, "text": "Having completed our systematic study, we wrap up by first recapping some of our most significant findings. Our results provide some high-level perspective on which avenues of research might be more or less promising. To conclude, we outline some topics we think might provide effective approaches for further progressing the field."}
{"doc_id": "1910.10683", "para_id": 207, "text": "Text-to-text Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS-B. In spite of its simplicity, we found the text-to- text framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale."}
{"doc_id": "1910.10683", "para_id": 208, "text": "Architectures While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoder-decoder form worked best in our text-to-text framework. Though an encoder-decoder model uses twice as many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model) architectures, it has a similar computational cost. We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"doc_id": "1910.10683", "para_id": 209, "text": "Unsupervised objectives Overall, we found that most “denoising” objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to- text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"doc_id": "1910.10683", "para_id": 210, "text": "Data sets We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to data sets that use additional filtering, we found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set. We separately showed that performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks."}
{"doc_id": "1910.10683", "para_id": 211, "text": "Training strategies We found that the basic approach of updating all of a pre-trained model’s parameters during fine-tuning outperformed methods that are designed to update fewer parameters, although updating all parameters is most expensive. We also experimented with various approaches for training the model on multiple tasks at once, which in our text-to-text setting simply corresponds to mixing examples from different data sets when constructing batches. The primary concern in multi-task learning is setting the proportion of each task to train on. We ultimately did not find a strategy for setting mixing proportions that matched the performance of the basic approach of unsupervised pre-training followed by supervised fine-tuning. However, we found that fine-tuning after pre-training on a mixture of tasks produced comparable performance to unsupervised pre-training."}
{"doc_id": "1910.10683", "para_id": 212, "text": "Scaling We compared various strategies for taking advantage of additional compute, includ- ing training the model on more data, training a larger model, and using an ensemble of models. We found each approach conferred a significant boost in performance, though training a smaller model on more data was often outperformed by training a larger model for fewer steps. We also showed an ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation. Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model."}
{"doc_id": "1910.10683", "para_id": 213, "text": "Pushing the limits We combined our above insights and trained substantially larger models (up to 11 billion parameters) to achieve state-of-the-art results across many of the benchmarks we considered. For unsupervised training, we extracted text from our C4 data set and applied a denoising objective that corrupts contiguous spans of tokens. We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall, our models were trained on over 1 trillion tokens. In the interest of facilitating the replication, extension, and application of our results, we release our code, the C4 data set, and pre-trained model weights for each T5 variant.1"}
{"doc_id": "1910.10683", "para_id": 214, "text": "The inconvenience of large models An unsurprising but important result from our study is that larger models tend to perform better. The fact that the hardware used for running these models is continually getting cheaper and more powerful suggests that scaling up may continue to be a promising way to achieve better performance (Sutton, 2019). However, it will always be the case that there are applications and scenarios where using a smaller or less expensive model is helpful, for example when performing client-side inference or federated learning (Konečn`y et al., 2015, 2016). Relatedly, one beneficial use of transfer learning is the possibility of attaining good performance on low-resource tasks. Low-resource tasks often occur (by definition) in settings where one lacks the assets to label more data. It follows that low-resource applications often also have limited access to computational resources which can incur additional costs. As a result, we advocate for research on methods that achieve stronger performance with cheaper models so that transfer learning can be applied where it will have the most impact. Some current work along these lines include distillation (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and conditional computation (Shazeer et al., 2017)."}
{"doc_id": "1910.10683", "para_id": 215, "text": "More efficient knowledge extraction Recall that one of the goals of pre-training is (loosely speaking) to provide the model with general-purpose “knowledge” that improves its performance on downstream tasks. The method we use in this work, which is currently common practice, is to train the model to denoise corrupted spans of text. We suspect that this simplistic technique may not be a very efficient way to teach the model general-purpose knowledge. More concretely, it would be useful to be able to attain good fine-tuning performance without needing to train our models on 1 trillion tokens of text first. Some concurrent work along these lines improves efficiency by pre-training a model to distinguish between real and machine-generated text (Clark et al., 2020)."}
{"doc_id": "1910.10683", "para_id": 216, "text": "Formalizing the similarity between tasks We observed that pre-training on unlabeled in-domain data can improve performance on downstream tasks (Section 3.4). This finding mostly relies on basic observations like the fact that SQuAD was created using data from Wikipedia. It would be useful to formulate a more rigorous notion of the “similarity” between the pre-training and downstream tasks, so that we could make more principled choices about what source of unlabeled data to use. There is some early empirical work along these lines in the field of computer vision (Huh et al., 2016; Kornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could also help choose supervised pre-training tasks, which has been shown to be helpful for the GLUE benchmark (Phang et al., 2018)."}
{"doc_id": "1910.10683", "para_id": 217, "text": "Language-agnostic models We were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks we studied. We also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time. To address these issues, we are interested in further investigating language-agnostic models, i.e. models that can perform a given NLP task with good performance regardless of the text’s language. This is an especially"}
{"doc_id": "1910.10683", "para_id": 218, "text": "pertinent issue given that English is not the native language for the majority of the world’s population."}
{"doc_id": "1910.10683", "para_id": 219, "text": "The motivation for this paper was the flurry of recent work on transfer learning for NLP. Before we began this work, these advances had already enabled breakthroughs in settings where learning-based methods had not yet been shown to be effective. We are happy to be able to continue this trend, for example by nearly matching human-level performance on the SuperGLUE benchmark, a task specifically designed to be difficult for modern transfer-learning pipelines. Our results stem from the combination of a straightforward and unified text-to-text framework, our new C4 data set, and insights from our systematic study. Additionally, we provided an empirical overview of the field and a perspective on where it stands. We are excited to see continued work using transfer learning towards the goal of general language understanding."}
{"doc_id": "1910.10683", "para_id": 220, "text": "We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight."}
{"doc_id": "1910.10683", "para_id": 221, "text": "Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase. Michael owned all aspects of the Winograd data sets, ingested many of the data sets we used, contributed various improvements and fixes to our infrastructure, and ran some preliminary experiments. Yanqi ran experiments and implemented methods to help settle on a reasonable baseline and helped with the final fine-tuning of the models in Section 3.7. Wei also helped with final fine-tuning and improved some of our preprocessors. Peter prototyped an early version of the pre-training data set and resolved issues pertaining to the SQuAD and CNN/DM tasks. All authors helped set the scope and research direction we followed in this work."}
{"doc_id": "1910.10683", "para_id": 222, "text": "Appendix B. Converting WNLI to Our Text-to-Text Format"}
{"doc_id": "1910.10683", "para_id": 223, "text": "Note that as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead, when evaluating on the WNLI test set (for the results in Section 3.7), we convert the WNLI test set to the “referent noun prediction” text-to-text format so that we can evaluate using a model trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed by He et al. (2019). Recall that examples from WNLI consist of a premise, a hypothesis, and a label that indicates whether the hypothesis is True or False. Using the example from Section 2.4, the hypothesis would be “The city councilmen refused the demonstrators a permit because they feared violence.” with the premise “The demonstrators feared violence.” and the label False. We first find the location of all pronouns in the premise (“they” in our example). Then, we find the maximum number of words that precede or follow each pronoun that are a substring in the hypothesis (“feared violence” in our example), ignoring case and punctuation. When the premise contains multiple candidate pronouns, we choose the pronoun that is preceded or followed by the largest substring of the hypothesis. We then highlight the pronoun in the premise by surrounding it with asterisks. For the candidate noun (which is compared to our model’s prediction to obtain a True or False label), we remove the matching substring from the hypothesis and optionally make it non-possessive (resulting in “the demonstrators”)."}
{"doc_id": "1910.10683", "para_id": 224, "text": "To show that our model is generating fluent summaries, we include a few example decodes from our best model (T5-11B) on the validation set along with the ground-truth summaries. These examples selected at random and were not cherry-picked."}
{"doc_id": "1910.10683", "para_id": 225, "text": "1. Ground-truth: leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside - watched by ms moolman ."}
{"doc_id": "1910.10683", "para_id": 226, "text": "Prediction: leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera ."}
{"doc_id": "1910.10683", "para_id": 227, "text": "2. Ground-truth: researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 228, "text": "Prediction: researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 229, "text": "3. Ground-truth: executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn’t work . it is ’essential that executions are carried out in a constitutional manner,’ said georgia attorney general sam olens ."}
{"doc_id": "1910.10683", "para_id": 230, "text": "Prediction: the execution of kelly gissendaner, the only woman on georgia’s death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country ."}
{"doc_id": "1910.10683", "para_id": 231, "text": "4. Ground-truth: dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain ."}
{"doc_id": "1910.10683", "para_id": 232, "text": "Prediction: dani alves was not selected for brazil’s friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter"}
{"doc_id": "1910.10683", "para_id": 233, "text": "of a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news ."}
{"doc_id": "1910.10683", "para_id": 234, "text": "In this section, we provide examples of our preprocessing for each of the data sets we consider."}
{"doc_id": "1910.10683", "para_id": 235, "text": "Processed input: cola sentence: John made Bill master of himself."}
{"doc_id": "1910.10683", "para_id": 236, "text": "Sentence 1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians)."}
{"doc_id": "1910.10683", "para_id": 237, "text": "Processed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). sentence2: Slovenia has 3,000 inhabitants."}
{"doc_id": "1910.10683", "para_id": 238, "text": "Hypothesis: The St. Louis Cardinals have always won."}
{"doc_id": "1910.10683", "para_id": 239, "text": "Premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but"}
{"doc_id": "1910.10683", "para_id": 240, "text": "Processed input: mnli hypothesis: The St. Louis Cardinals have always won. premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but"}
{"doc_id": "1910.10683", "para_id": 241, "text": "Sentence 1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . Sentence 2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" ."}
{"doc_id": "1910.10683", "para_id": 242, "text": "Processed input: mrpc sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" ."}
{"doc_id": "1910.10683", "para_id": 243, "text": "Question: Where did Jebe die? Sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand."}
{"doc_id": "1910.10683", "para_id": 244, "text": "Processed input: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand."}
{"doc_id": "1910.10683", "para_id": 245, "text": "Question 1: What attributes would have made you highly desirable in ancient Rome? Question 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?"}
{"doc_id": "1910.10683", "para_id": 246, "text": "Processed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?"}
{"doc_id": "1910.10683", "para_id": 247, "text": "Sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight ."}
{"doc_id": "1910.10683", "para_id": 248, "text": "Processed input: sst2 sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight ."}
{"doc_id": "1910.10683", "para_id": 249, "text": "Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday."}
{"doc_id": "1910.10683", "para_id": 250, "text": "Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit."}
{"doc_id": "1910.10683", "para_id": 251, "text": "Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit."}
{"doc_id": "1910.10683", "para_id": 252, "text": "Premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping?"}
{"doc_id": "1910.10683", "para_id": 253, "text": "Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping?"}
{"doc_id": "1910.10683", "para_id": 254, "text": "Premise: Political violence broke out in the nation."}
{"doc_id": "1910.10683", "para_id": 255, "text": "Choice 2: Many citizens took refuge in other territories."}
{"doc_id": "1910.10683", "para_id": 256, "text": "Processed input: copa choice1: Many citizens relocated to the capitol. choice2: Many citizens took refuge in other territories. premise: Political violence broke out in the nation. question: effect"}
{"doc_id": "1910.10683", "para_id": 257, "text": "Answer: There was only pie to eat, rather than traditional breakfast foods"}
{"doc_id": "1910.10683", "para_id": 258, "text": "Paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br>"}
{"doc_id": "1910.10683", "para_id": 259, "text": "Question: Why was Joey surprised the morning he woke up for breakfast?"}
{"doc_id": "1910.10683", "para_id": 260, "text": "Processed input: multirc question: Why was Joey surprised the morning he woke up for breakfast? answer: There was only pie to eat, rather than traditional breakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together"}
{"doc_id": "1910.10683", "para_id": 261, "text": "at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br>"}
{"doc_id": "1910.10683", "para_id": 262, "text": "Sentence 1: It was the deliberation of his act that was insulting ."}
{"doc_id": "1910.10683", "para_id": 263, "text": "Processed input: wic pos: N sentence1: It was the deliberation of his act that was insulting . sentence2: The deliberations of the jury . word: deliberation"}
{"doc_id": "1910.10683", "para_id": 264, "text": "Text: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made it pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 265, "text": "Processed input: wsc: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 266, "text": "Original input: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game ."}
{"doc_id": "1910.10683", "para_id": 267, "text": "Processed input: summarize: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom"}
{"doc_id": "1910.10683", "para_id": 268, "text": "from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game ."}
{"doc_id": "1910.10683", "para_id": 269, "text": "Original target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth ."}
{"doc_id": "1910.10683", "para_id": 270, "text": "Processed target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth ."}
{"doc_id": "1910.10683", "para_id": 271, "text": "Question: What does increased oxygen concentrations in the patient’s lungs displace?"}
{"doc_id": "1910.10683", "para_id": 272, "text": "Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment."}
{"doc_id": "1910.10683", "para_id": 273, "text": "Processed input: question: What does increased oxygen concentrations in the patient’s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment."}
{"doc_id": "1910.10683", "para_id": 274, "text": "Original input: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 275, "text": "Processed input: translate English to German: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 276, "text": "Original target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 277, "text": "Processed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 278, "text": "Original input: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 279, "text": "Processed input: translate English to French: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 280, "text": "Original target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 281, "text": "Processed target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 282, "text": "Original input: Taco Bell said it plans to add 2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 283, "text": "Processed input: translate English to Romanian: Taco Bell said it plans to add 2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 284, "text": "Original target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă 2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 285, "text": "Processed target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă 2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 286, "text": "Appendix E. Scores on Every Task for All Experiments"}
{"doc_id": "1910.10683", "para_id": 287, "text": "The following table lists the scores achieved on every task in the experiments described in Sections 3.2 to 3.6."}
{"doc_id": "1910.10683", "para_id": 288, "text": "GLUE SuperGLUE WMT Score CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLIm MNLImm QNLI RTE CNN/DM SQuAD Score BoolQ CB CB COPA MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC EnDe EnFr EnRo Table Experiment Average MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc R-1-F R-2-F R-L-F EM F1 Average Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc BLEU BLEU BLEU"}
{"doc_id": "1910.10683", "para_id": 289, "text": "1 ⋆Baseline average 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 1 Baseline standard deviation 0.235 1.111 0.569 0.729 1.019 0.374 0.418 0.108 0.070 0.291 0.231 0.361 1.393 0.065 0.065 0.058 0.343 0.226 0.416 0.365 3.237 2.560 2.741 0.716 1.011 0.370 0.379 1.228 0.850 2.029 0.112 0.090 0.108 1 No pre-training 66.22 12.29 80.62 81.42 73.04 72.58 72.97 81.94 86.62 68.02 67.98 75.69 58.84 39.19 17.60 36.69 50.31 61.97 53.04 65.38 71.61 76.79 62.00 59.10 0.84 20.33 17.95 54.15 54.08 65.38 25.86 39.77 24.04"}
{"doc_id": "1910.10683", "para_id": 290, "text": "2 ⋆Enc/dec, denoising 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 2 Enc/dec, shared, denoising 82.81 55.24 91.86 91.58 88.24 87.43 87.58 88.69 91.60 83.88 84.01 90.23 73.65 41.11 18.78 38.48 80.63 88.49 70.73 77.13 95.04 96.43 65.00 66.16 22.98 68.95 68.09 70.76 68.18 75.96 26.72 39.03 27.46 2 Enc/dec, 6 layers, denoising 80.88 46.26 92.09 91.51 87.99 87.01 86.76 87.93 90.97 82.20 82.41 88.83 71.48 40.83 18.97 38.31 77.59 86.07 68.42 73.79 91.70 92.86 67.00 61.02 19.62 61.26 60.33 72.20 65.99 75.00 26.38 38.40 26.95 2 Language model, denoising 74.70 24.50 90.60 86.08 78.92 85.22 85.42 85.40 88.99 76.72 77.05 86.02 64.62 39.49 17.93 36.91 61.14 71.37 55.02 65.47 60.08 71.43 58.00 43.03 2.94 53.35 52.31 53.07 58.62 63.46 25.09 35.28 25.86 2 Prefix LM, denoising 81.82 49.99 92.43 91.43 88.24 87.20 86.98 88.41 91.39 82.32 82.93 88.71 74.01 40.46 18.61 37.90 78.94 87.31 68.11 75.50 93.37 91.07 60.00 63.43 21.20 65.03 64.11 71.48 65.67 73.08 26.43 37.98 27.39 2 Enc/dec, LM 79.56 42.03 91.86 91.64 88.24 87.13 87.00 88.21 91.15 81.68 81.66 88.54 65.70 40.67 18.59 38.13 76.02 84.85 64.29 72.23 85.74 89.29 57.00 60.53 16.26 59.28 58.30 65.34 64.89 70.19 26.27 39.17 26.86 2 Enc/dec, shared, LM 79.60 44.83 92.09 90.20 85.78 86.03 85.87 87.77 91.02 81.74 82.29 89.16 65.34 40.16 18.13 37.59 76.35 84.86 63.50 70.49 91.41 87.50 55.00 60.21 16.89 57.83 56.73 63.54 63.48 70.19 26.62 39.17 27.05 2 Enc/dec, 6 layers, LM 78.67 38.72 91.40 90.40 86.52 86.82 86.49 87.87 91.03 80.99 80.92 88.05 65.70 40.29 18.26 37.70 75.32 84.06 64.06 71.38 85.25 89.29 60.00 57.56 16.79 55.22 54.30 66.79 63.95 71.15 26.13 38.42 26.89 2 Language model, LM 73.78 28.53 89.79 85.23 78.68 84.22 84.00 84.88 88.70 74.94 75.77 84.84 58.84 38.97 17.54 36.37 53.81 64.55 56.51 64.22 59.92 71.43 64.00 53.04 1.05 46.81 45.78 58.84 56.74 69.23 25.23 34.31 25.38 2 Prefix LM, LM 79.68 41.26 92.09 90.11 86.27 86.82 86.32 88.35 91.35 81.71 82.02 89.04 68.59 39.66 17.84 37.13 76.87 85.39 64.86 71.47 93.37 91.07 57.00 58.67 16.89 59.25 58.16 64.26 66.30 71.15 26.28 37.51 26.76"}
{"doc_id": "1910.10683", "para_id": 291, "text": "4 Language modeling with prefix 80.69 44.22 93.00 91.68 88.48 87.20 87.18 88.39 91.41 82.66 83.09 89.29 68.95 40.71 18.94 38.15 77.99 86.43 65.27 73.55 83.95 87.50 55.00 59.65 18.89 61.76 60.76 68.59 65.67 73.08 26.86 39.73 27.49 4 BERT-style (Devlin et al., 2018) 82.96 52.49 92.55 92.79 89.95 87.68 87.66 88.47 91.44 83.60 84.05 90.33 75.45 41.27 19.17 38.72 80.65 88.24 69.85 76.48 94.37 94.64 61.00 63.29 25.08 66.76 65.85 72.20 69.12 75.00 26.78 40.03 27.41 4 Deshuffling 73.17 22.82 87.16 86.88 81.13 84.03 83.82 86.38 89.90 76.30 76.34 84.18 58.84 40.75 18.59 38.10 67.61 76.76 58.47 69.17 63.70 78.57 56.00 59.85 12.70 45.52 44.36 57.04 64.89 68.27 26.11 39.30 25.62"}
{"doc_id": "1910.10683", "para_id": 292, "text": "5 BERT-style (Devlin et al., 2018) 82.96 52.49 92.55 92.79 89.95 87.68 87.66 88.47 91.44 83.60 84.05 90.33 75.45 41.27 19.17 38.72 80.65 88.24 69.85 76.48 94.37 94.64 61.00 63.29 25.08 66.76 65.85 72.20 69.12 75.00 26.78 40.03 27.41 5 MASS-style (Song et al., 2019) 82.32 47.01 91.63 92.53 89.71 88.21 88.18 88.58 91.44 82.96 83.67 90.02 77.26 41.16 19.16 38.55 80.10 88.07 69.28 75.08 84.98 89.29 63.00 64.46 23.50 66.71 65.91 72.20 67.71 78.85 26.79 39.89 27.55 5 ⋆Replace corrupted spans 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 5 Drop corrupted tokens 84.44 60.04 92.89 92.79 89.95 87.28 86.85 88.56 91.54 83.94 83.92 90.74 79.42 41.27 19.31 38.70 80.52 88.28 68.67 75.90 96.02 94.64 56.00 65.06 23.92 65.54 64.60 71.12 67.40 74.04 27.07 39.76 27.82"}
{"doc_id": "1910.10683", "para_id": 293, "text": "6 Corruption rate = 10% 82.82 52.71 92.09 91.55 88.24 88.19 88.15 88.47 91.40 83.50 84.51 90.33 75.45 41.05 19.00 38.53 80.38 88.36 69.55 74.98 92.37 92.86 62.00 66.04 24.66 67.93 67.09 70.76 67.24 75.96 26.87 39.28 27.44 6 ⋆Corruption rate = 15% 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 6 Corruption rate = 25% 83.00 53.47 93.00 92.44 89.46 87.36 87.36 88.68 91.53 84.44 84.15 90.77 74.01 41.69 19.54 39.14 80.96 88.61 70.48 76.39 93.02 92.86 68.00 65.46 24.66 68.20 67.39 73.65 67.87 72.12 27.04 39.83 27.47 6 Corruption rate = 50% 81.27 46.26 91.63 91.11 87.99 87.87 87.64 88.70 91.57 83.64 84.10 90.24 70.76 41.51 19.32 38.89 79.80 87.76 70.33 75.02 93.05 92.86 68.00 62.97 24.13 64.94 64.13 72.20 68.50 77.88 27.01 39.90 27.49"}
{"doc_id": "1910.10683", "para_id": 294, "text": "7 ⋆Baseline (i.i.d.) 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 7 Average span length = 2 83.54 53.82 92.20 93.05 90.44 87.85 87.71 88.42 91.40 84.28 84.46 90.88 77.62 41.23 19.39 38.69 82.09 89.69 72.20 77.06 90.43 91.07 70.00 66.28 26.13 71.34 70.61 75.45 68.34 78.85 26.76 39.99 27.63 7 Average span length = 3 83.49 53.90 92.43 92.25 89.46 87.49 87.53 88.72 91.51 84.85 84.84 90.99 77.26 41.50 19.62 38.94 81.84 89.66 72.53 76.85 94.37 94.64 70.00 67.64 28.75 70.84 69.90 74.73 67.71 77.88 26.86 39.65 27.62 7 Average span length = 5 83.40 52.12 93.12 92.63 89.71 88.70 88.47 88.84 91.64 84.32 84.29 90.79 76.90 41.39 19.24 38.82 82.05 89.79 72.23 77.06 83.06 89.29 69.00 68.16 30.12 71.36 70.53 75.81 69.91 79.81 26.88 39.40 27.53 7 Average span length = 10 82.85 50.11 92.09 91.95 88.97 88.45 88.22 88.86 91.63 84.34 84.28 91.07 76.17 41.38 19.33 38.80 81.84 89.39 70.44 76.45 87.40 89.29 65.00 66.87 29.59 69.82 68.94 72.56 67.55 75.96 26.79 39.49 27.69"}
{"doc_id": "1910.10683", "para_id": 295, "text": "8 ⋆C4 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 8 C4, unfiltered 81.46 48.01 91.63 92.72 89.95 87.79 87.60 88.31 91.27 82.30 82.34 88.71 72.20 41.09 19.14 38.54 78.78 87.04 68.04 75.75 89.17 91.07 62.00 65.52 25.60 62.42 61.58 69.68 67.08 72.12 26.55 39.34 27.21 8 RealNews-like 83.83 56.55 92.66 92.06 88.97 87.71 87.37 88.51 91.49 84.35 84.46 90.61 78.34 41.38 19.23 38.84 80.39 88.50 72.38 77.00 93.09 94.64 66.00 65.92 23.82 74.56 73.72 75.81 66.61 80.77 26.75 39.90 27.48 8 WebText-like 84.03 56.38 93.12 92.31 89.22 88.69 88.68 88.65 91.56 84.70 84.84 90.83 77.62 41.23 19.31 38.70 81.42 89.15 71.40 76.88 83.08 89.29 66.00 64.10 24.24 72.24 71.36 75.45 68.03 82.69 26.80 39.74 27.59 8 Wikipedia 81.85 45.53 92.32 91.67 88.24 85.62 86.40 88.37 91.34 82.61 83.25 90.96 77.26 41.39 19.31 38.81 81.29 89.18 68.01 76.12 56.03 80.36 67.00 65.01 25.92 69.03 68.06 74.73 67.08 76.92 26.94 39.69 27.67 8 Wikipedia + TBC 83.65 55.53 92.78 92.41 89.22 86.67 86.27 89.47 92.29 84.38 83.45 91.94 76.90 41.22 19.28 38.67 82.08 89.70 73.24 76.22 95.40 92.86 69.00 51.59 50.93 69.53 68.51 77.62 66.93 81.73 26.77 39.63 27.57"}
{"doc_id": "1910.10683", "para_id": 296, "text": "9 ⋆Full data set 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 9 229 (64 repeats) 82.87 53.82 92.78 91.79 88.73 87.56 87.58 88.73 91.54 84.07 84.21 90.59 73.65 41.18 19.19 38.67 80.97 88.90 72.03 76.76 92.96 92.86 66.00 65.11 26.76 69.35 68.49 75.81 67.24 82.69 26.83 39.74 27.63 9 227 (256 repeats) 82.62 50.60 92.32 92.07 88.73 87.83 87.60 88.65 91.54 83.43 84.37 90.12 75.81 41.24 19.20 38.70 79.78 87.63 69.97 75.29 93.42 91.07 63.00 61.82 23.61 66.27 65.39 73.65 66.30 80.77 27.02 39.71 27.33 9 225 (1,024 repeats) 79.55 43.84 91.28 89.32 85.05 85.92 85.74 88.05 91.09 81.29 81.72 87.90 69.31 40.66 18.57 38.13 76.27 84.58 64.76 72.63 83.97 82.14 64.00 59.39 17.94 56.94 56.04 64.98 65.20 73.08 26.38 39.56 26.80 9 223 (4,096 repeats) 76.34 32.68 89.45 89.84 86.03 83.49 83.42 87.18 90.61 77.80 78.69 85.47 64.62 40.16 18.33 37.66 70.92 80.20 59.29 69.85 73.48 73.21 56.00 57.66 14.38 46.69 45.79 59.57 65.05 68.27 26.37 38.84 25.81"}
{"doc_id": "1910.10683", "para_id": 297, "text": "10 ⋆All parameters 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 10 Adapter layers, d = 32 80.52 45.33 91.63 90.59 86.76 88.38 88.06 86.99 90.26 83.63 83.94 90.72 67.15 34.50 15.08 32.15 79.32 87.70 60.40 65.32 50.87 73.21 52.00 58.61 19.41 65.50 64.58 62.09 64.58 73.08 13.84 17.88 15.54 10 Adapter layers, d = 128 81.51 45.35 92.89 91.49 88.24 87.73 87.65 87.73 90.93 83.64 84.09 90.52 72.56 36.71 16.62 34.37 79.47 87.61 63.03 69.20 52.21 75.00 56.00 61.08 18.05 67.94 66.97 68.59 66.77 73.08 19.83 27.50 22.63 10 Adapter layers, d = 512 81.54 44.25 93.35 91.00 87.25 88.74 88.44 88.02 91.15 83.08 83.80 89.62 74.37 38.63 17.78 36.25 79.18 87.32 64.30 73.18 59.86 71.43 56.00 62.94 18.57 66.56 65.74 70.76 67.87 74.04 23.45 33.98 25.81 10 Adapter layers, d = 2048 82.62 49.86 92.55 91.30 87.99 88.46 88.35 88.36 91.40 83.63 83.18 90.66 76.53 39.44 18.30 37.06 79.40 87.36 68.61 74.53 88.00 91.07 58.00 61.10 18.89 66.73 66.06 73.29 71.16 75.96 25.64 36.92 26.93 10 Gradual Unfreezing 82.50 51.74 91.97 92.61 89.71 87.27 86.90 88.26 91.35 83.42 83.49 89.71 75.09 40.88 18.95 38.40 79.17 87.30 70.79 75.51 93.09 94.64 70.00 62.03 21.51 65.69 64.79 72.92 69.12 77.89 26.71 39.02 26.93"}
{"doc_id": "1910.10683", "para_id": 298, "text": "11 ⋆Baseline (pre-train/fine-tune) 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 11 Equal 76.13 39.47 90.94 82.90 75.74 78.83 78.44 86.45 89.71 82.08 82.92 90.13 59.93 40.95 19.02 38.39 76.51 85.61 63.37 73.06 82.37 83.93 65.00 60.89 17.52 60.51 59.70 61.01 60.03 65.38 23.89 34.31 26.78 11 Examples-proportional, K = 216 80.45 42.07 91.97 90.97 87.50 85.41 85.04 86.89 90.10 83.01 83.66 90.74 72.56 41.16 19.04 38.59 77.25 85.72 69.95 76.67 86.38 89.29 70.00 65.93 27.91 62.78 61.95 76.90 65.83 73.08 24.35 34.99 27.10 11 Examples-proportional, K = 217 81.56 47.35 91.40 91.55 88.24 86.15 85.93 86.94 90.06 82.76 84.12 90.79 75.09 41.06 19.12 38.47 77.00 85.87 67.91 77.89 77.54 85.71 57.00 67.78 27.07 61.51 60.54 79.06 65.20 74.04 24.36 35.00 27.25 11 Examples-proportional, K = 218 81.67 46.85 91.63 91.99 88.73 87.68 87.20 86.93 90.35 83.30 84.01 91.47 73.29 40.96 19.07 38.43 78.17 86.74 67.94 76.57 78.88 87.50 62.00 67.70 30.85 63.43 62.54 76.53 65.67 67.31 24.57 35.19 27.39 11 Examples-proportional, K = 219 81.42 45.94 91.63 92.20 89.22 88.44 88.32 86.84 90.10 83.73 84.29 91.84 70.40 41.26 19.24 38.71 79.78 88.15 67.30 75.66 75.59 87.50 59.00 68.22 30.64 65.32 64.29 73.65 65.05 69.23 25.21 36.30 27.76 11 Examples-proportional, K = 220 80.80 42.55 92.78 91.27 87.99 88.36 88.10 86.10 89.62 84.15 84.26 92.20 68.95 41.05 19.24 38.46 80.36 88.27 67.38 73.21 76.18 83.93 62.00 67.57 26.86 66.12 65.22 76.90 64.73 69.23 25.66 36.93 27.68 11 Examples-proportional, K = 221 79.83 44.45 91.28 89.00 84.31 87.54 87.40 84.93 88.53 82.54 84.16 90.85 67.87 40.51 18.79 37.92 79.50 87.48 65.10 71.16 68.88 85.71 57.00 62.75 23.40 64.50 63.65 72.92 64.11 71.15 25.82 37.22 27.13 11 Temperature-scaled, T = 2 81.90 54.00 91.74 90.56 86.76 85.11 84.60 86.40 89.74 83.47 84.15 91.51 72.56 41.09 19.28 38.54 79.42 87.77 69.92 76.73 92.37 92.86 57.00 69.80 31.90 66.65 65.74 72.92 67.08 75.96 25.42 36.72 27.20 11 Temperature-scaled, T = 4 80.56 45.38 91.97 89.68 85.78 83.13 82.76 86.39 90.00 82.78 84.19 91.16 73.65 41.09 19.22 38.51 77.99 86.81 69.54 76.76 97.36 96.43 59.00 68.10 31.48 64.26 63.27 74.73 64.26 71.15 25.04 35.82 27.45 11 Temperature-scaled, T = 8 77.21 40.07 91.06 88.11 83.33 79.20 79.06 86.60 89.90 83.05 83.56 90.21 59.93 41.01 19.10 38.40 77.14 85.99 66.07 73.94 93.70 94.64 60.00 66.36 26.86 63.46 62.60 62.09 63.32 65.38 24.55 35.35 27.17"}
{"doc_id": "1910.10683", "para_id": 299, "text": "12 ⋆Unsupervised pre-training + fine-tuning 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 12 Multi-task training 81.42 45.94 91.63 92.20 89.22 88.44 88.32 86.84 90.10 83.73 84.29 91.84 70.40 41.26 19.24 38.71 79.78 88.15 67.30 75.66 75.59 87.50 59.00 68.22 30.64 65.32 64.29 73.65 65.05 69.23 25.21 36.30 27.76 12 Multi-task pre-training + fine-tuning 83.11 51.42 92.66 91.73 88.73 88.06 87.70 88.61 91.61 84.09 84.31 91.85 76.53 41.15 19.12 38.59 80.26 88.50 71.03 79.54 81.69 87.50 65.00 70.72 31.48 65.94 65.03 81.23 68.18 73.08 27.08 39.80 28.07 12 Leave-one-out multi-task training 81.98 48.00 93.23 91.72 88.24 87.76 87.32 88.61 91.44 84.00 84.11 90.79 72.20 41.34 19.05 38.77 79.97 88.10 71.68 78.35 86.76 89.29 66.00 68.09 29.49 66.23 65.27 79.06 68.65 78.85 26.93 39.79 27.87 12 Supervised multi-task pre-training 79.93 36.60 92.43 91.58 88.24 87.03 86.78 88.15 91.20 82.87 83.16 90.13 70.76 41.12 18.96 38.49 77.38 85.65 65.36 75.66 68.87 83.93 58.00 64.81 21.93 55.37 54.61 71.12 67.40 75.96 26.81 40.13 28.04"}
{"doc_id": "1910.10683", "para_id": 300, "text": "13 ⋆Baseline 83.28 53.84 92.68 92.07 88.92 88.02 87.94 88.67 91.56 84.24 84.57 90.48 76.28 41.33 19.24 38.77 80.88 88.81 71.36 76.62 91.22 91.96 66.20 66.13 25.78 69.05 68.16 75.34 68.04 78.56 26.98 39.82 27.65 13 1× size, 4× training steps 85.33 60.29 93.81 94.06 91.67 89.42 89.25 89.15 91.87 86.01 85.70 91.63 78.34 41.52 19.33 38.96 82.45 90.19 74.72 79.17 94.75 92.86 71.00 67.34 29.70 72.63 71.59 78.34 72.10 82.69 27.08 40.66 27.93 13 1× size, 4× batch size 84.60 56.08 93.12 92.31 89.22 88.85 88.84 89.35 92.07 85.98 86.13 91.07 80.14 41.70 19.42 39.08 82.52 90.21 74.64 78.78 93.69 94.64 72.00 68.09 30.95 74.73 73.90 76.53 70.06 81.73 27.07 40.60 27.84 13 2× size, 2× training steps 86.18 62.04 93.69 93.36 90.69 89.18 89.23 89.35 92.05 87.23 87.05 92.68 81.95 41.74 19.66 39.14 84.18 91.29 77.18 80.98 97.36 96.43 74.00 71.34 35.68 77.11 76.34 80.51 69.28 85.58 27.52 41.03 28.19 13 4× size, 1× training steps 85.91 57.58 94.38 92.67 89.95 89.60 89.60 89.44 92.14 87.05 87.12 93.12 83.39 41.60 19.73 39.08 83.86 91.32 78.04 81.38 89.09 94.64 73.00 73.74 40.40 78.25 77.40 81.59 70.22 91.35 27.47 40.71 28.10 13 4× ensembled 84.77 56.14 93.46 93.31 90.67 89.71 89.60 89.62 92.24 86.22 86.53 91.60 77.98 42.10 20.10 39.56 83.09 90.40 71.74 77.58 89.85 91.07 66.00 69.32 29.49 72.67 71.94 76.90 69.12 72.12 28.05 40.53 28.09 13 4× ensembled, fine-tune only 84.05 54.78 92.78 93.15 90.44 88.34 88.12 89.27 91.97 85.33 85.88 90.98 77.62 41.66 19.57 39.12 82.36 89.86 71.56 77.43 90.07 92.86 69.00 67.31 26.34 70.47 69.64 75.45 68.18 74.04 27.55 40.22 28.09"}
{"doc_id": "1910.10683", "para_id": 301, "text": "Table 16: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with ⋆denotes our baseline model (described in Section 3.1)."}
{"doc_id": "1910.10683", "para_id": 302, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019."}
{"doc_id": "1910.10683", "para_id": 303, "text": "Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efficient adaptive optimization for large-scale learning. arXiv preprint arXiv:1901.11150, 2019."}
{"doc_id": "1910.10683", "para_id": 304, "text": "Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multi- lingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019."}
{"doc_id": "1910.10683", "para_id": 305, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016."}
{"doc_id": "1910.10683", "para_id": 306, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze- driven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019."}
{"doc_id": "1910.10683", "para_id": 307, "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Third International Conference on Learning Representations, 2015."}
{"doc_id": "1910.10683", "para_id": 308, "text": "Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural machine translation. arXiv preprint arXiv:1909.08478, 2019."}
{"doc_id": "1910.10683", "para_id": 309, "text": "Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019."}
{"doc_id": "1910.10683", "para_id": 310, "text": "Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo- hannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014."}
{"doc_id": "1910.10683", "para_id": 311, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015."}
{"doc_id": "1910.10683", "para_id": 312, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation, 2016."}
{"doc_id": "1910.10683", "para_id": 313, "text": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015."}
{"doc_id": "1910.10683", "para_id": 314, "text": "Christian Buck, Kenneth Heafield, and Bas Van Ooyen. N-gram counts and language models from the common crawl. In LREC, 2014."}
{"doc_id": "1910.10683", "para_id": 315, "text": "Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "1910.10683", "para_id": 316, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017."}
{"doc_id": "1910.10683", "para_id": 317, "text": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1910.10683", "para_id": 318, "text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019."}
{"doc_id": "1910.10683", "para_id": 319, "text": "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020."}
{"doc_id": "1910.10683", "para_id": 320, "text": "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. arXiv preprint arXiv:1803.05449, 2018."}
{"doc_id": "1910.10683", "para_id": 321, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Super- vised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017."}
{"doc_id": "1910.10683", "para_id": 322, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005."}
{"doc_id": "1910.10683", "para_id": 323, "text": "Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 324, "text": "Marie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. In Sinn und Bedeutung 23, 2019."}
{"doc_id": "1910.10683", "para_id": 325, "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 2009."}
{"doc_id": "1910.10683", "para_id": 326, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre- training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."}
{"doc_id": "1910.10683", "para_id": 327, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para- phrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005."}
{"doc_id": "1910.10683", "para_id": 328, "text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197, 2019."}
{"doc_id": "1910.10683", "para_id": 329, "text": "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "1910.10683", "para_id": 330, "text": "Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. arXiv preprint arXiv:1802.06893, 2018."}
{"doc_id": "1910.10683", "para_id": 331, "text": "Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013."}
{"doc_id": "1910.10683", "para_id": 332, "text": "Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus with free license. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 914–922, 2016."}
{"doc_id": "1910.10683", "para_id": 333, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016."}
{"doc_id": "1910.10683", "para_id": 334, "text": "Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv preprint arXiv:1811.08883, 2018."}
{"doc_id": "1910.10683", "para_id": 335, "text": "Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for commonsense reasoning. arXiv preprint arXiv:1907.11983, 2019."}
{"doc_id": "1910.10683", "para_id": 336, "text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in neural information processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 337, "text": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "1910.10683", "para_id": 338, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016."}
{"doc_id": "1910.10683", "para_id": 339, "text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015."}
{"doc_id": "1910.10683", "para_id": 340, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. arXiv preprint arXiv:1902.00751, 2019."}
{"doc_id": "1910.10683", "para_id": 341, "text": "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifi- cation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1910.10683", "para_id": 342, "text": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Dou- glas Eck. Music transformer: Generating music with long-term structure. In Seventh International Conference on Learning Representations, 2018a."}
{"doc_id": "1910.10683", "para_id": 343, "text": "Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965, 2018b."}
{"doc_id": "1910.10683", "para_id": 344, "text": "Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016."}
{"doc_id": "1910.10683", "para_id": 345, "text": "Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First Quora dataset release: Question pairs. https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs, 2017."}
{"doc_id": "1910.10683", "para_id": 346, "text": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, 2014."}
{"doc_id": "1910.10683", "para_id": 347, "text": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019."}
{"doc_id": "1910.10683", "para_id": 348, "text": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017."}
{"doc_id": "1910.10683", "para_id": 349, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019."}
{"doc_id": "1910.10683", "para_id": 350, "text": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1910.10683", "para_id": 351, "text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014."}
{"doc_id": "1910.10683", "para_id": 352, "text": "Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019a."}
{"doc_id": "1910.10683", "para_id": 353, "text": "Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question answering and text classification via span extraction. arXiv preprint arXiv:1904.09286, 2019b."}
{"doc_id": "1910.10683", "para_id": 354, "text": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018."}
{"doc_id": "1910.10683", "para_id": 355, "text": "Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 356, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for Winograd schema challenge. arXiv preprint arXiv:1905.06290, 2019."}
{"doc_id": "1910.10683", "para_id": 357, "text": "Jakub Konečn`y, Brendan McMahan, and Daniel Ramage. Federated optimization: Dis- tributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015."}
{"doc_id": "1910.10683", "para_id": 358, "text": "Jakub Konečn`y, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016."}
{"doc_id": "1910.10683", "para_id": 359, "text": "Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better ImageNet models transfer better? arXiv preprint arXiv:1805.08974, 2018."}
{"doc_id": "1910.10683", "para_id": 360, "text": "Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014."}
{"doc_id": "1910.10683", "para_id": 361, "text": "Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018."}
{"doc_id": "1910.10683", "para_id": 362, "text": "Taku Kudo and John Richardson. SentencePiece: A simple and language independent sub- word tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018."}
{"doc_id": "1910.10683", "para_id": 363, "text": "Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019."}
{"doc_id": "1910.10683", "para_id": 364, "text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representa- tions. arXiv preprint arXiv:1909.11942, 2019."}
{"doc_id": "1910.10683", "para_id": 365, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012."}
{"doc_id": "1910.10683", "para_id": 366, "text": "Qi Li. Literature survey: domain adaptation algorithms for natural language processing. 2012."}
{"doc_id": "1910.10683", "para_id": 367, "text": "Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text summarization branches out, 2004."}
{"doc_id": "1910.10683", "para_id": 368, "text": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018."}
{"doc_id": "1910.10683", "para_id": 369, "text": "Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization using length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998, 2019a."}
{"doc_id": "1910.10683", "para_id": 370, "text": "Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Rep- resentation learning using multi-task deep neural networks for semantic classification and information retrieval. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015."}
{"doc_id": "1910.10683", "para_id": 371, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019b."}
{"doc_id": "1910.10683", "para_id": 372, "text": "Yang Liu. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318, 2019."}
{"doc_id": "1910.10683", "para_id": 373, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019c."}
{"doc_id": "1910.10683", "para_id": 374, "text": "Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893, 2018."}
{"doc_id": "1910.10683", "para_id": 375, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), 2018."}
{"doc_id": "1910.10683", "para_id": 376, "text": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The nat- ural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "1910.10683", "para_id": 377, "text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013a."}
{"doc_id": "1910.10683", "para_id": 378, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 2013b."}
{"doc_id": "1910.10683", "para_id": 379, "text": "Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond. arXiv preprint arXiv:1602.06023, 2016."}
{"doc_id": "1910.10683", "para_id": 380, "text": "Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2014."}
{"doc_id": "1910.10683", "para_id": 381, "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002."}
{"doc_id": "1910.10683", "para_id": 382, "text": "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1910.10683", "para_id": 383, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014."}
{"doc_id": "1910.10683", "para_id": 384, "text": "Matthew Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting pretrained representations to diverse tasks. arXiv preprint arXiv:1903.05987, 2019."}
{"doc_id": "1910.10683", "para_id": 385, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018."}
{"doc_id": "1910.10683", "para_id": 386, "text": "Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Sup- plementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018."}
{"doc_id": "1910.10683", "para_id": 387, "text": "Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "1910.10683", "para_id": 388, "text": "Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018."}
{"doc_id": "1910.10683", "para_id": 389, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018."}
{"doc_id": "1910.10683", "para_id": 390, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019."}
{"doc_id": "1910.10683", "para_id": 391, "text": "Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the Winograd schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012."}
{"doc_id": "1910.10683", "para_id": 392, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016."}
{"doc_id": "1910.10683", "para_id": 393, "text": "Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016."}
{"doc_id": "1910.10683", "para_id": 394, "text": "Alex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré. Snorkel MeTaL: Weak supervision for multi-task learning. In Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning, 2018."}
{"doc_id": "1910.10683", "para_id": 395, "text": "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011."}
{"doc_id": "1910.10683", "para_id": 396, "text": "Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017."}
{"doc_id": "1910.10683", "para_id": 397, "text": "Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI Galway, 2019."}
{"doc_id": "1910.10683", "para_id": 398, "text": "Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning in natural language processing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials, pages 15–18, 2019."}
{"doc_id": "1910.10683", "para_id": 399, "text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International journal of computer vision, 2015."}
{"doc_id": "1910.10683", "para_id": 400, "text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."}
{"doc_id": "1910.10683", "para_id": 401, "text": "Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017."}
{"doc_id": "1910.10683", "para_id": 402, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1910.10683", "para_id": 403, "text": "Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018."}
{"doc_id": "1910.10683", "para_id": 404, "text": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018."}
{"doc_id": "1910.10683", "para_id": 405, "text": "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv preprint arXiv:1804.04235, 2018."}
{"doc_id": "1910.10683", "para_id": 406, "text": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1910.10683", "para_id": 407, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, 2018."}
{"doc_id": "1910.10683", "para_id": 408, "text": "Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison- Burch, and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013."}
{"doc_id": "1910.10683", "para_id": 409, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, 2013."}
{"doc_id": "1910.10683", "para_id": 410, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "1910.10683", "para_id": 411, "text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 2014."}
{"doc_id": "1910.10683", "para_id": 412, "text": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018."}
{"doc_id": "1910.10683", "para_id": 413, "text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 414, "text": "Richard S. Sutton. The bitter lesson. http://www.incompleteideas.net/IncIdeas/ BitterLesson.html, 2019."}
{"doc_id": "1910.10683", "para_id": 415, "text": "Wilson L. Taylor. “Cloze procedure”: A new tool for measuring readability. Journalism Bulletin, 1953."}
{"doc_id": "1910.10683", "para_id": 416, "text": "Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018."}
{"doc_id": "1910.10683", "para_id": 417, "text": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. arXiv preprint arXiv:1611.09830, 2016."}
{"doc_id": "1910.10683", "para_id": 418, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017."}
{"doc_id": "1910.10683", "para_id": 419, "text": "Elena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. arXiv preprint arXiv:1909.01380, 2019."}
{"doc_id": "1910.10683", "para_id": 420, "text": "Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018."}
{"doc_id": "1910.10683", "para_id": 421, "text": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Can you tell me how to get past Sesame Street? Sentence-level pretraining beyond language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019a."}
{"doc_id": "1910.10683", "para_id": 422, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general- purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019b."}
{"doc_id": "1910.10683", "para_id": 423, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT: Incorporating language structures into pre-training for deep language understanding. arXiv preprint arXiv:1908.04577, 2019c."}
{"doc_id": "1910.10683", "para_id": 424, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1910.10683", "para_id": 425, "text": "Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017."}
{"doc_id": "1910.10683", "para_id": 426, "text": "Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1989."}
{"doc_id": "1910.10683", "para_id": 427, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016."}
{"doc_id": "1910.10683", "para_id": 428, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019."}
{"doc_id": "1910.10683", "para_id": 429, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 430, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. QAnet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018."}
{"doc_id": "1910.10683", "para_id": 431, "text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes- ner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019."}
{"doc_id": "1910.10683", "para_id": 432, "text": "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018."}
{"doc_id": "1910.10683", "para_id": 433, "text": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: En- hanced adversarial training for language understanding. arXiv preprint arXiv:1909.11764, 2019."}
{"doc_id": "1910.10683", "para_id": 434, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla- nations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, 2015."}
{"doc_id": "2106.09685", "para_id": 0, "text": "LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS"}
{"doc_id": "2106.09685", "para_id": 1, "text": "Edward Hu∗ Yelong Shen∗ Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen Microsoft Corporation {edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com yuanzhil@andrew.cmu.edu (Version 2)"}
{"doc_id": "2106.09685", "para_id": 2, "text": "An important paradigm of natural language processing consists of large-scale pre- training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying indepen- dent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre- trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable pa- rameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁne- tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav- ing fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."}
{"doc_id": "2106.09685", "para_id": 3, "text": "Many applications in natural language processing rely on adapt- ing one large-scale, pre-trained language model to multiple down- stream applications. Such adaptation is usually done via ﬁne-tuning, which updates all the parameters of the pre-trained model. The ma- jor downside of ﬁne-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere “inconvenience” for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters.1"}
{"doc_id": "2106.09685", "para_id": 4, "text": "Many sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-speciﬁc parameters in ad- dition to the pre-trained model for each task, greatly boosting the operational efﬁciency when deployed. However, existing techniques"}
{"doc_id": "2106.09685", "para_id": 5, "text": "Figure 1: Our reparametriza- tion. We only train A and B."}
{"doc_id": "2106.09685", "para_id": 6, "text": "∗Equal contribution. 0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1While GPT-3 175B achieves non-trivial performance with few-shot learning, ﬁne-tuning boosts its perfor- mance signiﬁcantly as shown in Appendix A."}
{"doc_id": "2106.09685", "para_id": 7, "text": "often introduce inference latency (Houlsby et al., 2019; Rebufﬁet al., 2017) by extending model depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham- bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality."}
{"doc_id": "2106.09685", "para_id": 8, "text": "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers’ change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) sufﬁces even when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient."}
{"doc_id": "2106.09685", "para_id": 9, "text": "• A pre-trained model can be shared and used to build many small LoRA modules for dif- ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the matrices A and B in Figure 1, reducing the storage requirement and task-switching over- head signiﬁcantly."}
{"doc_id": "2106.09685", "para_id": 10, "text": "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices."}
{"doc_id": "2106.09685", "para_id": 11, "text": "• Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by construction."}
{"doc_id": "2106.09685", "para_id": 12, "text": "• LoRA is orthogonal to many prior methods and can be combined with many of them, such as preﬁx-tuning. We provide an example in Appendix E."}
{"doc_id": "2106.09685", "para_id": 13, "text": "Terminologies and Conventions We make frequent references to the Transformer architecture and use the conventional terminologies for its dimensions. We call the input and output di- mension size of a Transformer layer dmodel. We use Wq, Wk, Wv, and Wo to refer to the query/key/value/output projection matrices in the self-attention module. W or W0 refers to a pre- trained weight matrix and ∆W its accumulated gradient update during adaptation. We use r to denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017; Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model optimization and use a Transformer MLP feedforward dimension dffn = 4 × dmodel."}
{"doc_id": "2106.09685", "para_id": 14, "text": "While our proposal is agnostic to training objective, we focus on language modeling as our motivat- ing use case. Below is a brief description of the language modeling problem and, in particular, the maximization of conditional probabilities given a task-speciﬁc prompt."}
{"doc_id": "2106.09685", "para_id": 15, "text": "Suppose we are given a pre-trained autoregressive language model PΦ(y|x) parametrized by Φ. For instance, PΦ(y|x) can be a generic multi-task learner such as GPT (Radford et al., b; Brown et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this pre-trained model to downstream conditional text generation tasks, such as summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is represented by a training dataset of context-target pairs: Z = {(xi, yi)}i=1,..,N, where both xi and yi are sequences of tokens. For example, in NL2SQL, xi is a natural language query and yi its corresponding SQL command; for summarization, xi is the content of an article and yi its summary."}
{"doc_id": "2106.09685", "para_id": 16, "text": "During full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ by repeatedly following the gradient to maximize the conditional language modeling objective:"}
{"doc_id": "2106.09685", "para_id": 17, "text": "One of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different set of parameters ∆Φ whose dimension |∆Φ| equals |Φ0|. Thus, if the pre-trained model is large (such as GPT-3 with |Φ0| ≈175 Billion), storing and deploying many independent instances of ﬁne-tuned models can be challenging, if at all feasible."}
{"doc_id": "2106.09685", "para_id": 18, "text": "In this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter increment ∆Φ = ∆Φ(Θ) is further encoded by a much smaller-sized set of parameters Θ with |Θ| ≪|Φ0|. The task of ﬁnding ∆Φ thus becomes optimizing over Θ:"}
{"doc_id": "2106.09685", "para_id": 19, "text": "In the subsequent sections, we propose to use a low-rank representation to encode ∆Φ that is both compute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train- able parameters |Θ| can be as small as 0.01% of |Φ0|."}
{"doc_id": "2106.09685", "para_id": 20, "text": "The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec- tion 6 for a survey of some of the well-known works. Using language modeling as an example, there are two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby et al., 2019; Rebufﬁet al., 2017; Pfeiffer et al., 2021; R¨uckl´e et al., 2020) or optimizing some forms of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). However, both strategies have their limitations, especially in a large-scale and latency-sensitive production scenario."}
{"doc_id": "2106.09685", "para_id": 21, "text": "Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus on the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block and a more recent one by Lin et al. (2020) which has only one per block but with an additional LayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit- ing multi-task settings (R¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed to have few parameters (sometimes <1% of the original model) by having a small bottleneck di- mension, which limits the FLOPs they can add. However, large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one. In a generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b) medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension (Table 1)."}
{"doc_id": "2106.09685", "para_id": 22, "text": "This problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep- ikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as AllReduce and Broadcast, unless we store the adapter parameters redundantly many times."}
{"doc_id": "2106.09685", "para_id": 23, "text": "Directly Optimizing the Prompt is Hard The other direction, as exempliﬁed by preﬁx tuning (Li & Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize and that its performance changes non-monotonically in trainable parameters, conﬁrming similar observations in the original paper. More fundamentally, reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods. We defer the study on task performance to Section 5."}
{"doc_id": "2106.09685", "para_id": 24, "text": "Batch Size 32 16 1 Sequence Length 512 256 128 |Θ| 0.5M 11M 11M"}
{"doc_id": "2106.09685", "para_id": 25, "text": "AdapterL 1482.0±1.0 (+2.2%) 354.8±0.5 (+5.0%) 23.9±2.1 (+20.7%) AdapterH 1492.2±1.0 (+3.0%) 366.3±0.5 (+8.4%) 25.8±2.2 (+30.3%)"}
{"doc_id": "2106.09685", "para_id": 26, "text": "Table 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av- eraged over 100 trials. We use an NVIDIA Quadro RTX8000. “|Θ|” denotes the number of trainable parameters in adapter layers. AdapterL and AdapterH are two variants of adapter tuning, which we describe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an online, short-sequence-length scenario. See the full study in Appendix B."}
{"doc_id": "2106.09685", "para_id": 27, "text": "We describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case."}
{"doc_id": "2106.09685", "para_id": 28, "text": "A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low “instrisic dimension” and can still learn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe- size the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained weight matrix W0 ∈Rd×k, we constrain its update by representing the latter with a low-rank de- composition W0 + ∆W = W0 + BA, where B ∈Rd×r, A ∈Rr×k, and the rank r ≪min(d, k). During training, W0 is frozen and does not receive gradient updates, while A and B contain trainable parameters. Note both W0 and ∆W = BA are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For h = W0x, our modiﬁed forward pass yields:"}
{"doc_id": "2106.09685", "para_id": 29, "text": "We illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training. We then scale ∆Wx by α"}
{"doc_id": "2106.09685", "para_id": 30, "text": "r , where α is a constant in r. When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set α to the ﬁrst r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r (Yang & Hu, 2021)."}
{"doc_id": "2106.09685", "para_id": 31, "text": "A Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of a subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu- lated gradient update to weight matrices to have full-rank during adaptation. This means that when applying LoRA to all weight matrices and training all biases2, we roughly recover the expressive- ness of full ﬁne-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters 3, training LoRA roughly converges to training the original model, while adapter-based methods converges to an MLP and preﬁx-based methods to a model that cannot take long input sequences."}
{"doc_id": "2106.09685", "para_id": 32, "text": "No Additional Inference Latency. When deployed in production, we can explicitly compute and store W = W0 + BA and perform inference as usual. Note that both W0 and BA are in Rd×k. When we need to switch to another downstream task, we can recover W0 by subtracting BA and then adding a different B′A′, a quick operation with very little memory overhead. Critically, this"}
{"doc_id": "2106.09685", "para_id": 33, "text": "2They represent a negligible number of parameters compared to weights. 3An inevitability when adapting to hard tasks."}
{"doc_id": "2106.09685", "para_id": 34, "text": "guarantees that we do not introduce any additional latency during inference compared to a ﬁne-tuned model by construction."}
{"doc_id": "2106.09685", "para_id": 35, "text": "In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv) as a single matrix of dimension dmodel ×dmodel, even though the output dimension is usually sliced into attention heads. We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efﬁciency.We further study the effect on adapting different types of attention weight matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work."}
{"doc_id": "2106.09685", "para_id": 36, "text": "Practical Beneﬁts and Limitations. The most signiﬁcant beneﬁt comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r ≪dmodel as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000× (from 350GB to 35MB)4. This allows us to train with signiﬁ- cantly fewer GPUs and avoid I/O bottlenecks. Another beneﬁt is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the ﬂy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full ﬁne-tuning5 as we do not need to calculate the gradient for the vast majority of the parameters."}
{"doc_id": "2106.09685", "para_id": 37, "text": "LoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks with different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate additional inference latency. Though it is possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not critical."}
{"doc_id": "2106.09685", "para_id": 38, "text": "We evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De- BERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown et al., 2020). Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Speciﬁcally, we evaluate on the GLUE (Wang et al., 2019) benchmark for RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com- parison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al., 2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments."}
{"doc_id": "2106.09685", "para_id": 39, "text": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. This, however, means that some baselines might only appear in certain experiments."}
{"doc_id": "2106.09685", "para_id": 40, "text": "Fine-Tuning (FT) is a common approach for adaptation. During ﬁne-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple variant is to update only some layers while freezing others. We include one such baseline reported in prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers (FTTop2)."}
{"doc_id": "2106.09685", "para_id": 41, "text": "4We still need the 350GB model during deployment; however, storing 100 adapted models only requires 350GB + 35MB * 100 ≈354GB as opposed to 100 * 350GB ≈35TB. 5For GPT-3 175B, the training throughput for full ﬁne-tuning is 32.5 tokens/s per V100 GPU; with the same number of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA."}
{"doc_id": "2106.09685", "para_id": 42, "text": "Parameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg."}
{"doc_id": "2106.09685", "para_id": 43, "text": "RoBbase (FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4 RoBbase (BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2 RoBbase (AdptD)* 0.3M 87.1±.0 94.2±.1 88.5±1.1 60.8±.4 93.1±.1 90.2±.0 71.5±2.7 89.7±.3 84.4 RoBbase (AdptD)* 0.9M 87.3±.1 94.7±.3 88.4±.1 62.6±.9 93.0±.2 90.6±.0 75.9±2.2 90.3±.1 85.4 RoBbase (LoRA) 0.3M 87.5±.3 95.1±.2 89.7±.7 63.4±1.2 93.3±.3 90.8±.1 86.6±.7 91.5±.2 87.2"}
{"doc_id": "2106.09685", "para_id": 44, "text": "RoBlarge (FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9 RoBlarge (LoRA) 0.8M 90.6±.2 96.2±.5 90.9±1.2 68.2±1.9 94.9±.3 91.6±.1 87.4±2.5 92.6±.2 89.0"}
{"doc_id": "2106.09685", "para_id": 45, "text": "RoBlarge (AdptP)† 3.0M 90.2±.3 96.1±.3 90.2±.7 68.3±1.0 94.8±.2 91.9±.1 83.8±2.9 92.1±.7 88.4 RoBlarge (AdptP)† 0.8M 90.5±.3 96.6±.2 89.7±1.2 67.8±2.5 94.8±.3 91.7±.2 80.1±2.9 91.9±.4 87.9 RoBlarge (AdptH)† 6.0M 89.9±.5 96.2±.3 88.7±2.9 66.5±4.4 94.7±.2 92.1±.1 83.4±1.1 91.0±1.7 87.8 RoBlarge (AdptH)† 0.8M 90.3±.3 96.3±.5 87.7±1.7 66.3±2.0 94.7±.2 91.5±.1 72.9±2.9 91.5±.5 86.4 RoBlarge (LoRA)† 0.8M 90.6±.2 96.2±.5 90.2±1.0 68.2±1.9 94.8±.3 91.6±.2 85.2±1.1 92.3±.5 88.6"}
{"doc_id": "2106.09685", "para_id": 46, "text": "DeBXXL (FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1 DeBXXL (LoRA) 4.7M 91.9±.2 96.9±.2 92.6±.6 72.4±1.1 96.0±.1 92.9±.1 94.9±.4 93.0±.2 91.3"}
{"doc_id": "2106.09685", "para_id": 47, "text": "Table 2: RoBERTabase, RoBERTalarge, and DeBERTaXXL with different adaptation methods on the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. * indicates numbers published in prior works. † indicates runs conﬁgured in a setup similar to Houlsby et al. (2019) for a fair comparison."}
{"doc_id": "2106.09685", "para_id": 48, "text": "Bias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else. Contemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021)."}
{"doc_id": "2106.09685", "para_id": 49, "text": "Preﬁx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe- cial tokens have trainable word embeddings and are generally not in the model’s vocabulary. Where to place such tokens can have an impact on performance. We focus on “preﬁxing”, which prepends such tokens to the prompt, and “inﬁxing”, which appends to the prompt; both are discussed in Li & Liang (2021). We use lp (resp. li) denote the number of preﬁx (resp. inﬁx) tokens. The number of trainable parameters is |Θ| = dmodel × (lp + li)."}
{"doc_id": "2106.09685", "para_id": 50, "text": "Preﬁx-layer tuning (PreLayer) is an extension to preﬁx-embedding tuning. Instead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer. The activations computed from pre- vious layers are simply replaced by trainable ones. The resulting number of trainable parameters is |Θ| = L × dmodel × (lp + li), where L is the number of Transformer layers."}
{"doc_id": "2106.09685", "para_id": 51, "text": "Adapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self- attention module (and the MLP module) and the subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between. We call this original design AdapterH. Recently, Lin et al. (2020) proposed a more efﬁcient design with the adapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This is very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also include another baseline call AdapterDrop (R¨uckl´e et al., 2020) which drops some adapter layers for greater efﬁciency (AdapterD). We cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the ﬁrst column. In all cases, we have |Θ| = ˆLAdpt ×(2×dmodel ×r+r+dmodel)+2× ˆLLN ×dmodel where ˆLAdpt is the number of adapter layers and ˆLLN the number of trainable LayerNorms (e.g., in AdapterL)."}
{"doc_id": "2106.09685", "para_id": 52, "text": "LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to Wq and Wv in most experiments for simplicity. The number of trainable parameters is determined by the rank r and the shape of the original weights: |Θ| = 2 × ˆLLoRA × dmodel × r, where ˆLLoRA is the number of weight matrices we apply LoRA to."}
{"doc_id": "2106.09685", "para_id": 53, "text": "Model & Method # Trainable E2E NLG Challenge Parameters BLEU NIST MET ROUGE-L CIDEr"}
{"doc_id": "2106.09685", "para_id": 54, "text": "GPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47 GPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40 GPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47 GPT-2 M (AdapterH) 11.09M 67.3±.6 8.50±.07 46.0±.2 70.7±.2 2.44±.01 GPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41 GPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49 GPT-2 M (LoRA) 0.35M 70.4±.1 8.85±.02 46.8±.2 71.8±.1 2.53±.02 GPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45 GPT-2 L (AdapterL) 0.88M 69.1±.1 8.68±.03 46.3±.0 71.4±.2 2.49±.0 GPT-2 L (AdapterL) 23.00M 68.9±.3 8.70±.04 46.1±.1 71.3±.2 2.45±.02 GPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47 GPT-2 L (LoRA) 0.77M 70.4±.1 8.89±.02 46.8±.2 72.0±.2 2.47±.02"}
{"doc_id": "2106.09685", "para_id": 55, "text": "Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Conﬁdence intervals are shown for experiments we ran. * indicates numbers published in prior works."}
{"doc_id": "2106.09685", "para_id": 56, "text": "RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter’s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efﬁcient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the ﬁne-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 57, "text": "DeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su- perGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully ﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section). See Section D.2 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 58, "text": "Having shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3."}
{"doc_id": "2106.09685", "para_id": 59, "text": "Model&Method # Trainable WikiSQL MNLI-m SAMSum Parameters Acc. (%) Acc. (%) R1/R2/RL"}
{"doc_id": "2106.09685", "para_id": 60, "text": "GPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5 GPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5 GPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5 GPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5 GPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8 GPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1"}
{"doc_id": "2106.09685", "para_id": 61, "text": "GPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9 GPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1"}
{"doc_id": "2106.09685", "para_id": 62, "text": "Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full ﬁne-tuning. The results on WikiSQL have a ﬂuctuation around ±0.5%, MNLI-m around ±0.1%, and SAMSum around ±0.2/±0.2/±0.1 for the three metrics."}
{"doc_id": "2106.09685", "para_id": 63, "text": "As a ﬁnal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 64, "text": "As shown in Table 4, LoRA matches or exceeds the ﬁne-tuning baseline on all three datasets. Note that not all methods beneﬁt monotonically from having more trainable parameters, as shown in Fig- ure 2. We observe a signiﬁcant performance drop when we use more than 256 special tokens for preﬁx-embedding tuning or more than 32 special tokens for preﬁx-layer tuning. This corroborates similar observations in Li & Liang (2021). While a thorough investigation into this phenomenon is out-of-scope for this work, we suspect that having more special tokens causes the input distri- bution to shift further away from the pre-training data distribution. Separately, we investigate the performance of different adaptation approaches in the low-data regime in Section F.3."}
{"doc_id": "2106.09685", "para_id": 65, "text": "Figure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation methods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance. See Section F.2 for more details on the plotted data points."}
{"doc_id": "2106.09685", "para_id": 66, "text": "Transformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence architecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan- guage modeling by using a stack of Transformer decoders. Since then, Transformer-based language models have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged with BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) – both are large Transformer lan-"}
{"doc_id": "2106.09685", "para_id": 67, "text": "guage models trained on a large amount of text – where ﬁne-tuning on task-speciﬁc data after pre- training on general domain data provides a signiﬁcant performance gain compared to training on task-speciﬁc data directly. Training larger Transformers generally results in better performance and remains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer language model trained to-date with 175B parameters."}
{"doc_id": "2106.09685", "para_id": 68, "text": "Prompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a few additional training examples, the result depends heavily on the input prompt (Brown et al., 2020). This necessitates an empirical art of composing and formatting the prompt to maximize a model’s performance on a desired task, which is known as prompt engineering or prompt hacking. Fine-tuning retrains a model pre-trained on general domains to a speciﬁc task Devlin et al. (2019b); Radford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b); Collobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream performance. However, the enormity of GPT-3 175B makes it challenging to perform ﬁne-tuning in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as pre-training."}
{"doc_id": "2106.09685", "para_id": 69, "text": "Parameter-Efﬁcient Adaptation. Many have proposed inserting adapter layers between existing layers in a neural network (Houlsby et al., 2019; Rebufﬁet al., 2017; Lin et al., 2020). Our method uses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The key functional difference is that our learned weights can be merged with the main weights during inference, thus not introducing any latency, which is not the case for the adapter layers (Section 3). A comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially parametrizes the adapter layers using Kronecker products with some predetermined weight sharing scheme. Similarly, combining LoRA with other tensor product-based methods could potentially improve its parameter efﬁciency, which we leave to future work. More recently, many proposed optimizing the input word embeddings in lieu of ﬁne-tuning, akin to a continuous and differentiable generalization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section. However, this line of works can only scale up by using more special tokens in the prompt, which take up available sequence length for task tokens when positional embeddings are learned."}
{"doc_id": "2106.09685", "para_id": 70, "text": "Low-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn- ing. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many deep learning tasks, especially those with a heavily over-parametrized neural network, the learned neural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works even explicitly impose the low-rank constraint when training the original neural network (Sainath et al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho- dak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works considers low-rank update to a frozen model for adaptation to downstream tasks. In theory liter- ature, it is known that neural networks outperform other classical learning methods, including the corresponding (ﬁnite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when the underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li, 2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that low-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed low-rank adaptation update is well-motivated by the literature."}
{"doc_id": "2106.09685", "para_id": 71, "text": "Given the empirical advantage of LoRA, we hope to further explain the properties of the low-rank adaptation learned from downstream tasks. Note that the low-rank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel, but also gives better interpretability of how the update weights are correlated with the pre-trained weights. We focus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters (up to 10,000×) without adversely affecting task performances."}
{"doc_id": "2106.09685", "para_id": 72, "text": "We perform a sequence of empirical studies to answer the following questions: 1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt"}
{"doc_id": "2106.09685", "para_id": 73, "text": "to maximize downstream performance? 2) Is the “optimal” adaptation matrix ∆W really rank- deﬁcient? If so, what is a good rank to use in practice? 3) What is the connection between ∆W and W? Does ∆W highly correlate with W? How large is ∆W comparing to W?"}
{"doc_id": "2106.09685", "para_id": 74, "text": "We believe that our answers to question (2) and (3) shed light on the fundamental principles of using pre-trained language models for downstream tasks, which is a critical topic in NLP."}
{"doc_id": "2106.09685", "para_id": 75, "text": "7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?"}
{"doc_id": "2106.09685", "para_id": 76, "text": "Given a limited parameter budget, which types of weights should we adapt with LoRA to obtain the best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight matrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored in FP16) on GPT-3 175B, which corresponds to r = 8 if we adapt one type of attention weights or r = 4 if we adapt two types, for all 96 layers. The result is presented in Table 5."}
{"doc_id": "2106.09685", "para_id": 77, "text": "Weight Type Wq Wk Wv Wo Wq, Wk Wq, Wv Wq, Wk, Wv, Wo Rank r 8 8 8 8 4 4 2"}
{"doc_id": "2106.09685", "para_id": 78, "text": "WikiSQL (±0.5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7 MultiNLI (±0.1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7"}
{"doc_id": "2106.09685", "para_id": 79, "text": "Table 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both Wq and Wv gives the best performance overall. We ﬁnd the standard deviation across random seeds to be consistent for a given dataset, which we report in the ﬁrst column."}
{"doc_id": "2106.09685", "para_id": 80, "text": "Note that putting all the parameters in ∆Wq or ∆Wk results in signiﬁcantly lower performance, while adapting both Wq and Wv yields the best result. This suggests that even a rank of four captures enough information in ∆W such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank."}
{"doc_id": "2106.09685", "para_id": 81, "text": "We turn our attention to the effect of rank r on model performance. We adapt {Wq, Wv}, {Wq, Wk, Wv, Wc}, and just Wq for a comparison."}
{"doc_id": "2106.09685", "para_id": 82, "text": "WikiSQL(±0.5%) Wq 68.8 69.6 70.5 70.4 70.0 Wq, Wv 73.4 73.3 73.7 73.8 73.5 Wq, Wk, Wv, Wo 74.1 73.7 74.0 74.0 73.9"}
{"doc_id": "2106.09685", "para_id": 83, "text": "Wq 90.7 90.9 91.1 90.7 90.7 Wq, Wv 91.3 91.4 91.3 91.6 91.4 Wq, Wk, Wv, Wo 91.2 91.7 91.7 91.5 91.4"}
{"doc_id": "2106.09685", "para_id": 84, "text": "Table 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a rank as small as one sufﬁces for adapting both Wq and Wv on these datasets while training Wq alone needs a larger r. We conduct a similar experiment on GPT-2 in Section H.2."}
{"doc_id": "2106.09685", "para_id": 85, "text": "Table 6 shows that, surprisingly, LoRA already performs competitively with a very small r (more so for {Wq, Wv} than just Wq). This suggests the update matrix ∆W could have a very small “intrinsic rank”.6 To further support this ﬁnding, we check the overlap of the subspaces learned by different choices of r and by different random seeds. We argue that increasing r does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufﬁcient."}
{"doc_id": "2106.09685", "para_id": 86, "text": "6However, we do not expect a small r to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with r = dmodel) could certainly outperform LoRA with a small r."}
{"doc_id": "2106.09685", "para_id": 87, "text": "Subspace similarity between different r. Given Ar=8 and Ar=64 which are the learned adapta- tion matrices with rank r = 8 and 64 using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices UAr=8 and UAr=64.7 We hope to an- swer: how much of the subspace spanned by the top i singular vectors in UAr=8 (for 1 ≤i ≤8) is contained in the subspace spanned by top j singular vectors of UAr=64 (for 1 ≤j ≤64)? We mea- sure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap- pendix G for a more formal discussion)"}
{"doc_id": "2106.09685", "para_id": 88, "text": "φ(Ar=8, Ar=64, i, j) = ||U i⊤ Ar=8U j Ar=64||2 F min(i, j) ∈[0, 1] (4)"}
{"doc_id": "2106.09685", "para_id": 89, "text": "where U i Ar=8 represents the columns of UAr=8 corresponding to the top-i singular vectors."}
{"doc_id": "2106.09685", "para_id": 90, "text": "φ(·) has a range of [0, 1], where 1 represents a complete overlap of subspaces and 0 a complete separation. See Figure 3 for how φ changes as we vary i and j. We only look at the 48th layer (out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown in Section H.1."}
{"doc_id": "2106.09685", "para_id": 91, "text": "Figure 3: Subspace similarity between column vectors of Ar=8 and Ar=64 for both ∆Wq and ∆Wv. The third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top directions in r = 8 are included in r = 64, and vice versa."}
{"doc_id": "2106.09685", "para_id": 92, "text": "Directions corresponding to the top singular vector overlap signiﬁcantly between Ar=8 and Ar=64, while others do not. Speciﬁcally, ∆Wv (resp. ∆Wq) of Ar=8 and ∆Wv (resp. ∆Wq) of Ar=64 share a subspace of dimension 1 with normalized similarity > 0.5, providing an explanation of why r = 1 performs quite well in our downstream tasks for GPT-3."}
{"doc_id": "2106.09685", "para_id": 93, "text": "Since both Ar=8 and Ar=64 are learned using the same pre-trained model, Figure 3 indicates that the top singular-vector directions of Ar=8 and Ar=64 are the most useful, while other directions potentially contain mostly random noises accumulated during training. Hence, the adaptation matrix can indeed have a very low rank."}
{"doc_id": "2106.09685", "para_id": 94, "text": "Subspace similarity between different random seeds. We further conﬁrm this by plotting the normalized subspace similarity between two randomly seeded runs with r = 64, shown in Figure 4. ∆Wq appears to have a higher “intrinsic rank” than ∆Wv, since more common singular value direc- tions are learned by both runs for ∆Wq, which is in line with our empirical observation in Table 6. As a comparison, we also plot two random Gaussian matrices, which do not share any common singular value directions with each other."}
{"doc_id": "2106.09685", "para_id": 95, "text": "7.3 HOW DOES THE ADAPTATION MATRIX ∆W COMPARE TO W ?"}
{"doc_id": "2106.09685", "para_id": 96, "text": "We further investigate the relationship between ∆W and W. In particular, does ∆W highly correlate with W? (Or mathematically, is ∆W mostly contained in the top singular directions of W?) Also,"}
{"doc_id": "2106.09685", "para_id": 97, "text": "7Note that a similar analysis can be carried out with B and the left-singular unitary matrices – we stick with A for our experiments."}
{"doc_id": "2106.09685", "para_id": 98, "text": "Figure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64 from two random seeds, for both ∆Wq and ∆Wv in the 48-th layer. Right: the same heat-map between the column vectors of two random Gaussian matrices. See Section H.1 for other layers."}
{"doc_id": "2106.09685", "para_id": 99, "text": "how “large” is ∆W comparing to its corresponding directions in W? This can shed light on the underlying mechanism for adapting pre-trained language models."}
{"doc_id": "2106.09685", "para_id": 100, "text": "To answer these questions, we project W onto the r-dimensional subspace of ∆W by comput- ing U ⊤WV ⊤, with U/V being the left/right singular-vector matrix of ∆W. Then, we com- pare the Frobenius norm between ∥U ⊤WV ⊤∥F and ∥W∥F . As a comparison, we also compute ∥U ⊤WV ⊤∥F by replacing U, V with the top r singular vectors of W or a random matrix."}
{"doc_id": "2106.09685", "para_id": 101, "text": "Table 7: The Frobenius norm of U ⊤WqV ⊤where U and V are the left/right top r singular vector directions of either (1) ∆Wq, (2) Wq, or (3) a random matrix. The weight matrices are taken from the 48th layer of GPT-3."}
{"doc_id": "2106.09685", "para_id": 102, "text": "We draw several conclusions from Table 7. First, ∆W has a stronger correlation with W compared to a random matrix, indicating that ∆W ampliﬁes some features that are already in W. Second, instead of repeating the top singular directions of W, ∆W only ampliﬁes directions that are not emphasized in W. Third, the ampliﬁcation factor is rather huge: 21.5 ≈6.91/0.32 for r = 4. See Section H.4 for why r = 64 has a smaller ampliﬁcation factor. We also provide a visualization in Section H.3 for how the correlation changes as we include more top singular directions from Wq. This suggests that the low-rank adaptation matrix potentially ampliﬁes the important features for speciﬁc downstream tasks that were learned but not emphasized in the general pre-training model."}
{"doc_id": "2106.09685", "para_id": 103, "text": "Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers."}
{"doc_id": "2106.09685", "para_id": 104, "text": "There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta- tion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning or LoRA is far from clear – how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-"}
{"doc_id": "2106.09685", "para_id": 105, "text": "tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are there more principled ways to do it? 4) Finally, the rank-deﬁciency of ∆W suggests that W could be rank-deﬁcient as well, which can also be a source of inspiration for future works."}
{"doc_id": "2106.09685", "para_id": 106, "text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255."}
{"doc_id": "2106.09685", "para_id": 107, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Kernels? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337."}
{"doc_id": "2106.09685", "para_id": 108, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020a."}
{"doc_id": "2106.09685", "para_id": 109, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020b."}
{"doc_id": "2106.09685", "para_id": 110, "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811. 03962."}
{"doc_id": "2106.09685", "para_id": 111, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."}
{"doc_id": "2106.09685", "para_id": 112, "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165."}
{"doc_id": "2106.09685", "para_id": 113, "text": "Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010."}
{"doc_id": "2106.09685", "para_id": 114, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/ v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001."}
{"doc_id": "2106.09685", "para_id": 115, "text": "Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association for Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177."}
{"doc_id": "2106.09685", "para_id": 116, "text": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning, 2014."}
{"doc_id": "2106.09685", "para_id": 117, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019a."}
{"doc_id": "2106.09685", "para_id": 118, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805."}
{"doc_id": "2106.09685", "para_id": 119, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002."}
{"doc_id": "2106.09685", "para_id": 120, "text": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pp. 124–133, 2017."}
{"doc_id": "2106.09685", "para_id": 121, "text": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020."}
{"doc_id": "2106.09685", "para_id": 122, "text": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human- annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237."}
{"doc_id": "2106.09685", "para_id": 123, "text": "Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013."}
{"doc_id": "2106.09685", "para_id": 124, "text": "Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156. 1390204."}
{"doc_id": "2106.09685", "para_id": 125, "text": "Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/ 2101.00121. arXiv: 2101.00121."}
{"doc_id": "2106.09685", "para_id": 126, "text": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021."}
{"doc_id": "2106.09685", "para_id": 127, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902. 00751."}
{"doc_id": "2106.09685", "para_id": 128, "text": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014."}
{"doc_id": "2106.09685", "para_id": 129, "text": "Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol`o Fusi. Initialization and regularization of factorized neural layers, 2021."}
{"doc_id": "2106.09685", "para_id": 130, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."}
{"doc_id": "2106.09685", "para_id": 131, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020."}
{"doc_id": "2106.09685", "para_id": 132, "text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt Tuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691."}
{"doc_id": "2106.09685", "para_id": 133, "text": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di- mension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838."}
{"doc_id": "2106.09685", "para_id": 134, "text": "Xiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190."}
{"doc_id": "2106.09685", "para_id": 135, "text": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018."}
{"doc_id": "2106.09685", "para_id": 136, "text": "Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap- proximation via alternating minimization. In International Conference on Machine Learning, pp. 2358–2367. PMLR, 2016."}
{"doc_id": "2106.09685", "para_id": 137, "text": "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning The- ory, pp. 2–47. PMLR, 2018b."}
{"doc_id": "2106.09685", "para_id": 138, "text": "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.41. URL https://aclanthology. org/2020.findings-emnlp.41."}
{"doc_id": "2106.09685", "para_id": 139, "text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT Understands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/ 2103.10385. arXiv: 2103.10385."}
{"doc_id": "2106.09685", "para_id": 140, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019."}
{"doc_id": "2106.09685", "para_id": 141, "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017."}
{"doc_id": "2106.09685", "para_id": 142, "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019."}
{"doc_id": "2106.09685", "para_id": 143, "text": "Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers, 2021."}
{"doc_id": "2106.09685", "para_id": 144, "text": "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020."}
{"doc_id": "2106.09685", "para_id": 145, "text": "Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end- to-end generation. arXiv preprint arXiv:1706.09254, 2017."}
{"doc_id": "2106.09685", "para_id": 146, "text": "Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran- tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint arXiv:1906.05392, 2019."}
{"doc_id": "2106.09685", "para_id": 147, "text": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter- fusion: Non-destructive task composition for transfer learning, 2021."}
{"doc_id": "2106.09685", "para_id": 148, "text": "Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San- jeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, pp. 3743–3747, 2018."}
{"doc_id": "2106.09685", "para_id": 149, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under- standing by Generative Pre-Training. pp. 12, a."}
{"doc_id": "2106.09685", "para_id": 150, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. pp. 24, b."}
{"doc_id": "2106.09685", "para_id": 151, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822."}
{"doc_id": "2106.09685", "para_id": 152, "text": "Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/ abs/1705.08045. arXiv: 1705.08045."}
{"doc_id": "2106.09685", "para_id": 153, "text": "Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers, 2020."}
{"doc_id": "2106.09685", "para_id": 154, "text": "Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low- rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655– 6659. IEEE, 2013."}
{"doc_id": "2106.09685", "para_id": 155, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model par- allelism, 2020."}
{"doc_id": "2106.09685", "para_id": 156, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa- tional Linguistics. URL https://aclanthology.org/D13-1170."}
{"doc_id": "2106.09685", "para_id": 157, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In- ternational Conference on Neural Information Processing Systems, pp. 6000–6010, 2017."}
{"doc_id": "2106.09685", "para_id": 158, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019."}
{"doc_id": "2106.09685", "para_id": 159, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020."}
{"doc_id": "2106.09685", "para_id": 160, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "2106.09685", "para_id": 161, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb. org/anthology/N18-1101."}
{"doc_id": "2106.09685", "para_id": 162, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug- ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As- sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6."}
{"doc_id": "2106.09685", "para_id": 163, "text": "Greg Yang and Edward J. Hu. Feature Learning in Inﬁnite-Width Neural Networks. arXiv:2011.14522 [cond-mat], May 2021. URL http://arxiv.org/abs/2011.14522. arXiv: 2011.14522."}
{"doc_id": "2106.09685", "para_id": 164, "text": "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer-based masked language-models, 2021."}
{"doc_id": "2106.09685", "para_id": 165, "text": "Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 185–189. IEEE, 2014."}
{"doc_id": "2106.09685", "para_id": 166, "text": "Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5005–5009. IEEE, 2016."}
{"doc_id": "2106.09685", "para_id": 167, "text": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL http:// arxiv.org/abs/1709.00103."}
{"doc_id": "2106.09685", "para_id": 168, "text": "A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES"}
{"doc_id": "2106.09685", "para_id": 169, "text": "Few-shot learning, or prompt engineering, is very advantageous when we only have a handful of training samples. However, in practice, we can often afford to curate a few thousand or more training examples for performance-sensitive applications. As shown in Table 8, ﬁne-tuning improves the model performance drastically compared to few-shot learning on datasets large and small. We take the GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched, we use two demonstrations per class and six in-context examples in total."}
{"doc_id": "2106.09685", "para_id": 170, "text": "GPT-3 Few-Shot 40.6 69.0 GPT-3 Fine-Tuned 89.5 85.4"}
{"doc_id": "2106.09685", "para_id": 171, "text": "Table 8: Fine-tuning signiﬁcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020)."}
{"doc_id": "2106.09685", "para_id": 172, "text": "Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently, adapter layers must be computed in addition to the base model, inevitably introducing additional latency. While as pointed out in R¨uckl´e et al. (2020), the latency introduced by adapter layers can be mitigated when the model batch size and/or sequence length is large enough to full utilize the hardware parallelism. We conﬁrm their observation with a similar latency study on GPT-2 medium and point out that there are scenarios, notably online inference where the batch size is small, where the added latency can be signiﬁcant."}
{"doc_id": "2106.09685", "para_id": 173, "text": "We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension r. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH, and a recent, more efﬁcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1 for more details on the designs. We plot the slow-down in percentage compared to the no-adapter baseline in Figure 5."}
{"doc_id": "2106.09685", "para_id": 174, "text": "Figure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline. The top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an online, short-sequence-length scenario. We tweak the colormap for better visibility."}
{"doc_id": "2106.09685", "para_id": 175, "text": "GLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes MNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC (paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al. (2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8 (question-answering), RTE (inference),"}
{"doc_id": "2106.09685", "para_id": 176, "text": "8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs"}
{"doc_id": "2106.09685", "para_id": 177, "text": "and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a standard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets are released under different permissive licenses."}
{"doc_id": "2106.09685", "para_id": 178, "text": "WikiSQL is introduced in Zhong et al. (2017) and contains 56, 355/8, 421 training/validation ex- amples. The task is to generate SQL queries from natural language questions and table schemata. We encode context as x = {table schema, query} and target as y = {SQL}. The dataset is release under the BSD 3-Clause License."}
{"doc_id": "2106.09685", "para_id": 179, "text": "SAMSum is introduced in Gliwa et al. (2019) and contains 14, 732/819 training/test examples. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. We encode context as ”\\n” concatenated utterances followed by a ”\\n\\n”, and target as y = {summary}. The dataset is released under the non-commercial licence: Creative Commons BY-NC-ND 4.0."}
{"doc_id": "2106.09685", "para_id": 180, "text": "E2E NLG Challenge was ﬁrst introduced in Novikova et al. (2017) as a dataset for training end-to- end, data-driven natural language generation systems and is commonly used for data-to-text evalua- tion. The E2E dataset consists of roughly 42, 000 training, 4, 600 validation, and 4, 600 test exam- ples from the restaurant domain. Each source table used as input can have multiple references. Each sample input (x, y) consists of a sequence of slot-value pairs, along with a corresponding natural language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 181, "text": "DART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are structured as sequences of ENTITY — RELATION — ENTITY triples. With 82K examples in total, DART is a signiﬁcantly larger and more complex data-to-text task compared to E2E. The dataset is released under the MIT license."}
{"doc_id": "2106.09685", "para_id": 182, "text": "WebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With 22K examples in total WebNLG comprises 14 distinct categories, nine of which are seen during training. Since ﬁve of the 14 total categories are not seen during training, but are represented in the test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U) and “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY — OBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 183, "text": "We train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. For a fair comparison with the setup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128 and used a ﬁxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI. The runs with this restricted setup are marked with †. See the hyperparameters used in our runs in Table 9."}
{"doc_id": "2106.09685", "para_id": 184, "text": "We again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021), we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model sequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021), we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. See the hyperparameters used in our runs in Table 10."}
{"doc_id": "2106.09685", "para_id": 185, "text": "Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B"}
{"doc_id": "2106.09685", "para_id": 186, "text": "Optimizer AdamW Warmup Ratio 0.06 LR Schedule Linear"}
{"doc_id": "2106.09685", "para_id": 187, "text": "Batch Size 16 16 16 32 32 16 32 16 # Epochs 30 60 30 80 25 25 80 40 Learning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04 LoRA Conﬁg. rq = rv = 8 LoRA α 8 Max Seq. Len. 512"}
{"doc_id": "2106.09685", "para_id": 188, "text": "Batch Size 4 4 4 4 4 4 8 8 # Epochs 10 10 20 20 10 20 20 30 Learning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04 LoRA Conﬁg. rq = rv = 8 LoRA α 16 Max Seq. Len. 128 128 512 128 512 512 512 512"}
{"doc_id": "2106.09685", "para_id": 189, "text": "Batch Size 4 # Epochs 10 10 20 20 10 20 20 10 Learning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04 LoRA Conﬁg. rq = rv = 8 LoRA α 16 Max Seq. Len. 128"}
{"doc_id": "2106.09685", "para_id": 190, "text": "Batch Size 32 # Epochs 10 20 20 20 10 20 20 20 Learning Rate 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 Bottleneck r 64 Max Seq. Len. 128"}
{"doc_id": "2106.09685", "para_id": 191, "text": "Batch Size 32 # Epochs 5 20 20 20 10 20 20 20 Learning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 Bottleneck r 16 Max Seq. Len. 128"}
{"doc_id": "2106.09685", "para_id": 192, "text": "Batch Size 32 # Epochs 10 5 10 10 5 20 20 10 Learning Rate 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 Bottleneck r 64 Max Seq. Len. 128"}
{"doc_id": "2106.09685", "para_id": 193, "text": "Batch Size 32 # Epochs 10 5 10 10 5 20 20 10 Learning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 Bottleneck r 8 Max Seq. Len. 128"}
{"doc_id": "2106.09685", "para_id": 194, "text": "Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 195, "text": "We train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang (2021)."}
{"doc_id": "2106.09685", "para_id": 196, "text": "For all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for"}
{"doc_id": "2106.09685", "para_id": 197, "text": "Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B"}
{"doc_id": "2106.09685", "para_id": 198, "text": "Optimizer AdamW Warmup Ratio 0.1 LR Schedule Linear"}
{"doc_id": "2106.09685", "para_id": 199, "text": "Batch Size 8 8 32 4 6 8 4 4 # Epochs 5 16 30 10 8 11 11 10 Learning Rate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04 Weight Decay 0 0.01 0.01 0 0.01 0.01 0.01 0.1 CLS Dropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2 LoRA Conﬁg. rq = rv = 8 LoRA α 8 Max Seq. Len. 256 128 128 64 512 320 320 128"}
{"doc_id": "2106.09685", "para_id": 200, "text": "Table 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 201, "text": "Optimizer AdamW Weight Decay 0.01 0.01 0.0 Dropout Prob 0.1 0.1 0.0 Batch Size 8 # Epoch 5 Warmup Steps 500 Learning Rate Schedule Linear Label Smooth 0.1 0.1 0.0 Learning Rate 0.0002 Adaptation rq = rv = 4 LoRA α 32"}
{"doc_id": "2106.09685", "para_id": 202, "text": "Beam Size 10 Length Penalty 0.9 0.8 0.8 no repeat ngram size 4"}
{"doc_id": "2106.09685", "para_id": 203, "text": "Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART."}
{"doc_id": "2106.09685", "para_id": 204, "text": "WikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more details on the hyperparameters used. For preﬁx-embedding tuning, we ﬁnd the optimal lp and li to be 256 and 8, respectively, totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for preﬁx-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We present two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8 or rq = rk = rv = ro = 2). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in Table 12."}
{"doc_id": "2106.09685", "para_id": 205, "text": "LoRA can be naturally combined with existing preﬁx-based approaches. In this section, we evaluate two combinations of LoRA and variants of preﬁx-tuning on WikiSQL and MNLI."}
{"doc_id": "2106.09685", "para_id": 206, "text": "LoRA+PreﬁxEmbed (LoRA+PE) combines LoRA with preﬁx-embedding tuning, where we insert lp + li special tokens whose embeddings are treated as trainable parameters. For more on preﬁx- embedding tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 207, "text": "LoRA+PreﬁxLayer (LoRA+PL) combines LoRA with preﬁx-layer tuning. We also insert lp + li special tokens; however, instead of letting the hidden representations of these tokens evolve natu-"}
{"doc_id": "2106.09685", "para_id": 208, "text": "Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterH LoRA"}
{"doc_id": "2106.09685", "para_id": 209, "text": "Optimizer AdamW Batch Size 128 # Epoch 2 Warmup Tokens 250,000 LR Schedule Linear"}
{"doc_id": "2106.09685", "para_id": 210, "text": "Learning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04"}
{"doc_id": "2106.09685", "para_id": 211, "text": "Table 12: The training hyperparameters used for different GPT-3 adaption methods. We use the same hyperparameters for all datasets after tuning learning rate."}
{"doc_id": "2106.09685", "para_id": 212, "text": "rally, we replace them after every Transformer block with an input agnostic vector. Thus, both the embeddings and subsequent Transformer block activations are treated as trainable parameters. For more on preﬁx-layer tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 213, "text": "In Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI. First of all, LoRA+PE signiﬁcantly outperforms both LoRA and preﬁx-embedding tuning on WikiSQL, which indicates that LoRA is somewhat orthogonal to preﬁx-embedding tuning. On MultiNLI, the combination of LoRA+PE doesn’t perform better than LoRA, possibly because LoRA on its own already achieves performance comparable to the human baseline. Secondly, we notice that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at- tribute this to the fact that preﬁx-layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of LoRA weights more difﬁcult in LoRA+PL."}
{"doc_id": "2106.09685", "para_id": 214, "text": "We also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017) following the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with preﬁx-based approaches given the same number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 215, "text": "Method # Trainable DART Parameters BLEU↑ MET↑ TER↓"}
{"doc_id": "2106.09685", "para_id": 216, "text": "GPT-2 Medium Fine-Tune 354M 46.2 0.39 0.46 AdapterL 0.37M 42.4 0.36 0.48 AdapterL 11M 45.2 0.38 0.46 FTTop2 24M 41.0 0.34 0.56 PrefLayer 0.35M 46.4 0.38 0.46 LoRA 0.35M 47.1±.2 0.39 0.46"}
{"doc_id": "2106.09685", "para_id": 217, "text": "GPT-2 Large Fine-Tune 774M 47.0 0.39 0.46 AdapterL 0.88M 45.7±.1 0.38 0.46 AdapterL 23M 47.1±.1 0.39 0.45 PrefLayer 0.77M 46.7 0.38 0.45 LoRA 0.77M 47.5±.1 0.39 0.45"}
{"doc_id": "2106.09685", "para_id": 218, "text": "Table 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are less than 0.01 for all adaption approaches."}
{"doc_id": "2106.09685", "para_id": 219, "text": "GPT-2 Medium Fine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53 AdapterL (0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43 AdapterL (11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39 FTTop2 (24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72 Preﬁx (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40 LoRA (0.35M) 46.7±.4 62.1±.2 55.3±.2 .38 .44 .41 .46 .33 .39"}
{"doc_id": "2106.09685", "para_id": 220, "text": "GPT-2 Large Fine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42 AdapterL (0.88M) 49.8±.0 61.1±.0 56.0±.0 .38 .43 .41 .44 .35 .39 AdapterL (23M) 49.2±.1 64.7±.2 57.7±.1 .39 .46 .43 .46 .33 .39 Preﬁx (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40 LoRA (0.77M) 48.4±.3 64.0±.3 57.0±.1 .39 .45 .42 .45 .32 .38"}
{"doc_id": "2106.09685", "para_id": 221, "text": "Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER are less than 0.01 for all the experiments we ran. “U” indicates unseen categories, “S” indicates seen categories, and “A” indicates all categories in the test set of WebNLG."}
{"doc_id": "2106.09685", "para_id": 222, "text": "We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on identifying the trade-off between performance and the number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 223, "text": "To evaluate the performance of different adaptation approaches in the low-data regime. we randomly sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI- n. To our surprise, PreﬁxEmbed and PreﬁxLayer performs very poorly on MNLI-100 dataset, with PreﬁxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreﬁxLayer performs better than PreﬁxEmbed but is still signiﬁcantly worse than Fine-Tune or LoRA on MNLI- 100. The gap between preﬁx-based approaches and LoRA/Fine-tuning becomes smaller as we in- crease the number of training examples, which might suggest that preﬁx-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than ﬁne-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the (±0.3) variance due to random seeds."}
{"doc_id": "2106.09685", "para_id": 224, "text": "The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta- ble 17. We use a smaller learning rate for PreﬁxLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate."}
{"doc_id": "2106.09685", "para_id": 225, "text": "In this paper we use the measure φ(A, B, i, j) = ψ(U i A, U j B) = ∥U i⊤ A UB∥2 F min{i,j} to measure the subspace"}
{"doc_id": "2106.09685", "para_id": 226, "text": "similarity between two column orthonormal matrices U i A ∈Rd×i and U j B ∈Rd×j, obtained by taking columns of the left singular matrices of A and B. We point out that this similarity is simply a reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee (2008)."}
{"doc_id": "2106.09685", "para_id": 227, "text": "Method Hyperparameters # Trainable Parameters WikiSQL MNLI-m"}
{"doc_id": "2106.09685", "para_id": 228, "text": "lp = 32, li = 8 0.4 M 55.9 84.9 lp = 64, li = 8 0.9 M 58.7 88.1 lp = 128, li = 8 1.7 M 60.6 88.0 lp = 256, li = 8 3.2 M 63.1 88.6 lp = 512, li = 8 6.4 M 55.9 85.8"}
{"doc_id": "2106.09685", "para_id": 229, "text": "lp = 2, li = 2 5.1 M 68.5 89.2 lp = 8, li = 0 10.1 M 69.8 88.2 lp = 8, li = 8 20.2 M 70.1 89.5 lp = 32, li = 4 44.1 M 66.4 89.6 lp = 64, li = 0 76.1 M 64.9 87.9"}
{"doc_id": "2106.09685", "para_id": 230, "text": "r = 1 7.1 M 71.9 89.8 r = 4 21.2 M 73.2 91.0 r = 8 40.1 M 73.2 91.5 r = 16 77.9 M 73.2 91.5 r = 64 304.4 M 72.6 91.5"}
{"doc_id": "2106.09685", "para_id": 231, "text": "rv = 2 4.7 M 73.4 91.7 rq = rv = 1 4.7 M 73.4 91.3 rq = rv = 2 9.4 M 73.3 91.4 rq = rk = rv = ro = 1 9.4 M 74.1 91.2 rq = rv = 4 18.8 M 73.7 91.3 rq = rk = rv = ro = 2 18.8 M 73.7 91.7 rq = rv = 8 37.7 M 73.8 91.6 rq = rk = rv = ro = 4 37.7 M 74.0 91.7 rq = rv = 64 301.9 M 73.6 91.4 rq = rk = rv = ro = 64 603.8 M 73.9 91.4"}
{"doc_id": "2106.09685", "para_id": 232, "text": "rq = rv = 8, lp = 8, li = 4 37.8 M 75.0 91.4 rq = rv = 32, lp = 8, li = 4 151.1 M 75.9 91.1 rq = rv = 64, lp = 8, li = 4 302.1 M 76.2 91.3"}
{"doc_id": "2106.09685", "para_id": 233, "text": "LoRA+PL rq = rv = 8, lp = 8, li = 4 52.8 M 72.9 90.2"}
{"doc_id": "2106.09685", "para_id": 234, "text": "Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both preﬁx-embedding tuning (PreﬁxEmbed) and preﬁx-layer tuning (PreﬁxLayer) perform worse as we increase the number of trainable parameters, while LoRA’s performance stabilizes. Performance is measured in validation accuracy."}
{"doc_id": "2106.09685", "para_id": 235, "text": "Method MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k MNLI(m)-392K"}
{"doc_id": "2106.09685", "para_id": 236, "text": "GPT-3 (Fine-Tune) 60.2 85.8 88.9 89.5 GPT-3 (PreﬁxEmbed) 37.6 75.2 79.5 88.6 GPT-3 (PreﬁxLayer) 48.3 82.5 85.9 89.6 GPT-3 (LoRA) 63.8 85.6 89.2 91.7"}
{"doc_id": "2106.09685", "para_id": 237, "text": "Table 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI- n describes a subset with n training examples. We evaluate with the full validation set. LoRA performs exhibits favorable sample-efﬁciency compared to other methods, including ﬁne-tuning."}
{"doc_id": "2106.09685", "para_id": 238, "text": "To be concrete, let the singular values of U i⊤ A U j B to be σ1, σ2, · · · , σp where p = min{i, j}. We know that the Projection Metric Ham & Lee (2008) is deﬁned as:"}
{"doc_id": "2106.09685", "para_id": 239, "text": "Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K"}
{"doc_id": "2106.09685", "para_id": 240, "text": "Optimizer - AdamW Warmup Tokens - 250,000 LR Schedule - Linear Batch Size - 20 20 100 128 # Epoch - 40 40 4 2"}
{"doc_id": "2106.09685", "para_id": 241, "text": "FineTune 5.00E-6 PreﬁxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04 PreﬁxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04 LoRA 2.00E-4"}
{"doc_id": "2106.09685", "para_id": 242, "text": "PreﬁxEmbed lp 16 32 64 256 Adaptation- PreﬁxEmbed li 8 Speciﬁc PreﬁxTune lp = li = 8 LoRA rq = rv = 8"}
{"doc_id": "2106.09685", "para_id": 243, "text": "Table 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n."}
{"doc_id": "2106.09685", "para_id": 244, "text": "φ(A, B, i, j) = ψ(U i A, U j B) = Pp i=1 σ2 i p = 1"}
{"doc_id": "2106.09685", "para_id": 245, "text": "This similarity satisﬁes that if U i A and U j B share the same column span, then φ(A, B, i, j) = 1. If they are completely orthogonal, then φ(A, B, i, j) = 0. Otherwise, φ(A, B, i, j) ∈(0, 1)."}
{"doc_id": "2106.09685", "para_id": 246, "text": "We present additional results from our investigation into the low-rank update matrices."}
{"doc_id": "2106.09685", "para_id": 247, "text": "See Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other layers."}
{"doc_id": "2106.09685", "para_id": 248, "text": "We repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge dataset as an example, we report the validation loss and test metrics achieved by different choices of r after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2 Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B. Note that the relationship between model size and the optimal rank for adaptation is still an open question."}
{"doc_id": "2106.09685", "para_id": 249, "text": "See Figure 8 for the normalized subspace similarity between W and ∆W with varying r."}
{"doc_id": "2106.09685", "para_id": 250, "text": "Note again that ∆W does not contain the top singular directions of W, since the similarity between the top 4 directions in ∆W and the top-10% of those in W barely exceeds 0.2. This gives evidence that ∆W contains those “task-speciﬁc” directions that are otherwise not emphasized in W."}
{"doc_id": "2106.09685", "para_id": 251, "text": "An interesting next question to answer, is how “strong” do we need to amplify those task-speciﬁc directions, in order for the model adaptation to work well?"}
{"doc_id": "2106.09685", "para_id": 252, "text": "Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both ∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer."}
{"doc_id": "2106.09685", "para_id": 253, "text": "One can naturally consider a feature ampliﬁcation factor as the ratio ∥∆W ∥F ∥U ⊤W V ⊤∥F , where U and V are the left- and right-singular matrices of the SVD decomposition of ∆W. (Recall UU ⊤WV ⊤V gives the “projection” of W onto the subspace spanned by ∆W.)"}
{"doc_id": "2106.09685", "para_id": 254, "text": "Intuitively, when ∆W mostly contains task-speciﬁc directions, this quantity measures how much of them are ampliﬁed by ∆W. As shown in Section 7.3, for r = 4, this ampliﬁcation factor is as large as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the entire feature space from the pre-trained model W), that need to be ampliﬁed by a very large factor 20, in order to achieve our reported accuracy for the downstream speciﬁc task. And, one should expect a very different set of feature directions to be ampliﬁed for each different downstream task."}
{"doc_id": "2106.09685", "para_id": 255, "text": "One may notice, however, for r = 64, this ampliﬁcation factor is only around 2, meaning that most directions learned in ∆W with r = 64 are not being ampliﬁed by much. This should not be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent the “task-speciﬁc directions” (thus for model adaptation) is low. In contrast, those directions in the rank-4 version of ∆W (corresponding to r = 4) are ampliﬁed by a much larger factor 20."}
{"doc_id": "2106.09685", "para_id": 256, "text": "Figure 7: Normalized subspace similarity between the column vectors of Ar=64 from two randomly seeded runs, for both ∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans- former."}
{"doc_id": "2106.09685", "para_id": 257, "text": "1 1.23 68.72 8.7215 0.4565 0.7052 2.4329 2 1.21 69.17 8.7413 0.4590 0.7052 2.4639 4 1.18 70.38 8.8439 0.4689 0.7186 2.5349 8 1.17 69.57 8.7457 0.4636 0.7196 2.5196 16 1.16 69.61 8.7483 0.4629 0.7177 2.4985 32 1.16 69.33 8.7736 0.4642 0.7105 2.5255 64 1.16 69.24 8.7174 0.4651 0.7180 2.5070 128 1.16 68.73 8.6718 0.4628 0.7127 2.5030 256 1.16 68.92 8.6982 0.4629 0.7128 2.5012 512 1.16 68.78 8.6857 0.4637 0.7128 2.5025 1024 1.17 69.37 8.7495 0.4659 0.7149 2.5090"}
{"doc_id": "2106.09685", "para_id": 258, "text": "Table 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with different rank r using GPT-2 Medium. Unlike on GPT-3 where r = 1 sufﬁces for many tasks, here the performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2 Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our hyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and thus might not be optimal for other choices of r."}
{"doc_id": "2106.09685", "para_id": 259, "text": "Figure 8: Normalized subspace similarity between the singular directions of Wq and those of ∆Wq with varying r and a random baseline. ∆Wq ampliﬁes directions that are important but not empha- sized in W. ∆W with a larger r tends to pick up more directions that are already emphasized in W."}
{"doc_id": "gutenberg_1342", "para_id": 0, "text": "CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO. TOOKS COURT, CHANCERY LANE, LONDON."}
{"doc_id": "gutenberg_1342", "para_id": 1, "text": "_To J. Comyns Carr in acknowledgment of all I owe to his friendship and advice, these illustrations are gratefully inscribed_"}
{"doc_id": "gutenberg_1342", "para_id": 2, "text": "_Walt Whitman has somewhere a fine and just distinction between “loving by allowance” and “loving with personal love.” This distinction applies to books as well as to men and women; and in the case of the not very numerous authors who are the objects of the personal affection, it brings a curious consequence with it. There is much more difference as to their best work than in the case of those others who are loved “by allowance” by convention, and because it is felt to be the right and proper thing to love them. And in the sect--fairly large and yet unusually choice--of Austenians or Janites, there would probably be found partisans of the claim to primacy of almost every one of the novels. To some the delightful freshness and humour of_ Northanger Abbey, _its completeness, finish, and_ entrain, _obscure the undoubted critical facts that its scale is small, and its scheme, after all, that of burlesque or parody, a kind in which the first rank is reached with difficulty._ Persuasion, _relatively faint in tone, and not enthralling in interest, has devotees who exalt above all the others its exquisite delicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly theatrical, the hero and heroine are insipid, and the author has almost wickedly destroyed all romantic interest by expressly admitting that Edmund only took Fanny because Mary shocked him, and that Fanny might very likely have taken Crawford if he had been a little more assiduous; yet the matchless rehearsal-scenes and the characters of Mrs. Norris and others have secured, I believe, a considerable party for it._ Sense and Sensibility _has perhaps the fewest out-and-out admirers; but it does not want them._"}
{"doc_id": "gutenberg_1342", "para_id": 3, "text": "_I suppose, however, that the majority of at least competent votes would, all things considered, be divided between_ Emma _and the present book; and perhaps the vulgar verdict (if indeed a fondness for Miss Austen be not of itself a patent of exemption from any possible charge of vulgarity) would go for_ Emma. _It is the larger, the more varied, the more popular; the author had by the time of its composition seen rather more of the world, and had improved her general, though not her most peculiar and characteristic dialogue; such figures as Miss Bates, as the Eltons, cannot but unite the suffrages of everybody. On the other hand, I, for my part, declare for_ Pride and Prejudice _unhesitatingly. It seems to me the most perfect, the most characteristic, the most eminently quintessential of its author’s works; and for this contention in such narrow space as is permitted to me, I propose here to show cause._"}
{"doc_id": "gutenberg_1342", "para_id": 4, "text": "_In the first place, the book (it may be barely necessary to remind the reader) was in its first shape written very early, somewhere about 1796, when Miss Austen was barely twenty-one; though it was revised and finished at Chawton some fifteen years later, and was not published till 1813, only four years before her death. I do not know whether, in this combination of the fresh and vigorous projection of youth, and the critical revision of middle life, there may be traced the distinct superiority in point of construction, which, as it seems to me, it possesses over all the others. The plot, though not elaborate, is almost regular enough for Fielding; hardly a character, hardly an incident could be retrenched without loss to the story. The elopement of Lydia and Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de théâtre; _it connects itself in the strictest way with the course of the story earlier, and brings about the denouement with complete propriety. All the minor passages--the loves of Jane and Bingley, the advent of Mr. Collins, the visit to Hunsford, the Derbyshire tour--fit in after the same unostentatious, but masterly fashion. There is no attempt at the hide-and-seek, in-and-out business, which in the transactions between Frank Churchill and Jane Fairfax contributes no doubt a good deal to the intrigue of_ Emma, _but contributes it in a fashion which I do not think the best feature of that otherwise admirable book. Although Miss Austen always liked something of the misunderstanding kind, which afforded her opportunities for the display of the peculiar and incomparable talent to be noticed presently, she has been satisfied here with the perfectly natural occasions provided by the false account of Darcy’s conduct given by Wickham, and by the awkwardness (arising with equal naturalness) from the gradual transformation of Elizabeth’s own feelings from positive aversion to actual love. I do not know whether the all-grasping hand of the playwright has ever been laid upon_ Pride and Prejudice; _and I dare say that, if it were, the situations would prove not startling or garish enough for the footlights, the character-scheme too subtle and delicate for pit and gallery. But if the attempt were made, it would certainly not be hampered by any of those loosenesses of construction, which, sometimes disguised by the conveniences of which the novelist can avail himself, appear at once on the stage._"}
{"doc_id": "gutenberg_1342", "para_id": 5, "text": "_I think, however, though the thought will doubtless seem heretical to more than one school of critics, that construction is not the highest merit, the choicest gift, of the novelist. It sets off his other gifts and graces most advantageously to the critical eye; and the want of it will sometimes mar those graces--appreciably, though not quite consciously--to eyes by no means ultra-critical. But a very badly-built novel which excelled in pathetic or humorous character, or which displayed consummate command of dialogue--perhaps the rarest of all faculties--would be an infinitely better thing than a faultless plot acted and told by puppets with pebbles in their mouths. And despite the ability which Miss Austen has shown in working out the story, I for one should put_ Pride and Prejudice _far lower if it did not contain what seem to me the very masterpieces of Miss Austen’s humour and of her faculty of character-creation--masterpieces who may indeed admit John Thorpe, the Eltons, Mrs. Norris, and one or two others to their company, but who, in one instance certainly, and perhaps in others, are still superior to them._"}
{"doc_id": "gutenberg_1342", "para_id": 6, "text": "_The characteristics of Miss Austen’s humour are so subtle and delicate that they are, perhaps, at all times easier to apprehend than to express, and at any particular time likely to be differently apprehended by different persons. To me this humour seems to possess a greater affinity, on the whole, to that of Addison than to any other of the numerous species of this great British genus. The differences of scheme, of time, of subject, of literary convention, are, of course, obvious enough; the difference of sex does not, perhaps, count for much, for there was a distinctly feminine element in “Mr. Spectator,” and in Jane Austen’s genius there was, though nothing mannish, much that was masculine. But the likeness of quality consists in a great number of common subdivisions of quality--demureness, extreme minuteness of touch, avoidance of loud tones and glaring effects. Also there is in both a certain not inhuman or unamiable cruelty. It is the custom with those who judge grossly to contrast the good nature of Addison with the savagery of Swift, the mildness of Miss Austen with the boisterousness of Fielding and Smollett, even with the ferocious practical jokes that her immediate predecessor, Miss Burney, allowed without very much protest. Yet, both in Mr. Addison and in Miss Austen there is, though a restrained and well-mannered, an insatiable and ruthless delight in roasting and cutting up a fool. A man in the early eighteenth century, of course, could push this taste further than a lady in the early nineteenth; and no doubt Miss Austen’s principles, as well as her heart, would have shrunk from such things as the letter from the unfortunate husband in the_ Spectator, _who describes, with all the gusto and all the innocence in the world, how his wife and his friend induce him to play at blind-man’s-buff. But another_ Spectator _letter--that of the damsel of fourteen who wishes to marry Mr. Shapely, and assures her selected Mentor that “he admires your_ Spectators _mightily”--might have been written by a rather more ladylike and intelligent Lydia Bennet in the days of Lydia’s great-grandmother; while, on the other hand, some (I think unreasonably) have found “cynicism” in touches of Miss Austen’s own, such as her satire of Mrs. Musgrove’s self-deceiving regrets over her son. But this word “cynical” is one of the most misused in the English language, especially when, by a glaring and gratuitous falsification of its original sense, it is applied, not to rough and snarling invective, but to gentle and oblique satire. If cynicism means the perception of “the other side,” the sense of “the accepted hells beneath,” the consciousness that motives are nearly always mixed, and that to seem is not identical with to be--if this be cynicism, then every man and woman who is not a fool, who does not care to live in a fool’s paradise, who has knowledge of nature and the world and life, is a cynic. And in that sense Miss Austen certainly was one. She may even have been one in the further sense that, like her own Mr. Bennet, she took an epicurean delight in dissecting, in displaying, in setting at work her fools and her mean persons. I think she did take this delight, and I do not think at all the worse of her for it as a woman, while she was immensely the better for it as an artist._"}
{"doc_id": "gutenberg_1342", "para_id": 7, "text": "_In respect of her art generally, Mr. Goldwin Smith has truly observed that “metaphor has been exhausted in depicting the perfection of it, combined with the narrowness of her field;” and he has justly added that we need not go beyond her own comparison to the art of a miniature painter. To make this latter observation quite exact we must not use the term miniature in its restricted sense, and must think rather of Memling at one end of the history of painting and Meissonier at the other, than of Cosway or any of his kind. And I am not so certain that I should myself use the word “narrow” in connection with her. If her world is a microcosm, the cosmic quality of it is at least as eminent as the littleness. She does not touch what she did not feel herself called to paint; I am not so sure that she could not have painted what she did not feel herself called to touch. It is at least remarkable that in two very short periods of writing--one of about three years, and another of not much more than five--she executed six capital works, and has not left a single failure. It is possible that the romantic paste in her composition was defective: we must always remember that hardly anybody born in her decade--that of the eighteenth-century seventies--independently exhibited the full romantic quality. Even Scott required hill and mountain and ballad, even Coleridge metaphysics and German to enable them to chip the classical shell. Miss Austen was an English girl, brought up in a country retirement, at the time when ladies went back into the house if there was a white frost which might pierce their kid shoes, when a sudden cold was the subject of the gravest fears, when their studies, their ways, their conduct were subject to all those fantastic limits and restrictions against which Mary Wollstonecraft protested with better general sense than particular taste or judgment. Miss Austen, too, drew back when the white frost touched her shoes; but I think she would have made a pretty good journey even in a black one._"}
{"doc_id": "gutenberg_1342", "para_id": 8, "text": "_For if her knowledge was not very extended, she knew two things which only genius knows. The one was humanity, and the other was art. On the first head she could not make a mistake; her men, though limited, are true, and her women are, in the old sense, “absolute.” As to art, if she has never tried idealism, her realism is real to a degree which makes the false realism of our own day look merely dead-alive. Take almost any Frenchman, except the late M. de Maupassant, and watch him laboriously piling up strokes in the hope of giving a complete impression. You get none; you are lucky if, discarding two-thirds of what he gives, you can shape a real impression out of the rest. But with Miss Austen the myriad, trivial, unforced strokes build up the picture like magic. Nothing is false; nothing is superfluous. When (to take the present book only) Mr. Collins changed his mind from Jane to Elizabeth “while Mrs. Bennet was stirring the fire” (and we know_ how _Mrs. Bennet would have stirred the fire), when Mr. Darcy “brought his coffee-cup back_ himself,” _the touch in each case is like that of Swift--“taller by the breadth of my nail”--which impressed the half-reluctant Thackeray with just and outspoken admiration. Indeed, fantastic as it may seem, I should put Miss Austen as near to Swift in some ways, as I have put her to Addison in others._"}
{"doc_id": "gutenberg_1342", "para_id": 9, "text": "_This Swiftian quality appears in the present novel as it appears nowhere else in the character of the immortal, the ineffable Mr. Collins. Mr. Collins is really_ great; _far greater than anything Addison ever did, almost great enough for Fielding or for Swift himself. It has been said that no one ever was like him. But in the first place,_ he _was like him; he is there--alive, imperishable, more real than hundreds of prime ministers and archbishops, of “metals, semi-metals, and distinguished philosophers.” In the second place, it is rash, I think, to conclude that an actual Mr. Collins was impossible or non-existent at the end of the eighteenth century. It is very interesting that we possess, in this same gallery, what may be called a spoiled first draught, or an unsuccessful study of him, in John Dashwood. The formality, the under-breeding, the meanness, are there; but the portrait is only half alive, and is felt to be even a little unnatural. Mr. Collins is perfectly natural, and perfectly alive. In fact, for all the “miniature,” there is something gigantic in the way in which a certain side, and more than one, of humanity, and especially eighteenth-century humanity, its Philistinism, its well-meaning but hide-bound morality, its formal pettiness, its grovelling respect for rank, its materialism, its selfishness, receives exhibition. I will not admit that one speech or one action of this inestimable man is incapable of being reconciled with reality, and I should not wonder if many of these words and actions are historically true._"}
{"doc_id": "gutenberg_1342", "para_id": 10, "text": "_But the greatness of Mr. Collins could not have been so satisfactorily exhibited if his creatress had not adjusted so artfully to him the figures of Mr. Bennet and of Lady Catherine de Bourgh. The latter, like Mr. Collins himself, has been charged with exaggeration. There is, perhaps, a very faint shade of colour for the charge; but it seems to me very faint indeed. Even now I do not think that it would be impossible to find persons, especially female persons, not necessarily of noble birth, as overbearing, as self-centred, as neglectful of good manners, as Lady Catherine. A hundred years ago, an earl’s daughter, the Lady Powerful (if not exactly Bountiful) of an out-of-the-way country parish, rich, long out of marital authority, and so forth, had opportunities of developing these agreeable characteristics which seldom present themselves now. As for Mr. Bennet, Miss Austen, and Mr. Darcy, and even Miss Elizabeth herself, were, I am inclined to think, rather hard on him for the “impropriety” of his conduct. His wife was evidently, and must always have been, a quite irreclaimable fool; and unless he had shot her or himself there was no way out of it for a man of sense and spirit but the ironic. From no other point of view is he open to any reproach, except for an excusable and not unnatural helplessness at the crisis of the elopement, and his utterances are the most acutely delightful in the consciously humorous kind--in the kind that we laugh with, not at--that even Miss Austen has put into the mouth of any of her characters. It is difficult to know whether he is most agreeable when talking to his wife, or when putting Mr. Collins through his paces; but the general sense of the world has probably been right in preferring to the first rank his consolation to the former when she maunders over the entail, “My dear, do not give way to such gloomy thoughts. Let us hope for better things. Let us flatter ourselves that_ I _may be the survivor;” and his inquiry to his colossal cousin as to the compliments which Mr. Collins has just related as made by himself to Lady Catherine, “May I ask whether these pleasing attentions proceed from the impulse of the moment, or are the result of previous study?” These are the things which give Miss Austen’s readers the pleasant shocks, the delightful thrills, which are felt by the readers of Swift, of Fielding, and we may here add, of Thackeray, as they are felt by the readers of no other English author of fiction outside of these four._"}
{"doc_id": "gutenberg_1342", "para_id": 11, "text": "_The goodness of the minor characters in_ Pride and Prejudice _has been already alluded to, and it makes a detailed dwelling on their beauties difficult in any space, and impossible in this. Mrs. Bennet we have glanced at, and it is not easy to say whether she is more exquisitely amusing or more horribly true. Much the same may be said of Kitty and Lydia; but it is not every author, even of genius, who would have differentiated with such unerring skill the effects of folly and vulgarity of intellect and disposition working upon the common weaknesses of woman at such different ages. With Mary, Miss Austen has taken rather less pains, though she has been even more unkind to her; not merely in the text, but, as we learn from those interesting traditional appendices which Mr. Austen Leigh has given us, in dooming her privately to marry “one of Mr. Philips’s clerks.” The habits of first copying and then retailing moral sentiments, of playing and singing too long in public, are, no doubt, grievous and criminal; but perhaps poor Mary was rather the scapegoat of the sins of blue stockings in that Fordyce-belectured generation. It is at any rate difficult not to extend to her a share of the respect and affection (affection and respect of a peculiar kind; doubtless), with which one regards Mr. Collins, when she draws the moral of Lydia’s fall. I sometimes wish that the exigencies of the story had permitted Miss Austen to unite these personages, and thus at once achieve a notable mating and soothe poor Mrs. Bennet’s anguish over the entail._"}
{"doc_id": "gutenberg_1342", "para_id": 12, "text": "_The Bingleys and the Gardiners and the Lucases, Miss Darcy and Miss de Bourgh, Jane, Wickham, and the rest, must pass without special comment, further than the remark that Charlotte Lucas (her egregious papa, though delightful, is just a little on the thither side of the line between comedy and farce) is a wonderfully clever study in drab of one kind, and that Wickham (though something of Miss Austen’s hesitation of touch in dealing with young men appears) is a not much less notable sketch in drab of another. Only genius could have made Charlotte what she is, yet not disagreeable; Wickham what he is, without investing him either with a cheap Don Juanish attractiveness or a disgusting rascality. But the hero and the heroine are not tints to be dismissed._"}
{"doc_id": "gutenberg_1342", "para_id": 13, "text": "_Darcy has always seemed to me by far the best and most interesting of Miss Austen’s heroes; the only possible competitor being Henry Tilney, whose part is so slight and simple that it hardly enters into comparison. It has sometimes, I believe, been urged that his pride is unnatural at first in its expression and later in its yielding, while his falling in love at all is not extremely probable. Here again I cannot go with the objectors. Darcy’s own account of the way in which his pride had been pampered, is perfectly rational and sufficient; and nothing could be, psychologically speaking, a_ causa verior _for its sudden restoration to healthy conditions than the shock of Elizabeth’s scornful refusal acting on a nature_ ex hypothesi _generous. Nothing in even our author is finer and more delicately touched than the change of his demeanour at the sudden meeting in the grounds of Pemberley. Had he been a bad prig or a bad coxcomb, he might have been still smarting under his rejection, or suspicious that the girl had come husband-hunting. His being neither is exactly consistent with the probable feelings of a man spoilt in the common sense, but not really injured in disposition, and thoroughly in love. As for his being in love, Elizabeth has given as just an exposition of the causes of that phenomenon as Darcy has of the conditions of his unregenerate state, only she has of course not counted in what was due to her own personal charm._"}
{"doc_id": "gutenberg_1342", "para_id": 14, "text": "_The secret of that charm many men and not a few women, from Miss Austen herself downwards, have felt, and like most charms it is a thing rather to be felt than to be explained. Elizabeth of course belongs to the_ allegro _or_ allegra _division of the army of Venus. Miss Austen was always provokingly chary of description in regard to her beauties; and except the fine eyes, and a hint or two that she had at any rate sometimes a bright complexion, and was not very tall, we hear nothing about her looks. But her chief difference from other heroines of the lively type seems to lie first in her being distinctly clever--almost strong-minded, in the better sense of that objectionable word--and secondly in her being entirely destitute of ill-nature for all her propensity to tease and the sharpness of her tongue. Elizabeth can give at least as good as she gets when she is attacked; but she never “scratches,” and she never attacks first. Some of the merest obsoletenesses of phrase and manner give one or two of her early speeches a slight pertness, but that is nothing, and when she comes to serious business, as in the great proposal scene with Darcy (which is, as it should be, the climax of the interest of the book), and in the final ladies’ battle with Lady Catherine, she is unexceptionable. Then too she is a perfectly natural girl. She does not disguise from herself or anybody that she resents Darcy’s first ill-mannered personality with as personal a feeling. (By the way, the reproach that the ill-manners of this speech are overdone is certainly unjust; for things of the same kind, expressed no doubt less stiltedly but more coarsely, might have been heard in more than one ball-room during this very year from persons who ought to have been no worse bred than Darcy.) And she lets the injury done to Jane and the contempt shown to the rest of her family aggravate this resentment in the healthiest way in the world._"}
{"doc_id": "gutenberg_1342", "para_id": 15, "text": "_Still, all this does not explain her charm, which, taking beauty as a common form of all heroines, may perhaps consist in the addition to her playfulness, her wit, her affectionate and natural disposition, of a certain fearlessness very uncommon in heroines of her type and age. Nearly all of them would have been in speechless awe of the magnificent Darcy; nearly all of them would have palpitated and fluttered at the idea of proposals, even naughty ones, from the fascinating Wickham. Elizabeth, with nothing offensive, nothing_ viraginous, _nothing of the “New Woman” about her, has by nature what the best modern (not “new”) women have by education and experience, a perfect freedom from the idea that all men may bully her if they choose, and that most will away with her if they can. Though not in the least “impudent and mannish grown,” she has no mere sensibility, no nasty niceness about her. The form of passion common and likely to seem natural in Miss Austen’s day was so invariably connected with the display of one or the other, or both of these qualities, that she has not made Elizabeth outwardly passionate. But I, at least, have not the slightest doubt that she would have married Darcy just as willingly without Pemberley as with it, and anybody who can read between lines will not find the lovers’ conversations in the final chapters so frigid as they might have looked to the Della Cruscans of their own day, and perhaps do look to the Della Cruscans of this._"}
{"doc_id": "gutenberg_1342", "para_id": 16, "text": "_And, after all, what is the good of seeking for the reason of charm?--it is there. There were better sense in the sad mechanic exercise of determining the reason of its absence where it is not. In the novels of the last hundred years there are vast numbers of young ladies with whom it might be a pleasure to fall in love; there are at least five with whom, as it seems to me, no man of taste and spirit can help doing so. Their names are, in chronological order, Elizabeth Bennet, Diana Vernon, Argemone Lavington, Beatrix Esmond, and Barbara Grant. I should have been most in love with Beatrix and Argemone; I should, I think, for mere occasional companionship, have preferred Diana and Barbara. But to live with and to marry, I do not know that any one of the four can come into competition with Elizabeth._"}
{"doc_id": "gutenberg_1342", "para_id": 17, "text": "It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife."}
{"doc_id": "gutenberg_1342", "para_id": 18, "text": "However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters."}
{"doc_id": "gutenberg_1342", "para_id": 19, "text": "“My dear Mr. Bennet,” said his lady to him one day, “have you heard that Netherfield Park is let at last?”"}
{"doc_id": "gutenberg_1342", "para_id": 20, "text": "“But it is,” returned she; “for Mrs. Long has just been here, and she told me all about it.”"}
{"doc_id": "gutenberg_1342", "para_id": 21, "text": "“Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.”"}
{"doc_id": "gutenberg_1342", "para_id": 22, "text": "“Oh, single, my dear, to be sure! A single man of large fortune; four or five thousand a year. What a fine thing for our girls!”"}
{"doc_id": "gutenberg_1342", "para_id": 23, "text": "“My dear Mr. Bennet,” replied his wife, “how can you be so tiresome? You must know that I am thinking of his marrying one of them.”"}
{"doc_id": "gutenberg_1342", "para_id": 24, "text": "“Design? Nonsense, how can you talk so! But it is very likely that he _may_ fall in love with one of them, and therefore you must visit him as soon as he comes.”"}
{"doc_id": "gutenberg_1342", "para_id": 25, "text": "“I see no occasion for that. You and the girls may go--or you may send them by themselves, which perhaps will be still better; for as you are as handsome as any of them, Mr. Bingley might like you the best of the party.”"}
{"doc_id": "gutenberg_1342", "para_id": 26, "text": "“My dear, you flatter me. I certainly _have_ had my share of beauty, but I do not pretend to be anything extraordinary now. When a woman has five grown-up daughters, she ought to give over thinking of her own beauty.”"}
{"doc_id": "gutenberg_1342", "para_id": 27, "text": "“But, my dear, you must indeed go and see Mr. Bingley when he comes into the neighbourhood.”"}
{"doc_id": "gutenberg_1342", "para_id": 28, "text": "“But consider your daughters. Only think what an establishment it would be for one of them. Sir William and Lady Lucas are determined to go, merely on that account; for in general, you know, they visit no new comers. Indeed you must go, for it will be impossible for _us_ to visit him, if you do not.”"}
{"doc_id": "gutenberg_1342", "para_id": 29, "text": "“You are over scrupulous, surely. I dare say Mr. Bingley will be very glad to see you; and I will send a few lines by you to assure him of my hearty consent to his marrying whichever he chooses of the girls--though I must throw in a good word for my little Lizzy.”"}
{"doc_id": "gutenberg_1342", "para_id": 30, "text": "“I desire you will do no such thing. Lizzy is not a bit better than the others: and I am sure she is not half so handsome as Jane, nor half so good-humoured as Lydia. But you are always giving _her_ the preference.”"}
{"doc_id": "gutenberg_1342", "para_id": 31, "text": "“They have none of them much to recommend them,” replied he: “they are all silly and ignorant like other girls; but Lizzy has something more of quickness than her sisters.”"}
{"doc_id": "gutenberg_1342", "para_id": 32, "text": "“Mr. Bennet, how can you abuse your own children in such a way? You take delight in vexing me. You have no compassion on my poor nerves.”"}
{"doc_id": "gutenberg_1342", "para_id": 33, "text": "“You mistake me, my dear. I have a high respect for your nerves. They are my old friends. I have heard you mention them with consideration these twenty years at least.”"}
{"doc_id": "gutenberg_1342", "para_id": 34, "text": "“But I hope you will get over it, and live to see many young men of four thousand a year come into the neighbourhood.”"}
{"doc_id": "gutenberg_1342", "para_id": 35, "text": "“It will be no use to us, if twenty such should come, since you will not visit them.”"}
{"doc_id": "gutenberg_1342", "para_id": 36, "text": "Mr. Bennet was so odd a mixture of quick parts, sarcastic humour, reserve, and caprice, that the experience of three-and-twenty years had been insufficient to make his wife understand his character. _Her_ mind was less difficult to develope. She was a woman of mean understanding, little information, and uncertain temper. When she was discontented, she fancied herself nervous. The business of her life was to get her daughters married: its solace was visiting and news."}
{"doc_id": "gutenberg_1342", "para_id": 37, "text": "Mr. Bennet was among the earliest of those who waited on Mr. Bingley. He had always intended to visit him, though to the last always assuring his wife that he should not go; and till the evening after the visit was paid she had no knowledge of it. It was then disclosed in the following manner. Observing his second daughter employed in trimming a hat, he suddenly addressed her with,--"}
{"doc_id": "gutenberg_1342", "para_id": 38, "text": "“We are not in a way to know _what_ Mr. Bingley likes,” said her mother, resentfully, “since we are not to visit.”"}
{"doc_id": "gutenberg_1342", "para_id": 39, "text": "“But you forget, mamma,” said Elizabeth, “that we shall meet him at the assemblies, and that Mrs. Long has promised to introduce him.”"}
{"doc_id": "gutenberg_1342", "para_id": 40, "text": "“I do not believe Mrs. Long will do any such thing. She has two nieces of her own. She is a selfish, hypocritical woman, and I have no opinion of her.”"}
{"doc_id": "gutenberg_1342", "para_id": 41, "text": "“No more have I,” said Mr. Bennet; “and I am glad to find that you do not depend on her serving you.”"}
{"doc_id": "gutenberg_1342", "para_id": 42, "text": "Mrs. Bennet deigned not to make any reply; but, unable to contain herself, began scolding one of her daughters."}
{"doc_id": "gutenberg_1342", "para_id": 43, "text": "“Don’t keep coughing so, Kitty, for heaven’s sake! Have a little compassion on my nerves. You tear them to pieces.”"}
{"doc_id": "gutenberg_1342", "para_id": 44, "text": "“I do not cough for my own amusement,” replied Kitty, fretfully. “When is your next ball to be, Lizzy?”"}
{"doc_id": "gutenberg_1342", "para_id": 45, "text": "“Ay, so it is,” cried her mother, “and Mrs. Long does not come back till the day before; so, it will be impossible for her to introduce him, for she will not know him herself.”"}
{"doc_id": "gutenberg_1342", "para_id": 46, "text": "“Then, my dear, you may have the advantage of your friend, and introduce Mr. Bingley to _her_.”"}
{"doc_id": "gutenberg_1342", "para_id": 47, "text": "“Impossible, Mr. Bennet, impossible, when I am not acquainted with him myself; how can you be so teasing?”"}
{"doc_id": "gutenberg_1342", "para_id": 48, "text": "“I honour your circumspection. A fortnight’s acquaintance is certainly very little. One cannot know what a man really is by the end of a fortnight. But if _we_ do not venture, somebody else will; and after all, Mrs. Long and her nieces must stand their chance; and, therefore, as she will think it an act of kindness, if you decline the office, I will take it on myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 49, "text": "“What can be the meaning of that emphatic exclamation?” cried he. “Do you consider the forms of introduction, and the stress that is laid on them, as nonsense? I cannot quite agree with you _there_. What say you, Mary? For you are a young lady of deep reflection, I know, and read great books, and make extracts.”"}
{"doc_id": "gutenberg_1342", "para_id": 50, "text": "“While Mary is adjusting her ideas,” he continued, “let us return to Mr. Bingley.”"}
{"doc_id": "gutenberg_1342", "para_id": 51, "text": "“I am sorry to hear _that_; but why did you not tell me so before? If I had known as much this morning, I certainly would not have called on him. It is very unlucky; but as I have actually paid the visit, we cannot escape the acquaintance now.”"}
{"doc_id": "gutenberg_1342", "para_id": 52, "text": "The astonishment of the ladies was just what he wished--that of Mrs. Bennet perhaps surpassing the rest; though when the first tumult of joy was over, she began to declare that it was what she had expected all the while."}
{"doc_id": "gutenberg_1342", "para_id": 53, "text": "“How good it was in you, my dear Mr. Bennet! But I knew I should persuade you at last. I was sure you loved your girls too well to neglect such an acquaintance. Well, how pleased I am! And it is such a good joke, too, that you should have gone this morning, and never said a word about it till now.”"}
{"doc_id": "gutenberg_1342", "para_id": 54, "text": "“Now, Kitty, you may cough as much as you choose,” said Mr. Bennet; and, as he spoke, he left the room, fatigued with the raptures of his wife."}
{"doc_id": "gutenberg_1342", "para_id": 55, "text": "“What an excellent father you have, girls,” said she, when the door was shut. “I do not know how you will ever make him amends for his kindness; or me either, for that matter. At our time of life, it is not so pleasant, I can tell you, to be making new acquaintances every day; but for your sakes we would do anything. Lydia, my love, though you _are_ the youngest, I dare say Mr. Bingley will dance with you at the next ball.”"}
{"doc_id": "gutenberg_1342", "para_id": 56, "text": "“Oh,” said Lydia, stoutly, “I am not afraid; for though I _am_ the youngest, I’m the tallest.”"}
{"doc_id": "gutenberg_1342", "para_id": 57, "text": "The rest of the evening was spent in conjecturing how soon he would return Mr. Bennet’s visit, and determining when they should ask him to dinner."}
{"doc_id": "gutenberg_1342", "para_id": 58, "text": "Not all that Mrs. Bennet, however, with the assistance of her five daughters, could ask on the subject, was sufficient to draw from her husband any satisfactory description of Mr. Bingley. They attacked him in various ways, with barefaced questions, ingenious suppositions, and distant surmises; but he eluded the skill of them all; and they were at last obliged to accept the second-hand intelligence of their neighbour, Lady Lucas. Her report was highly favourable. Sir William had been delighted with him. He was quite young, wonderfully handsome, extremely agreeable, and, to crown the whole, he meant to be at the next assembly with a large party. Nothing could be more delightful! To be fond of dancing was a certain step towards falling in love; and very lively hopes of Mr. Bingley’s heart were entertained."}
{"doc_id": "gutenberg_1342", "para_id": 59, "text": "“If I can but see one of my daughters happily settled at Netherfield,” said Mrs. Bennet to her husband, “and all the others equally well married, I shall have nothing to wish for.”"}
{"doc_id": "gutenberg_1342", "para_id": 60, "text": "In a few days Mr. Bingley returned Mr. Bennet’s visit, and sat about ten minutes with him in his library. He had entertained hopes of being admitted to a sight of the young ladies, of whose beauty he had heard much; but he saw only the father. The ladies were somewhat more fortunate, for they had the advantage of ascertaining, from an upper window, that he wore a blue coat and rode a black horse."}
{"doc_id": "gutenberg_1342", "para_id": 61, "text": "An invitation to dinner was soon afterwards despatched; and already had Mrs. Bennet planned the courses that were to do credit to her housekeeping, when an answer arrived which deferred it all. Mr. Bingley was obliged to be in town the following day, and consequently unable to accept the honour of their invitation, etc. Mrs. Bennet was quite disconcerted. She could not imagine what business he could have in town so soon after his arrival in Hertfordshire; and she began to fear that he might always be flying about from one place to another, and never settled at Netherfield as he ought to be. Lady Lucas quieted her fears a little by starting the idea of his"}
{"doc_id": "gutenberg_1342", "para_id": 62, "text": "being gone to London only to get a large party for the ball; and a report soon followed that Mr. Bingley was to bring twelve ladies and seven gentlemen with him to the assembly. The girls grieved over such a number of ladies; but were comforted the day before the ball by hearing that, instead of twelve, he had brought only six with him from London, his five sisters and a cousin. And when the party entered the assembly-room, it consisted of only five altogether: Mr. Bingley, his two sisters, the husband of the eldest, and another young man."}
{"doc_id": "gutenberg_1342", "para_id": 63, "text": "Mr. Bingley was good-looking and gentlemanlike: he had a pleasant countenance, and easy, unaffected manners. His sisters were fine women, with an air of decided fashion. His brother-in-law, Mr. Hurst, merely looked the gentleman; but his friend Mr. Darcy soon drew the attention of the room by his fine, tall person, handsome features, noble mien, and the report, which was in general circulation within five minutes after his entrance, of his having ten thousand a year. The gentlemen pronounced him to be a fine figure of a man, the ladies declared he was much handsomer than Mr. Bingley, and he was looked at with great admiration for about half the evening, till his manners gave a disgust which turned the tide of his popularity; for he was discovered to be proud, to be above his company, and above being pleased; and not all his large estate in Derbyshire could save him from having a most forbidding, disagreeable countenance, and being unworthy to be compared with his friend."}
{"doc_id": "gutenberg_1342", "para_id": 64, "text": "Mr. Bingley had soon made himself acquainted with all the principal people in the room: he was lively and unreserved, danced every dance, was angry that the ball closed so early, and talked of giving one himself at Netherfield. Such amiable qualities must speak for themselves. What a contrast between him and his friend! Mr. Darcy danced only once with Mrs. Hurst and once with Miss Bingley, declined being introduced to any other lady, and spent the rest of the evening in walking about the room, speaking occasionally to one of his own party. His character was decided. He was the proudest, most disagreeable man in the world, and everybody hoped that he would never come there again. Amongst the most violent against him was Mrs. Bennet, whose dislike of his general behaviour was sharpened into particular resentment by his having slighted one of her daughters."}
{"doc_id": "gutenberg_1342", "para_id": 65, "text": "Elizabeth Bennet had been obliged, by the scarcity of gentlemen, to sit down for two dances; and during part of that time, Mr. Darcy had been standing near enough for her to overhear a conversation between him and Mr. Bingley, who came from the dance for a few minutes to press his friend to join it."}
{"doc_id": "gutenberg_1342", "para_id": 66, "text": "“Come, Darcy,” said he, “I must have you dance. I hate to see you standing about by yourself in this stupid manner. You had much better dance.”"}
{"doc_id": "gutenberg_1342", "para_id": 67, "text": "“I certainly shall not. You know how I detest it, unless I am particularly acquainted with my partner. At such an assembly as this, it would be insupportable. Your sisters are engaged, and there is not another woman in the room whom it would not be a punishment to me to stand up with.”"}
{"doc_id": "gutenberg_1342", "para_id": 68, "text": "“I would not be so fastidious as you are,” cried Bingley, “for a kingdom! Upon my honour, I never met with so many pleasant girls in my life as I have this evening; and there are several of them, you see, uncommonly pretty.”"}
{"doc_id": "gutenberg_1342", "para_id": 69, "text": "“_You_ are dancing with the only handsome girl in the room,” said Mr. Darcy, looking at the eldest Miss Bennet."}
{"doc_id": "gutenberg_1342", "para_id": 70, "text": "“Oh, she is the most beautiful creature I ever beheld! But there is one of her sisters sitting down just behind you, who is very pretty, and I dare say very agreeable. Do let me ask my partner to introduce you.”"}
{"doc_id": "gutenberg_1342", "para_id": 71, "text": "“Which do you mean?” and turning round, he looked for a moment at Elizabeth, till, catching her eye, he withdrew his own, and coldly said, “She is tolerable: but not handsome enough to tempt _me_; and I am in no humour at present to give consequence to young ladies who are slighted by other men. You had better return to your partner and enjoy her smiles, for you are wasting your time with me.”"}
{"doc_id": "gutenberg_1342", "para_id": 72, "text": "Mr. Bingley followed his advice. Mr. Darcy walked off; and Elizabeth remained with no very cordial feelings towards him. She told the story, however, with great spirit among her friends; for she had a lively, playful disposition, which delighted in anything ridiculous."}
{"doc_id": "gutenberg_1342", "para_id": 73, "text": "The evening altogether passed off pleasantly to the whole family. Mrs. Bennet had seen her eldest daughter much admired by the Netherfield party. Mr. Bingley had danced with her twice, and she had been distinguished by his sisters. Jane was as much gratified by this as her mother could be, though in a quieter way. Elizabeth felt Jane’s pleasure. Mary had heard herself mentioned to Miss Bingley as the most accomplished girl in the neighbourhood; and Catherine and Lydia had been fortunate enough to be never without partners, which was all that they had yet learnt to care for at a ball. They returned, therefore, in good spirits to Longbourn, the village where they lived, and of which they were the principal inhabitants. They found Mr. Bennet still up. With a book, he was regardless of time; and on the present occasion he had a good deal of curiosity as to the event of an evening which had raised such splendid expectations. He had rather hoped that all his wife’s views on the stranger would be disappointed; but he soon found that he had a very different story to hear."}
{"doc_id": "gutenberg_1342", "para_id": 74, "text": "“Oh, my dear Mr. Bennet,” as she entered the room, “we have had a most delightful evening, a most excellent ball. I wish you had been there. Jane was so admired, nothing could be like it. Everybody said how well she looked; and Mr. Bingley thought her quite beautiful, and danced with her twice. Only think of _that_, my dear: he actually danced with her twice; and she was the only creature in the room that he asked a second time. First of all, he asked Miss Lucas. I was so vexed to see him stand up with her; but, however, he did not admire her at all; indeed, nobody can, you know; and he seemed quite struck with Jane as she was going down the dance. So he inquired who she was, and got introduced, and asked her for the two next. Then, the two third he danced with Miss King, and the two fourth with Maria Lucas, and the two fifth with Jane again, and the two sixth with Lizzy, and the _Boulanger_----”"}
{"doc_id": "gutenberg_1342", "para_id": 75, "text": "“If he had had any compassion for _me_,” cried her husband impatiently, “he would not have danced half so much! For God’s sake, say no more of his partners. O that he had sprained his ancle in the first dance!”"}
{"doc_id": "gutenberg_1342", "para_id": 76, "text": "“Oh, my dear,” continued Mrs. Bennet, “I am quite delighted with him. He is so excessively handsome! and his sisters are charming women. I never in my life saw anything more elegant than their dresses. I dare say the lace upon Mrs. Hurst’s gown----”"}
{"doc_id": "gutenberg_1342", "para_id": 77, "text": "Here she was interrupted again. Mr. Bennet protested against any description of finery. She was therefore obliged to seek another branch of the subject, and related, with much bitterness of spirit, and some exaggeration, the shocking rudeness of Mr. Darcy."}
{"doc_id": "gutenberg_1342", "para_id": 78, "text": "“But I can assure you,” she added, “that Lizzy does not lose much by not suiting _his_ fancy; for he is a most disagreeable, horrid man, not at all worth pleasing. So high and so conceited, that there was no enduring him! He walked here, and he walked there, fancying himself so very great! Not handsome enough to dance with! I wish you had been there, my dear, to have given him one of your set-downs. I quite detest the man.”"}
{"doc_id": "gutenberg_1342", "para_id": 79, "text": "When Jane and Elizabeth were alone, the former, who had been cautious in her praise of Mr. Bingley before, expressed to her sister how very much she admired him."}
{"doc_id": "gutenberg_1342", "para_id": 80, "text": "“He is just what a young-man ought to be,” said she, “sensible, good-humoured, lively; and I never saw such happy manners! so much ease, with such perfect good breeding!”"}
{"doc_id": "gutenberg_1342", "para_id": 81, "text": "“He is also handsome,” replied Elizabeth, “which a young man ought likewise to be if he possibly can. His character is thereby complete.”"}
{"doc_id": "gutenberg_1342", "para_id": 82, "text": "“I was very much flattered by his asking me to dance a second time. I did not expect such a compliment.”"}
{"doc_id": "gutenberg_1342", "para_id": 83, "text": "“Did not you? _I_ did for you. But that is one great difference between us. Compliments always take _you_ by surprise, and _me_ never. What could be more natural than his asking you again? He could not help seeing that you were about five times as pretty as every other woman in the room. No thanks to his gallantry for that. Well, he certainly is very agreeable, and I give you leave to like him. You have liked many a stupider person.”"}
{"doc_id": "gutenberg_1342", "para_id": 84, "text": "“Oh, you are a great deal too apt, you know, to like people in general. You never see a fault in anybody. All the world are good and agreeable in your eyes. I never heard you speak ill of a human being in my life.”"}
{"doc_id": "gutenberg_1342", "para_id": 85, "text": "“I would wish not to be hasty in censuring anyone; but I always speak what I think.”"}
{"doc_id": "gutenberg_1342", "para_id": 86, "text": "“I know you do: and it is _that_ which makes the wonder. With _your_ good sense, to be so honestly blind to the follies and nonsense of others! Affectation of candour is common enough; one meets with it everywhere. But to be candid without ostentation or design,--to take the good of everybody’s character and make it still better, and say nothing of the bad,--belongs to you alone. And so, you like this man’s sisters, too, do you? Their manners are not equal to his.”"}
{"doc_id": "gutenberg_1342", "para_id": 87, "text": "“Certainly not, at first; but they are very pleasing women when you converse with them. Miss Bingley is to live with her brother, and keep his house; and I am much mistaken if we shall not find a very charming neighbour in her.”"}
{"doc_id": "gutenberg_1342", "para_id": 88, "text": "Elizabeth listened in silence, but was not convinced: their behaviour at the assembly had not been calculated to please in general; and with more quickness of observation and less pliancy of temper than her sister, and with a judgment, too, unassailed by any attention to herself, she was very little disposed to approve them. They were, in fact, very fine ladies; not deficient in good-humour when they were pleased, nor in the power of being agreeable where they chose it; but proud and conceited. They were rather handsome; had been educated in one of the first private seminaries in town; had a fortune of twenty thousand pounds; were in the habit of spending more than they ought, and of associating with people of rank; and were, therefore, in every respect entitled to think well of themselves and meanly of others. They were of a respectable family in the north of England; a circumstance more deeply impressed on their memories than that their brother’s fortune and their own had been acquired by trade."}
{"doc_id": "gutenberg_1342", "para_id": 89, "text": "Mr. Bingley inherited property to the amount of nearly a hundred thousand pounds from his father, who had intended to purchase an estate, but did not live to do it. Mr. Bingley intended it likewise, and sometimes made choice of his county; but, as he was now provided with a good house and the liberty of a manor, it was doubtful to many of those who best knew the easiness of his temper, whether he might not spend the remainder of his days at Netherfield, and leave the next generation to purchase."}
{"doc_id": "gutenberg_1342", "para_id": 90, "text": "His sisters were very anxious for his having an estate of his own; but though he was now established only as a tenant, Miss Bingley was by no means unwilling to preside at his table; nor was Mrs. Hurst, who had married a man of more fashion than fortune, less disposed to consider his house as her home when it suited her. Mr. Bingley had not been of age two years when he was tempted, by an accidental recommendation, to look at Netherfield House. He did look at it, and into it, for half an hour; was pleased with the situation and the principal rooms, satisfied with what the owner said in its praise, and took it immediately."}
{"doc_id": "gutenberg_1342", "para_id": 91, "text": "Between him and Darcy there was a very steady friendship, in spite of a great opposition of character. Bingley was endeared to Darcy by the easiness, openness, and ductility of his temper, though no disposition could offer a greater contrast to his own, and though with his own he never appeared dissatisfied. On the strength of Darcy’s regard, Bingley had the firmest reliance, and of his judgment the highest opinion. In understanding, Darcy was the superior. Bingley was by no means deficient; but Darcy was clever. He was at the same time haughty, reserved, and fastidious; and his manners, though well bred, were not inviting. In that respect his friend had greatly the advantage. Bingley was sure of being liked wherever he appeared; Darcy was continually giving offence."}
{"doc_id": "gutenberg_1342", "para_id": 92, "text": "The manner in which they spoke of the Meryton assembly was sufficiently characteristic. Bingley had never met with pleasanter people or prettier girls in his life; everybody had been most kind and attentive to him; there had been no formality, no stiffness; he had soon felt acquainted with all the room; and as to Miss Bennet, he could not conceive an angel more beautiful. Darcy, on the contrary, had seen a collection of people in whom there was little beauty and no fashion, for none of whom he had felt the smallest interest, and from none received either attention or pleasure. Miss Bennet he acknowledged to be pretty; but she smiled too much."}
{"doc_id": "gutenberg_1342", "para_id": 93, "text": "Mrs. Hurst and her sister allowed it to be so; but still they admired her and liked her, and pronounced her to be a sweet girl, and one whom they should not object to know more of. Miss Bennet was therefore established as a sweet girl; and their brother felt authorized by such commendation to think of her as he chose."}
{"doc_id": "gutenberg_1342", "para_id": 94, "text": "Within a short walk of Longbourn lived a family with whom the Bennets were particularly intimate. Sir William Lucas had been formerly in trade in Meryton, where he had made a tolerable fortune, and risen to the honour of knighthood by an address to the king during his mayoralty. The distinction had, perhaps, been felt too strongly. It had given him a disgust to his business and to his residence in a small market town; and, quitting them both, he had removed with his family to a house about a mile from Meryton, denominated from that period Lucas Lodge; where he could think with pleasure of his own importance, and, unshackled by business, occupy himself solely in being civil to all the world. For, though elated by his rank, it did not render him supercilious; on the contrary, he was all attention to everybody. By nature inoffensive, friendly, and obliging, his presentation at St. James’s had made him courteous."}
{"doc_id": "gutenberg_1342", "para_id": 95, "text": "Lady Lucas was a very good kind of woman, not too clever to be a valuable neighbour to Mrs. Bennet. They had several children. The eldest of them, a sensible, intelligent young woman, about twenty-seven, was Elizabeth’s intimate friend."}
{"doc_id": "gutenberg_1342", "para_id": 96, "text": "That the Miss Lucases and the Miss Bennets should meet to talk over a ball was absolutely necessary; and the morning after the assembly brought the former to Longbourn to hear and to communicate."}
{"doc_id": "gutenberg_1342", "para_id": 97, "text": "“_You_ began the evening well, Charlotte,” said Mrs. Bennet, with civil self-command, to Miss Lucas. “_You_ were Mr. Bingley’s first choice.”"}
{"doc_id": "gutenberg_1342", "para_id": 98, "text": "“Oh, you mean Jane, I suppose, because he danced with her twice. To be sure that _did_ seem as if he admired her--indeed, I rather believe he _did_--I heard something about it--but I hardly know what--something about Mr. Robinson.”"}
{"doc_id": "gutenberg_1342", "para_id": 99, "text": "“Perhaps you mean what I overheard between him and Mr. Robinson: did not I mention it to you? Mr. Robinson’s asking him how he liked our Meryton assemblies, and whether he did not think there were a great many pretty women in the room, and _which_ he thought the prettiest? and his answering immediately to the last question, ‘Oh, the eldest Miss Bennet, beyond a doubt: there cannot be two opinions on that point.’”"}
{"doc_id": "gutenberg_1342", "para_id": 100, "text": "“Upon my word! Well, that was very decided, indeed--that does seem as if--but, however, it may all come to nothing, you know.”"}
{"doc_id": "gutenberg_1342", "para_id": 101, "text": "“_My_ overhearings were more to the purpose than _yours_, Eliza,” said Charlotte. “Mr. Darcy is not so well worth listening to as his friend, is he? Poor Eliza! to be only just _tolerable_.”"}
{"doc_id": "gutenberg_1342", "para_id": 102, "text": "“I beg you will not put it into Lizzy’s head to be vexed by his ill-treatment, for he is such a disagreeable man that it would be quite a misfortune to be liked by him. Mrs. Long told me last night that he sat close to her for half an hour without once opening his lips.”"}
{"doc_id": "gutenberg_1342", "para_id": 103, "text": "“Are you quite sure, ma’am? Is not there a little mistake?” said Jane. “I certainly saw Mr. Darcy speaking to her.”"}
{"doc_id": "gutenberg_1342", "para_id": 104, "text": "“Ay, because she asked him at last how he liked Netherfield, and he could not help answering her; but she said he seemed very angry at being spoke to.”"}
{"doc_id": "gutenberg_1342", "para_id": 105, "text": "“Miss Bingley told me,” said Jane, “that he never speaks much unless among his intimate acquaintance. With _them_ he is remarkably agreeable.”"}
{"doc_id": "gutenberg_1342", "para_id": 106, "text": "“I do not believe a word of it, my dear. If he had been so very agreeable, he would have talked to Mrs. Long. But I can guess how it was; everybody says that he is eat up with pride, and I dare say he had heard somehow that Mrs. Long does not keep a carriage, and had to come to the ball in a hack chaise.”"}
{"doc_id": "gutenberg_1342", "para_id": 107, "text": "“I do not mind his not talking to Mrs. Long,” said Miss Lucas, “but I wish he had danced with Eliza.”"}
{"doc_id": "gutenberg_1342", "para_id": 108, "text": "“Another time, Lizzy,” said her mother, “I would not dance with _him_, if I were you.”"}
{"doc_id": "gutenberg_1342", "para_id": 109, "text": "“His pride,” said Miss Lucas, “does not offend _me_ so much as pride often does, because there is an excuse for it. One cannot wonder that so very fine a young man, with family, fortune, everything in his favour, should think highly of himself. If I may so express it, he has a _right_ to be proud.”"}
{"doc_id": "gutenberg_1342", "para_id": 110, "text": "“That is very true,” replied Elizabeth, “and I could easily forgive _his_ pride, if he had not mortified _mine_.”"}
{"doc_id": "gutenberg_1342", "para_id": 111, "text": "“Pride,” observed Mary, who piqued herself upon the solidity of her reflections, “is a very common failing, I believe. By all that I have ever read, I am convinced that it is very common indeed; that human nature is particularly prone to it, and that there are very few of us who do not cherish a feeling of self-complacency on the score of some quality or other, real or imaginary. Vanity and pride are different things, though the words are often used synonymously. A person may be proud without being vain. Pride relates more to our opinion of ourselves; vanity to what we would have others think of us.”"}
{"doc_id": "gutenberg_1342", "para_id": 112, "text": "“If I were as rich as Mr. Darcy,” cried a young Lucas, who came with his sisters, “I should not care how proud I was. I would keep a pack of foxhounds, and drink a bottle of wine every day.”"}
{"doc_id": "gutenberg_1342", "para_id": 113, "text": "“Then you would drink a great deal more than you ought,” said Mrs. Bennet; “and if I were to see you at it, I should take away your bottle directly.”"}
{"doc_id": "gutenberg_1342", "para_id": 114, "text": "The boy protested that she should not; she continued to declare that she would; and the argument ended only with the visit."}
{"doc_id": "gutenberg_1342", "para_id": 115, "text": "The ladies of Longbourn soon waited on those of Netherfield. The visit was returned in due form. Miss Bennet’s pleasing manners grew on the good-will of Mrs. Hurst and Miss Bingley; and though the mother was found to be intolerable, and the younger sisters not worth speaking to, a wish of being better acquainted with _them_ was expressed towards the two eldest. By Jane this attention was received with the greatest pleasure; but Elizabeth still saw superciliousness in their treatment of everybody, hardly excepting even her sister, and could not like them; though their kindness to Jane, such as it was, had a value, as arising, in all probability, from the influence of their brother’s admiration. It was generally evident, whenever they met, that he _did_ admire her; and to _her_ it was equally evident that Jane was yielding to the preference which she had begun to entertain for him from the first, and was in a way to be very much in love; but she considered with pleasure that it was not likely to be discovered by the world in general, since Jane united with great strength of feeling, a composure of temper and an uniform cheerfulness of manner, which would guard her from the suspicions of the impertinent. She mentioned this to her friend, Miss Lucas."}
{"doc_id": "gutenberg_1342", "para_id": 116, "text": "“It may, perhaps, be pleasant,” replied Charlotte, “to be able to impose on the public in such a case; but it is sometimes a disadvantage to be so very guarded. If a woman conceals her affection with the same skill from the object of it, she may lose the opportunity of fixing him; and it will then be but poor consolation to believe the world equally in the dark. There is so much of gratitude or vanity in almost every attachment, that it is not safe to leave any to itself. We can all _begin_ freely--a slight preference is natural enough; but there are very few of us who have heart enough to be really in love without encouragement. In nine cases out of ten, a woman had better show _more_ affection than she feels. Bingley likes your sister undoubtedly; but he may never do more than like her, if she does not help him on.”"}
{"doc_id": "gutenberg_1342", "para_id": 117, "text": "“But she does help him on, as much as her nature will allow. If _I_ can perceive her regard for him, he must be a simpleton indeed not to discover it too.”"}
{"doc_id": "gutenberg_1342", "para_id": 118, "text": "“But if a woman is partial to a man, and does not endeavor to conceal it, he must find it out.”"}
{"doc_id": "gutenberg_1342", "para_id": 119, "text": "“Perhaps he must, if he sees enough of her. But though Bingley and Jane meet tolerably often, it is never for many hours together; and as they always see each other in large mixed parties, it is impossible that every moment should be employed in conversing together. Jane should therefore make the most of every half hour in which she can command his attention. When she is secure of him, there will be leisure for falling in love as much as she chooses.”"}
{"doc_id": "gutenberg_1342", "para_id": 120, "text": "“Your plan is a good one,” replied Elizabeth, “where nothing is in question but the desire of being well married; and if I were determined to get a rich husband, or any husband, I dare say I should adopt it. But these are not Jane’s feelings; she is not acting by design. As yet she cannot even be certain of the degree of her own regard, nor of its reasonableness. She has known him only a fortnight. She danced four dances with him at Meryton; she saw him one morning at his own house, and has since dined in company with him four times. This is not quite enough to make her understand his character.”"}
{"doc_id": "gutenberg_1342", "para_id": 121, "text": "“Not as you represent it. Had she merely _dined_ with him, she might only have discovered whether he had a good appetite; but you must remember that four evenings have been also spent together--and four evenings may do a great deal.”"}
{"doc_id": "gutenberg_1342", "para_id": 122, "text": "“Yes: these four evenings have enabled them to ascertain that they both like Vingt-un better than Commerce, but with respect to any other leading characteristic, I do not imagine that much has been unfolded.”"}
{"doc_id": "gutenberg_1342", "para_id": 123, "text": "“Well,” said Charlotte, “I wish Jane success with all my heart; and if she were married to him to-morrow, I should think she had as good a chance of happiness as if she were to be studying his character for a twelvemonth. Happiness in marriage is entirely a matter of chance. If the dispositions of the parties are ever so well known to each other, or ever so similar beforehand, it does not advance their felicity in the least. They always continue to grow sufficiently unlike afterwards to have their share of vexation; and it is better to know as little as possible of the defects of the person with whom you are to pass your life.”"}
{"doc_id": "gutenberg_1342", "para_id": 124, "text": "“You make me laugh, Charlotte; but it is not sound. You know it is not sound, and that you would never act in this way yourself.”"}
{"doc_id": "gutenberg_1342", "para_id": 125, "text": "Occupied in observing Mr. Bingley’s attention to her sister, Elizabeth was far from suspecting that she was herself becoming an object of some interest in the eyes of his friend. Mr. Darcy had at first scarcely allowed her to be pretty: he had looked at her without admiration at the ball; and when they next met, he looked at her only to criticise. But no sooner had he made it clear to himself and his friends that she had hardly a good feature in her face, than he began to find it was rendered uncommonly intelligent by the beautiful expression of her dark eyes. To this discovery succeeded some others equally mortifying. Though he had detected with a critical eye more than one failure of perfect symmetry in her form, he was forced to acknowledge her figure to be light and pleasing; and in spite of his asserting that her manners were not those of the fashionable world, he was caught by their easy playfulness. Of this she was perfectly unaware: to her he was only the man who made himself agreeable nowhere, and who had not thought her handsome enough to dance with."}
{"doc_id": "gutenberg_1342", "para_id": 126, "text": "He began to wish to know more of her; and, as a step towards conversing with her himself, attended to her conversation with others. His doing so drew her notice. It was at Sir William Lucas’s, where a large party were assembled."}
{"doc_id": "gutenberg_1342", "para_id": 127, "text": "“What does Mr. Darcy mean,” said she to Charlotte, “by listening to my conversation with Colonel Forster?”"}
{"doc_id": "gutenberg_1342", "para_id": 128, "text": "“But if he does it any more, I shall certainly let him know that I see what he is about. He has a very satirical eye, and if I do not begin by being impertinent myself, I shall soon grow afraid of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 129, "text": "On his approaching them soon afterwards, though without seeming to have any intention of speaking, Miss Lucas defied her friend to mention such a subject to him, which immediately provoking Elizabeth to do it, she turned to him and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 130, "text": "“Did not you think, Mr. Darcy, that I expressed myself uncommonly well just now, when I was teasing Colonel Forster to give us a ball at Meryton?”"}
{"doc_id": "gutenberg_1342", "para_id": 131, "text": "“It will be _her_ turn soon to be teased,” said Miss Lucas. “I am going to open the instrument, Eliza, and you know what follows.”"}
{"doc_id": "gutenberg_1342", "para_id": 132, "text": "“You are a very strange creature by way of a friend!--always wanting me to play and sing before anybody and everybody! If my vanity had taken a musical turn, you would have been invaluable; but as it is, I would really rather not sit down before those who must be in the habit of hearing the very best performers.” On Miss Lucas’s persevering, however, she added, “Very well; if it must be so, it must.” And gravely glancing at Mr. Darcy, “There is a very fine old saying, which everybody here is of course familiar with--‘Keep your breath to cool your porridge,’--and I shall keep mine to swell my song.”"}
{"doc_id": "gutenberg_1342", "para_id": 133, "text": "Her performance was pleasing, though by no means capital. After a song or two, and before she could reply to the entreaties of several that she would sing again, she was eagerly succeeded at the instrument by her sister Mary, who having, in consequence of being the only plain one in the family, worked hard for knowledge and accomplishments, was always impatient for display."}
{"doc_id": "gutenberg_1342", "para_id": 134, "text": "Mary had neither genius nor taste; and though vanity had given her application, it had given her likewise a pedantic air and conceited manner, which would have injured a higher degree of excellence than she had reached. Elizabeth, easy and unaffected, had been listened to with much more pleasure, though not playing half so well; and Mary, at the end of a long concerto, was glad to purchase praise and gratitude by Scotch and Irish airs, at the request of her younger sisters, who with some of the Lucases, and two or three officers, joined eagerly in dancing at one end of the room."}
{"doc_id": "gutenberg_1342", "para_id": 135, "text": "Mr. Darcy stood near them in silent indignation at such a mode of passing the evening, to the exclusion of all conversation, and was too much engrossed by his own thoughts to perceive that Sir William Lucas was his neighbour, till Sir William thus began:--"}
{"doc_id": "gutenberg_1342", "para_id": 136, "text": "“What a charming amusement for young people this is, Mr. Darcy! There is nothing like dancing, after all. I consider it as one of the first refinements of polished societies.”"}
{"doc_id": "gutenberg_1342", "para_id": 137, "text": "“Certainly, sir; and it has the advantage also of being in vogue amongst the less polished societies of the world: every savage can dance.”"}
{"doc_id": "gutenberg_1342", "para_id": 138, "text": "Sir William only smiled. “Your friend performs delightfully,” he continued, after a pause, on seeing Bingley join the group; “and I doubt not that you are an adept in the science yourself, Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 139, "text": "“Yes, indeed, and received no inconsiderable pleasure from the sight. Do you often dance at St. James’s?”"}
{"doc_id": "gutenberg_1342", "para_id": 140, "text": "“I had once some thoughts of fixing in town myself, for I am fond of superior society; but I did not feel quite certain that the air of London would agree with Lady Lucas.”"}
{"doc_id": "gutenberg_1342", "para_id": 141, "text": "He paused in hopes of an answer: but his companion was not disposed to make any; and Elizabeth at that instant moving towards them, he was struck with the notion of doing a very gallant thing, and called out to her,--"}
{"doc_id": "gutenberg_1342", "para_id": 142, "text": "“My dear Miss Eliza, why are not you dancing? Mr. Darcy, you must allow me to present this young lady to you as a very desirable partner. You cannot refuse to dance, I am sure, when so much beauty is before you.” And, taking her hand, he would have given it to Mr. Darcy, who, though extremely surprised, was not unwilling to receive it, when she instantly drew back, and said with some discomposure to Sir William,--"}
{"doc_id": "gutenberg_1342", "para_id": 143, "text": "“Indeed, sir, I have not the least intention of dancing. I entreat you not to suppose that I moved this way in order to beg for a partner.”"}
{"doc_id": "gutenberg_1342", "para_id": 144, "text": "Mr. Darcy, with grave propriety, requested to be allowed the honour of her hand, but in vain. Elizabeth was determined; nor did Sir William at all shake her purpose by his attempt at persuasion."}
{"doc_id": "gutenberg_1342", "para_id": 145, "text": "“You excel so much in the dance, Miss Eliza, that it is cruel to deny me the happiness of seeing you; and though this gentleman dislikes the amusement in general, he can have no objection, I am sure, to oblige us for one half hour.”"}
{"doc_id": "gutenberg_1342", "para_id": 146, "text": "“He is, indeed: but considering the inducement, my dear Miss Eliza, we cannot wonder at his complaisance; for who would object to such a partner?”"}
{"doc_id": "gutenberg_1342", "para_id": 147, "text": "Elizabeth looked archly, and turned away. Her resistance had not injured her with the gentleman, and he was thinking of her with some complacency, when thus accosted by Miss Bingley,--"}
{"doc_id": "gutenberg_1342", "para_id": 148, "text": "“You are considering how insupportable it would be to pass many evenings in this manner,--in such society; and, indeed, I am quite of your opinion. I was never more annoyed! The insipidity, and yet the noise--the nothingness, and yet the self-importance, of all these people! What would I give to hear your strictures on them!”"}
{"doc_id": "gutenberg_1342", "para_id": 149, "text": "“Your conjecture is totally wrong, I assure you. My mind was more agreeably engaged. I have been meditating on the very great pleasure which a pair of fine eyes in the face of a pretty woman can bestow.”"}
{"doc_id": "gutenberg_1342", "para_id": 150, "text": "Miss Bingley immediately fixed her eyes on his face, and desired he would tell her what lady had the credit of inspiring such reflections. Mr. Darcy replied, with great intrepidity,--"}
{"doc_id": "gutenberg_1342", "para_id": 151, "text": "“Miss Elizabeth Bennet!” repeated Miss Bingley. “I am all astonishment. How long has she been such a favourite? and pray when am I to wish you joy?”"}
{"doc_id": "gutenberg_1342", "para_id": 152, "text": "“That is exactly the question which I expected you to ask. A lady’s imagination is very rapid; it jumps from admiration to love, from love to matrimony, in a moment. I knew you would be wishing me joy.”"}
{"doc_id": "gutenberg_1342", "para_id": 153, "text": "“Nay, if you are so serious about it, I shall consider the matter as absolutely settled. You will have a charming mother-in-law, indeed, and of course she will be always at Pemberley with you.”"}
{"doc_id": "gutenberg_1342", "para_id": 154, "text": "He listened to her with perfect indifference, while she chose to entertain herself in this manner; and as his composure convinced her that all was safe, her wit flowed along."}
{"doc_id": "gutenberg_1342", "para_id": 155, "text": "Mr. Bennet’s property consisted almost entirely in an estate of two thousand a year, which, unfortunately for his daughters, was entailed, in default of heirs male, on a distant relation; and their mother’s fortune, though ample for her situation in life, could but ill supply the deficiency of his. Her father had been an attorney in Meryton, and had left her four thousand pounds."}
{"doc_id": "gutenberg_1342", "para_id": 156, "text": "She had a sister married to a Mr. Philips, who had been a clerk to their father and succeeded him in the business, and a brother settled in London in a respectable line of trade."}
{"doc_id": "gutenberg_1342", "para_id": 157, "text": "The village of Longbourn was only one mile from Meryton; a most convenient distance for the young ladies, who were usually tempted thither three or four times a week, to pay their duty to their aunt, and to a milliner’s shop just over the way. The two youngest of the family, Catherine and Lydia, were particularly frequent in these attentions: their minds were more vacant than their sisters’, and when nothing better offered, a walk to Meryton was necessary to amuse their morning hours and furnish conversation for the evening; and, however bare of news the country in general might be, they always contrived to learn some from their aunt. At present, indeed, they were well supplied both with news and happiness by the recent arrival of a militia regiment in the neighbourhood; it was to remain the whole winter, and Meryton was the head-quarters."}
{"doc_id": "gutenberg_1342", "para_id": 158, "text": "Their visits to Mrs. Philips were now productive of the most interesting intelligence. Every day added something to their knowledge of the officers’ names and connections. Their lodgings were not long a secret, and at length they began to know the officers themselves. Mr. Philips visited them all, and this opened to his nieces a source of felicity unknown before. They could talk of nothing but officers; and Mr. Bingley’s large fortune, the mention of which gave animation to their mother, was worthless in their eyes when opposed to the regimentals of an ensign."}
{"doc_id": "gutenberg_1342", "para_id": 159, "text": "After listening one morning to their effusions on this subject, Mr. Bennet coolly observed,--"}
{"doc_id": "gutenberg_1342", "para_id": 160, "text": "“From all that I can collect by your manner of talking, you must be two of the silliest girls in the country. I have suspected it some time, but I am now convinced.”"}
{"doc_id": "gutenberg_1342", "para_id": 161, "text": "Catherine was disconcerted, and made no answer; but Lydia, with perfect indifference, continued to express her admiration of Captain Carter, and her hope of seeing him in the course of the day, as he was going the next morning to London."}
{"doc_id": "gutenberg_1342", "para_id": 162, "text": "“I am astonished, my dear,” said Mrs. Bennet, “that you should be so ready to think your own children silly. If I wished to think slightingly of anybody’s children, it should not be of my own, however.”"}
{"doc_id": "gutenberg_1342", "para_id": 163, "text": "“This is the only point, I flatter myself, on which we do not agree. I had hoped that our sentiments coincided in every particular, but I must so far differ from you as to think our two youngest daughters uncommonly foolish.”"}
{"doc_id": "gutenberg_1342", "para_id": 164, "text": "“My dear Mr. Bennet, you must not expect such girls to have the sense of their father and mother. When they get to our age, I dare say they will not think about officers any more than we do. I remember the time when I liked a red coat myself very well--and, indeed, so I do still at my heart; and if a smart young colonel, with five or six thousand a year, should want one of my girls, I shall not say nay to him; and I thought Colonel Forster looked very becoming the other night at Sir William’s in his regimentals.”"}
{"doc_id": "gutenberg_1342", "para_id": 165, "text": "“Mamma,” cried Lydia, “my aunt says that Colonel Forster and Captain Carter do not go so often to Miss Watson’s as they did when they first came; she sees them now very often standing in Clarke’s library.”"}
{"doc_id": "gutenberg_1342", "para_id": 166, "text": "Mrs. Bennet was prevented replying by the entrance of the footman with a note for Miss Bennet; it came from Netherfield, and the servant waited for an answer. Mrs. Bennet’s eyes sparkled with pleasure, and she was eagerly calling out, while her daughter read,--"}
{"doc_id": "gutenberg_1342", "para_id": 167, "text": "“Well, Jane, who is it from? What is it about? What does he say? Well, Jane, make haste and tell us; make haste, my love.”"}
{"doc_id": "gutenberg_1342", "para_id": 168, "text": "“If you are not so compassionate as to dine to-day with Louisa and me, we shall be in danger of hating each other for the rest of our lives; for a whole day’s _tête-à-tête_ between two women can never end without a quarrel. Come as soon as you can on the receipt of this. My brother and the gentlemen are to dine with the officers. Yours ever,"}
{"doc_id": "gutenberg_1342", "para_id": 169, "text": "“No, my dear, you had better go on horseback, because it seems likely to rain; and then you must stay all night.”"}
{"doc_id": "gutenberg_1342", "para_id": 170, "text": "“That would be a good scheme,” said Elizabeth, “if you were sure that they would not offer to send her home.”"}
{"doc_id": "gutenberg_1342", "para_id": 171, "text": "“Oh, but the gentlemen will have Mr. Bingley’s chaise to go to Meryton; and the Hursts have no horses to theirs.”"}
{"doc_id": "gutenberg_1342", "para_id": 172, "text": "“But, my dear, your father cannot spare the horses, I am sure. They are wanted in the farm, Mr. Bennet, are not they?”"}
{"doc_id": "gutenberg_1342", "para_id": 173, "text": "“But if you have got them to-day,” said Elizabeth, “my mother’s purpose will be answered.”"}
{"doc_id": "gutenberg_1342", "para_id": 174, "text": "She did at last extort from her father an acknowledgment that the horses were engaged; Jane was therefore obliged to go on horseback, and her mother attended her to the door with many cheerful prognostics of a bad day. Her hopes were answered; Jane had not been gone long before it rained hard. Her sisters were uneasy for her, but her mother was delighted. The rain continued the whole evening without intermission; Jane certainly could not come back."}
{"doc_id": "gutenberg_1342", "para_id": 175, "text": "“This was a lucky idea of mine, indeed!” said Mrs. Bennet, more than once, as if the credit of making it rain were all her own. Till the next morning, however, she was not aware of all the felicity of her contrivance. Breakfast was scarcely over when a servant from Netherfield brought the following note for Elizabeth:--"}
{"doc_id": "gutenberg_1342", "para_id": 176, "text": "“I find myself very unwell this morning, which, I suppose, is to be imputed to my getting wet through yesterday. My kind friends will not hear of my returning home till I am better. They insist also on my seeing Mr. Jones--therefore do not be alarmed if you should hear of his having been to me--and, excepting a sore throat and a headache, there is not much the matter with me."}
{"doc_id": "gutenberg_1342", "para_id": 177, "text": "“Well, my dear,” said Mr. Bennet, when Elizabeth had read the note aloud, “if your daughter should have a dangerous fit of illness--if she should die--it would be a comfort to know that it was all in pursuit of Mr. Bingley, and under your orders.”"}
{"doc_id": "gutenberg_1342", "para_id": 178, "text": "“Oh, I am not at all afraid of her dying. People do not die of little trifling colds. She will be taken good care of. As long as she stays there, it is all very well. I would go and see her if I could have the carriage.”"}
{"doc_id": "gutenberg_1342", "para_id": 179, "text": "Elizabeth, feeling really anxious, determined to go to her, though the carriage was not to be had: and as she was no horsewoman, walking was her only alternative. She declared her resolution."}
{"doc_id": "gutenberg_1342", "para_id": 180, "text": "“How can you be so silly,” cried her mother, “as to think of such a thing, in all this dirt! You will not be fit to be seen when you get there.”"}
{"doc_id": "gutenberg_1342", "para_id": 181, "text": "“No, indeed. I do not wish to avoid the walk. The distance is nothing, when one has a motive; only three miles. I shall be back by dinner.”"}
{"doc_id": "gutenberg_1342", "para_id": 182, "text": "“I admire the activity of your benevolence,” observed Mary, “but every impulse of feeling should be guided by reason; and, in my opinion, exertion should always be in proportion to what is required.”"}
{"doc_id": "gutenberg_1342", "para_id": 183, "text": "“We will go as far as Meryton with you,” said Catherine and Lydia. Elizabeth accepted their company, and the three young ladies set off together."}
{"doc_id": "gutenberg_1342", "para_id": 184, "text": "“If we make haste,” said Lydia, as they walked along, “perhaps we may see something of Captain Carter, before he goes.”"}
{"doc_id": "gutenberg_1342", "para_id": 185, "text": "In Meryton they parted: the two youngest repaired to the lodgings of one of the officers’ wives, and Elizabeth continued her walk alone, crossing field after field at a quick pace, jumping over stiles and springing over puddles, with impatient activity, and finding herself at last within view of the house, with weary ancles, dirty stockings, and a face glowing with the warmth of exercise."}
{"doc_id": "gutenberg_1342", "para_id": 186, "text": "She was shown into the breakfast parlour, where all but Jane were assembled, and where her appearance created a great deal of surprise. That she should have walked three miles so early in the day in such dirty weather, and by herself, was almost incredible to Mrs. Hurst and Miss Bingley; and Elizabeth was convinced that they held her in contempt for it. She was received, however, very politely by them; and in their brother’s manners there was something better than politeness--there was good-humour and kindness. Mr. Darcy said very little, and Mr. Hurst nothing at all. The former was divided between admiration of the brilliancy which exercise had given to her complexion and doubt as to the occasion’s justifying her coming so far alone. The latter was thinking only of his breakfast."}
{"doc_id": "gutenberg_1342", "para_id": 187, "text": "Her inquiries after her sister were not very favourably answered. Miss Bennet had slept ill, and though up, was very feverish, and not well enough to leave her room. Elizabeth was glad to be taken to her immediately; and Jane, who had only been withheld by the fear of giving alarm or inconvenience, from expressing in her note how much she longed for such a visit, was delighted at her entrance. She was not equal, however, to much conversation; and when Miss Bingley left them together, could attempt little beside expressions of gratitude for the extraordinary kindness she was treated with. Elizabeth silently attended her."}
{"doc_id": "gutenberg_1342", "para_id": 188, "text": "When breakfast was over, they were joined by the sisters; and Elizabeth began to like them herself, when she saw how much affection and solicitude they showed for Jane. The apothecary came; and having examined his patient, said, as might be supposed, that she had caught a violent cold, and that they must endeavour to get the better of it; advised her to return to bed, and promised her some draughts. The advice was followed readily, for the feverish symptoms increased, and her head ached acutely. Elizabeth did not quit her room for a moment, nor were the other ladies often absent; the gentlemen being out, they had in fact nothing to do elsewhere."}
{"doc_id": "gutenberg_1342", "para_id": 189, "text": "When the clock struck three, Elizabeth felt that she must go, and very unwillingly said so. Miss Bingley offered her the carriage, and she only wanted a little pressing to accept it, when Jane testified such concern at parting with her that Miss Bingley was obliged to convert the offer of the chaise into an invitation to remain at Netherfield for the present. Elizabeth most thankfully consented, and a servant was despatched to Longbourn, to acquaint the family with her stay, and bring back a supply of clothes."}
{"doc_id": "gutenberg_1342", "para_id": 190, "text": "At five o’clock the two ladies retired to dress, and at half-past six Elizabeth was summoned to dinner. To the civil inquiries which then poured in, and amongst which she had the pleasure of distinguishing the much superior solicitude of Mr. Bingley, she could not make a very favourable answer. Jane was by no means better. The sisters, on hearing this, repeated three or four times how much they were grieved, how shocking it was to have a bad cold, and how excessively they disliked being ill themselves; and then thought no more of the matter: and their indifference towards Jane, when not immediately before them, restored Elizabeth to the enjoyment of all her original dislike."}
{"doc_id": "gutenberg_1342", "para_id": 191, "text": "Their brother, indeed, was the only one of the party whom she could regard with any complacency. His anxiety for Jane was evident, and his attentions to herself most pleasing; and they prevented her feeling herself so much an intruder as she believed she was considered by the others. She had very little notice from any but him. Miss Bingley was engrossed by Mr. Darcy, her sister scarcely less so; and as for Mr. Hurst, by whom Elizabeth sat, he was an indolent man, who lived only to eat, drink, and play at cards, who, when he found her prefer a plain dish to a ragout, had nothing to say to her."}
{"doc_id": "gutenberg_1342", "para_id": 192, "text": "When dinner was over, she returned directly to Jane, and Miss Bingley began abusing her as soon as she was out of the room. Her manners were pronounced to be very bad indeed,--a mixture of pride and impertinence: she had no conversation, no style, no taste, no beauty. Mrs. Hurst thought the same, and added,--"}
{"doc_id": "gutenberg_1342", "para_id": 193, "text": "“She has nothing, in short, to recommend her, but being an excellent walker. I shall never forget her appearance this morning. She really looked almost wild.”"}
{"doc_id": "gutenberg_1342", "para_id": 194, "text": "“She did indeed, Louisa. I could hardly keep my countenance. Very nonsensical to come at all! Why must _she_ be scampering about the country, because her sister had a cold? Her hair so untidy, so blowzy!”"}
{"doc_id": "gutenberg_1342", "para_id": 195, "text": "“Yes, and her petticoat; I hope you saw her petticoat, six inches deep in mud, I am absolutely certain, and the gown which had been let down to hide it not doing its office.”"}
{"doc_id": "gutenberg_1342", "para_id": 196, "text": "“Your picture may be very exact, Louisa,” said Bingley; “but this was all lost upon me. I thought Miss Elizabeth Bennet looked remarkably well when she came into the room this morning. Her dirty petticoat quite escaped my notice.”"}
{"doc_id": "gutenberg_1342", "para_id": 197, "text": "“_You_ observed it, Mr. Darcy, I am sure,” said Miss Bingley; “and I am inclined to think that you would not wish to see _your sister_ make such an exhibition.”"}
{"doc_id": "gutenberg_1342", "para_id": 198, "text": "“To walk three miles, or four miles, or five miles, or whatever it is, above her ancles in dirt, and alone, quite alone! what could she mean by it? It seems to me to show an abominable sort of conceited independence, a most country-town indifference to decorum.”"}
{"doc_id": "gutenberg_1342", "para_id": 199, "text": "“I am afraid, Mr. Darcy,” observed Miss Bingley, in a half whisper, “that this adventure has rather affected your admiration of her fine eyes.”"}
{"doc_id": "gutenberg_1342", "para_id": 200, "text": "“Not at all,” he replied: “they were brightened by the exercise.” A short pause followed this speech, and Mrs. Hurst began again,--"}
{"doc_id": "gutenberg_1342", "para_id": 201, "text": "“I have an excessive regard for Jane Bennet,--she is really a very sweet girl,--and I wish with all my heart she were well settled. But with such a father and mother, and such low connections, I am afraid there is no chance of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 202, "text": "“If they had uncles enough to fill _all_ Cheapside,” cried Bingley, “it would not make them one jot less agreeable.”"}
{"doc_id": "gutenberg_1342", "para_id": 203, "text": "“But it must very materially lessen their chance of marrying men of any consideration in the world,” replied Darcy."}
{"doc_id": "gutenberg_1342", "para_id": 204, "text": "To this speech Bingley made no answer; but his sisters gave it their hearty assent, and indulged their mirth for some time at the expense of their dear friend’s vulgar relations."}
{"doc_id": "gutenberg_1342", "para_id": 205, "text": "With a renewal of tenderness, however, they repaired to her room on leaving the dining-parlour, and sat with her till summoned to coffee. She was still very poorly, and Elizabeth would not quit her at all, till late in the evening, when she had the comfort of seeing her asleep, and when it appeared to her rather right than pleasant that she should go down stairs herself. On entering the drawing-room, she found the whole party at loo, and was immediately invited to join them; but suspecting them to be playing high, she declined it, and making her sister the excuse, said she would amuse herself, for the short time she could stay below, with a book. Mr. Hurst looked at her with astonishment."}
{"doc_id": "gutenberg_1342", "para_id": 206, "text": "“Miss Eliza Bennet,” said Miss Bingley, “despises cards. She is a great reader, and has no pleasure in anything else.”"}
{"doc_id": "gutenberg_1342", "para_id": 207, "text": "“I deserve neither such praise nor such censure,” cried Elizabeth; “I am _not_ a great reader, and I have pleasure in many things.”"}
{"doc_id": "gutenberg_1342", "para_id": 208, "text": "“In nursing your sister I am sure you have pleasure,” said Bingley; “and I hope it will soon be increased by seeing her quite well.”"}
{"doc_id": "gutenberg_1342", "para_id": 209, "text": "Elizabeth thanked him from her heart, and then walked towards a table where a few books were lying. He immediately offered to fetch her others; all that his library afforded."}
{"doc_id": "gutenberg_1342", "para_id": 210, "text": "“And I wish my collection were larger for your benefit and my own credit; but I am an idle fellow; and though I have not many, I have more than I ever looked into.”"}
{"doc_id": "gutenberg_1342", "para_id": 211, "text": "Elizabeth assured him that she could suit herself perfectly with those in the room."}
{"doc_id": "gutenberg_1342", "para_id": 212, "text": "“I am astonished,” said Miss Bingley, “that my father should have left so small a collection of books. What a delightful library you have at Pemberley, Mr. Darcy!”"}
{"doc_id": "gutenberg_1342", "para_id": 213, "text": "“Neglect! I am sure you neglect nothing that can add to the beauties of that noble place. Charles, when you build _your_ house, I wish it may be half as delightful as Pemberley.”"}
{"doc_id": "gutenberg_1342", "para_id": 214, "text": "“But I would really advise you to make your purchase in that neighbourhood, and take Pemberley for a kind of model. There is not a finer county in England than Derbyshire.”"}
{"doc_id": "gutenberg_1342", "para_id": 215, "text": "“Upon my word, Caroline, I should think it more possible to get Pemberley by purchase than by imitation.”"}
{"doc_id": "gutenberg_1342", "para_id": 216, "text": "Elizabeth was so much caught by what passed, as to leave her very little attention for her book; and, soon laying it wholly aside, she drew near the card-table, and stationed herself between Mr. Bingley and his eldest sister, to observe the game."}
{"doc_id": "gutenberg_1342", "para_id": 217, "text": "“Is Miss Darcy much grown since the spring?” said Miss Bingley: “will she be as tall as I am?”"}
{"doc_id": "gutenberg_1342", "para_id": 218, "text": "“I think she will. She is now about Miss Elizabeth Bennet’s height, or rather taller.”"}
{"doc_id": "gutenberg_1342", "para_id": 219, "text": "“How I long to see her again! I never met with anybody who delighted me so much. Such a countenance, such manners, and so extremely accomplished for her age! Her performance on the pianoforte is exquisite.”"}
{"doc_id": "gutenberg_1342", "para_id": 220, "text": "“It is amazing to me,” said Bingley, “how young ladies can have patience to be so very accomplished as they all are.”"}
{"doc_id": "gutenberg_1342", "para_id": 221, "text": "“Yes, all of them, I think. They all paint tables, cover screens, and net purses. I scarcely know any one who cannot do all this; and I am sure I never heard a young lady spoken of for the first time, without being informed that she was very accomplished.”"}
{"doc_id": "gutenberg_1342", "para_id": 222, "text": "“Your list of the common extent of accomplishments,” said Darcy, “has too much truth. The word is applied to many a woman who deserves it no otherwise than by netting a purse or covering a screen; but I am very far from agreeing with you in your estimation of ladies in general. I cannot boast of knowing more than half-a-dozen in the whole range of my acquaintance that are really accomplished.”"}
{"doc_id": "gutenberg_1342", "para_id": 223, "text": "“Then,” observed Elizabeth, “you must comprehend a great deal in your idea of an accomplished woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 224, "text": "“Oh, certainly,” cried his faithful assistant, “no one can be really esteemed accomplished who does not greatly surpass what is usually met with. A woman must have a thorough knowledge of music, singing, drawing, dancing, and the modern languages, to deserve the word; and, besides all this, she must possess a certain something in her air and manner of walking, the tone of her voice, her address and expressions, or the word will be but half deserved.”"}
{"doc_id": "gutenberg_1342", "para_id": 225, "text": "“All this she must possess,” added Darcy; “and to all she must yet add something more substantial in the improvement of her mind by extensive reading.”"}
{"doc_id": "gutenberg_1342", "para_id": 226, "text": "“I am no longer surprised at your knowing _only_ six accomplished women. I rather wonder now at your knowing _any_.”"}
{"doc_id": "gutenberg_1342", "para_id": 227, "text": "“_I_ never saw such a woman. _I_ never saw such capacity, and taste, and application, and elegance, as you describe, united.”"}
{"doc_id": "gutenberg_1342", "para_id": 228, "text": "Mrs. Hurst and Miss Bingley both cried out against the injustice of her implied doubt, and were both protesting that they knew many women who answered this description, when Mr. Hurst called them to order, with bitter complaints of their inattention to what was going forward. As all conversation was thereby at an end, Elizabeth soon afterwards left the room."}
{"doc_id": "gutenberg_1342", "para_id": 229, "text": "“Eliza Bennet,” said Miss Bingley, when the door was closed on her, “is one of those young ladies who seek to recommend themselves to the other sex by undervaluing their own; and with many men, I daresay, it succeeds; but, in my opinion, it is a paltry device, a very mean art.”"}
{"doc_id": "gutenberg_1342", "para_id": 230, "text": "“Undoubtedly,” replied Darcy, to whom this remark was chiefly addressed, “there is meanness in _all_ the arts which ladies sometimes condescend to employ for captivation. Whatever bears affinity to cunning is despicable.”"}
{"doc_id": "gutenberg_1342", "para_id": 231, "text": "Miss Bingley was not so entirely satisfied with this reply as to continue the subject."}
{"doc_id": "gutenberg_1342", "para_id": 232, "text": "Elizabeth joined them again only to say that her sister was worse, and that she could not leave her. Bingley urged Mr. Jones’s being sent for immediately; while his sisters, convinced that no country advice could be of any service, recommended an express to town for one of the most eminent physicians. This she would not hear of; but she was not so unwilling to comply with their brother’s proposal; and it was settled that Mr. Jones should be sent for early in the morning, if Miss Bennet were not decidedly better. Bingley was quite uncomfortable; his sisters declared that they were miserable. They solaced their wretchedness, however, by duets after supper; while he could find no better relief to his feelings than by giving his housekeeper directions that every possible attention might be paid to the sick lady and her sister."}
{"doc_id": "gutenberg_1342", "para_id": 233, "text": "Elizabeth passed the chief of the night in her sister’s room, and in the morning had the pleasure of being able to send a tolerable answer to the inquiries which she very early received from Mr. Bingley by a housemaid, and some time afterwards from the two elegant ladies who waited on his sisters. In spite of this amendment, however, she requested to have a note sent to Longbourn, desiring her mother to visit Jane, and form her own judgment of her situation. The note was immediately despatched, and its contents as quickly complied with. Mrs. Bennet, accompanied by her two youngest girls, reached Netherfield soon after the family breakfast."}
{"doc_id": "gutenberg_1342", "para_id": 234, "text": "Had she found Jane in any apparent danger, Mrs. Bennet would have been very miserable; but being satisfied on seeing her that her illness was not alarming, she had no wish of her recovering immediately, as her restoration to health would probably remove her from Netherfield. She would not listen, therefore, to her daughter’s proposal of being carried home; neither did the apothecary, who arrived about the same time, think it at all advisable. After sitting a little while with Jane, on Miss Bingley’s appearance and invitation, the mother and three daughters all attended her into the breakfast parlour. Bingley met them with hopes that Mrs. Bennet had not found Miss Bennet worse than she expected."}
{"doc_id": "gutenberg_1342", "para_id": 235, "text": "“Indeed I have, sir,” was her answer. “She is a great deal too ill to be moved. Mr. Jones says we must not think of moving her. We must trespass a little longer on your kindness.”"}
{"doc_id": "gutenberg_1342", "para_id": 236, "text": "“Removed!” cried Bingley. “It must not be thought of. My sister, I am sure, will not hear of her removal.”"}
{"doc_id": "gutenberg_1342", "para_id": 237, "text": "“You may depend upon it, madam,” said Miss Bingley, with cold civility, “that Miss Bennet shall receive every possible attention while she remains with us.”"}
{"doc_id": "gutenberg_1342", "para_id": 238, "text": "“I am sure,” she added, “if it was not for such good friends, I do not know what would become of her, for she is very ill indeed, and suffers a vast deal, though with the greatest patience in the world, which is always the way with her, for she has, without exception, the sweetest temper I ever met with. I often tell my other girls they are nothing to _her_. You have a sweet room here, Mr. Bingley, and a charming prospect over that gravel walk. I do not know a place in the country that is equal to Netherfield. You will not think of quitting it in a hurry, I hope, though you have but a short lease.”"}
{"doc_id": "gutenberg_1342", "para_id": 239, "text": "“Whatever I do is done in a hurry,” replied he; “and therefore if I should resolve to quit Netherfield, I should probably be off in five minutes. At present, however, I consider myself as quite fixed here.”"}
{"doc_id": "gutenberg_1342", "para_id": 240, "text": "“I wish I might take this for a compliment; but to be so easily seen through, I am afraid, is pitiful.”"}
{"doc_id": "gutenberg_1342", "para_id": 241, "text": "“That is as it happens. It does not necessarily follow that a deep, intricate character is more or less estimable than such a one as yours.”"}
{"doc_id": "gutenberg_1342", "para_id": 242, "text": "“Lizzy,” cried her mother, “remember where you are, and do not run on in the wild manner that you are suffered to do at home.”"}
{"doc_id": "gutenberg_1342", "para_id": 243, "text": "“I did not know before,” continued Bingley, immediately, “that you were a studier of character. It must be an amusing study.”"}
{"doc_id": "gutenberg_1342", "para_id": 244, "text": "“Yes; but intricate characters are the _most_ amusing. They have at least that advantage.”"}
{"doc_id": "gutenberg_1342", "para_id": 245, "text": "“The country,” said Darcy, “can in general supply but few subjects for such a study. In a country neighbourhood you move in a very confined and unvarying society.”"}
{"doc_id": "gutenberg_1342", "para_id": 246, "text": "“But people themselves alter so much, that there is something new to be observed in them for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 247, "text": "“Yes, indeed,” cried Mrs. Bennet, offended by his manner of mentioning a country neighbourhood. “I assure you there is quite as much of _that_ going on in the country as in town.”"}
{"doc_id": "gutenberg_1342", "para_id": 248, "text": "Everybody was surprised; and Darcy, after looking at her for a moment, turned silently away. Mrs. Bennet, who fancied she had gained a complete victory over him, continued her triumph,--"}
{"doc_id": "gutenberg_1342", "para_id": 249, "text": "“I cannot see that London has any great advantage over the country, for my part, except the shops and public places. The country is a vast deal pleasanter, is not it, Mr. Bingley?”"}
{"doc_id": "gutenberg_1342", "para_id": 250, "text": "“When I am in the country,” he replied, “I never wish to leave it; and when I am in town, it is pretty much the same. They have each their advantages, and I can be equally happy in either.”"}
{"doc_id": "gutenberg_1342", "para_id": 251, "text": "“Ay, that is because you have the right disposition. But that gentleman,” looking at Darcy, “seemed to think the country was nothing at all.”"}
{"doc_id": "gutenberg_1342", "para_id": 252, "text": "“Indeed, mamma, you are mistaken,” said Elizabeth, blushing for her mother. “You quite mistook Mr. Darcy. He only meant that there was not such a variety of people to be met with in the country as in town, which you must acknowledge to be true.”"}
{"doc_id": "gutenberg_1342", "para_id": 253, "text": "“Certainly, my dear, nobody said there were; but as to not meeting with many people in this neighbourhood, I believe there are few neighbourhoods larger. I know we dine with four-and-twenty families.”"}
{"doc_id": "gutenberg_1342", "para_id": 254, "text": "Nothing but concern for Elizabeth could enable Bingley to keep his countenance. His sister was less delicate, and directed her eye towards Mr. Darcy with a very expressive smile. Elizabeth, for the sake of saying something that might turn her mother’s thoughts, now asked her if Charlotte Lucas had been at Longbourn since _her_ coming away."}
{"doc_id": "gutenberg_1342", "para_id": 255, "text": "“Yes, she called yesterday with her father. What an agreeable man Sir William is, Mr. Bingley--is not he? so much the man of fashion! so genteel and so easy! He has always something to say to everybody. _That_ is my idea of good breeding; and those persons who fancy themselves very important and never open their mouths quite mistake the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 256, "text": "“No, she would go home. I fancy she was wanted about the mince-pies. For my part, Mr. Bingley, _I_ always keep servants that can do their own work; _my_ daughters are brought up differently. But everybody is to judge for themselves, and the Lucases are a very good sort of girls, I assure you. It is a pity they are not handsome! Not that _I_ think Charlotte so _very_ plain; but then she is our particular friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 257, "text": "“Oh dear, yes; but you must own she is very plain. Lady Lucas herself has often said so, and envied me Jane’s beauty. I do not like to boast of my own child; but to be sure, Jane--one does not often see anybody better looking. It is what everybody says. I do not trust my own partiality. When she was only fifteen there was a gentleman at my brother Gardiner’s in town so much in love with her, that my sister-in-law was sure he would make her an offer before we came away. But, however, he did not. Perhaps he thought her too young. However, he wrote some verses on her, and very pretty they were.”"}
{"doc_id": "gutenberg_1342", "para_id": 258, "text": "“And so ended his affection,” said Elizabeth, impatiently. “There has been many a one, I fancy, overcome in the same way. I wonder who first discovered the efficacy of poetry in driving away love!”"}
{"doc_id": "gutenberg_1342", "para_id": 259, "text": "“Of a fine, stout, healthy love it may. Everything nourishes what is strong already. But if it be only a slight, thin sort of inclination, I am convinced that one good sonnet will starve it entirely away.”"}
{"doc_id": "gutenberg_1342", "para_id": 260, "text": "Darcy only smiled; and the general pause which ensued made Elizabeth tremble lest her mother should be exposing herself again. She longed to speak, but could think of nothing to say; and after a short silence Mrs. Bennet began repeating her thanks to Mr. Bingley for his kindness to Jane, with an apology for troubling him also with Lizzy. Mr. Bingley was unaffectedly civil in his answer, and forced his younger sister to be civil also, and say what the occasion required. She performed her part, indeed, without much graciousness, but Mrs. Bennet was satisfied, and soon afterwards ordered her carriage. Upon this signal, the youngest of her daughters put herself forward. The two girls had been whispering to each other during the whole visit; and the result of it was, that the youngest should tax Mr. Bingley with having promised on his first coming into the country to give a ball at Netherfield."}
{"doc_id": "gutenberg_1342", "para_id": 261, "text": "Lydia was a stout, well-grown girl of fifteen, with a fine complexion and good-humoured countenance; a favourite with her mother, whose affection had brought her into public at an early age. She had high animal spirits, and a sort of natural self-consequence, which the attentions of the officers, to whom her uncle’s good dinners and her own easy manners recommended her, had increased into assurance. She was very equal, therefore, to address Mr. Bingley on the subject of the ball, and abruptly reminded him of his promise; adding, that it would be the most shameful thing in the world if he did not keep it. His answer to this sudden attack was delightful to her mother’s ear."}
{"doc_id": "gutenberg_1342", "para_id": 262, "text": "“I am perfectly ready, I assure you, to keep my engagement; and, when your sister is recovered, you shall, if you please, name the very day of the ball. But you would not wish to be dancing while she is ill?”"}
{"doc_id": "gutenberg_1342", "para_id": 263, "text": "Lydia declared herself satisfied. “Oh yes--it would be much better to wait till Jane was well; and by that time, most likely, Captain Carter would be at Meryton again. And when you have given _your_ ball,” she added, “I shall insist on their giving one also. I shall tell Colonel Forster it will be quite a shame if he does not.”"}
{"doc_id": "gutenberg_1342", "para_id": 264, "text": "Mrs. Bennet and her daughters then departed, and Elizabeth returned instantly to Jane, leaving her own and her relations’ behaviour to the remarks of the two ladies and Mr. Darcy; the latter of whom, however, could not be prevailed on to join in their censure of _her_, in spite of all Miss Bingley’s witticisms on _fine eyes_."}
{"doc_id": "gutenberg_1342", "para_id": 265, "text": "The day passed much as the day before had done. Mrs. Hurst and Miss Bingley had spent some hours of the morning with the invalid, who continued, though slowly, to mend; and, in the evening, Elizabeth joined their party in the drawing-room. The loo table, however, did not appear. Mr. Darcy was writing, and Miss Bingley, seated near him, was watching the progress of his letter, and repeatedly calling off his attention by messages to his sister. Mr. Hurst and Mr. Bingley were at piquet, and Mrs. Hurst was observing their game."}
{"doc_id": "gutenberg_1342", "para_id": 266, "text": "Elizabeth took up some needlework, and was sufficiently amused in attending to what passed between Darcy and his companion. The perpetual commendations of the lady either on his hand-writing, or on the evenness of his lines, or on the length of his letter, with the perfect unconcern with which her praises were received, formed a curious dialogue, and was exactly in unison with her opinion of each."}
{"doc_id": "gutenberg_1342", "para_id": 267, "text": "“How many letters you must have occasion to write in the course of a year! Letters of business, too! How odious I should think them!”"}
{"doc_id": "gutenberg_1342", "para_id": 268, "text": "“I am afraid you do not like your pen. Let me mend it for you. I mend pens remarkably well.”"}
{"doc_id": "gutenberg_1342", "para_id": 269, "text": "“Tell your sister I am delighted to hear of her improvement on the harp, and pray let her know that I am quite in raptures with her beautiful little design for a table, and I think it infinitely superior to Miss Grantley’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 270, "text": "“Will you give me leave to defer your raptures till I write again? At present I have not room to do them justice.”"}
{"doc_id": "gutenberg_1342", "para_id": 271, "text": "“Oh, it is of no consequence. I shall see her in January. But do you always write such charming long letters to her, Mr. Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 272, "text": "“They are generally long; but whether always charming, it is not for me to determine.”"}
{"doc_id": "gutenberg_1342", "para_id": 273, "text": "“It is a rule with me, that a person who can write a long letter with ease cannot write ill.”"}
{"doc_id": "gutenberg_1342", "para_id": 274, "text": "“That will not do for a compliment to Darcy, Caroline,” cried her brother, “because he does _not_ write with ease. He studies too much for words of four syllables. Do not you, Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 275, "text": "“Oh,” cried Miss Bingley, “Charles writes in the most careless way imaginable. He leaves out half his words, and blots the rest.”"}
{"doc_id": "gutenberg_1342", "para_id": 276, "text": "“My ideas flow so rapidly that I have not time to express them; by which means my letters sometimes convey no ideas at all to my correspondents.”"}
{"doc_id": "gutenberg_1342", "para_id": 277, "text": "“Nothing is more deceitful,” said Darcy, “than the appearance of humility. It is often only carelessness of opinion, and sometimes an indirect boast.”"}
{"doc_id": "gutenberg_1342", "para_id": 278, "text": "“The indirect boast; for you are really proud of your defects in writing, because you consider them as proceeding from a rapidity of thought and carelessness of execution, which, if not estimable, you think at least highly interesting. The power of doing anything with quickness is always much prized by the possessor, and often without any attention to the imperfection of the performance. When you told Mrs. Bennet this morning, that if you ever resolved on quitting Netherfield you should be gone in five minutes, you meant it to be a sort of panegyric, of compliment to yourself; and yet what is there so very laudable in a precipitance which must leave very necessary business undone, and can be of no real advantage to yourself or anyone else?”"}
{"doc_id": "gutenberg_1342", "para_id": 279, "text": "“Nay,” cried Bingley, “this is too much, to remember at night all the foolish things that were said in the morning. And yet, upon my honour, I believed what I said of myself to be true, and I believe it at this moment. At least, therefore, I did not assume the character of needless precipitance merely to show off before the ladies.”"}
{"doc_id": "gutenberg_1342", "para_id": 280, "text": "“I daresay you believed it; but I am by no means convinced that you would be gone with such celerity. Your conduct would be quite as dependent on chance as that of any man I know; and if, as you were mounting your horse, a friend were to say, ‘Bingley, you had better stay till next week,’ you would probably do it--you would probably not go--and, at another word, might stay a month.”"}
{"doc_id": "gutenberg_1342", "para_id": 281, "text": "“You have only proved by this,” cried Elizabeth, “that Mr. Bingley did not do justice to his own disposition. You have shown him off now much more than he did himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 282, "text": "“I am exceedingly gratified,” said Bingley, “by your converting what my friend says into a compliment on the sweetness of my temper. But I am afraid you are giving it a turn which that gentleman did by no means intend; for he would certainly think the better of me if, under such a circumstance, I were to give a flat denial, and ride off as fast as I could.”"}
{"doc_id": "gutenberg_1342", "para_id": 283, "text": "“Would Mr. Darcy then consider the rashness of your original intention as atoned for by your obstinacy in adhering to it?”"}
{"doc_id": "gutenberg_1342", "para_id": 284, "text": "“Upon my word, I cannot exactly explain the matter--Darcy must speak for himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 285, "text": "“You expect me to account for opinions which you choose to call mine, but which I have never acknowledged. Allowing the case, however, to stand according to your representation, you must remember, Miss Bennet, that the friend who is supposed to desire his return to the house, and the delay of his plan, has merely desired it, asked it without offering one argument in favour of its propriety.”"}
{"doc_id": "gutenberg_1342", "para_id": 286, "text": "“To yield readily--easily--to the _persuasion_ of a friend is no merit with you.”"}
{"doc_id": "gutenberg_1342", "para_id": 287, "text": "“You appear to me, Mr. Darcy, to allow nothing for the influence of friendship and affection. A regard for the requester would often make one readily yield to a request, without waiting for arguments to reason one into it. I am not particularly speaking of such a case as you have supposed about Mr. Bingley. We may as well wait, perhaps, till the circumstance occurs, before we discuss the discretion of his behaviour thereupon. But in general and ordinary cases, between friend and friend, where one of them is desired by the other to change a resolution of no very great moment, should you think ill of that person for complying with the desire, without waiting to be argued into it?”"}
{"doc_id": "gutenberg_1342", "para_id": 288, "text": "“Will it not be advisable, before we proceed on this subject, to arrange with rather more precision the degree of importance which is to appertain to this request, as well as the degree of intimacy subsisting between the parties?”"}
{"doc_id": "gutenberg_1342", "para_id": 289, "text": "“By all means,” cried Bingley; “let us hear all the particulars, not forgetting their comparative height and size, for that will have more weight in the argument, Miss Bennet, than you may be aware of. I assure you that if Darcy were not such a great tall fellow, in comparison with myself, I should not pay him half so much deference. I declare I do not know a more awful object than Darcy on particular occasions, and in particular places; at his own house especially, and of a Sunday evening, when he has nothing to do.”"}
{"doc_id": "gutenberg_1342", "para_id": 290, "text": "Mr. Darcy smiled; but Elizabeth thought she could perceive that he was rather offended, and therefore checked her laugh. Miss Bingley warmly resented the indignity he had received, in an expostulation with her brother for talking such nonsense."}
{"doc_id": "gutenberg_1342", "para_id": 291, "text": "“I see your design, Bingley,” said his friend. “You dislike an argument, and want to silence this.”"}
{"doc_id": "gutenberg_1342", "para_id": 292, "text": "“Perhaps I do. Arguments are too much like disputes. If you and Miss Bennet will defer yours till I am out of the room, I shall be very thankful; and then you may say whatever you like of me.”"}
{"doc_id": "gutenberg_1342", "para_id": 293, "text": "“What you ask,” said Elizabeth, “is no sacrifice on my side; and Mr. Darcy had much better finish his letter.”"}
{"doc_id": "gutenberg_1342", "para_id": 294, "text": "When that business was over, he applied to Miss Bingley and Elizabeth for the indulgence of some music. Miss Bingley moved with alacrity to the pianoforte, and after a polite request that Elizabeth would lead the way, which the other as politely and more earnestly negatived, she seated herself."}
{"doc_id": "gutenberg_1342", "para_id": 295, "text": "Mrs. Hurst sang with her sister; and while they were thus employed, Elizabeth could not help observing, as she turned over some music-books that lay on the instrument, how frequently Mr. Darcy’s eyes were fixed on her. She hardly knew how to suppose that she could be an object of admiration to so great a man, and yet that he should look at her because he disliked her was still more strange. She could only imagine, however, at last, that she drew his notice because there was something about her more wrong and reprehensible, according to his ideas of right, than in any other person present. The supposition did not pain her. She liked him too little to care for his approbation."}
{"doc_id": "gutenberg_1342", "para_id": 296, "text": "After playing some Italian songs, Miss Bingley varied the charm by a lively Scotch air; and soon afterwards Mr. Darcy, drawing near Elizabeth, said to her,--"}
{"doc_id": "gutenberg_1342", "para_id": 297, "text": "“Do you not feel a great inclination, Miss Bennet, to seize such an opportunity of dancing a reel?”"}
{"doc_id": "gutenberg_1342", "para_id": 298, "text": "She smiled, but made no answer. He repeated the question, with some surprise at her silence."}
{"doc_id": "gutenberg_1342", "para_id": 299, "text": "“Oh,” said she, “I heard you before; but I could not immediately determine what to say in reply. You wanted me, I know, to say ‘Yes,’ that you might have the pleasure of despising my taste; but I always delight in overthrowing those kind of schemes, and cheating a person of their premeditated contempt. I have, therefore, made up my mind to tell you that I do not want to dance a reel at all; and now despise me if you dare.”"}
{"doc_id": "gutenberg_1342", "para_id": 300, "text": "Elizabeth, having rather expected to affront him, was amazed at his gallantry; but there was a mixture of sweetness and archness in her manner which made it difficult for her to affront anybody, and Darcy had never been so bewitched by any woman as he was by her. He really believed that, were it not for the inferiority of her connections, he should be in some danger."}
{"doc_id": "gutenberg_1342", "para_id": 301, "text": "Miss Bingley saw, or suspected, enough to be jealous; and her great anxiety for the recovery of her dear friend Jane received some assistance from her desire of getting rid of Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 302, "text": "She often tried to provoke Darcy into disliking her guest, by talking of their supposed marriage, and planning his happiness in such an alliance."}
{"doc_id": "gutenberg_1342", "para_id": 303, "text": "“I hope,” said she, as they were walking together in the shrubbery the next day, “you will give your mother-in-law a few hints, when this desirable event takes place, as to the advantage of holding her tongue; and if you can compass it, to cure the younger girls of running after the officers. And, if I may mention so delicate a subject, endeavour to check that little something, bordering on conceit and impertinence, which your lady possesses.”"}
{"doc_id": "gutenberg_1342", "para_id": 304, "text": "“Oh yes. Do let the portraits of your uncle and aunt Philips be placed in the gallery at Pemberley. Put them next to your great-uncle the judge. They are in the same profession, you know, only in different lines. As for your Elizabeth’s picture, you must not attempt to have it taken, for what painter could do justice to those beautiful eyes?”"}
{"doc_id": "gutenberg_1342", "para_id": 305, "text": "“It would not be easy, indeed, to catch their expression; but their colour and shape, and the eyelashes, so remarkably fine, might be copied.”"}
{"doc_id": "gutenberg_1342", "para_id": 306, "text": "At that moment they were met from another walk by Mrs. Hurst and Elizabeth herself."}
{"doc_id": "gutenberg_1342", "para_id": 307, "text": "“I did not know that you intended to walk,” said Miss Bingley, in some confusion, lest they had been overheard."}
{"doc_id": "gutenberg_1342", "para_id": 308, "text": "“You used us abominably ill,” answered Mrs. Hurst, “running away without telling us that you were coming out.”"}
{"doc_id": "gutenberg_1342", "para_id": 309, "text": "Then taking the disengaged arm of Mr. Darcy, she left Elizabeth to walk by herself. The path just admitted three. Mr. Darcy felt their rudeness, and immediately said,--"}
{"doc_id": "gutenberg_1342", "para_id": 310, "text": "But Elizabeth, who had not the least inclination to remain with them, laughingly answered,--"}
{"doc_id": "gutenberg_1342", "para_id": 311, "text": "“No, no; stay where you are. You are charmingly grouped, and appear to uncommon advantage. The picturesque would be spoilt by admitting a fourth. Good-bye.”"}
{"doc_id": "gutenberg_1342", "para_id": 312, "text": "She then ran gaily off, rejoicing, as she rambled about, in the hope of being at home again in a day or two. Jane was already so much recovered as to intend leaving her room for a couple of hours that evening."}
{"doc_id": "gutenberg_1342", "para_id": 313, "text": "When the ladies removed after dinner Elizabeth ran up to her sister, and seeing her well guarded from cold, attended her into the drawing-room, where she was welcomed by her two friends with many professions of pleasure; and Elizabeth had never seen them so agreeable as they were during the hour which passed before the gentlemen appeared. Their powers of conversation were considerable. They could describe an entertainment with accuracy, relate an anecdote with humour, and laugh at their acquaintance with spirit."}
{"doc_id": "gutenberg_1342", "para_id": 314, "text": "But when the gentlemen entered, Jane was no longer the first object; Miss Bingley’s eyes were instantly turned towards Darcy, and she had something to say to him before he had advanced many steps. He addressed himself directly to Miss Bennet with a polite congratulation; Mr. Hurst also made her a slight bow, and said he was “very glad;” but diffuseness and warmth remained for Bingley’s salutation. He was full of joy and attention. The first half hour was spent in piling up the fire, lest she should suffer from the change of room; and she removed, at his desire, to the other side of the fireplace, that she might be farther from the door. He then sat down by her, and talked scarcely to anyone else. Elizabeth, at work in the opposite corner, saw it all with great delight."}
{"doc_id": "gutenberg_1342", "para_id": 315, "text": "When tea was over Mr. Hurst reminded his sister-in-law of the card-table--but in vain. She had obtained private intelligence that Mr. Darcy did not wish for cards, and Mr. Hurst soon found even his open petition rejected. She assured him that no one intended to play, and the silence of the whole party on the subject seemed to justify her. Mr. Hurst had, therefore, nothing to do but to stretch himself on one of the sofas and go to sleep. Darcy took up a book. Miss Bingley did the same; and Mrs. Hurst, principally occupied in playing with her bracelets and rings, joined now and then in her brother’s conversation with Miss Bennet."}
{"doc_id": "gutenberg_1342", "para_id": 316, "text": "Miss Bingley’s attention was quite as much engaged in watching Mr. Darcy’s progress through _his_ book, as in reading her own; and she was perpetually either making some inquiry, or looking at his page. She could not win him, however, to any conversation; he merely answered her question and read on. At length, quite exhausted by the attempt to be amused with her own book, which she had only chosen because it was the second volume of his, she gave a great yawn and said, “How pleasant it is to spend an evening in this way! I declare, after all, there is no enjoyment like reading! How much sooner one tires of anything than of a book! When I have a house of my own, I shall be miserable if I have not an excellent library.”"}
{"doc_id": "gutenberg_1342", "para_id": 317, "text": "No one made any reply. She then yawned again, threw aside her book, and cast her eyes round the room in quest of some amusement; when, hearing her brother mentioning a ball to Miss Bennet, she turned suddenly towards him and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 318, "text": "“By the bye Charles, are you really serious in meditating a dance at Netherfield? I would advise you, before you determine on it, to consult the wishes of the present party; I am much mistaken if there are not some among us to whom a ball would be rather a punishment than a pleasure.”"}
{"doc_id": "gutenberg_1342", "para_id": 319, "text": "“If you mean Darcy,” cried her brother, “he may go to bed, if he chooses, before it begins; but as for the ball, it is quite a settled thing, and as soon as Nicholls has made white soup enough I shall send round my cards.”"}
{"doc_id": "gutenberg_1342", "para_id": 320, "text": "“I should like balls infinitely better,” she replied, “if they were carried on in a different manner; but there is something insufferably tedious in the usual process of such a meeting. It would surely be much more rational if conversation instead of dancing made the order of the day.”"}
{"doc_id": "gutenberg_1342", "para_id": 321, "text": "“Much more rational, my dear Caroline, I dare say; but it would not be near so much like a ball.”"}
{"doc_id": "gutenberg_1342", "para_id": 322, "text": "Miss Bingley made no answer, and soon afterwards got up and walked about the room. Her figure was elegant, and she walked well; but Darcy, at whom it was all aimed, was still inflexibly studious. In the desperation of her feelings, she resolved on one effort more; and, turning to Elizabeth, said,--"}
{"doc_id": "gutenberg_1342", "para_id": 323, "text": "“Miss Eliza Bennet, let me persuade you to follow my example, and take a turn about the room. I assure you it is very refreshing after sitting so long in one attitude.”"}
{"doc_id": "gutenberg_1342", "para_id": 324, "text": "Elizabeth was surprised, but agreed to it immediately. Miss Bingley succeeded no less in the real object of her civility: Mr. Darcy looked up. He was as much awake to the novelty of attention in that quarter as Elizabeth herself could be, and unconsciously closed his book. He was directly invited to join their party, but he declined it, observing that he could imagine but two motives for their choosing to walk up and down the room together, with either of which motives his joining them would interfere. What could he mean? She was dying to know what could be his meaning--and asked Elizabeth whether she could at all understand him."}
{"doc_id": "gutenberg_1342", "para_id": 325, "text": "“Not at all,” was her answer; “but, depend upon it, he means to be severe on us, and our surest way of disappointing him will be to ask nothing about it.”"}
{"doc_id": "gutenberg_1342", "para_id": 326, "text": "Miss Bingley, however, was incapable of disappointing Mr. Darcy in anything, and persevered, therefore, in requiring an explanation of his two motives."}
{"doc_id": "gutenberg_1342", "para_id": 327, "text": "“I have not the smallest objection to explaining them,” said he, as soon as she allowed him to speak. “You either choose this method of passing the evening because you are in each other’s confidence, and have secret affairs to discuss, or because you are conscious that your figures appear to the greatest advantage in walking: if the first, I should be completely in your way; and if the second, I can admire you much better as I sit by the fire.”"}
{"doc_id": "gutenberg_1342", "para_id": 328, "text": "“Oh, shocking!” cried Miss Bingley. “I never heard anything so abominable. How shall we punish him for such a speech?”"}
{"doc_id": "gutenberg_1342", "para_id": 329, "text": "“Nothing so easy, if you have but the inclination,” said Elizabeth. “We can all plague and punish one another. Tease him--laugh at him. Intimate as you are, you must know how it is to be done.”"}
{"doc_id": "gutenberg_1342", "para_id": 330, "text": "“But upon my honour I do _not_. I do assure you that my intimacy has not yet taught me _that_. Tease calmness of temper and presence of mind! No, no; I feel he may defy us there. And as to laughter, we will not expose ourselves, if you please, by attempting to laugh without a subject. Mr. Darcy may hug himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 331, "text": "“Mr. Darcy is not to be laughed at!” cried Elizabeth. “That is an uncommon advantage, and uncommon I hope it will continue, for it would be a great loss to _me_ to have many such acquaintance. I dearly love a laugh.”"}
{"doc_id": "gutenberg_1342", "para_id": 332, "text": "“Miss Bingley,” said he, “has given me credit for more than can be. The wisest and best of men,--nay, the wisest and best of their actions,--may be rendered ridiculous by a person whose first object in life is a joke.”"}
{"doc_id": "gutenberg_1342", "para_id": 333, "text": "“Certainly,” replied Elizabeth, “there are such people, but I hope I am not one of _them_. I hope I never ridicule what is wise or good. Follies and nonsense, whims and inconsistencies, _do_ divert me, I own, and I laugh at them whenever I can. But these, I suppose, are precisely what you are without.”"}
{"doc_id": "gutenberg_1342", "para_id": 334, "text": "“Perhaps that is not possible for anyone. But it has been the study of my life to avoid those weaknesses which often expose a strong understanding to ridicule.”"}
{"doc_id": "gutenberg_1342", "para_id": 335, "text": "“Yes, vanity is a weakness indeed. But pride--where there is a real superiority of mind--pride will be always under good regulation.”"}
{"doc_id": "gutenberg_1342", "para_id": 336, "text": "“Your examination of Mr. Darcy is over, I presume,” said Miss Bingley; “and pray what is the result?”"}
{"doc_id": "gutenberg_1342", "para_id": 337, "text": "“I am perfectly convinced by it that Mr. Darcy has no defect. He owns it himself without disguise.”"}
{"doc_id": "gutenberg_1342", "para_id": 338, "text": "“No,” said Darcy, “I have made no such pretension. I have faults enough, but they are not, I hope, of understanding. My temper I dare not vouch for. It is, I believe, too little yielding; certainly too little for the convenience of the world. I cannot forget the follies and vices of others so soon as I ought, nor their offences against myself. My feelings are not puffed about with every attempt to move them. My temper would perhaps be called resentful. My good opinion once lost is lost for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 339, "text": "“_That_ is a failing, indeed!” cried Elizabeth. “Implacable resentment _is_ a shade in a character. But you have chosen your fault well. I really cannot _laugh_ at it. You are safe from me.”"}
{"doc_id": "gutenberg_1342", "para_id": 340, "text": "“There is, I believe, in every disposition a tendency to some particular evil, a natural defect, which not even the best education can overcome.”"}
{"doc_id": "gutenberg_1342", "para_id": 341, "text": "“Do let us have a little music,” cried Miss Bingley, tired of a conversation in which she had no share. “Louisa, you will not mind my waking Mr. Hurst.”"}
{"doc_id": "gutenberg_1342", "para_id": 342, "text": "Her sister made not the smallest objection, and the pianoforte was opened; and Darcy, after a few moments’ recollection, was not sorry for it. He began to feel the danger of paying Elizabeth too much attention."}
{"doc_id": "gutenberg_1342", "para_id": 343, "text": "In consequence of an agreement between the sisters, Elizabeth wrote the next morning to her mother, to beg that the carriage might be sent for them in the course of the day. But Mrs. Bennet, who had calculated on her daughters remaining at Netherfield till the following Tuesday, which would exactly finish Jane’s week, could not bring herself to receive them with pleasure before. Her answer, therefore, was not propitious, at least not to Elizabeth’s wishes, for she was impatient to get home. Mrs. Bennet sent them word that they could not possibly have the carriage before Tuesday; and in her postscript it was added, that if Mr. Bingley and his sister pressed them to stay longer, she could spare them very well. Against staying longer, however, Elizabeth was positively resolved--nor did she much expect it would be asked; and fearful, on the contrary, of being considered as intruding themselves needlessly long, she urged Jane to borrow Mr. Bingley’s carriage immediately, and at length it was settled that their original design of leaving Netherfield that morning should be mentioned, and the request made."}
{"doc_id": "gutenberg_1342", "para_id": 344, "text": "The communication excited many professions of concern; and enough was said of wishing them to stay at least till the following day to work on Jane; and till the morrow their going was deferred. Miss Bingley was then sorry that she had proposed the delay; for her jealousy and dislike of one sister much exceeded her affection for the other."}
{"doc_id": "gutenberg_1342", "para_id": 345, "text": "The master of the house heard with real sorrow that they were to go so soon, and repeatedly tried to persuade Miss Bennet that it would not be safe for her--that she was not enough recovered; but Jane was firm where she felt herself to be right."}
{"doc_id": "gutenberg_1342", "para_id": 346, "text": "To Mr. Darcy it was welcome intelligence: Elizabeth had been at Netherfield long enough. She attracted him more than he liked; and Miss Bingley was uncivil to _her_ and more teasing than usual to himself. He wisely resolved to be particularly careful that no sign of admiration should _now_ escape him--nothing that could elevate her with the hope of influencing his felicity; sensible that, if such an idea had been suggested, his behaviour during the last day must have material weight in confirming or crushing it. Steady to his purpose, he scarcely spoke ten words to her through the whole of Saturday: and though they were at one time left by themselves for half an hour, he adhered most conscientiously to his book, and would not even look at her."}
{"doc_id": "gutenberg_1342", "para_id": 347, "text": "On Sunday, after morning service, the separation, so agreeable to almost all, took place. Miss Bingley’s civility to Elizabeth increased at last very rapidly, as well as her affection for Jane; and when they parted, after assuring the latter of the pleasure it would always give her to see her either at Longbourn or Netherfield, and embracing her most tenderly, she even shook hands with the former. Elizabeth took leave of the whole party in the liveliest spirits."}
{"doc_id": "gutenberg_1342", "para_id": 348, "text": "They were not welcomed home very cordially by their mother. Mrs. Bennet wondered at their coming, and thought them very wrong to give so much trouble, and was sure Jane would have caught cold again. But their father, though very laconic in his expressions of pleasure, was really glad to see them; he had felt their importance in the family circle. The evening conversation, when they were all assembled, had lost much of its animation, and almost all its sense, by the absence of Jane and Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 349, "text": "They found Mary, as usual, deep in the study of thorough bass and human nature; and had some new extracts to admire and some new observations of threadbare morality to listen to. Catherine and Lydia had information for them of a different sort. Much had been done, and much had been said in the regiment since the preceding Wednesday; several of the officers had dined lately with their uncle; a private had been flogged; and it had actually been hinted that Colonel Forster was going to be married."}
{"doc_id": "gutenberg_1342", "para_id": 350, "text": "“I hope, my dear,” said Mr. Bennet to his wife, as they were at breakfast the next morning, “that you have ordered a good dinner to-day, because I have reason to expect an addition to our family party.”"}
{"doc_id": "gutenberg_1342", "para_id": 351, "text": "“Who do you mean, my dear? I know of nobody that is coming, I am sure, unless Charlotte Lucas should happen to call in; and I hope _my_ dinners are good enough for her. I do not believe she often sees such at home.”"}
{"doc_id": "gutenberg_1342", "para_id": 352, "text": "Mrs. Bennet’s eyes sparkled. “A gentleman and a stranger! It is Mr. Bingley, I am sure. Why, Jane--you never dropped a word of this--you sly thing! Well, I am sure I shall be extremely glad to see Mr. Bingley. But--good Lord! how unlucky! there is not a bit of fish to be got to-day. Lydia, my love, ring the bell. I must speak to Hill this moment.”"}
{"doc_id": "gutenberg_1342", "para_id": 353, "text": "“It is _not_ Mr. Bingley,” said her husband; “it is a person whom I never saw in the whole course of my life.”"}
{"doc_id": "gutenberg_1342", "para_id": 354, "text": "This roused a general astonishment; and he had the pleasure of being eagerly questioned by his wife and five daughters at once."}
{"doc_id": "gutenberg_1342", "para_id": 355, "text": "After amusing himself some time with their curiosity, he thus explained:--“About a month ago I received this letter, and about a fortnight ago I answered it; for I thought it a case of some delicacy, and requiring early attention. It is from my cousin, Mr. Collins, who, when I am dead, may turn you all out of this house as soon as he pleases.”"}
{"doc_id": "gutenberg_1342", "para_id": 356, "text": "“Oh, my dear,” cried his wife, “I cannot bear to hear that mentioned. Pray do not talk of that odious man. I do think it is the hardest thing in the world, that your estate should be entailed away from your own children; and I am sure, if I had been you, I should have tried long ago to do something or other about it.”"}
{"doc_id": "gutenberg_1342", "para_id": 357, "text": "Jane and Elizabeth attempted to explain to her the nature of an entail. They had often attempted it before: but it was a subject on which Mrs. Bennet was beyond the reach of reason; and she continued to rail bitterly against the cruelty of settling an estate away from a family of five daughters, in favour of a man whom nobody cared anything about."}
{"doc_id": "gutenberg_1342", "para_id": 358, "text": "“It certainly is a most iniquitous affair,” said Mr. Bennet; “and nothing can clear Mr. Collins from the guilt of inheriting Longbourn. But if you will listen to his letter, you may, perhaps, be a little softened by his manner of expressing himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 359, "text": "“No, that I am sure I shall not: and I think it was very impertinent of him to write to you at all, and very hypocritical. I hate such false friends. Why could not he keep on quarrelling with you, as his father did before him?”"}
{"doc_id": "gutenberg_1342", "para_id": 360, "text": "“Why, indeed, he does seem to have had some filial scruples on that head, as you will hear.”"}
{"doc_id": "gutenberg_1342", "para_id": 361, "text": "“The disagreement subsisting between yourself and my late honoured father always gave me much uneasiness; and, since I have had the misfortune to lose him, I have frequently wished to heal the breach: but, for some time, I was kept back by my own doubts, fearing lest it might seem disrespectful to his memory for me to be on good terms with anyone with whom it had always pleased him to be at variance.”--‘There, Mrs. Bennet.’--“My mind, however, is now made up on the subject; for, having received ordination at Easter, I have been so fortunate as to be distinguished by the patronage of the Right Honourable Lady Catherine de Bourgh, widow of Sir Lewis de Bourgh, whose bounty and beneficence has preferred me to the valuable rectory of this parish, where it shall be my earnest endeavour to demean myself with grateful respect towards her Ladyship, and be ever ready to perform those rites and ceremonies which are instituted by the Church of England. As a clergyman, moreover, I feel it my duty to promote and establish the blessing of peace in all families within the reach of my influence; and on these grounds I flatter myself that my present overtures of good-will are highly commendable, and that the circumstance of my being next in the entail of Longbourn estate will be kindly overlooked on your side, and not lead you to reject the offered olive branch. I cannot be otherwise than concerned at being the means of injuring your amiable daughters, and beg leave to apologize for it, as well as to assure you of my readiness to make them every possible amends; but of this hereafter. If you should have no objection to receive me into your house, I propose myself the satisfaction of waiting on you and your family, Monday, November 18th, by four o’clock, and shall probably trespass on your hospitality till the Saturday se’nnight following, which I can do without any inconvenience, as Lady Catherine is far from objecting to my occasional absence on a Sunday, provided that some other clergyman is engaged to do the duty of the day. I remain, dear sir, with respectful compliments to your lady and daughters, your well-wisher and friend,"}
{"doc_id": "gutenberg_1342", "para_id": 362, "text": "“At four o’clock, therefore, we may expect this peace-making gentleman,” said Mr. Bennet, as he folded up the letter. “He seems to be a most conscientious and polite young man, upon my word; and, I doubt not, will prove a valuable acquaintance, especially if Lady Catherine should be so indulgent as to let him come to us again.”"}
{"doc_id": "gutenberg_1342", "para_id": 363, "text": "“There is some sense in what he says about the girls, however; and, if he is disposed to make them any amends, I shall not be the person to discourage him.”"}
{"doc_id": "gutenberg_1342", "para_id": 364, "text": "“Though it is difficult,” said Jane, “to guess in what way he can mean to make us the atonement he thinks our due, the wish is certainly to his credit.”"}
{"doc_id": "gutenberg_1342", "para_id": 365, "text": "Elizabeth was chiefly struck with his extraordinary deference for Lady Catherine, and his kind intention of christening, marrying, and burying his parishioners whenever it were required."}
{"doc_id": "gutenberg_1342", "para_id": 366, "text": "“He must be an oddity, I think,” said she. “I cannot make him out. There is something very pompous in his style. And what can he mean by apologizing for being next in the entail? We cannot suppose he would help it, if he could. Can he be a sensible man, sir?”"}
{"doc_id": "gutenberg_1342", "para_id": 367, "text": "“No, my dear; I think not. I have great hopes of finding him quite the reverse. There is a mixture of servility and self-importance in his letter which promises well. I am impatient to see him.”"}
{"doc_id": "gutenberg_1342", "para_id": 368, "text": "“In point of composition,” said Mary, “his letter does not seem defective. The idea of the olive branch perhaps is not wholly new, yet I think it is well expressed.”"}
{"doc_id": "gutenberg_1342", "para_id": 369, "text": "To Catherine and Lydia neither the letter nor its writer were in any degree interesting. It was next to impossible that their cousin should come in a scarlet coat, and it was now some weeks since they had received pleasure from the society of a man in any other colour. As for their mother, Mr. Collins’s letter had done away much of her ill-will, and she was preparing to see him with a degree of composure which astonished her husband and daughters."}
{"doc_id": "gutenberg_1342", "para_id": 370, "text": "Mr. Collins was punctual to his time, and was received with great politeness by the whole family. Mr. Bennet indeed said little; but the ladies were ready enough to talk, and Mr. Collins seemed neither in need of encouragement, nor inclined to be silent himself. He was a tall, heavy-looking young man of five-and-twenty. His air was grave and stately, and his manners were very formal. He had not been long seated before he complimented Mrs. Bennet on having so fine a family of daughters, said he had heard much of their beauty, but that, in this instance, fame had fallen short of the truth; and added, that he did not doubt her seeing them all in due time well disposed of in marriage. This gallantry was not much to the taste of some of his hearers; but Mrs. Bennet, who quarrelled with no compliments, answered most readily,--"}
{"doc_id": "gutenberg_1342", "para_id": 371, "text": "“You are very kind, sir, I am sure; and I wish with all my heart it may prove so; for else they will be destitute enough. Things are settled so oddly.”"}
{"doc_id": "gutenberg_1342", "para_id": 372, "text": "“Ah, sir, I do indeed. It is a grievous affair to my poor girls, you must confess. Not that I mean to find fault with _you_, for such things, I know, are all chance in this world. There is no knowing how estates will go when once they come to be entailed.”"}
{"doc_id": "gutenberg_1342", "para_id": 373, "text": "“I am very sensible, madam, of the hardship to my fair cousins, and could say much on the subject, but that I am cautious of appearing forward and precipitate. But I can assure the young ladies that I come prepared to admire them. At present I will not say more, but, perhaps, when we are better acquainted----”"}
{"doc_id": "gutenberg_1342", "para_id": 374, "text": "He was interrupted by a summons to dinner; and the girls smiled on each other. They were not the only objects of Mr. Collins’s admiration. The hall, the dining-room, and all its furniture, were examined and praised; and his commendation of everything would have touched Mrs. Bennet’s heart, but for the mortifying supposition of his viewing it all as his own future property. The dinner, too, in its turn, was highly admired; and he begged to know to which of his fair cousins the excellence of its cookery was owing. But here he was set right by Mrs. Bennet, who assured him, with some asperity, that they were very well able to keep a good cook, and that her daughters had nothing to do in the kitchen. He begged pardon for having displeased her. In a softened tone she declared herself not at all offended; but he continued to apologize for about a quarter of an hour."}
{"doc_id": "gutenberg_1342", "para_id": 375, "text": "During dinner, Mr. Bennet scarcely spoke at all; but when the servants were withdrawn, he thought it time to have some conversation with his guest, and therefore started a subject in which he expected him to shine, by observing that he seemed very fortunate in his patroness. Lady Catherine de Bourgh’s attention to his wishes, and consideration for his comfort, appeared very remarkable. Mr. Bennet could not have chosen better. Mr. Collins was eloquent in her praise. The subject elevated him to more than usual solemnity of manner; and with a most important aspect he protested that he had never in his life witnessed such behaviour in a person of rank--such affability and condescension, as he had himself experienced from Lady Catherine. She had been graciously pleased to approve of both the discourses which he had already had the honour of preaching before her. She had also asked him twice to dine at Rosings, and had sent for him only the Saturday before, to make up her pool of quadrille in the evening. Lady Catherine was reckoned proud by many people, he knew, but _he_ had never seen anything but affability in her. She had always spoken to him as she would to any other gentleman; she made not the smallest objection to his joining in the society of the neighbourhood, nor to his leaving his parish occasionally for a week or two to visit his relations. She had even condescended to advise him to marry as soon as he could, provided he chose with discretion; and had once paid him a visit in his humble parsonage, where she had perfectly approved all the alterations he had been making, and had even vouchsafed to suggest some herself,--some shelves in the closets upstairs."}
{"doc_id": "gutenberg_1342", "para_id": 376, "text": "“That is all very proper and civil, I am sure,” said Mrs. Bennet, “and I dare say she is a very agreeable woman. It is a pity that great ladies in general are not more like her. Does she live near you, sir?”"}
{"doc_id": "gutenberg_1342", "para_id": 377, "text": "“The garden in which stands my humble abode is separated only by a lane from Rosings Park, her Ladyship’s residence.”"}
{"doc_id": "gutenberg_1342", "para_id": 378, "text": "“She has one only daughter, the heiress of Rosings, and of very extensive property.”"}
{"doc_id": "gutenberg_1342", "para_id": 379, "text": "“Ah,” cried Mrs. Bennet, shaking her head, “then she is better off than many girls. And what sort of young lady is she? Is she handsome?”"}
{"doc_id": "gutenberg_1342", "para_id": 380, "text": "“She is a most charming young lady, indeed. Lady Catherine herself says that, in point of true beauty, Miss de Bourgh is far superior to the handsomest of her sex; because there is that in her features which marks the young woman of distinguished birth. She is unfortunately of a sickly constitution, which has prevented her making that progress in many accomplishments which she could not otherwise have failed of, as I am informed by the lady who superintended her education, and who still resides with them. But she is perfectly amiable, and often condescends to drive by my humble abode in her little phaeton and ponies.”"}
{"doc_id": "gutenberg_1342", "para_id": 381, "text": "“Her indifferent state of health unhappily prevents her being in town; and by that means, as I told Lady Catherine myself one day, has deprived the British Court of its brightest ornament. Her Ladyship seemed pleased with the idea; and you may imagine that I am happy on every occasion to offer those little delicate compliments which are always acceptable to ladies. I have more than once observed to Lady Catherine, that her charming daughter seemed born to be a duchess; and that the most elevated rank, instead of giving her consequence, would be adorned by her. These are the kind of little things which please her Ladyship, and it is a sort of attention which I conceive myself peculiarly bound to pay.”"}
{"doc_id": "gutenberg_1342", "para_id": 382, "text": "“You judge very properly,” said Mr. Bennet; “and it is happy for you that you possess the talent of flattering with delicacy. May I ask whether these pleasing attentions proceed from the impulse of the moment, or are the result of previous study?”"}
{"doc_id": "gutenberg_1342", "para_id": 383, "text": "“They arise chiefly from what is passing at the time; and though I sometimes amuse myself with suggesting and arranging such little elegant compliments as may be adapted to ordinary occasions, I always wish to give them as unstudied an air as possible.”"}
{"doc_id": "gutenberg_1342", "para_id": 384, "text": "Mr. Bennet’s expectations were fully answered. His cousin was as absurd as he had hoped; and he listened to him with the keenest enjoyment, maintaining at the same time the most resolute composure of countenance, and, except in an occasional glance at Elizabeth, requiring no partner in his pleasure."}
{"doc_id": "gutenberg_1342", "para_id": 385, "text": "By tea-time, however, the dose had been enough, and Mr. Bennet was glad to take his guest into the drawing-room again, and when tea was over, glad to invite him"}
{"doc_id": "gutenberg_1342", "para_id": 386, "text": "to read aloud to the ladies. Mr. Collins readily assented, and a book was produced; but on beholding it (for everything announced it to be from a circulating library) he started back, and, begging pardon, protested that he never read novels. Kitty stared at him, and Lydia exclaimed. Other books were produced, and after some deliberation he chose “Fordyce’s Sermons.” Lydia gaped as he opened the volume; and before he had, with very monotonous solemnity, read three pages, she interrupted him with,--"}
{"doc_id": "gutenberg_1342", "para_id": 387, "text": "“Do you know, mamma, that my uncle Philips talks of turning away Richard? and if he does, Colonel Forster will hire him. My aunt told me so herself on Saturday. I shall walk to Meryton to-morrow to hear more about it, and to ask when Mr. Denny comes back from town.”"}
{"doc_id": "gutenberg_1342", "para_id": 388, "text": "Lydia was bid by her two eldest sisters to hold her tongue; but Mr. Collins, much offended, laid aside his book, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 389, "text": "“I have often observed how little young ladies are interested by books of a serious stamp, though written solely for their benefit. It amazes me, I confess; for certainly there can be nothing so advantageous to them as instruction. But I will no longer importune my young cousin.”"}
{"doc_id": "gutenberg_1342", "para_id": 390, "text": "Then, turning to Mr. Bennet, he offered himself as his antagonist at backgammon. Mr. Bennet accepted the challenge, observing that he acted very wisely in leaving the girls to their own trifling amusements. Mrs. Bennet and her daughters apologized most civilly for Lydia’s interruption, and promised that it should not occur again, if he would resume his book; but Mr. Collins, after assuring them that he bore his young cousin no ill-will, and should never resent her behaviour as any affront, seated himself at another table with Mr. Bennet, and prepared for backgammon."}
{"doc_id": "gutenberg_1342", "para_id": 391, "text": "Mr. Collins was not a sensible man, and the deficiency of nature had been but little assisted by education or society; the greatest part of his life having been spent under the guidance of an illiterate and miserly father; and though he belonged to one of the universities, he had merely kept the necessary terms without forming at it any useful acquaintance. The subjection in which his father had brought him up had given him originally great humility of manner; but it was now a good deal counteracted by the self-conceit of a weak head, living in retirement, and the consequential feelings of early and unexpected prosperity. A fortunate chance had recommended him to Lady Catherine de Bourgh when the living of Hunsford was vacant; and the respect which he felt for her high rank, and his veneration for her as his patroness, mingling with a very good opinion of himself, of his authority as a clergyman, and his right as a rector, made him altogether a mixture of pride and obsequiousness, self-importance and humility."}
{"doc_id": "gutenberg_1342", "para_id": 392, "text": "Having now a good house and a very sufficient income, he intended to marry; and in seeking a reconciliation with the Longbourn family he had a wife in view, as he meant to choose one of the daughters, if he found them as handsome and amiable as they were represented by common report. This was his plan of amends--of atonement--for inheriting their father’s estate; and he thought it an excellent one, full of eligibility and suitableness, and excessively generous and disinterested on his own part."}
{"doc_id": "gutenberg_1342", "para_id": 393, "text": "His plan did not vary on seeing them. Miss Bennet’s lovely face confirmed his views, and established all his strictest notions of what was due to seniority; and for the first evening _she_ was his settled choice. The next morning, however, made an alteration; for in a quarter of an hour’s _tête-à-tête_ with Mrs. Bennet before breakfast, a conversation beginning with his parsonage-house, and leading naturally to the avowal of his hopes, that a mistress for it might be found at Longbourn, produced from her, amid very complaisant smiles and general encouragement, a caution against the very Jane he had fixed on. “As to her _younger_ daughters, she could not take upon her to say--she could not positively answer--but she did not _know_ of any prepossession;--her _eldest_ daughter she must just mention--she felt it incumbent on her to hint, was likely to be very soon engaged.”"}
{"doc_id": "gutenberg_1342", "para_id": 394, "text": "Mr. Collins had only to change from Jane to Elizabeth--and it was soon done--done while Mrs. Bennet was stirring the fire. Elizabeth, equally next to Jane in birth and beauty, succeeded her of course."}
{"doc_id": "gutenberg_1342", "para_id": 395, "text": "Mrs. Bennet treasured up the hint, and trusted that she might soon have two daughters married; and the man whom she could not bear to speak of the day before, was now high in her good graces."}
{"doc_id": "gutenberg_1342", "para_id": 396, "text": "Lydia’s intention of walking to Meryton was not forgotten: every sister except Mary agreed to go with her; and Mr. Collins was to attend them, at the request of Mr. Bennet, who was most anxious to get rid of him, and have his library to himself; for thither Mr. Collins had followed him after breakfast, and there he would continue, nominally engaged with one of the largest folios in the collection, but really talking to Mr. Bennet, with little cessation, of his house and garden at Hunsford. Such doings discomposed Mr. Bennet exceedingly. In his library he had been always sure of leisure and tranquillity; and though prepared, as he told Elizabeth, to meet with folly and conceit in every other room in the house, he was used to be free from them there: his civility, therefore, was most prompt in inviting Mr. Collins to join his daughters in their walk; and Mr. Collins, being in fact much better fitted for a walker than a reader, was extremely well pleased to close his large book, and go."}
{"doc_id": "gutenberg_1342", "para_id": 397, "text": "In pompous nothings on his side, and civil assents on that of his cousins, their time passed till they entered Meryton. The attention of the younger ones was then no longer to be gained by _him_. Their eyes were immediately wandering up the street in quest of the officers, and nothing less than a very smart bonnet, indeed, or a really new muslin in a shop window, could recall them."}
{"doc_id": "gutenberg_1342", "para_id": 398, "text": "But the attention of every lady was soon caught by a young man, whom they had never seen before, of most gentlemanlike appearance, walking with an officer on the other side of the way. The officer was the very Mr. Denny concerning whose return from London Lydia came to inquire, and he bowed as they passed. All were struck with the stranger’s air, all wondered who he could be; and Kitty and Lydia, determined if possible to find out, led the way across the street, under pretence of wanting something in an opposite shop, and fortunately had just gained the pavement, when the two gentlemen, turning back, had reached the same spot. Mr. Denny addressed them directly, and entreated permission to introduce his friend, Mr. Wickham, who had returned with him the day before from town, and, he was happy to say, had accepted a commission in their corps. This was exactly as it should be; for the young man wanted only regimentals to make him completely charming. His appearance was greatly in his favour: he had all the best parts of beauty, a fine countenance, a good figure, and very pleasing address. The introduction was followed up on his side by a happy readiness of conversation--a readiness at the same time perfectly correct and unassuming; and the whole party were still standing and talking together very agreeably, when the sound of horses drew their notice, and Darcy and Bingley were seen riding down the street. On distinguishing the ladies of the group the two gentlemen came directly towards them, and began the usual civilities. Bingley was the principal spokesman, and Miss Bennet the principal object. He was then, he said, on his way to Longbourn on purpose to inquire after her. Mr. Darcy corroborated it with a bow, and was beginning to determine not to fix his eyes on Elizabeth, when they were suddenly arrested by the sight of the stranger; and Elizabeth happening to see the countenance of both as they looked at each other, was all astonishment at the effect of the meeting. Both changed colour, one looked white, the other red. Mr. Wickham, after a few moments, touched his hat--a salutation which Mr. Darcy just deigned to return. What could be the meaning of it? It was impossible to imagine; it was impossible not to long to know."}
{"doc_id": "gutenberg_1342", "para_id": 399, "text": "In another minute Mr. Bingley, but without seeming to have noticed what passed, took leave and rode on with his friend."}
{"doc_id": "gutenberg_1342", "para_id": 400, "text": "Mr. Denny and Mr. Wickham walked with the young ladies to the door of Mr. Philips’s house, and then made their bows, in spite of Miss Lydia’s pressing entreaties that they would come in, and even in spite of Mrs. Philips’s throwing up the parlour window, and loudly seconding the invitation."}
{"doc_id": "gutenberg_1342", "para_id": 401, "text": "Mrs. Philips was always glad to see her nieces; and the two eldest, from their recent absence, were particularly welcome; and she was eagerly expressing her surprise at their sudden return home, which, as their own carriage had not fetched them, she should have known nothing about, if she had not happened to see Mr. Jones’s shopboy in the street, who had told her that they were not to send any more draughts to Netherfield, because the Miss Bennets were come away, when her civility was claimed towards Mr. Collins by Jane’s introduction of him. She received him with her very best politeness, which he returned with as much more, apologizing for his intrusion, without any previous acquaintance with her, which he could not help flattering himself, however, might be justified by his relationship to the young ladies who introduced him to her notice. Mrs. Philips was quite awed by such an excess of good breeding; but her contemplation of one stranger was soon put an end to by exclamations and inquiries about the other, of whom, however, she could only tell her nieces what they already knew, that Mr. Denny had brought him from London, and that he was to have a lieutenant’s commission in the ----shire. She had been watching him the last hour, she said, as he walked up and down the street,--and had Mr. Wickham appeared, Kitty and Lydia would certainly have continued the occupation; but unluckily no one passed the windows now except a few of the officers, who, in comparison with the stranger, were become “stupid, disagreeable fellows.” Some of them were to dine with the Philipses the next day, and their aunt promised to make her husband call on Mr. Wickham, and give him an invitation also, if the family from Longbourn would come in the evening. This was agreed to; and Mrs. Philips protested that they would have a nice comfortable noisy game of lottery tickets, and a little bit of hot supper afterwards. The prospect of such delights was very cheering, and they parted in mutual good spirits. Mr. Collins repeated his apologies in quitting the room, and was assured, with unwearying civility, that they were perfectly needless."}
{"doc_id": "gutenberg_1342", "para_id": 402, "text": "As they walked home, Elizabeth related to Jane what she had seen pass between the two gentlemen; but though Jane would have defended either or both, had they appeared to be wrong, she could no more explain such behaviour than her sister."}
{"doc_id": "gutenberg_1342", "para_id": 403, "text": "Mr. Collins on his return highly gratified Mrs. Bennet by admiring Mrs. Philips’s manners and politeness. He protested that, except Lady Catherine and her daughter, he had never seen a more elegant woman; for she had not only received him with the utmost civility, but had even pointedly included him in her invitation for the next evening, although utterly unknown to her before. Something, he supposed, might be attributed to his connection with them, but yet he had never met with so much attention in the whole course of his life."}
{"doc_id": "gutenberg_1342", "para_id": 404, "text": "As no objection was made to the young people’s engagement with their aunt, and all Mr. Collins’s scruples of leaving Mr. and Mrs. Bennet for a single evening during his visit were most steadily resisted, the coach conveyed him and his five cousins at a suitable hour to Meryton; and the girls had the pleasure of hearing, as they entered the drawing-room, that Mr. Wickham had accepted their uncle’s invitation, and was then in the house."}
{"doc_id": "gutenberg_1342", "para_id": 405, "text": "When this information was given, and they had all taken their seats, Mr. Collins was at leisure to look around him and admire, and he was so much struck with the size and furniture of the apartment, that he declared he might almost have supposed himself in the small summer breakfast parlour at Rosings; a comparison that did not at first convey much gratification; but when Mrs. Philips understood from him what Rosings was, and who was its proprietor, when she had listened to the description of only one of Lady Catherine’s drawing-rooms, and found that the chimney-piece alone had cost eight hundred pounds, she felt all the force of the compliment, and would hardly have resented a comparison with the housekeeper’s room."}
{"doc_id": "gutenberg_1342", "para_id": 406, "text": "In describing to her all the grandeur of Lady Catherine and her mansion, with occasional digressions in praise of his own humble abode, and the improvements it was receiving, he was happily employed until the gentlemen joined them; and he found in Mrs. Philips a very attentive listener, whose opinion of his consequence increased with what she heard, and who was resolving to retail it all among her neighbours as soon as she could. To the girls, who could not listen to their cousin, and who had nothing to do but to wish for an instrument, and examine their own indifferent imitations of china on the mantel-piece, the interval of waiting appeared very long. It was over at last, however. The gentlemen did approach: and when Mr. Wickham walked into the room, Elizabeth felt that she had neither been seeing him before, nor thinking of him since, with the smallest degree of unreasonable admiration. The officers of the ----shire were in general a very creditable, gentlemanlike set and the best of them were of the present party; but Mr, Wickham was as far beyond them all in person, countenance, air, and walk, as _they_ were superior to the broad-faced stuffy uncle Philips, breathing port wine, who followed them into the room."}
{"doc_id": "gutenberg_1342", "para_id": 407, "text": "Mr. Wickham was the happy man towards whom almost every female eye was turned, and Elizabeth was the happy woman by whom he finally seated himself; and the agreeable manner in which he immediately fell into conversation, though it was only on its being a wet night, and on the probability of a rainy season, made her feel that the commonest, dullest, most threadbare topic might be rendered interesting by the skill of the speaker."}
{"doc_id": "gutenberg_1342", "para_id": 408, "text": "With such rivals for the notice of the fair as Mr. Wickham and the officers, Mr. Collins seemed to sink into insignificance; to the young ladies he certainly was nothing; but he had still at intervals a kind listener in Mrs. Philips, and was, by her watchfulness, most abundantly supplied with coffee and muffin."}
{"doc_id": "gutenberg_1342", "para_id": 409, "text": "When the card tables were placed, he had an opportunity of obliging her, in return, by sitting down to whist."}
{"doc_id": "gutenberg_1342", "para_id": 410, "text": "“I know little of the game at present,” said he, “but I shall be glad to improve myself; for in my situation of life----” Mrs. Philips was very thankful for his compliance, but could not wait for his reason."}
{"doc_id": "gutenberg_1342", "para_id": 411, "text": "Mr. Wickham did not play at whist, and with ready delight was he received at the other table between Elizabeth and Lydia. At first there seemed danger of Lydia’s engrossing him entirely, for she was a most determined talker; but being likewise extremely fond of lottery tickets, she soon grew too much interested in the game, too eager in making bets and exclaiming after prizes, to have attention for anyone in particular. Allowing for the common demands of the game, Mr. Wickham was therefore at leisure to talk to Elizabeth, and she was very willing to hear him, though what she chiefly wished to hear she could not hope to be told, the history of his acquaintance with Mr. Darcy. She dared not even mention that gentleman. Her curiosity, however, was unexpectedly relieved. Mr. Wickham began the subject himself. He inquired how far Netherfield was from Meryton; and, after receiving her answer, asked in a hesitating manner how long Mr. Darcy had been staying there."}
{"doc_id": "gutenberg_1342", "para_id": 412, "text": "“About a month,” said Elizabeth; and then, unwilling to let the subject drop, added, “he is a man of very large property in Derbyshire, I understand.”"}
{"doc_id": "gutenberg_1342", "para_id": 413, "text": "“Yes,” replied Wickham; “his estate there is a noble one. A clear ten thousand per annum. You could not have met with a person more capable of giving you certain information on that head than myself--for I have been connected with his family, in a particular manner, from my infancy.”"}
{"doc_id": "gutenberg_1342", "para_id": 414, "text": "“You may well be surprised, Miss Bennet, at such an assertion, after seeing, as you probably might, the very cold manner of our meeting yesterday. Are you much acquainted with Mr. Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 415, "text": "“As much as I ever wish to be,” cried Elizabeth, warmly. “I have spent four days in the same house with him, and I think him very disagreeable.”"}
{"doc_id": "gutenberg_1342", "para_id": 416, "text": "“I have no right to give _my_ opinion,” said Wickham, “as to his being agreeable or otherwise. I am not qualified to form one. I have known him too long and too well to be a fair judge. It is impossible for _me_ to be impartial. But I believe your opinion of him would in general astonish--and, perhaps, you would not express it quite so strongly anywhere else. Here you are in your own family.”"}
{"doc_id": "gutenberg_1342", "para_id": 417, "text": "“Upon my word I say no more _here_ than I might say in any house in the neighbourhood, except Netherfield. He is not at all liked in Hertfordshire. Everybody is disgusted with his pride. You will not find him more favourably spoken of by anyone.”"}
{"doc_id": "gutenberg_1342", "para_id": 418, "text": "“I cannot pretend to be sorry,” said Wickham, after a short interruption, “that he or that any man should not be estimated beyond their deserts; but with _him_ I believe it does not often happen. The world is blinded by his fortune and consequence, or frightened by his high and imposing manners, and sees him only as he chooses to be seen.”"}
{"doc_id": "gutenberg_1342", "para_id": 419, "text": "“I should take him, even on _my_ slight acquaintance, to be an ill-tempered man.”"}
{"doc_id": "gutenberg_1342", "para_id": 420, "text": "“I wonder,” said he, at the next opportunity of speaking, “whether he is likely to be in this country much longer.”"}
{"doc_id": "gutenberg_1342", "para_id": 421, "text": "“I do not at all know; but I _heard_ nothing of his going away when I was at Netherfield. I hope your plans in favour of the ----shire will not be affected by his being in the neighbourhood.”"}
{"doc_id": "gutenberg_1342", "para_id": 422, "text": "“Oh no--it is not for _me_ to be driven away by Mr. Darcy. If _he_ wishes to avoid seeing _me_ he must go. We are not on friendly terms, and it always gives me pain to meet him, but I have no reason for avoiding _him_ but what I might proclaim to all the world--a sense of very great ill-usage, and most painful regrets at his being what he is. His father, Miss Bennet, the late Mr. Darcy, was one of the best men that ever breathed, and the truest friend I ever had; and I can never be in company with this Mr. Darcy without being grieved to the soul by a thousand tender recollections. His behaviour to myself has been scandalous; but I verily believe I could forgive him anything and everything, rather than his disappointing the hopes and disgracing the memory of his father.”"}
{"doc_id": "gutenberg_1342", "para_id": 423, "text": "Elizabeth found the interest of the subject increase, and listened with all her heart; but the delicacy of it prevented further inquiry."}
{"doc_id": "gutenberg_1342", "para_id": 424, "text": "Mr. Wickham began to speak on more general topics, Meryton, the neighbourhood, the society, appearing highly pleased with all that he had yet seen, and speaking of the latter, especially, with gentle but very intelligible gallantry."}
{"doc_id": "gutenberg_1342", "para_id": 425, "text": "“It was the prospect of constant society, and good society,” he added, “which was my chief inducement to enter the ----shire. I know it to be a most respectable, agreeable corps; and my friend Denny tempted me further by his account of their present quarters, and the very great attentions and excellent acquaintance Meryton had procured them. Society, I own, is necessary to me. I have been a disappointed man, and my spirits will not bear solitude. I _must_ have employment and society. A military life is not what I was intended for, but circumstances have now made it eligible. The church _ought_ to have been my profession--I was brought up for the church; and I should at this time have been in possession of a most valuable living, had it pleased the gentleman we were speaking of just now.”"}
{"doc_id": "gutenberg_1342", "para_id": 426, "text": "“Yes--the late Mr. Darcy bequeathed me the next presentation of the best living in his gift. He was my godfather, and excessively attached to me. I cannot do justice to his kindness. He meant to provide for me amply, and thought he had done it; but when the living fell, it was given elsewhere.”"}
{"doc_id": "gutenberg_1342", "para_id": 427, "text": "“Good heavens!” cried Elizabeth; “but how could _that_ be? How could his will be disregarded? Why did not you seek legal redress?”"}
{"doc_id": "gutenberg_1342", "para_id": 428, "text": "“There was just such an informality in the terms of the bequest as to give me no hope from law. A man of honour could not have doubted the intention, but Mr. Darcy chose to doubt it--or to treat it as a merely conditional recommendation, and to assert that I had forfeited all claim to it by extravagance, imprudence, in short, anything or nothing. Certain it is that the living became vacant two years ago, exactly as I was of an age to hold it, and that it was given to another man; and no less certain is it, that I cannot accuse myself of having really done anything to deserve to lose it. I have a warm unguarded temper, and I may perhaps have sometimes spoken my opinion _of_ him, and _to_ him, too freely. I can recall nothing worse. But the fact is, that we are very different sort of men, and that he hates me.”"}
{"doc_id": "gutenberg_1342", "para_id": 429, "text": "“Some time or other he _will_ be--but it shall not be by _me_. Till I can forget his father, I can never defy or expose _him_.”"}
{"doc_id": "gutenberg_1342", "para_id": 430, "text": "Elizabeth honoured him for such feelings, and thought him handsomer than ever as he expressed them."}
{"doc_id": "gutenberg_1342", "para_id": 431, "text": "“But what,” said she, after a pause, “can have been his motive? what can have induced him to behave so cruelly?”"}
{"doc_id": "gutenberg_1342", "para_id": 432, "text": "“A thorough, determined dislike of me--a dislike which I cannot but attribute in some measure to jealousy. Had the late Mr. Darcy liked me less, his son might have borne with me better; but his father’s uncommon attachment to me irritated him, I believe, very early in life. He had not a temper to bear the sort of competition in which we stood--the sort of preference which was often given me.”"}
{"doc_id": "gutenberg_1342", "para_id": 433, "text": "“I had not thought Mr. Darcy so bad as this--though I have never liked him, I had not thought so very ill of him--I had supposed him to be despising his fellow-creatures in general, but did not suspect him of descending to such malicious revenge, such injustice, such inhumanity as this!”"}
{"doc_id": "gutenberg_1342", "para_id": 434, "text": "After a few minutes’ reflection, however, she continued, “I _do_ remember his boasting one day, at Netherfield, of the implacability of his resentments, of his having an unforgiving temper. His disposition must be dreadful.”"}
{"doc_id": "gutenberg_1342", "para_id": 435, "text": "“I will not trust myself on the subject,” replied Wickham; “_I_ can hardly be just to him.”"}
{"doc_id": "gutenberg_1342", "para_id": 436, "text": "Elizabeth was again deep in thought, and after a time exclaimed, “To treat in such a manner the godson, the friend, the favourite of his father!” She could have added, “A young man, too, like _you_, whose very countenance may vouch for your being amiable.” But she contented herself with--“And one, too, who had probably been his own companion from childhood, connected together, as I think you said, in the closest manner.”"}
{"doc_id": "gutenberg_1342", "para_id": 437, "text": "“We were born in the same parish, within the same park; the greatest part of our youth was passed together: inmates of the same house, sharing the same amusements, objects of the same parental care. _My_ father began life in the profession which your uncle, Mr. Philips, appears to do so much credit to; but he gave up everything to be of use to the late Mr. Darcy, and devoted all his time to the care of the Pemberley property. He was most highly esteemed by Mr. Darcy, a most intimate, confidential friend. Mr. Darcy often acknowledged himself to be under the greatest obligations to my father’s active superintendence; and when, immediately before my father’s death, Mr. Darcy gave him a voluntary promise of providing for me, I am convinced that he felt it to be as much a debt of gratitude to _him_ as of affection to myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 438, "text": "“How strange!” cried Elizabeth. “How abominable! I wonder that the very pride of this Mr. Darcy has not made him just to you. If from no better motive, that he should not have been too proud to be dishonest,--for dishonesty I must call it.”"}
{"doc_id": "gutenberg_1342", "para_id": 439, "text": "“It _is_ wonderful,” replied Wickham; “for almost all his actions may be traced to pride; and pride has often been his best friend. It has connected him nearer with virtue than any other feeling. But we are none of us consistent; and in his behaviour to me there were stronger impulses even than pride.”"}
{"doc_id": "gutenberg_1342", "para_id": 440, "text": "“Yes; it has often led him to be liberal and generous; to give his money freely, to display hospitality, to assist his tenants, and relieve the poor. Family pride, and _filial_ pride, for he is very proud of what his father was, have done this. Not to appear to disgrace his family, to degenerate from the popular qualities, or lose the influence of the Pemberley House, is a powerful motive. He has also _brotherly_ pride, which, with _some_ brotherly affection, makes him a very kind and careful guardian of his sister; and you will hear him generally cried up as the most attentive and best of brothers.”"}
{"doc_id": "gutenberg_1342", "para_id": 441, "text": "He shook his head. “I wish I could call her amiable. It gives me pain to speak ill of a Darcy; but she is too much like her brother,--very, very proud. As a child, she was affectionate and pleasing, and extremely fond of me; and I have devoted hours and hours to her amusement. But she is nothing to me now. She is a handsome girl, about fifteen or sixteen, and, I understand, highly accomplished. Since her father’s death her home has been London, where a lady lives with her, and superintends her education.”"}
{"doc_id": "gutenberg_1342", "para_id": 442, "text": "After many pauses and many trials of other subjects, Elizabeth could not help reverting once more to the first, and saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 443, "text": "“I am astonished at his intimacy with Mr. Bingley. How can Mr. Bingley, who seems good-humour itself, and is, I really believe, truly amiable, be in friendship with such a man? How can they suit each other? Do you know Mr. Bingley?”"}
{"doc_id": "gutenberg_1342", "para_id": 444, "text": "“He is a sweet-tempered, amiable, charming man. He cannot know what Mr. Darcy is.”"}
{"doc_id": "gutenberg_1342", "para_id": 445, "text": "“Probably not; but Mr. Darcy can please where he chooses. He does not want abilities. He can be a conversible companion if he thinks it worth his while. Among those who are at all his equals in consequence, he is a very different man from what he is to the less prosperous. His pride never deserts him; but with the rich he is liberal-minded, just, sincere, rational, honourable, and, perhaps, agreeable,--allowing something for fortune and figure.”"}
{"doc_id": "gutenberg_1342", "para_id": 446, "text": "The whist party soon afterwards breaking up, the players gathered round the other table, and Mr. Collins took his station between his cousin Elizabeth and Mrs. Philips. The usual inquiries as to his success were made by the latter. It had not been very great; he had lost every point; but when Mrs. Philips began to express her concern thereupon, he assured her, with much earnest gravity, that it was not of the least importance; that he considered the money as a mere trifle, and begged she would not make herself uneasy."}
{"doc_id": "gutenberg_1342", "para_id": 447, "text": "“I know very well, madam,” said he, “that when persons sit down to a card table they must take their chance of these things,--and happily I am not in such circumstances as to make five shillings any object. There are, undoubtedly, many who could not say the same; but, thanks to Lady Catherine de Bourgh, I am removed far beyond the necessity of regarding little matters.”"}
{"doc_id": "gutenberg_1342", "para_id": 448, "text": "Mr. Wickham’s attention was caught; and after observing Mr. Collins for a few moments, he asked Elizabeth in a low voice whether her relations were very intimately acquainted with the family of De Bourgh."}
{"doc_id": "gutenberg_1342", "para_id": 449, "text": "“Lady Catherine de Bourgh,” she replied, “has very lately given him a living. I hardly know how Mr. Collins was first introduced to her notice, but he certainly has not known her long.”"}
{"doc_id": "gutenberg_1342", "para_id": 450, "text": "“You know of course that Lady Catherine de Bourgh and Lady Anne Darcy were sisters; consequently that she is aunt to the present Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 451, "text": "“No, indeed, I did not. I knew nothing at all of Lady Catherine’s connections. I never heard of her existence till the day before yesterday.”"}
{"doc_id": "gutenberg_1342", "para_id": 452, "text": "“Her daughter, Miss de Bourgh, will have a very large fortune, and it is believed that she and her cousin will unite the two estates.”"}
{"doc_id": "gutenberg_1342", "para_id": 453, "text": "This information made Elizabeth smile, as she thought of poor Miss Bingley. Vain indeed must be all her attentions, vain and useless her affection for his sister and her praise of himself, if he were already self-destined to another."}
{"doc_id": "gutenberg_1342", "para_id": 454, "text": "“Mr. Collins,” said she, “speaks highly both of Lady Catherine and her daughter; but, from some particulars that he has related of her Ladyship, I suspect his gratitude misleads him; and that, in spite of her being his patroness, she is an arrogant, conceited woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 455, "text": "“I believe her to be both in a great degree,” replied Wickham; “I have not seen her for many years; but I very well remember that I never liked her, and that her manners were dictatorial and insolent. She has the reputation of being remarkably sensible and clever; but I rather believe she derives part of her abilities from her rank and fortune, part from her authoritative manner, and the rest from the pride of her nephew, who chooses that everyone connected with him should have an understanding of the first class.”"}
{"doc_id": "gutenberg_1342", "para_id": 456, "text": "Elizabeth allowed that he had given a very rational account of it, and they continued talking together with mutual satisfaction till supper put an end to cards, and gave the rest of the ladies their share of Mr. Wickham’s attentions. There could be no conversation in the noise of Mrs. Philips’s supper party, but his manners recommended him to everybody. Whatever he said, was said well; and whatever he did, done gracefully. Elizabeth went away with her head full of him. She could think of nothing but of Mr. Wickham, and of what he had told her, all the way home; but there was not time for her even to mention his name as they went, for neither Lydia nor Mr. Collins were once silent. Lydia talked incessantly of lottery tickets, of the fish she had lost and the fish she had won; and Mr. Collins, in describing the civility of Mr. and Mrs. Philips, protesting that he did not in the least regard his losses at whist, enumerating all the dishes at supper, and repeatedly fearing that he crowded his cousins, had more to say than he could well manage before the carriage stopped at Longbourn House."}
{"doc_id": "gutenberg_1342", "para_id": 457, "text": "Elizabeth related to Jane, the next day, what had passed between Mr. Wickham and herself. Jane listened with astonishment and concern: she knew not how to believe that Mr. Darcy could be so unworthy of Mr. Bingley’s regard; and yet it was not in her nature to question the veracity of a young man of such amiable appearance as Wickham. The possibility of his having really endured such unkindness was enough to interest all her tender feelings; and nothing therefore remained to be done but to think well of them both, to defend the conduct of each, and throw into the account of accident or mistake whatever could not be otherwise explained."}
{"doc_id": "gutenberg_1342", "para_id": 458, "text": "“They have both,” said she, “been deceived, I dare say, in some way or other, of which we can form no idea. Interested people have perhaps misrepresented each to the other. It is, in short, impossible for us to conjecture the causes or circumstances which may have alienated them, without actual blame on either side.”"}
{"doc_id": "gutenberg_1342", "para_id": 459, "text": "“Very true, indeed; and now, my dear Jane, what have you got to say in behalf of the interested people who have probably been concerned in the business? Do clear _them_, too, or we shall be obliged to think ill of somebody.”"}
{"doc_id": "gutenberg_1342", "para_id": 460, "text": "“Laugh as much as you choose, but you will not laugh me out of my opinion. My dearest Lizzy, do but consider in what a disgraceful light it places Mr. Darcy, to be treating his father’s favourite in such a manner,--one whom his father had promised to provide for. It is impossible. No man of common humanity, no man who had any value for his character, could be capable of it. Can his most intimate friends be so excessively deceived in him? Oh no.”"}
{"doc_id": "gutenberg_1342", "para_id": 461, "text": "“I can much more easily believe Mr. Bingley’s being imposed on than that Mr. Wickham should invent such a history of himself as he gave me last night; names, facts, everything mentioned without ceremony. If it be not so, let Mr. Darcy contradict it. Besides, there was truth in his looks.”"}
{"doc_id": "gutenberg_1342", "para_id": 462, "text": "But Jane could think with certainty on only one point,--that Mr. Bingley, if he _had been_ imposed on, would have much to suffer when the affair became public."}
{"doc_id": "gutenberg_1342", "para_id": 463, "text": "The two young ladies were summoned from the shrubbery, where this conversation passed, by the arrival of some of the very persons of whom they had been speaking; Mr. Bingley and his sisters came to give their personal invitation for the long expected ball at Netherfield, which was fixed for the following Tuesday. The two ladies were delighted to see their dear friend again, called it an age since they had met, and repeatedly asked what she had been doing with herself since their separation. To the rest of the family they paid little attention; avoiding Mrs. Bennet as much as possible, saying not much to Elizabeth, and nothing at all to the others. They were soon gone again, rising from their seats with an activity which took their brother by surprise, and hurrying off as if eager to escape from Mrs. Bennet’s civilities."}
{"doc_id": "gutenberg_1342", "para_id": 464, "text": "The prospect of the Netherfield ball was extremely agreeable to every female of the family. Mrs. Bennet chose to consider it as given in compliment to her eldest daughter, and was particularly flattered by receiving the invitation from Mr. Bingley himself, instead of a ceremonious card. Jane pictured to herself a happy evening in the society of her two friends, and the attentions of their brother; and Elizabeth thought with pleasure of dancing a great deal with Mr. Wickham, and of seeing a confirmation of everything in Mr. Darcy’s look and behaviour. The happiness anticipated by Catherine and Lydia depended less on any single event, or any particular person; for though they each, like Elizabeth, meant to dance half the evening with Mr. Wickham, he was by no means the only partner who could satisfy them, and a ball was, at any rate, a ball. And even Mary could assure her family that she had no disinclination for it."}
{"doc_id": "gutenberg_1342", "para_id": 465, "text": "“While I can have my mornings to myself,” said she, “it is enough. I think it is no sacrifice to join occasionally in evening engagements. Society has claims on us all; and I profess myself one of those who consider intervals of recreation and amusement as desirable for everybody.”"}
{"doc_id": "gutenberg_1342", "para_id": 466, "text": "Elizabeth’s spirits were so high on the occasion, that though she did not often speak unnecessarily to Mr. Collins, she could not help asking him whether he intended to accept Mr. Bingley’s invitation, and if he did, whether he would think it proper to join in the evening’s amusement; and she was rather surprised to find that he entertained no scruple whatever on that head, and was very far from dreading a rebuke, either from the Archbishop or Lady Catherine de Bourgh, by venturing to dance."}
{"doc_id": "gutenberg_1342", "para_id": 467, "text": "“I am by no means of opinion, I assure you,” said he, “that a ball of this kind, given by a young man of character, to respectable people, can have any evil tendency; and I am so far from objecting to dancing myself, that I shall hope to be honoured with the hands of all my fair cousins in the course of the evening; and I take this opportunity of soliciting yours, Miss Elizabeth, for the two first dances especially; a preference which I trust my cousin Jane will attribute to the right cause, and not to any disrespect for her.”"}
{"doc_id": "gutenberg_1342", "para_id": 468, "text": "Elizabeth felt herself completely taken in. She had fully proposed being engaged by Wickham for those very dances; and to have Mr. Collins instead!--her liveliness had been never worse timed. There was no help for it, however. Mr. Wickham’s happiness and her own was perforce delayed a little longer, and Mr. Collins’s proposal accepted with as good a grace as she could. She was not the better pleased with his gallantry, from the idea it suggested of something more. It now first struck her, that _she_ was selected from among her sisters as worthy of being the mistress of Hunsford Parsonage, and of assisting to form a quadrille table at Rosings, in the absence of more eligible visitors. The idea soon reached to conviction, as she observed his increasing civilities towards herself, and heard his frequent attempt at a compliment on her wit and vivacity; and though more astonished than gratified herself by this effect of her charms, it was not long before her mother gave her to understand that the probability of their marriage was exceedingly agreeable to _her_. Elizabeth, however, did not choose to take the hint, being well aware that a serious dispute must be the consequence of any reply. Mr. Collins might never make the offer, and, till he did, it was useless to quarrel about him."}
{"doc_id": "gutenberg_1342", "para_id": 469, "text": "If there had not been a Netherfield ball to prepare for and talk of, the younger Miss Bennets would have been in a pitiable state at this time; for, from the day of the invitation to the day of the ball, there was such a succession of rain as prevented their walking to Meryton once. No aunt, no officers, no news could be sought after; the very shoe-roses for Netherfield were got by proxy. Even Elizabeth might have found some trial of her patience in weather which totally suspended the improvement of her acquaintance with Mr. Wickham; and nothing less than a dance on Tuesday could have made such a Friday, Saturday, Sunday, and Monday endurable to Kitty and Lydia."}
{"doc_id": "gutenberg_1342", "para_id": 470, "text": "Till Elizabeth entered the drawing-room at Netherfield, and looked in vain for Mr. Wickham among the cluster of red coats there assembled, a doubt of his being present had never occurred to her. The certainty of meeting him had not been checked by any of those recollections that might not unreasonably have alarmed her. She had dressed with more than usual care, and prepared in the highest spirits for the conquest of all that remained unsubdued of his heart, trusting that it was not more than might be won in the course of the evening. But in an instant arose the dreadful suspicion of his being purposely omitted, for Mr. Darcy’s pleasure, in the Bingleys’ invitation to the officers; and though this was not exactly the case, the absolute fact of his absence was pronounced by his friend Mr. Denny, to whom Lydia eagerly applied, and who told them that Wickham had been obliged to go to town on business the day before, and was not yet returned; adding, with a significant smile,--"}
{"doc_id": "gutenberg_1342", "para_id": 471, "text": "“I do not imagine his business would have called him away just now, if he had not wished to avoid a certain gentleman here.”"}
{"doc_id": "gutenberg_1342", "para_id": 472, "text": "This part of his intelligence, though unheard by Lydia, was caught by Elizabeth; and, as it assured her that Darcy was not less answerable for Wickham’s absence than if her first surmise had been just, every feeling of displeasure against the former was so sharpened by immediate disappointment, that she could hardly reply with tolerable civility to the polite inquiries which he directly afterwards approached to make. Attention, forbearance, patience with Darcy, was injury to Wickham. She was resolved against any sort of conversation with him, and turned away with a degree of ill-humour which she could not wholly surmount even in speaking to Mr. Bingley, whose blind partiality provoked her."}
{"doc_id": "gutenberg_1342", "para_id": 473, "text": "But Elizabeth was not formed for ill-humour; and though every prospect of her own was destroyed for the evening, it could not dwell long on her spirits; and, having told all her griefs to Charlotte Lucas, whom she had not seen for a week, she was soon able to make a voluntary transition to the oddities of her cousin, and to point him out to her particular notice. The two first dances, however, brought a return of distress: they were dances of mortification. Mr. Collins, awkward and solemn, apologizing instead of attending, and often moving wrong without being aware of it, gave her all the shame and misery which a disagreeable partner for a couple of dances can give. The moment of her release from him was ecstasy."}
{"doc_id": "gutenberg_1342", "para_id": 474, "text": "She danced next with an officer, and had the refreshment of talking of Wickham, and of hearing that he was universally liked. When those dances were over, she returned to Charlotte Lucas, and was in conversation with her, when she found herself suddenly addressed by Mr. Darcy, who took her so much by surprise in his application for her hand, that, without knowing what she did, she accepted him. He walked away again immediately, and she was left to fret over her own want of presence of mind: Charlotte tried to console her."}
{"doc_id": "gutenberg_1342", "para_id": 475, "text": "“Heaven forbid! _That_ would be the greatest misfortune of all! To find a man agreeable whom one is determined to hate! Do not wish me such an evil.”"}
{"doc_id": "gutenberg_1342", "para_id": 476, "text": "When the dancing recommenced, however, and Darcy approached to claim her hand, Charlotte could not help cautioning her, in a whisper, not to be a simpleton, and allow her fancy for Wickham to make her appear unpleasant in the eyes of a man often times his consequence. Elizabeth made no answer, and took her place in the set, amazed at the dignity to which she was arrived in being allowed to stand opposite to Mr. Darcy, and reading in her neighbours’ looks their equal amazement in beholding it. They stood for some time without speaking a word; and she began to imagine that their silence was to last through the two dances, and, at first, was resolved not to break it; till suddenly fancying that it would be the greater punishment to her partner to oblige him to talk, she made some slight observation on the dance. He replied, and was again silent. After a pause of some minutes, she addressed him a second time, with--"}
{"doc_id": "gutenberg_1342", "para_id": 477, "text": "“It is _your_ turn to say something now, Mr. Darcy. _I_ talked about the dance, and _you_ ought to make some kind of remark on the size of the room, or the number of couples.”"}
{"doc_id": "gutenberg_1342", "para_id": 478, "text": "“Very well; that reply will do for the present. Perhaps, by-and-by, I may observe that private balls are much pleasanter than public ones; but _now_ we may be silent.”"}
{"doc_id": "gutenberg_1342", "para_id": 479, "text": "“Sometimes. One must speak a little, you know. It would look odd to be entirely silent for half an hour together; and yet, for the advantage of _some_, conversation ought to be so arranged as that they may have the trouble of saying as little as possible.”"}
{"doc_id": "gutenberg_1342", "para_id": 480, "text": "“Are you consulting your own feelings in the present case, or do you imagine that you are gratifying mine?”"}
{"doc_id": "gutenberg_1342", "para_id": 481, "text": "“Both,” replied Elizabeth archly; “for I have always seen a great similarity in the turn of our minds. We are each of an unsocial, taciturn disposition, unwilling to speak, unless we expect to say something that will amaze the whole room, and be handed down to posterity with all the _éclat_ of a proverb.”"}
{"doc_id": "gutenberg_1342", "para_id": 482, "text": "“This is no very striking resemblance of your own character, I am sure,” said he. “How near it may be to _mine_, I cannot pretend to say. _You_ think it a faithful portrait, undoubtedly.”"}
{"doc_id": "gutenberg_1342", "para_id": 483, "text": "He made no answer; and they were again silent till they had gone down the dance, when he asked her if she and her sisters did not very often walk to Meryton. She answered in the affirmative; and, unable to resist the temptation, added, “When you met us there the other day, we had just been forming a new acquaintance.”"}
{"doc_id": "gutenberg_1342", "para_id": 484, "text": "The effect was immediate. A deeper shade of _hauteur_ overspread his features, but he said not a word; and Elizabeth, though blaming herself for her own weakness, could not go on. At length Darcy spoke, and in a constrained manner said,--"}
{"doc_id": "gutenberg_1342", "para_id": 485, "text": "“Mr. Wickham is blessed with such happy manners as may insure his _making_ friends; whether he may be equally capable of _retaining_ them, is less certain.”"}
{"doc_id": "gutenberg_1342", "para_id": 486, "text": "“He has been so unlucky as to lose your friendship,” replied Elizabeth, with emphasis, “and in a manner which he is likely to suffer from all his life.”"}
{"doc_id": "gutenberg_1342", "para_id": 487, "text": "Darcy made no answer, and seemed desirous of changing the subject. At that moment Sir William Lucas appeared close to them, meaning to pass through the set to the other side of the room; but, on perceiving Mr. Darcy, he stopped, with a bow of superior courtesy, to compliment him on his dancing and his partner."}
{"doc_id": "gutenberg_1342", "para_id": 488, "text": "“I have been most highly gratified, indeed, my dear sir; such very superior dancing is not often seen. It is evident that you belong to the first circles. Allow me to say, however, that your fair partner does not disgrace you: and that I must hope to have this pleasure often repeated, especially when a certain desirable event, my dear Miss Eliza (glancing at her sister and Bingley), shall take place. What congratulations will then flow in! I appeal to Mr. Darcy;--but let me not interrupt you, sir. You will not thank me for detaining you from the bewitching converse of that young lady, whose bright eyes are also upbraiding me.”"}
{"doc_id": "gutenberg_1342", "para_id": 489, "text": "The latter part of this address was scarcely heard by Darcy; but Sir William’s allusion to his friend seemed to strike him forcibly, and his eyes were directed, with a very serious expression, towards Bingley and Jane, who were dancing together. Recovering himself, however, shortly, he turned to his partner, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 490, "text": "“I do not think we were speaking at all. Sir William could not have interrupted any two people in the room who had less to say for themselves. We have tried two or three subjects already without success, and what we are to talk of next I cannot imagine.”"}
{"doc_id": "gutenberg_1342", "para_id": 491, "text": "“Books--oh no!--I am sure we never read the same, or not with the same feelings.”"}
{"doc_id": "gutenberg_1342", "para_id": 492, "text": "“I am sorry you think so; but if that be the case, there can at least be no want of subject. We may compare our different opinions.”"}
{"doc_id": "gutenberg_1342", "para_id": 493, "text": "“No--I cannot talk of books in a ball-room; my head is always full of something else.”"}
{"doc_id": "gutenberg_1342", "para_id": 494, "text": "“The _present_ always occupies you in such scenes--does it?” said he, with a look of doubt."}
{"doc_id": "gutenberg_1342", "para_id": 495, "text": "“Yes, always,” she replied, without knowing what she said; for her thoughts had wandered far from the subject, as soon afterwards appeared by her suddenly exclaiming, “I remember hearing you once say, Mr. Darcy, that you hardly ever forgave;--that your resentment, once created, was unappeasable. You are very cautious, I suppose, as to its _being created_?”"}
{"doc_id": "gutenberg_1342", "para_id": 496, "text": "“It is particularly incumbent on those who never change their opinion, to be secure of judging properly at first.”"}
{"doc_id": "gutenberg_1342", "para_id": 497, "text": "“Merely to the illustration of _your_ character,” said she, endeavouring to shake off her gravity. “I am trying to make it out.”"}
{"doc_id": "gutenberg_1342", "para_id": 498, "text": "She shook her head. “I do not get on at all. I hear such different accounts of you as puzzle me exceedingly.”"}
{"doc_id": "gutenberg_1342", "para_id": 499, "text": "“I can readily believe,” answered he, gravely, “that reports may vary greatly with respect to me; and I could wish, Miss Bennet, that you were not to sketch my character at the present moment, as there is reason to fear that the performance would reflect no credit on either.”"}
{"doc_id": "gutenberg_1342", "para_id": 500, "text": "“I would by no means suspend any pleasure of yours,” he coldly replied. She said no more, and they went down the other dance and parted in silence; on each side dissatisfied, though not to an equal degree; for in Darcy’s breast there was a tolerably powerful feeling towards her, which soon procured her pardon, and directed all his anger against another."}
{"doc_id": "gutenberg_1342", "para_id": 501, "text": "They had not long separated when Miss Bingley came towards her, and, with an expression of civil disdain, thus accosted her,--"}
{"doc_id": "gutenberg_1342", "para_id": 502, "text": "“So, Miss Eliza, I hear you are quite delighted with George Wickham? Your sister has been talking to me about him, and asking me a thousand questions; and I find that the young man forgot to tell you, among his other communications, that he was the son of old Wickham, the late Mr. Darcy’s steward. Let me recommend you, however, as a friend, not to give implicit confidence to all his assertions; for, as to Mr. Darcy’s using him ill, it is perfectly false: for, on the contrary, he has been always remarkably kind to him, though George Wickham has treated Mr. Darcy in a most infamous manner. I do not know the particulars, but I know very well that Mr. Darcy is not in the least to blame; that he cannot bear to hear George Wickham mentioned; and that though my brother thought he could not well avoid including him in his invitation to the officers, he was excessively glad to find that he had taken himself out of the way. His coming into the country at all is a most insolent thing, indeed, and I wonder how he could presume to do it. I pity you, Miss Eliza, for this discovery of your favourite’s guilt; but really, considering his descent, one could not expect much better.”"}
{"doc_id": "gutenberg_1342", "para_id": 503, "text": "“His guilt and his descent appear, by your account, to be the same,” said Elizabeth, angrily; “for I have heard you accuse him of nothing worse than of being the son of Mr. Darcy’s steward, and of _that_, I can assure you, he informed me himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 504, "text": "“I beg your pardon,” replied Miss Bingley, turning away with a sneer. “Excuse my interference; it was kindly meant.”"}
{"doc_id": "gutenberg_1342", "para_id": 505, "text": "“Insolent girl!” said Elizabeth to herself. “You are much mistaken if you expect to influence me by such a paltry attack as this. I see nothing in it but your own wilful ignorance and the malice of Mr. Darcy.” She then sought her eldest sister, who had undertaken to make inquiries on the same subject of Bingley. Jane met her with a smile of such sweet complacency, a glow of such happy expression, as sufficiently marked how well she was satisfied with the occurrences of the evening. Elizabeth instantly read her feelings; and, at that moment, solicitude for Wickham, resentment against his enemies, and everything else, gave way before the hope of Jane’s being in the fairest way for happiness."}
{"doc_id": "gutenberg_1342", "para_id": 506, "text": "“I want to know,” said she, with a countenance no less smiling than her sister’s, “what you have learnt about Mr. Wickham. But perhaps you have been too pleasantly engaged to think of any third person, in which case you may be sure of my pardon.”"}
{"doc_id": "gutenberg_1342", "para_id": 507, "text": "“No,” replied Jane, “I have not forgotten him; but I have nothing satisfactory to tell you. Mr. Bingley does not know the whole of his history, and is quite ignorant of the circumstances which have principally offended Mr. Darcy; but he will vouch for the good conduct, the probity and honour, of his friend, and is perfectly convinced that Mr. Wickham has deserved much less attention from Mr. Darcy than he has received; and I am sorry to say that by his account, as well as his sister’s, Mr. Wickham is by no means a respectable young man. I am afraid he has been very imprudent, and has deserved to lose Mr. Darcy’s regard.”"}
{"doc_id": "gutenberg_1342", "para_id": 508, "text": "“This account then is what he has received from Mr. Darcy. I am perfectly satisfied. But what does he say of the living?”"}
{"doc_id": "gutenberg_1342", "para_id": 509, "text": "“He does not exactly recollect the circumstances, though he has heard them from Mr. Darcy more than once, but he believes that it was left to him _conditionally_ only.”"}
{"doc_id": "gutenberg_1342", "para_id": 510, "text": "“I have not a doubt of Mr. Bingley’s sincerity,” said Elizabeth warmly, “but you must excuse my not being convinced by assurances only. Mr. Bingley’s defence of his friend was a very able one, I dare say; but since he is unacquainted with several parts of the story, and has learnt the rest from that friend himself, I shall venture still to think of both gentlemen as I did before.”"}
{"doc_id": "gutenberg_1342", "para_id": 511, "text": "She then changed the discourse to one more gratifying to each, and on which there could be no difference of sentiment. Elizabeth listened with delight to the happy though modest hopes which Jane entertained of Bingley’s regard, and said all in her power to heighten her confidence in it. On their being joined by Mr. Bingley himself, Elizabeth withdrew to Miss Lucas; to whose inquiry after the pleasantness of her last partner she had scarcely replied, before Mr. Collins came up to them, and told her with great exultation, that he had just been so fortunate as to make a most important discovery."}
{"doc_id": "gutenberg_1342", "para_id": 512, "text": "“I have found out,” said he, “by a singular accident, that there is now in the room a near relation to my patroness. I happened to overhear the gentleman himself mentioning to the young lady who does the honours of this house the names of his cousin Miss De Bourgh, and of her mother, Lady Catherine. How wonderfully these sort of things occur! Who would have thought of my meeting with--perhaps--a nephew of Lady Catherine de Bourgh in this assembly! I am most thankful that the discovery is made in time for me to pay my respects to him, which I am now going to do, and trust he will excuse my not having done it before. My total ignorance of the connection must plead my apology.”"}
{"doc_id": "gutenberg_1342", "para_id": 513, "text": "“Indeed I am. I shall entreat his pardon for not having done it earlier. I believe him to be Lady Catherine’s _nephew_. It will be in my power to assure him that her Ladyship was quite well yesterday se’nnight.”"}
{"doc_id": "gutenberg_1342", "para_id": 514, "text": "Elizabeth tried hard to dissuade him from such a scheme; assuring him that Mr. Darcy would consider his addressing him without introduction as an impertinent freedom, rather than a compliment to his aunt; that it was not in the least necessary there should be any notice on either side, and that if it were, it must belong to Mr. Darcy, the superior in consequence, to begin the acquaintance. Mr. Collins listened to her with the determined air of following his own inclination, and when she ceased speaking, replied thus,--"}
{"doc_id": "gutenberg_1342", "para_id": 515, "text": "“My dear Miss Elizabeth, I have the highest opinion in the world of your excellent judgment in all matters within the scope of your understanding, but permit me to say that there must be a wide difference between the established forms of ceremony amongst the laity and those which regulate the clergy; for, give me leave to observe that I consider the clerical office as equal in point of dignity with the highest rank in the kingdom--provided that a proper humility of behaviour is at the same time maintained. You must, therefore, allow me to follow the dictates of my conscience on this occasion, which lead me to perform what I look on as a point of duty. Pardon me for neglecting to profit by your advice, which on every other subject shall be my constant guide, though in the case before us I consider myself more fitted by education and habitual study to decide on what is right than a young lady like yourself;” and with a low bow he left her to attack Mr. Darcy, whose reception of his advances she eagerly watched, and whose astonishment at being so addressed was very evident. Her cousin prefaced his speech with a solemn bow, and though she could not hear a word of it, she felt as if hearing it all, and saw in the motion of his lips the words “apology,” “Hunsford,” and “Lady Catherine de Bourgh.” It vexed her to see him expose himself to such a man. Mr. Darcy was eyeing him with unrestrained wonder; and when at last Mr. Collins allowed him to speak, replied with an air of distant civility. Mr. Collins, however, was not discouraged from speaking again, and Mr. Darcy’s contempt seemed abundantly increasing with the length of his second speech; and at the end of it he only made him a slight bow, and moved another way: Mr. Collins then returned to Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 516, "text": "“I have no reason, I assure you,” said he, “to be dissatisfied with my reception. Mr. Darcy seemed much pleased with the attention. He answered me with the utmost civility, and even paid me the compliment of saying, that he was so well convinced of Lady Catherine’s discernment as to be certain she could never bestow a favour unworthily. It was really a very handsome thought. Upon the whole, I am much pleased with him.”"}
{"doc_id": "gutenberg_1342", "para_id": 517, "text": "As Elizabeth had no longer any interest of her own to pursue, she turned her attention almost entirely on her sister and Mr. Bingley; and the train of agreeable reflections which her observations gave birth to made her perhaps almost as happy as Jane. She saw her in idea settled in that very house, in all the felicity which a marriage of true affection could bestow; and she felt capable, under such circumstances, of endeavouring even to like Bingley’s two sisters. Her mother’s thoughts she plainly saw were bent the same way, and she determined not to venture near her, lest she might hear too much. When they sat down to supper, therefore, she considered it a most unlucky perverseness which placed them within one of each other; and deeply was she vexed to find that her mother was talking to that one person (Lady Lucas) freely, openly, and of nothing else but of her expectation that Jane would be soon married to Mr. Bingley. It was an animating subject, and Mrs. Bennet seemed incapable of fatigue while enumerating the advantages of the match. His being such a charming young man, and so rich, and living but three miles from them, were the first points of self-gratulation; and then it was such a comfort to think how fond the two sisters were of Jane, and to be certain that they must desire the connection as much as she could do. It was, moreover, such a promising thing for her younger daughters, as Jane’s marrying so greatly must throw them in the way of other rich men; and, lastly, it was so pleasant at her time of life to be able to consign her single daughters to the care of their sister, that she might not be obliged to go into company more than she liked. It was necessary to make this circumstance a matter of pleasure, because on such occasions it is the etiquette; but no one was less likely than Mrs. Bennet to find comfort in staying at home at any period of her life. She concluded with many good wishes that Lady Lucas might soon be equally fortunate, though evidently and triumphantly believing there was no chance of it."}
{"doc_id": "gutenberg_1342", "para_id": 518, "text": "In vain did Elizabeth endeavour to check the rapidity of her mother’s words, or persuade her to describe her felicity in a less audible whisper; for to her inexpressible vexation she could perceive that the chief of it was overheard by Mr. Darcy, who sat opposite to them. Her mother only scolded her for being nonsensical."}
{"doc_id": "gutenberg_1342", "para_id": 519, "text": "“What is Mr. Darcy to me, pray, that I should be afraid of him? I am sure we owe him no such particular civility as to be obliged to say nothing _he_ may not like to hear.”"}
{"doc_id": "gutenberg_1342", "para_id": 520, "text": "“For heaven’s sake, madam, speak lower. What advantage can it be to you to offend Mr. Darcy? You will never recommend yourself to his friend by so doing.”"}
{"doc_id": "gutenberg_1342", "para_id": 521, "text": "Nothing that she could say, however, had any influence. Her mother would talk of her views in the same intelligible tone. Elizabeth blushed and blushed again with shame and vexation. She could not help frequently glancing her eye at Mr. Darcy, though every glance convinced her of what she dreaded; for though he was not always looking at her mother, she was convinced that his attention was invariably fixed by her. The expression of his face changed gradually from indignant contempt to a composed and steady gravity."}
{"doc_id": "gutenberg_1342", "para_id": 522, "text": "At length, however, Mrs. Bennet had no more to say; and Lady Lucas, who had been long yawning at the repetition of delights which she saw no likelihood of sharing, was left to the comforts of cold ham and chicken. Elizabeth now began to revive. But not long was the interval of tranquillity; for when supper was over, singing was talked of, and she had the mortification of seeing Mary, after very little entreaty, preparing to oblige the company. By many significant looks and silent entreaties did she endeavour to prevent such a proof of complaisance,--but in vain; Mary would not understand them; such an opportunity of exhibiting was delightful to her, and she began her song. Elizabeth’s eyes were fixed on her, with most painful sensations; and she watched her progress through the several stanzas with an impatience which was very ill rewarded at their close; for Mary, on receiving amongst the thanks of the table the hint of a hope that she might be prevailed on to favour them again, after the pause of half a minute began another. Mary’s powers were by no means fitted for such a display; her voice was weak, and her manner affected. Elizabeth was in agonies. She looked at Jane to see how she bore it; but Jane was very composedly talking to Bingley. She looked at his two sisters, and saw them making signs of derision at each other, and at Darcy, who continued, however, impenetrably grave. She looked at her father to entreat his interference, lest Mary should be singing all night. He took the hint, and, when Mary had finished her second song, said aloud,--"}
{"doc_id": "gutenberg_1342", "para_id": 523, "text": "“That will do extremely well, child. You have delighted us long enough. Let the other young ladies have time to exhibit.”"}
{"doc_id": "gutenberg_1342", "para_id": 524, "text": "Mary, though pretending not to hear, was somewhat disconcerted; and Elizabeth, sorry for her, and sorry for her father’s speech, was afraid her anxiety had done no good. Others of the party were now applied to."}
{"doc_id": "gutenberg_1342", "para_id": 525, "text": "“If I,” said Mr. Collins, “were so fortunate as to be able to sing, I should have great pleasure, I am sure, in obliging the company with an air; for I consider music as a very innocent diversion, and perfectly compatible with the profession of a clergyman. I do not mean, however, to assert that we can be justified in devoting too much of our time to music, for there are certainly other things to be attended to. The rector of a parish has much to do. In the first place, he must make such an agreement for tithes as may be beneficial to himself and not offensive to his patron. He must write his own sermons; and the time that remains will not be too much for his parish duties, and the care and improvement of his dwelling, which he cannot be excused from making as comfortable as possible. And I do not think it of light importance that he should have attentive and conciliatory manners towards everybody, especially towards those to whom he owes his preferment. I cannot acquit him of that duty; nor could I think well of the man who should omit an occasion of testifying his respect towards anybody connected with the family.” And with a bow to Mr. Darcy, he concluded his speech, which had been spoken so loud as to be heard by half the room. Many stared--many smiled; but no one looked more amused than Mr. Bennet himself, while his wife seriously commended Mr. Collins for having spoken so sensibly, and observed, in a half-whisper to Lady Lucas, that he was a remarkably clever, good kind of young man."}
{"doc_id": "gutenberg_1342", "para_id": 526, "text": "To Elizabeth it appeared, that had her family made an agreement to expose themselves as much as they could during the evening, it would have been impossible for them to play their parts with more spirit, or finer success; and happy did she think it for Bingley and her sister that some of the exhibition had escaped his notice, and that his feelings were not of a sort to be much distressed by the folly which he must have witnessed. That his two sisters and Mr. Darcy, however, should have such an opportunity of ridiculing her relations was bad enough; and she could not determine whether the silent contempt of the gentleman, or the insolent smiles of the ladies, were more intolerable."}
{"doc_id": "gutenberg_1342", "para_id": 527, "text": "The rest of the evening brought her little amusement. She was teased by Mr. Collins, who continued most perseveringly by her side; and though he could not prevail with her to dance with him again, put it out of her power to dance with others. In vain did she entreat him to stand up with somebody else, and offered to introduce him to any young lady in the room. He assured her that, as to dancing, he was perfectly indifferent to it; that his chief object was, by delicate attentions, to recommend himself to her; and that he should therefore make a point of remaining close to her the whole evening. There was no arguing upon such a project. She owed her greatest relief to her friend Miss Lucas, who often joined them, and good-naturedly engaged Mr. Collins’s conversation to herself."}
{"doc_id": "gutenberg_1342", "para_id": 528, "text": "She was at least free from the offence of Mr. Darcy’s further notice: though often standing within a very short distance of her, quite disengaged, he never came near enough to speak. She felt it to be the probable consequence of her allusions to Mr. Wickham, and rejoiced in it."}
{"doc_id": "gutenberg_1342", "para_id": 529, "text": "The Longbourn party were the last of all the company to depart; and by a manœuvre of Mrs. Bennet had to wait for their carriage a quarter of an hour after everybody else was gone, which gave them time to see how heartily they were wished away by some of the family. Mrs. Hurst and her sister scarcely opened their mouths except to complain of fatigue, and were evidently impatient to have the house to themselves. They repulsed every attempt of Mrs. Bennet at conversation, and, by so doing, threw a languor over the whole party, which was very little relieved by the long speeches of Mr. Collins, who was complimenting Mr. Bingley and his sisters on the elegance of their entertainment, and the hospitality and politeness which had marked their behaviour to their guests. Darcy said nothing at all. Mr. Bennet, in equal silence, was enjoying the scene. Mr. Bingley and Jane were standing together a little detached from the rest, and talked only to each other. Elizabeth preserved as steady a silence as either Mrs. Hurst or Miss Bingley; and even Lydia was too much fatigued to utter more than the occasional exclamation of “Lord, how tired I am!” accompanied by a violent yawn."}
{"doc_id": "gutenberg_1342", "para_id": 530, "text": "When at length they arose to take leave, Mrs. Bennet was most pressingly civil in her hope of seeing the whole family soon at Longbourn; and addressed herself particularly to Mr. Bingley, to assure him how happy he would make them, by eating a family dinner with them at any time, without the ceremony of a formal invitation. Bingley was all grateful pleasure; and he readily engaged for taking the earliest opportunity of waiting on her after his return from London, whither he was obliged to go the next day for a short time."}
{"doc_id": "gutenberg_1342", "para_id": 531, "text": "Mrs. Bennet was perfectly satisfied; and quitted the house under the delightful persuasion that, allowing for the necessary preparations of settlements, new carriages, and wedding clothes, she should undoubtedly see her daughter settled at Netherfield in the course of three or four months. Of having another daughter married to Mr. Collins she thought with equal certainty, and with considerable, though not equal, pleasure. Elizabeth was the least dear to her of all her children; and though the man and the match were quite good enough for _her_, the worth of each was eclipsed by Mr. Bingley and Netherfield."}
{"doc_id": "gutenberg_1342", "para_id": 532, "text": "The next day opened a new scene at Longbourn. Mr. Collins made his declaration in form. Having resolved to do it without loss of time, as his leave of absence extended only to the following Saturday, and having no feelings of diffidence to make it distressing to himself even at the moment, he set about it in a very orderly manner, with all the observances which he supposed a regular part of the business. On finding Mrs. Bennet, Elizabeth, and one of the younger girls together, soon after breakfast, he addressed the mother in these words,--"}
{"doc_id": "gutenberg_1342", "para_id": 533, "text": "“May I hope, madam, for your interest with your fair daughter Elizabeth, when I solicit for the honour of a private audience with her in the course of this morning?”"}
{"doc_id": "gutenberg_1342", "para_id": 534, "text": "Before Elizabeth had time for anything but a blush of surprise, Mrs. Bennet instantly answered,--"}
{"doc_id": "gutenberg_1342", "para_id": 535, "text": "“Oh dear! Yes, certainly. I am sure Lizzy will be very happy--I am sure she can have no objection. Come, Kitty, I want you upstairs.” And gathering her work together, she was hastening away, when Elizabeth called out,--"}
{"doc_id": "gutenberg_1342", "para_id": 536, "text": "“Dear ma’am, do not go. I beg you will not go. Mr. Collins must excuse me. He can have nothing to say to me that anybody need not hear. I am going away myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 537, "text": "“No, no, nonsense, Lizzy. I desire you will stay where you are.” And upon Elizabeth’s seeming really, with vexed and embarrassed looks, about to escape, she added, “Lizzy, I _insist_ upon your staying and hearing Mr. Collins.”"}
{"doc_id": "gutenberg_1342", "para_id": 538, "text": "Elizabeth would not oppose such an injunction; and a moment’s consideration making her also sensible that it would be wisest to get it over as soon and as quietly as possible, she sat down again, and tried to conceal, by incessant employment, the feelings which were divided between distress and diversion. Mrs. Bennet and Kitty walked off, and as soon as they were gone, Mr. Collins began,--"}
{"doc_id": "gutenberg_1342", "para_id": 539, "text": "“Believe me, my dear Miss Elizabeth, that your modesty, so far from doing you any disservice, rather adds to your other perfections. You would have been less amiable in my eyes had there _not_ been this little unwillingness; but allow me to assure you that I have your respected mother’s permission for this address. You can hardly doubt the purport of my discourse, however your natural delicacy may lead you to dissemble; my attentions have been too marked to be mistaken. Almost as soon as I entered the house I singled you out as the companion of my future life. But before I am run away with by my feelings on this subject, perhaps it will be advisable for me to state my reasons for marrying--and, moreover, for coming into Hertfordshire with the design of selecting a wife, as I certainly did.”"}
{"doc_id": "gutenberg_1342", "para_id": 540, "text": "The idea of Mr. Collins, with all his solemn composure, being run away with by his feelings, made Elizabeth so near laughing that she could not use the short pause he allowed in any attempt to stop him farther, and he continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 541, "text": "“My reasons for marrying are, first, that I think it a right thing for every clergyman in easy circumstances (like myself) to set the example of matrimony in his parish; secondly, that I am convinced it will add very greatly to my happiness; and, thirdly, which perhaps I ought to have mentioned earlier, that it is the particular advice and recommendation of the very noble lady whom I have the honour of calling patroness. Twice has she condescended to give me her opinion (unasked too!) on this subject; and it was but the very Saturday night before I left Hunsford,--between our pools at quadrille, while Mrs. Jenkinson was arranging Miss De Bourgh’s footstool,--that she said, ‘Mr. Collins, you must marry. A clergyman like you must marry. Choose properly, choose a gentlewoman for _my_ sake, and for your _own_; let her be an active, useful sort of person, not brought up high, but able to make a small income go a good way. This is my advice. Find such a woman as soon as you can, bring her to Hunsford, and I will visit her.’ Allow me, by the way, to observe, my fair cousin, that I do not reckon the notice and kindness of Lady Catherine de Bourgh as among the least of the advantages in my power to offer. You will find her manners beyond anything I can describe; and your wit and vivacity, I think, must be acceptable to her, especially when tempered with the silence and respect which her rank will inevitably excite. Thus much for my general intention in favour of matrimony; it remains to be told why my views were directed to Longbourn instead of my own neighbourhood, where I assure you there are many amiable young women. But the fact is, that being, as I am, to inherit this estate after the death of your honoured father (who, however, may live many years longer), I could not satisfy myself without resolving to choose a wife from among his daughters, that the loss to them might be as little as possible when the melancholy event takes place--which, however, as I have already said, may not be for several years. This has been my motive, my fair cousin, and I flatter myself it will not sink me in your esteem. And now nothing remains for me but to assure you in the most animated language of the violence of my affection. To fortune I am perfectly indifferent, and shall make no demand of that nature on your father, since I am well aware that it could not be complied with; and that one thousand pounds in the 4 per cents., which will not be yours till after your mother’s decease, is all that you may ever be entitled to. On that head, therefore, I shall be uniformly silent: and you may assure yourself that no ungenerous reproach shall ever pass my lips when we are married.”"}
{"doc_id": "gutenberg_1342", "para_id": 542, "text": "“You are too hasty, sir,” she cried. “You forget that I have made no answer. Let me do it without further loss of time. Accept my thanks for the compliment you are paying me. I am very sensible of the honour of your proposals, but it is impossible for me to do otherwise than decline them.”"}
{"doc_id": "gutenberg_1342", "para_id": 543, "text": "“I am not now to learn,” replied Mr. Collins, with a formal wave of the hand, “that it is usual with young ladies to reject the addresses of the man whom they secretly mean to accept, when he first applies for their favour; and that sometimes the refusal is repeated a second or even a third time. I am, therefore, by no means discouraged by what you have just said, and shall hope to lead you to the altar ere long.”"}
{"doc_id": "gutenberg_1342", "para_id": 544, "text": "“Upon my word, sir,” cried Elizabeth, “your hope is rather an extraordinary one after my declaration. I do assure you that I am not one of those young ladies (if such young ladies there are) who are so daring as to risk their happiness on the chance of being asked a second time. I am perfectly serious in my refusal. You could not make _me_ happy, and I am convinced that I am the last woman in the world who would make _you_ so. Nay, were your friend Lady Catherine to know me, I am persuaded she would find me in every respect ill qualified for the situation.”"}
{"doc_id": "gutenberg_1342", "para_id": 545, "text": "“Were it certain that Lady Catherine would think so,” said Mr. Collins, very gravely--“but I cannot imagine that her Ladyship would at all disapprove of you. And you may be certain that when I have the honour of seeing her again I shall speak in the highest terms of your modesty, economy, and other amiable qualifications.”"}
{"doc_id": "gutenberg_1342", "para_id": 546, "text": "“Indeed, Mr. Collins, all praise of me will be unnecessary. You must give me leave to judge for myself, and pay me the compliment of believing what I say. I wish you very happy and very rich, and by refusing your hand, do all in my power to prevent your being otherwise. In making me the offer, you must have satisfied the delicacy of your feelings with regard to my family, and may take possession of Longbourn estate whenever it falls, without any self-reproach. This matter may be considered, therefore, as finally settled.” And rising as she thus spoke, she would have quitted the room, had not Mr. Collins thus addressed her,--"}
{"doc_id": "gutenberg_1342", "para_id": 547, "text": "“When I do myself the honour of speaking to you next on the subject, I shall hope to receive a more favourable answer than you have now given me; though I am far from accusing you of cruelty at present, because I know it to be the established custom of your sex to reject a man on the first application, and, perhaps, you have even now said as much to encourage my suit as would be consistent with the true delicacy of the female character.”"}
{"doc_id": "gutenberg_1342", "para_id": 548, "text": "“Really, Mr. Collins,” cried Elizabeth, with some warmth, “you puzzle me exceedingly. If what I have hitherto said can appear to you in the form of encouragement, I know not how to express my refusal in such a way as may convince you of its being one.”"}
{"doc_id": "gutenberg_1342", "para_id": 549, "text": "“You must give me leave to flatter myself, my dear cousin, that your refusal of my addresses are merely words of course. My reasons for believing it are briefly these:--It does not appear to me that my hand is unworthy of your acceptance, or that the establishment I can offer would be any other than highly desirable. My situation in life, my connections with the family of De Bourgh, and my relationship to your own, are circumstances highly in my favour; and you should take it into further consideration that, in spite of your manifold attractions, it is by no means certain that another offer of marriage may ever be made you. Your portion is unhappily so small, that it will in all likelihood undo the effects of your loveliness and amiable qualifications. As I must, therefore, conclude that you are not serious in your rejection of me, I shall choose to attribute it to your wish of increasing my love by suspense, according to the usual practice of elegant females.”"}
{"doc_id": "gutenberg_1342", "para_id": 550, "text": "“I do assure you, sir, that I have no pretensions whatever to that kind of elegance which consists in tormenting a respectable man. I would rather be paid the compliment of being believed sincere. I thank you again and again for the honour you have done me in your proposals, but to accept them is absolutely impossible. My feelings in every respect forbid it. Can I speak plainer? Do not consider me now as an elegant female intending to plague you, but as a rational creature speaking the truth from her heart.”"}
{"doc_id": "gutenberg_1342", "para_id": 551, "text": "“You are uniformly charming!” cried he, with an air of awkward gallantry; “and I am persuaded that, when sanctioned by the express authority of both your excellent parents, my proposals will not fail of being acceptable.”"}
{"doc_id": "gutenberg_1342", "para_id": 552, "text": "To such perseverance in wilful self-deception Elizabeth would make no reply, and immediately and in silence withdrew; determined, that if he persisted in considering her repeated refusals as flattering encouragement, to apply to her father, whose negative might be uttered in such a manner as must be decisive, and whose behaviour at least could not be mistaken for the affectation and coquetry of an elegant female."}
{"doc_id": "gutenberg_1342", "para_id": 553, "text": "Mr. Collins was not left long to the silent contemplation of his successful love; for Mrs. Bennet, having dawdled about in the vestibule to watch for the end of the conference, no sooner saw Elizabeth open the door and with quick step pass her towards the staircase, than she entered the breakfast-room, and congratulated both him and herself in warm terms on the happy prospect of their nearer connection. Mr. Collins received and returned these felicitations with equal pleasure, and then proceeded to relate the particulars of their interview, with the result of which he trusted he had every reason to be satisfied, since the refusal which his cousin had steadfastly given him would naturally flow from her bashful modesty and the genuine delicacy of her character."}
{"doc_id": "gutenberg_1342", "para_id": 554, "text": "This information, however, startled Mrs. Bennet: she would have been glad to be equally satisfied that her daughter had meant to encourage him by protesting against his proposals, but she dared not believe it, and could not help saying so."}
{"doc_id": "gutenberg_1342", "para_id": 555, "text": "“But depend upon it, Mr. Collins,” she added, “that Lizzy shall be brought to reason. I will speak to her about it myself directly. She is a very headstrong, foolish girl, and does not know her own interest; but I will _make_ her know it.”"}
{"doc_id": "gutenberg_1342", "para_id": 556, "text": "“Pardon me for interrupting you, madam,” cried Mr. Collins; “but if she is really headstrong and foolish, I know not whether she would altogether be a very desirable wife to a man in my situation, who naturally looks for happiness in the marriage state. If, therefore, she actually persists in rejecting my suit, perhaps it were better not to force her into accepting me, because, if liable to such defects of temper, she could not contribute much to my felicity.”"}
{"doc_id": "gutenberg_1342", "para_id": 557, "text": "“Sir, you quite misunderstand me,” said Mrs. Bennet, alarmed. “Lizzy is only headstrong in such matters as these. In everything else she is as good-natured a girl as ever lived. I will go directly to Mr. Bennet, and we shall very soon settle it with her, I am sure.”"}
{"doc_id": "gutenberg_1342", "para_id": 558, "text": "She would not give him time to reply, but hurrying instantly to her husband, called out, as she entered the library,--"}
{"doc_id": "gutenberg_1342", "para_id": 559, "text": "“Oh, Mr. Bennet, you are wanted immediately; we are all in an uproar. You must come and make Lizzy marry Mr. Collins, for she vows she will not have him; and if you do not make haste he will change his mind and not have _her_.”"}
{"doc_id": "gutenberg_1342", "para_id": 560, "text": "Mr. Bennet raised his eyes from his book as she entered, and fixed them on her face with a calm unconcern, which was not in the least altered by her communication."}
{"doc_id": "gutenberg_1342", "para_id": 561, "text": "“I have not the pleasure of understanding you,” said he, when she had finished her speech. “Of what are you talking?”"}
{"doc_id": "gutenberg_1342", "para_id": 562, "text": "“Of Mr. Collins and Lizzy. Lizzy declares she will not have Mr. Collins, and Mr. Collins begins to say that he will not have Lizzy.”"}
{"doc_id": "gutenberg_1342", "para_id": 563, "text": "“Speak to Lizzy about it yourself. Tell her that you insist upon her marrying him.”"}
{"doc_id": "gutenberg_1342", "para_id": 564, "text": "“Come here, child,” cried her father as she appeared. “I have sent for you on an affair of importance. I understand that Mr. Collins has made you an offer of marriage. Is it true?”"}
{"doc_id": "gutenberg_1342", "para_id": 565, "text": "“Very well. We now come to the point. Your mother insists upon your accepting it. Is it not so, Mrs. Bennet?”"}
{"doc_id": "gutenberg_1342", "para_id": 566, "text": "“An unhappy alternative is before you, Elizabeth. From this day you must be a stranger to one of your parents. Your mother will never see you again if you do _not_ marry Mr. Collins, and I will never see you again if you _do_.”"}
{"doc_id": "gutenberg_1342", "para_id": 567, "text": "Elizabeth could not but smile at such a conclusion of such a beginning; but Mrs. Bennet, who had persuaded herself that her husband regarded the affair as she wished, was excessively disappointed."}
{"doc_id": "gutenberg_1342", "para_id": 568, "text": "“What do you mean, Mr. Bennet, by talking in this way? You promised me to _insist_ upon her marrying him.”"}
{"doc_id": "gutenberg_1342", "para_id": 569, "text": "“My dear,” replied her husband, “I have two small favours to request. First, that you will allow me the free use of my understanding on the present occasion; and, secondly, of my room. I shall be glad to have the library to myself as soon as may be.”"}
{"doc_id": "gutenberg_1342", "para_id": 570, "text": "Not yet, however, in spite of her disappointment in her husband, did Mrs. Bennet give up the point. She talked to Elizabeth again and again; coaxed and threatened her by turns. She endeavoured to secure Jane in her interest, but Jane, with all possible mildness, declined interfering; and Elizabeth, sometimes with real earnestness, and sometimes with playful gaiety, replied to her attacks. Though her manner varied, however, her determination never did."}
{"doc_id": "gutenberg_1342", "para_id": 571, "text": "Mr. Collins, meanwhile, was meditating in solitude on what had passed. He thought too well of himself to comprehend on what motive his cousin could refuse him; and though his pride was hurt, he suffered in no other way. His regard for her was quite imaginary; and the possibility of her deserving her mother’s reproach prevented his feeling any regret."}
{"doc_id": "gutenberg_1342", "para_id": 572, "text": "While the family were in this confusion, Charlotte Lucas came to spend the day with them. She was met in the vestibule by Lydia, who, flying to her, cried in a half whisper, “I am glad you are come, for there is such fun here! What do you think has happened this morning? Mr. Collins has made an offer to Lizzy, and she will not have him.”"}
{"doc_id": "gutenberg_1342", "para_id": 573, "text": "Charlotte had hardly time to answer before they were joined by Kitty, who came to tell the same news; and no sooner had they entered the breakfast-room, where Mrs. Bennet was alone, than she likewise began on the subject, calling on Miss Lucas for her compassion, and entreating her to persuade her friend Lizzy to comply with the wishes of her family. “Pray do, my dear Miss Lucas,” she added, in a melancholy tone; “for nobody is on my side, nobody takes part with me; I am cruelly used, nobody feels for my poor nerves.”"}
{"doc_id": "gutenberg_1342", "para_id": 574, "text": "“Ay, there she comes,” continued Mrs. Bennet, “looking as unconcerned as may be, and caring no more for us than if we were at York, provided she can have her own way. But I tell you what, Miss Lizzy, if you take it into your head to go on refusing every offer of marriage in this way, you will never get a husband at all--and I am sure I do not know who is to maintain you when your father is dead. _I_ shall not be able to keep you--and so I warn you. I have done with you from this very day. I told you in the library, you know, that I should never speak to you again, and you will find me as good as my word. I have no pleasure in talking to undutiful children. Not that I have much pleasure, indeed, in talking to anybody. People who suffer as I do from nervous complaints can have no great inclination for talking. Nobody can tell what I suffer! But it is always so. Those who do not complain are never pitied.”"}
{"doc_id": "gutenberg_1342", "para_id": 575, "text": "Her daughters listened in silence to this effusion, sensible that any attempt to reason with or soothe her would only increase the irritation. She talked on, therefore, without interruption from any of them till they were joined by Mr. Collins, who entered with an air more stately than usual, and on perceiving whom, she said to the girls,--"}
{"doc_id": "gutenberg_1342", "para_id": 576, "text": "“Now, I do insist upon it, that you, all of you, hold your tongues, and let Mr. Collins and me have a little conversation together.”"}
{"doc_id": "gutenberg_1342", "para_id": 577, "text": "Elizabeth passed quietly out of the room, Jane and Kitty followed, but Lydia stood her ground, determined to hear all she could; and Charlotte, detained first by the civility of Mr. Collins, whose inquiries after herself and all her family were very minute, and then by a little curiosity, satisfied herself with walking to the window and pretending not to hear. In a doleful voice Mrs. Bennet thus began the projected conversation:--"}
{"doc_id": "gutenberg_1342", "para_id": 578, "text": "“My dear madam,” replied he, “let us be for ever silent on this point. Far be it from me,” he presently continued, in a voice that marked his displeasure, “to resent the behaviour of your daughter. Resignation to inevitable evils is the duty of us all: the peculiar duty of a young man who has been so fortunate as I have been, in early preferment; and, I trust, I am resigned. Perhaps not the less so from feeling a doubt of my positive happiness had my fair cousin honoured me with her hand; for I have often observed, that resignation is never so perfect as when the blessing denied begins to lose somewhat of its value in our estimation. You will not, I hope, consider me as showing any disrespect to your family, my dear madam, by thus withdrawing my pretensions to your daughter’s favour, without having paid yourself and Mr. Bennet the compliment of requesting you to interpose your authority in my behalf. My conduct may, I fear, be objectionable in having accepted my dismission from your daughter’s lips instead of your own; but we are all liable to error. I have certainly meant well through the whole affair. My object has been to secure an amiable companion for myself, with due consideration for the advantage of all your family; and if my _manner_ has been at all reprehensible, I here beg leave to apologize.”"}
{"doc_id": "gutenberg_1342", "para_id": 579, "text": "The discussion of Mr. Collins’s offer was now nearly at an end, and Elizabeth had only to suffer from the uncomfortable feelings necessarily attending it, and occasionally from some peevish allusion of her mother. As for the gentleman himself, _his_ feelings were chiefly expressed, not by embarrassment or dejection, or by trying to avoid her, but by stiffness of manner and resentful silence. He scarcely ever spoke to her; and the assiduous attentions which he had been so sensible of himself were transferred for the rest of the day to Miss Lucas, whose civility in listening to him was a seasonable relief to them all, and especially to her friend."}
{"doc_id": "gutenberg_1342", "para_id": 580, "text": "The morrow produced no abatement of Mrs. Bennet’s ill humour or ill health. Mr. Collins was also in the same state of angry pride. Elizabeth had hoped that his resentment might shorten his visit, but his plan did not appear in the least affected by it. He was always to have gone on Saturday, and to Saturday he still meant to stay."}
{"doc_id": "gutenberg_1342", "para_id": 581, "text": "After breakfast, the girls walked to Meryton, to inquire if Mr. Wickham were returned, and to lament over his absence from the Netherfield ball. He joined them on their entering the town, and attended them to their aunt’s, where his regret and vexation and the concern of everybody were well talked over. To Elizabeth, however, he voluntarily acknowledged that the necessity of his absence _had_ been self-imposed."}
{"doc_id": "gutenberg_1342", "para_id": 582, "text": "“I found,” said he, “as the time drew near, that I had better not meet Mr. Darcy;--that to be in the same room, the same party with him for so many hours together, might be more than I could bear, and that scenes might arise unpleasant to more than myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 583, "text": "She highly approved his forbearance; and they had leisure for a full discussion of it, and for all the commendations which they civilly bestowed on each other, as Wickham and another officer walked back with them to Longbourn, and during the walk he particularly attended to her. His accompanying them was a double advantage: she felt all the compliment it offered to herself; and it was most acceptable as an occasion of introducing him to her father and mother."}
{"doc_id": "gutenberg_1342", "para_id": 584, "text": "Soon after their return, a letter was delivered to Miss Bennet; it came from Netherfield, and was opened immediately. The envelope contained a sheet of elegant, little, hot-pressed paper, well covered with a lady’s fair, flowing hand; and Elizabeth saw her sister’s countenance change as she read it, and saw her dwelling intently on some particular passages. Jane recollected herself soon; and putting the letter away, tried to join, with her usual cheerfulness, in the general conversation: but Elizabeth felt an anxiety on the subject which drew off her attention even from Wickham; and no sooner had he and his companion taken leave, than a glance from Jane invited her to follow her upstairs. When they had gained their own room, Jane, taking out her letter, said, “This is from Caroline Bingley: what it contains has surprised me a good deal. The whole party have left Netherfield by this time, and are on their way to town; and without any intention of coming back again. You shall hear what she says.”"}
{"doc_id": "gutenberg_1342", "para_id": 585, "text": "She then read the first sentence aloud, which comprised the information of their having just resolved to follow their brother to town directly, and of their meaning to dine that day in Grosvenor Street, where Mr. Hurst had a house. The next was in these words:--“‘I do not pretend to regret anything I shall leave in Hertfordshire except your society, my dearest friend; but we will hope, at some future period, to enjoy many returns of that delightful intercourse we have known, and in the meanwhile may lessen the pain of separation by a very frequent and most unreserved correspondence. I depend on you for that.’” To these high-flown expressions Elizabeth listened with all the insensibility of distrust; and though the suddenness of their removal surprised her, she saw nothing in it really to lament: it was not to be supposed that their absence from Netherfield would prevent Mr. Bingley’s being there; and as to the loss of their society, she was persuaded that Jane must soon cease to regard it in the enjoyment of his."}
{"doc_id": "gutenberg_1342", "para_id": 586, "text": "“It is unlucky,” said she, after a short pause, “that you should not be able to see your friends before they leave the country. But may we not hope that the period of future happiness, to which Miss Bingley looks forward, may arrive earlier than she is aware, and that the delightful intercourse you have known as friends will be renewed with yet greater satisfaction as sisters? Mr. Bingley will not be detained in London by them.”"}
{"doc_id": "gutenberg_1342", "para_id": 587, "text": "“Caroline decidedly says that none of the party will return into Hertfordshire this winter. I will read it to you."}
{"doc_id": "gutenberg_1342", "para_id": 588, "text": "“‘When my brother left us yesterday, he imagined that the business which took him to London might be concluded in three or four days; but as we are certain it cannot be so, and at the same time convinced that when Charles gets to town he will be in no hurry to leave it again, we have determined on following him thither, that he may not be obliged to spend his vacant hours in a comfortless hotel. Many of my acquaintance are already there for the winter: I wish I could hear that you, my dearest friend, had any intention of making one in the crowd, but of that I despair. I sincerely hope your Christmas in Hertfordshire may abound in the gaieties which that season generally brings, and that your beaux will be so numerous as to prevent your feeling the loss of the three of whom we shall deprive you.’"}
{"doc_id": "gutenberg_1342", "para_id": 589, "text": "“Why will you think so? It must be his own doing; he is his own master. But you do not know _all_. I _will_ read you the passage which particularly hurts me. I will have no reserves from _you_. ‘Mr. Darcy is impatient to see his sister; and to confess the truth, _we_ are scarcely less eager to meet her again. I really do not think Georgiana Darcy has her equal for beauty, elegance, and accomplishments; and the affection she inspires in Louisa and myself is heightened into something still more interesting from the hope we dare to entertain of her being hereafter our sister. I do not know whether I ever before mentioned to you my feelings on this subject, but I will not leave the country without confiding them, and I trust you will not esteem them unreasonable. My brother admires her greatly already; he will have frequent opportunity now of seeing her on the most intimate footing; her relations all wish the connection as much as his own; and a sister’s partiality is not misleading me, I think, when I call Charles most capable of engaging any woman’s heart. With all these circumstances to favour an attachment, and nothing to prevent it, am I wrong, my dearest Jane, in indulging the hope of an event which will secure the happiness of so many?’ What think you of _this_ sentence, my dear Lizzy?” said Jane, as she finished it. “Is it not clear enough? Does it not expressly declare that Caroline neither expects nor wishes me to be her sister; that she is perfectly convinced of her brother’s indifference; and that if she suspects the nature of my feelings for him she means (most kindly!) to put me on my guard. Can there be any other opinion on the subject?”"}
{"doc_id": "gutenberg_1342", "para_id": 590, "text": "“You shall have it in a few words. Miss Bingley sees that her brother is in love with you and wants him to marry Miss Darcy. She follows him to town in the hope of keeping him there, and tries to persuade you that he does not care about you.”"}
{"doc_id": "gutenberg_1342", "para_id": 591, "text": "“Indeed, Jane, you ought to believe me. No one who has ever seen you together can doubt his affection; Miss Bingley, I am sure, cannot: she is not such a simpleton. Could she have seen half as much love in Mr. Darcy for herself, she would have ordered her wedding clothes. But the case is this:--we are not rich enough or grand enough for them; and she is the more anxious to get Miss Darcy for her brother, from the notion that when there has been _one_ inter-marriage, she may have less trouble in achieving a second; in which there is certainly some ingenuity, and I dare say it would succeed if Miss de Bourgh were out of the way. But, my dearest Jane, you cannot seriously imagine that, because Miss Bingley tells you her brother greatly admires Miss Darcy, he is in the smallest degree less sensible of _your_ merit than when he took leave of you on Tuesday; or that it will be in her power to persuade him that, instead of being in love with you, he is very much in love with her friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 592, "text": "“If we thought alike of Miss Bingley,” replied Jane, “your representation of all this might make me quite easy. But I know the foundation is unjust. Caroline is incapable of wilfully deceiving anyone; and all that I can hope in this case is, that she is deceived herself.”"}
{"doc_id": "gutenberg_1342", "para_id": 593, "text": "“That is right. You could not have started a more happy idea, since you will not take comfort in mine: believe her to be deceived, by all means. You have now done your duty by her, and must fret no longer.”"}
{"doc_id": "gutenberg_1342", "para_id": 594, "text": "“But, my dear sister, can I be happy, even supposing the best, in accepting a man whose sisters and friends are all wishing him to marry elsewhere?”"}
{"doc_id": "gutenberg_1342", "para_id": 595, "text": "“You must decide for yourself,” said Elizabeth; “and if, upon mature deliberation, you find that the misery of disobliging his two sisters is more than equivalent to the happiness of being his wife, I advise you, by all means, to refuse him.”"}
{"doc_id": "gutenberg_1342", "para_id": 596, "text": "“How can you talk so?” said Jane, faintly smiling; “you must know, that, though I should be exceedingly grieved at their disapprobation, I could not hesitate.”"}
{"doc_id": "gutenberg_1342", "para_id": 597, "text": "“I did not think you would; and that being the case, I cannot consider your situation with much compassion.”"}
{"doc_id": "gutenberg_1342", "para_id": 598, "text": "“But if he returns no more this winter, my choice will never be required. A thousand things may arise in six months.”"}
{"doc_id": "gutenberg_1342", "para_id": 599, "text": "The idea of his returning no more Elizabeth treated with the utmost contempt. It appeared to her merely the suggestion of Caroline’s interested wishes; and she could not for a moment suppose that those wishes, however openly or artfully spoken, could influence a young man so totally independent of everyone."}
{"doc_id": "gutenberg_1342", "para_id": 600, "text": "She represented to her sister, as forcibly as possible, what she felt on the subject, and had soon the pleasure of seeing its happy effect. Jane’s temper was not desponding; and she was gradually led to hope, though the diffidence of affection sometimes overcame the hope, that Bingley would return to Netherfield, and answer every wish of her heart."}
{"doc_id": "gutenberg_1342", "para_id": 601, "text": "They agreed that Mrs. Bennet should only hear of the departure of the family, without being alarmed on the score of the gentleman’s conduct; but even this partial communication gave her a great deal of concern, and she bewailed it as exceedingly unlucky that the ladies should happen to go away just as they were all getting so intimate together. After lamenting it, however, at some length, she had the consolation of thinking that Mr. Bingley would be soon down again, and soon dining at Longbourn; and the conclusion of all was the comfortable declaration, that, though he had been invited only to a family dinner, she would take care to have two full courses."}
{"doc_id": "gutenberg_1342", "para_id": 602, "text": "The Bennets were engaged to dine with the Lucases; and again, during the chief of the day, was Miss Lucas so kind as to listen to Mr. Collins. Elizabeth took an opportunity of thanking her. “It keeps him in good humour,” said she, “and I am more obliged to you than I can express.”"}
{"doc_id": "gutenberg_1342", "para_id": 603, "text": "Charlotte assured her friend of her satisfaction in being useful, and that it amply repaid her for the little sacrifice of her time. This was very amiable; but Charlotte’s kindness extended farther than Elizabeth had any conception of:--its object was nothing less than to secure her from any return of Mr. Collins’s addresses, by engaging them towards herself. Such was Miss Lucas’s scheme; and appearances were so favourable, that when they parted at night, she would have felt almost sure of success if he had not been to leave Hertfordshire so very soon. But here she did injustice to the fire and independence of his character; for it led him to escape out of Longbourn House the next morning with admirable slyness, and hasten to Lucas Lodge to throw himself at her feet. He was anxious to avoid the notice of his cousins, from a conviction that, if they saw him depart, they could not fail to conjecture his design, and he was not willing to have the attempt known till its success could be known likewise; for, though feeling almost secure, and with reason, for Charlotte had been tolerably encouraging, he was comparatively diffident since the adventure of Wednesday. His reception, however, was of the most flattering kind. Miss Lucas perceived him from an upper window as he walked towards the house, and instantly set out to meet him accidentally in the lane. But little had she dared to hope that so much love and eloquence awaited her there."}
{"doc_id": "gutenberg_1342", "para_id": 604, "text": "In as short a time as Mr. Collins’s long speeches would allow, everything was settled between them to the satisfaction of both; and as they entered the house, he earnestly entreated her to name the day that was to make him the happiest of men; and though such a solicitation must be waived for the present, the lady felt no inclination to trifle with his happiness. The stupidity with which he was favoured by nature must guard his courtship from any charm that could make a woman wish for its continuance; and Miss Lucas, who accepted him solely from the pure and disinterested desire of an establishment, cared not how soon that establishment were gained."}
{"doc_id": "gutenberg_1342", "para_id": 605, "text": "Sir William and Lady Lucas were speedily applied to for their consent; and it was bestowed with a most joyful alacrity. Mr. Collins’s present circumstances made it a most eligible match for their daughter, to whom they could give little fortune; and his prospects of future wealth were exceedingly fair. Lady Lucas began directly to calculate, with more interest than the matter had ever"}
{"doc_id": "gutenberg_1342", "para_id": 606, "text": "excited before, how many years longer Mr. Bennet was likely to live; and Sir William gave it as his decided opinion, that whenever Mr. Collins should be in possession of the Longbourn estate, it would be highly expedient that both he and his wife should make their appearance at St. James’s. The whole family in short were properly overjoyed on the occasion. The younger girls formed hopes of _coming out_ a year or two sooner than they might otherwise have done; and the boys were relieved from their apprehension of Charlotte’s dying an old maid. Charlotte herself was tolerably composed. She had gained her point, and had time to consider of it. Her reflections were in general satisfactory. Mr. Collins, to be sure, was neither sensible nor agreeable: his society was irksome, and his attachment to her must be imaginary. But still he would be her husband. Without thinking highly either of men or of matrimony, marriage had always been her object: it was the only honourable provision for well-educated young women of small fortune, and, however uncertain of giving happiness, must be their pleasantest preservative from want. This preservative she had now obtained; and at the age of twenty-seven, without having ever been handsome, she felt all the good luck of it. The least agreeable circumstance in the business was the surprise it must occasion to Elizabeth Bennet, whose friendship she valued beyond that of any other person. Elizabeth would wonder, and probably would blame her; and though her resolution was not to be shaken, her feelings must be hurt by such a disapprobation. She resolved to give her the information herself; and therefore charged Mr. Collins, when he returned to Longbourn to dinner, to drop no hint of what had passed before any of the family. A promise of secrecy was of course very dutifully given, but it could not be kept without difficulty; for the curiosity excited by his long absence burst forth in such very direct questions on his return, as required some ingenuity to evade, and he was at the same time exercising great self-denial, for he was longing to publish his prosperous love."}
{"doc_id": "gutenberg_1342", "para_id": 607, "text": "As he was to begin his journey too early on the morrow to see any of the family, the ceremony of leave-taking was performed when the ladies moved for the night; and Mrs. Bennet, with great politeness and cordiality, said how happy they should be to see him at Longbourn again, whenever his other engagements might allow him to visit them."}
{"doc_id": "gutenberg_1342", "para_id": 608, "text": "“My dear madam,” he replied, “this invitation is particularly gratifying, because it is what I have been hoping to receive; and you may be very certain that I shall avail myself of it as soon as possible.”"}
{"doc_id": "gutenberg_1342", "para_id": 609, "text": "They were all astonished; and Mr. Bennet, who could by no means wish for so speedy a return, immediately said,--"}
{"doc_id": "gutenberg_1342", "para_id": 610, "text": "“But is there not danger of Lady Catherine’s disapprobation here, my good sir? You had better neglect your relations than run the risk of offending your patroness.”"}
{"doc_id": "gutenberg_1342", "para_id": 611, "text": "“My dear sir,” replied Mr. Collins, “I am particularly obliged to you for this friendly caution, and you may depend upon my not taking so material a step without her Ladyship’s concurrence.”"}
{"doc_id": "gutenberg_1342", "para_id": 612, "text": "“You cannot be too much on your guard. Risk anything rather than her displeasure; and if you find it likely to be raised by your coming to us again, which I should think exceedingly probable, stay quietly at home, and be satisfied that _we_ shall take no offence.”"}
{"doc_id": "gutenberg_1342", "para_id": 613, "text": "“Believe me, my dear sir, my gratitude is warmly excited by such affectionate attention; and, depend upon it, you will speedily receive from me a letter of thanks for this as well as for every other mark of your regard during my stay in Hertfordshire. As for my fair cousins, though my absence may not be long enough to render it necessary, I shall now take the liberty of wishing them health and happiness, not excepting my cousin Elizabeth.”"}
{"doc_id": "gutenberg_1342", "para_id": 614, "text": "With proper civilities, the ladies then withdrew; all of them equally surprised to find that he meditated a quick return. Mrs. Bennet wished to understand by it that he thought of paying his addresses to one of her younger girls, and Mary might have been prevailed on to accept him. She rated his abilities much higher than any of the others: there was a solidity in his reflections which often struck her; and though by no means so clever as herself, she thought that, if encouraged to read and improve himself by such an example as hers, he might become a very agreeable companion. But on the following morning every hope of this kind was done away. Miss Lucas called soon after breakfast, and in a private conference with Elizabeth related the event of the day before."}
{"doc_id": "gutenberg_1342", "para_id": 615, "text": "The possibility of Mr. Collins’s fancying himself in love with her friend had once occurred to Elizabeth within the last day or two: but that Charlotte could encourage him seemed almost as far from possibility as that she could encourage him herself; and her astonishment was consequently so great as to overcome at first the bounds of decorum, and she could not help crying out,--"}
{"doc_id": "gutenberg_1342", "para_id": 616, "text": "The steady countenance which Miss Lucas had commanded in telling her story gave way to a momentary confusion here on receiving so direct a reproach; though, as it was no more than she expected, she soon regained her composure, and calmly replied,--"}
{"doc_id": "gutenberg_1342", "para_id": 617, "text": "“Why should you be surprised, my dear Eliza? Do you think it incredible that Mr. Collins should be able to procure any woman’s good opinion, because he was not so happy as to succeed with you?”"}
{"doc_id": "gutenberg_1342", "para_id": 618, "text": "But Elizabeth had now recollected herself; and, making a strong effort for it, was able to assure her, with tolerable firmness, that the prospect of their relationship was highly grateful to her, and that she wished her all imaginable happiness."}
{"doc_id": "gutenberg_1342", "para_id": 619, "text": "“I see what you are feeling,” replied Charlotte; “you must be surprised, very much surprised, so lately as Mr. Collins was wishing to marry you. But when you have had time to think it all over, I hope you will be satisfied with what I have done. I am not romantic, you know. I never was. I ask only a comfortable home; and, considering Mr. Collins’s character, connections, and situation in life, I am convinced that my chance of happiness with him is as fair as most people can boast on entering the marriage state.”"}
{"doc_id": "gutenberg_1342", "para_id": 620, "text": "Elizabeth quietly answered “undoubtedly;” and, after an awkward pause, they returned to the rest of the family. Charlotte did not stay much longer; and Elizabeth was then left to reflect on what she had heard. It was a long time before she became at all reconciled to the idea of so unsuitable a match. The strangeness of Mr. Collins’s making two offers of marriage within three days was nothing in comparison of his being now accepted. She had always felt that Charlotte’s opinion of matrimony was not exactly like her own; but she could not have supposed it possible that, when called into action, she would have sacrificed every better feeling to worldly advantage. Charlotte, the wife of Mr. Collins, was a most humiliating picture! And to the pang of a friend disgracing herself, and sunk in her esteem, was added the distressing conviction that it was impossible for that friend to be tolerably happy in the lot she had chosen."}
{"doc_id": "gutenberg_1342", "para_id": 621, "text": "Elizabeth was sitting with her mother and sisters, reflecting on what she had heard, and doubting whether she was authorized to mention it, when Sir William Lucas himself appeared, sent by his daughter to announce her engagement to the family. With many compliments to them, and much self-gratulation on the prospect of a connection between the houses, he unfolded the matter,--to an audience not merely wondering, but incredulous; for Mrs. Bennet, with more perseverance than politeness, protested he must be entirely mistaken; and Lydia, always unguarded and often uncivil, boisterously exclaimed,--"}
{"doc_id": "gutenberg_1342", "para_id": 622, "text": "“Good Lord! Sir William, how can you tell such a story? Do not you know that Mr. Collins wants to marry Lizzy?”"}
{"doc_id": "gutenberg_1342", "para_id": 623, "text": "Nothing less than the complaisance of a courtier could have borne without anger such treatment: but Sir William’s good-breeding carried him through it all; and though he begged leave to be positive as to the truth of his information, he listened to all their impertinence with the most forbearing courtesy."}
{"doc_id": "gutenberg_1342", "para_id": 624, "text": "Elizabeth, feeling it incumbent on her to relieve him from so unpleasant a situation, now put herself forward to confirm his account, by mentioning her prior knowledge of it from Charlotte herself; and endeavoured to put a stop to the exclamations of her mother and sisters, by the earnestness of her congratulations to Sir William, in which she was readily joined by Jane, and by making a variety of remarks on the happiness that might be expected from the match, the excellent character of Mr. Collins, and the convenient distance of Hunsford from London."}
{"doc_id": "gutenberg_1342", "para_id": 625, "text": "Mrs. Bennet was, in fact, too much overpowered to say a great deal while Sir William remained; but no sooner had he left them than her feelings found a rapid vent. In the first place, she persisted in disbelieving the whole of the matter; secondly, she was very sure that Mr. Collins had been taken in; thirdly, she trusted that they would never be happy together; and, fourthly, that the match might be broken off. Two inferences, however, were plainly deduced from the whole: one, that Elizabeth was the real cause of all the mischief; and the other, that she herself had been barbarously used by them all; and on these two points she principally dwelt during the rest of the day. Nothing could console and nothing appease her. Nor did that day wear out her resentment. A week elapsed before she could see Elizabeth without scolding her: a month passed away before she could speak to Sir William or Lady Lucas without being rude; and many months were gone before she could at all forgive their daughter."}
{"doc_id": "gutenberg_1342", "para_id": 626, "text": "Mr. Bennet’s emotions were much more tranquil on the occasion, and such as he did experience he pronounced to be of a most agreeable sort; for it gratified him, he said, to discover that Charlotte Lucas, whom he had been used to think tolerably sensible, was as foolish as his wife, and more foolish than his daughter!"}
{"doc_id": "gutenberg_1342", "para_id": 627, "text": "Jane confessed herself a little surprised at the match: but she said less of her astonishment than of her earnest desire for their happiness; nor could Elizabeth persuade her to consider it as improbable. Kitty and Lydia were far from envying Miss Lucas, for Mr. Collins was only a clergyman; and it affected them in no other way than as a piece of news to spread at Meryton."}
{"doc_id": "gutenberg_1342", "para_id": 628, "text": "Lady Lucas could not be insensible of triumph on being able to retort on Mrs. Bennet the comfort of having a daughter well married; and she called at Longbourn rather oftener than usual to say how happy she was, though Mrs. Bennet’s sour looks and ill-natured remarks might have been enough to drive happiness away."}
{"doc_id": "gutenberg_1342", "para_id": 629, "text": "Between Elizabeth and Charlotte there was a restraint which kept them mutually silent on the subject; and Elizabeth felt persuaded that no real confidence could ever subsist between them again. Her disappointment in Charlotte made her turn with fonder regard to her sister, of whose rectitude and delicacy she was sure her opinion could never be shaken, and for whose happiness she grew daily more anxious, as Bingley had now been gone a week, and nothing was heard of his return."}
{"doc_id": "gutenberg_1342", "para_id": 630, "text": "Jane had sent Caroline an early answer to her letter, and was counting the days till she might reasonably hope to hear again. The promised letter of thanks from Mr. Collins arrived on Tuesday, addressed to their father, and written with all the solemnity of gratitude which a twelve-month’s abode in the family might have prompted. After discharging his conscience on that head, he proceeded to inform them, with many rapturous expressions, of his happiness in having obtained the affection of their amiable neighbour, Miss Lucas, and then explained that it was merely with the view of enjoying her society that he had been so ready to close with their kind wish of seeing him again at Longbourn, whither he hoped to be able to return on Monday fortnight; for Lady Catherine, he added, so heartily approved his marriage, that she wished it to take place as soon as possible, which he trusted would be an unanswerable argument with his amiable Charlotte to name an early day for making him the happiest of men."}
{"doc_id": "gutenberg_1342", "para_id": 631, "text": "Mr. Collins’s return into Hertfordshire was no longer a matter of pleasure to Mrs. Bennet. On the contrary, she was as much disposed to complain of it as her husband. It was very strange that he should come to Longbourn instead of to Lucas Lodge; it was also very inconvenient and exceedingly troublesome. She hated having visitors in the house while her health was so indifferent, and lovers were of all people the most disagreeable. Such were the gentle murmurs of Mrs. Bennet, and they gave way only to the greater distress of Mr. Bingley’s continued absence."}
{"doc_id": "gutenberg_1342", "para_id": 632, "text": "Neither Jane nor Elizabeth were comfortable on this subject. Day after day passed away without bringing any other tidings of him than the report which shortly prevailed in Meryton of his coming no more to Netherfield the whole winter; a report which highly incensed Mrs. Bennet, and which she never failed to contradict as a most scandalous falsehood."}
{"doc_id": "gutenberg_1342", "para_id": 633, "text": "Even Elizabeth began to fear--not that Bingley was indifferent--but that his sisters would be successful in keeping him away. Unwilling as she was to admit an idea so destructive to Jane’s happiness, and so dishonourable to the stability of her lover, she could not prevent its frequently recurring. The united efforts of his two unfeeling sisters, and of his overpowering friend, assisted by the attractions of Miss Darcy and the amusements of London, might be too much, she feared, for the strength of his attachment."}
{"doc_id": "gutenberg_1342", "para_id": 634, "text": "As for Jane, _her_ anxiety under this suspense was, of course, more painful than Elizabeth’s: but whatever she felt she was desirous of concealing; and between herself and Elizabeth, therefore, the subject was never alluded to. But as no such delicacy restrained her mother, an hour seldom passed in which she did not talk of Bingley, express her impatience for his arrival, or even require Jane to confess that if he did not come back she should think herself very ill-used. It needed all Jane’s steady mildness to bear these attacks with tolerable tranquillity."}
{"doc_id": "gutenberg_1342", "para_id": 635, "text": "Mr. Collins returned most punctually on the Monday fortnight, but his reception at Longbourn was not quite so gracious as it had been on his first introduction. He was too happy, however, to need much attention; and, luckily for the others, the business of love-making relieved them from a great deal of his company. The chief of every day was spent by him at Lucas Lodge, and he sometimes returned to Longbourn only in time to make an apology for his absence before the family went to bed."}
{"doc_id": "gutenberg_1342", "para_id": 636, "text": "Mrs. Bennet was really in a most pitiable state. The very mention of anything concerning the match threw her into an agony of ill-humour, and wherever she went she was sure of hearing it talked of. The sight of Miss Lucas was odious to her. As her successor in that house, she regarded her with jealous abhorrence. Whenever Charlotte came to see them, she concluded her to be anticipating the hour of possession; and whenever she spoke in a low voice to Mr. Collins, was convinced that they were talking of the Longbourn estate, and resolving to turn herself and her daughters out of the house as soon as Mr. Bennet was dead. She complained bitterly of all this to her husband."}
{"doc_id": "gutenberg_1342", "para_id": 637, "text": "“Indeed, Mr. Bennet,” said she, “it is very hard to think that Charlotte Lucas should ever be mistress of this house, that _I_ should be forced to make way for _her_, and live to see her take my place in it!”"}
{"doc_id": "gutenberg_1342", "para_id": 638, "text": "“My dear, do not give way to such gloomy thoughts. Let us hope for better things. Let us flatter ourselves that _I_ may be the survivor.”"}
{"doc_id": "gutenberg_1342", "para_id": 639, "text": "This was not very consoling to Mrs. Bennet; and, therefore, instead of making any answer, she went on as before."}
{"doc_id": "gutenberg_1342", "para_id": 640, "text": "“I cannot bear to think that they should have all this estate. If it was not for the entail, I should not mind it.”"}
{"doc_id": "gutenberg_1342", "para_id": 641, "text": "“I never can be thankful, Mr. Bennet, for anything about the entail. How anyone could have the conscience to entail away an estate from one’s own daughters I cannot understand; and all for the sake of Mr. Collins, too! Why should _he_ have it more than anybody else?”"}
{"doc_id": "gutenberg_1342", "para_id": 642, "text": "Miss Bingley’s letter arrived, and put an end to doubt. The very first sentence conveyed the assurance of their being all settled in London for the winter, and concluded with her brother’s regret at not having had time to pay his respects to his friends in Hertfordshire before he left the country."}
{"doc_id": "gutenberg_1342", "para_id": 643, "text": "Hope was over, entirely over; and when Jane could attend to the rest of the letter, she found little, except the professed affection of the writer, that could give her any comfort. Miss Darcy’s praise occupied the chief of it. Her many attractions were again dwelt on; and Caroline boasted joyfully of their increasing intimacy, and ventured to predict the accomplishment of the wishes which had been unfolded in her former letter. She wrote also with great pleasure of her brother’s being an inmate of Mr. Darcy’s house, and mentioned with raptures some plans of the latter with regard to new furniture."}
{"doc_id": "gutenberg_1342", "para_id": 644, "text": "Elizabeth, to whom Jane very soon communicated the chief of all this, heard it in silent indignation. Her heart was divided between concern for her sister and resentment against all others. To Caroline’s assertion of her brother’s being partial to Miss Darcy, she paid no credit. That he was really fond of Jane, she doubted no more than she had ever done; and much as she had always been disposed to like him, she could not think without anger, hardly without contempt, on that easiness of temper, that want of proper resolution, which now made him the slave of his designing friends, and led him to sacrifice his own happiness to the caprice of their inclinations. Had his own happiness, however, been the only sacrifice, he might have been allowed to sport with it in whatever manner he thought best; but her sister’s was involved in it, as she thought he must be sensible himself. It was a subject, in short, on which reflection would be long indulged, and must be unavailing. She could think of nothing else; and yet, whether Bingley’s regard had really died away, or were suppressed by his friends’ interference; whether he had been aware of Jane’s attachment, or whether it had escaped his observation; whichever were the case, though her opinion of him must be materially affected by the difference, her sister’s situation remained the same, her peace equally wounded."}
{"doc_id": "gutenberg_1342", "para_id": 645, "text": "A day or two passed before Jane had courage to speak of her feelings to Elizabeth; but at last, on Mrs. Bennet’s leaving them together, after a longer irritation than usual about Netherfield and its master, she could not help saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 646, "text": "“O that my dear mother had more command over herself! she can have no idea of the pain she gives me by her continual reflections on him. But I will not repine. It cannot last long. He will be forgot, and we shall all be as we were before.”"}
{"doc_id": "gutenberg_1342", "para_id": 647, "text": "“You doubt me,” cried Jane, slightly colouring; “indeed, you have no reason. He may live in my memory as the most amiable man of my acquaintance but that is all. I have nothing either to hope or fear, and nothing to reproach him with. Thank God I have not _that_ pain. A little time, therefore--I shall certainly try to get the better----”"}
{"doc_id": "gutenberg_1342", "para_id": 648, "text": "With a stronger voice she soon added, “I have this comfort immediately, that it has not been more than an error of fancy on my side, and that it has done no harm to anyone but myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 649, "text": "“My dear Jane,” exclaimed Elizabeth, “you are too good. Your sweetness and disinterestedness are really angelic; I do not know what to say to you. I feel as if I had never done you justice, or loved you as you deserve.”"}
{"doc_id": "gutenberg_1342", "para_id": 650, "text": "Miss Bennet eagerly disclaimed all extraordinary merit, and threw back the praise on her sister’s warm affection."}
{"doc_id": "gutenberg_1342", "para_id": 651, "text": "“Nay,” said Elizabeth, “this is not fair. _You_ wish to think all the world respectable, and are hurt if I speak ill of anybody. _I_ only want to think _you_ perfect, and you set yourself against it. Do not be afraid of my running into any excess, of my encroaching on your privilege of universal good-will. You need not. There are few people whom I really love, and still fewer of whom I think well. The more I see of the world the more am I dissatisfied with it; and every day confirms my belief of the inconsistency of all human characters, and of the little dependence that can be placed on the appearance of either merit or sense. I have met with two instances lately: one I will not mention, the other is Charlotte’s marriage. It is unaccountable! in every view it is unaccountable!”"}
{"doc_id": "gutenberg_1342", "para_id": 652, "text": "“My dear Lizzy, do not give way to such feelings as these. They will ruin your happiness. You do not make allowance enough for difference of situation and temper. Consider Mr. Collins’s respectability, and Charlotte’s prudent, steady character. Remember that she is one of a large family; that as to fortune it is a most eligible match; and be ready to believe, for everybody’s sake, that she may feel something like regard and esteem for our cousin.”"}
{"doc_id": "gutenberg_1342", "para_id": 653, "text": "“To oblige you, I would try to believe almost anything, but no one else could be benefited by such a belief as this; for were I persuaded that Charlotte had any regard for him, I should only think worse of her understanding than I now do of her heart. My dear Jane, Mr. Collins is a conceited, pompous, narrow-minded, silly man: you know he is, as well as I do; and you must feel, as well as I do, that the woman who marries him cannot have a proper way of thinking. You shall not defend her, though it is Charlotte Lucas. You shall not, for the sake of one individual, change the meaning of principle and integrity, nor endeavour to persuade yourself or me, that selfishness is prudence, and insensibility of danger security for happiness.”"}
{"doc_id": "gutenberg_1342", "para_id": 654, "text": "“I must think your language too strong in speaking of both,” replied Jane; “and I hope you will be convinced of it, by seeing them happy together. But enough of this. You alluded to something else. You mentioned _two_ instances. I cannot misunderstand you, but I entreat you, dear Lizzy, not to pain me by thinking _that person_ to blame, and saying your opinion of him is sunk. We must not be so ready to fancy ourselves intentionally injured. We must not expect a lively young man to be always so guarded and circumspect. It is very often nothing but our own vanity that deceives us. Women fancy admiration means more than it does.”"}
{"doc_id": "gutenberg_1342", "para_id": 655, "text": "“If it is designedly done, they cannot be justified; but I have no idea of there being so much design in the world as some persons imagine.”"}
{"doc_id": "gutenberg_1342", "para_id": 656, "text": "“I am far from attributing any part of Mr. Bingley’s conduct to design,” said Elizabeth; “but, without scheming to do wrong, or to make others unhappy, there may be error and there may be misery. Thoughtlessness, want of attention to other people’s feelings, and want of resolution, will do the business.”"}
{"doc_id": "gutenberg_1342", "para_id": 657, "text": "“Yes; to the last. But if I go on I shall displease you by saying what I think of persons you esteem. Stop me, whilst you can.”"}
{"doc_id": "gutenberg_1342", "para_id": 658, "text": "“I cannot believe it. Why should they try to influence him? They can only wish his happiness; and if he is attached to me no other woman can secure it.”"}
{"doc_id": "gutenberg_1342", "para_id": 659, "text": "“Your first position is false. They may wish many things besides his happiness: they may wish his increase of wealth and consequence; they may wish him to marry a girl who has all the importance of money, great connections, and pride.”"}
{"doc_id": "gutenberg_1342", "para_id": 660, "text": "“Beyond a doubt they do wish him to choose Miss Darcy,” replied Jane; “but this may be from better feelings than you are supposing. They have known her much longer than they have known me; no wonder if they love her better. But, whatever may be their own wishes, it is very unlikely they should have opposed their brother’s. What sister would think herself at liberty to do it, unless there were something very objectionable? If they believed him attached to me they would not try to part us; if he were so, they could not succeed. By supposing such an affection, you make everybody acting unnaturally and wrong, and me most unhappy. Do not distress me by the idea. I am not ashamed of having been mistaken--or, at least, it is slight, it is nothing in comparison of what I should feel in thinking ill of him or his sisters. Let me take it in the best light, in the light in which it may be understood.”"}
{"doc_id": "gutenberg_1342", "para_id": 661, "text": "Elizabeth could not oppose such a wish; and from this time Mr. Bingley’s name was scarcely ever mentioned between them."}
{"doc_id": "gutenberg_1342", "para_id": 662, "text": "Mrs. Bennet still continued to wonder and repine at his returning no more; and though a day seldom passed in which Elizabeth did not account for it clearly, there seemed little chance of her ever considering it with less perplexity. Her daughter endeavoured to convince her of what she did not believe herself, that his attentions to Jane had been merely the effect of a common and transient liking, which ceased when he saw her no more; but though the probability of the statement was admitted at the time, she had the same story to repeat every day. Mrs. Bennet’s best comfort was, that Mr. Bingley must be down again in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 663, "text": "Mr. Bennet treated the matter differently. “So, Lizzy,” said he, one day, “your sister is crossed in love, I find. I congratulate her. Next to being married, a girl likes to be crossed in love a little now and then. It is something to think of, and gives her a sort of distinction among her companions. When is your turn to come? You will hardly bear to be long outdone by Jane. Now is your time. Here are officers enough at Meryton to disappoint all the young ladies in the country. Let Wickham be your man. He is a pleasant fellow, and would jilt you creditably.”"}
{"doc_id": "gutenberg_1342", "para_id": 664, "text": "“Thank you, sir, but a less agreeable man would satisfy me. We must not all expect Jane’s good fortune.”"}
{"doc_id": "gutenberg_1342", "para_id": 665, "text": "“True,” said Mr. Bennet; “but it is a comfort to think that, whatever of that kind may befall you, you have an affectionate mother who will always make the most of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 666, "text": "Mr. Wickham’s society was of material service in dispelling the gloom which the late perverse occurrences had thrown on many of the Longbourn family. They saw him often, and to his other recommendations was now added that of general unreserve. The whole of what Elizabeth had already heard, his claims on Mr. Darcy, and all that he had suffered from him, was now openly acknowledged and publicly canvassed; and everybody was pleased to think how much they had always disliked Mr. Darcy before they had known anything of the matter."}
{"doc_id": "gutenberg_1342", "para_id": 667, "text": "Miss Bennet was the only creature who could suppose there might be any extenuating circumstances in the case unknown to the society of Hertfordshire: her mild and steady candour always pleaded for allowances, and urged the possibility of mistakes; but by everybody else Mr. Darcy was condemned as the worst of men."}
{"doc_id": "gutenberg_1342", "para_id": 668, "text": "After a week spent in professions of love and schemes of felicity, Mr. Collins was called from his amiable Charlotte by the arrival of Saturday. The pain of separation, however, might be alleviated on his side by preparations for the reception of his bride, as he had reason to hope, that shortly after his next return into Hertfordshire, the day would be fixed that was to make him the happiest of men. He took leave of his relations at Longbourn with as much solemnity as before; wished his fair cousins health and happiness again, and promised their father another letter of thanks."}
{"doc_id": "gutenberg_1342", "para_id": 669, "text": "On the following Monday, Mrs. Bennet had the pleasure of receiving her brother and his wife, who came, as usual, to spend the Christmas at Longbourn. Mr. Gardiner was a sensible, gentlemanlike man, greatly superior to his sister, as well by nature as education. The Netherfield ladies would have had difficulty in believing that a man who lived by trade, and within view of his own warehouses, could have been so well-bred and agreeable. Mrs. Gardiner, who was several years younger than Mrs. Bennet and Mrs. Philips, was an amiable, intelligent, elegant woman, and a great favourite with her Longbourn nieces. Between the two eldest and herself especially, there subsisted a very particular regard. They had frequently been staying with her in town."}
{"doc_id": "gutenberg_1342", "para_id": 670, "text": "The first part of Mrs. Gardiner’s business, on her arrival, was to distribute her presents and describe the newest fashions. When this was done, she had a less active part to play. It became her turn to listen. Mrs. Bennet had many grievances to relate, and much to complain of. They had all been very ill-used since she last saw her sister. Two of her girls had been on the point of marriage, and after all there was nothing in it."}
{"doc_id": "gutenberg_1342", "para_id": 671, "text": "“I do not blame Jane,” she continued, “for Jane would have got Mr. Bingley if she could. But, Lizzy! Oh, sister! it is very hard to think that she might have been Mr. Collins’s wife by this time, had not it been for her own perverseness. He made her an offer in this very room, and she refused him. The consequence of it is, that Lady Lucas will have a daughter married before I have, and that Longbourn estate is just as much entailed as ever. The Lucases are very artful people, indeed, sister. They are all for what they can get. I am sorry to say it of them, but so it is. It makes me very nervous and poorly, to be thwarted so in my own family, and to have neighbours who think of themselves before anybody else. However, your coming just at this time is the greatest of comforts, and I am very glad to hear what you tell us of long sleeves.”"}
{"doc_id": "gutenberg_1342", "para_id": 672, "text": "Mrs. Gardiner, to whom the chief of this news had been given before, in the course of Jane and Elizabeth’s correspondence with her, made her sister a slight answer, and, in compassion to her nieces, turned the conversation."}
{"doc_id": "gutenberg_1342", "para_id": 673, "text": "When alone with Elizabeth afterwards, she spoke more on the subject. “It seems likely to have been a desirable match for Jane,” said she. “I am sorry it went off. But these things happen so often! A young man, such as you describe Mr. Bingley, so easily falls in love with a pretty girl for a few weeks, and, when accident separates them, so easily forgets her, that these sort of inconstancies are very frequent.”"}
{"doc_id": "gutenberg_1342", "para_id": 674, "text": "“An excellent consolation in its way,” said Elizabeth; “but it will not do for _us_. We do not suffer by accident. It does not often happen that the interference of friends will persuade a young man of independent fortune to think no more of a girl whom he was violently in love with only a few days before.”"}
{"doc_id": "gutenberg_1342", "para_id": 675, "text": "“But that expression of ‘violently in love’ is so hackneyed, so doubtful, so indefinite, that it gives me very little idea. It is as often applied to feelings which arise only from a half hour’s acquaintance, as to a real, strong attachment. Pray, how _violent was_ Mr. Bingley’s love?”"}
{"doc_id": "gutenberg_1342", "para_id": 676, "text": "“I never saw a more promising inclination; he was growing quite inattentive to other people, and wholly engrossed by her. Every time they met, it was more decided and remarkable. At his own ball he offended two or three young ladies by not asking them to dance; and I spoke to him twice myself without receiving an answer. Could there be finer symptoms? Is not general incivility the very essence of love?”"}
{"doc_id": "gutenberg_1342", "para_id": 677, "text": "“Oh, yes! of that kind of love which I suppose him to have felt. Poor Jane! I am sorry for her, because, with her disposition, she may not get over it immediately. It had better have happened to _you_, Lizzy; you would have laughed yourself out of it sooner. But do you think she would be prevailed on to go back with us? Change of scene might be of service--and perhaps a little relief from home may be as useful as anything.”"}
{"doc_id": "gutenberg_1342", "para_id": 678, "text": "Elizabeth was exceedingly pleased with this proposal, and felt persuaded of her sister’s ready acquiescence."}
{"doc_id": "gutenberg_1342", "para_id": 679, "text": "“I hope,” added Mrs. Gardiner, “that no consideration with regard to this young man will influence her. We live in so different a part of town, all our connections are so different, and, as you well know, we go out so little, that it is very improbable they should meet at all, unless he really comes to see her.”"}
{"doc_id": "gutenberg_1342", "para_id": 680, "text": "“And _that_ is quite impossible; for he is now in the custody of his friend, and Mr. Darcy would no more suffer him to call on Jane in such a part of London! My dear aunt, how could you think of it? Mr. Darcy may, perhaps, have _heard_ of such a place as Gracechurch Street, but he would hardly think a month’s ablution enough to cleanse him from its impurities, were he once to enter it; and, depend upon it, Mr. Bingley never stirs without him.”"}
{"doc_id": "gutenberg_1342", "para_id": 681, "text": "“So much the better. I hope they will not meet at all. But does not Jane correspond with his sister? _She_ will not be able to help calling.”"}
{"doc_id": "gutenberg_1342", "para_id": 682, "text": "But, in spite of the certainty in which Elizabeth affected to place this point, as well as the still more interesting one of Bingley’s being withheld from seeing Jane, she felt a solicitude on the subject which convinced her, on examination, that she did not consider it entirely hopeless. It was possible, and sometimes she thought it probable, that his affection might be re-animated, and the influence of his friends successfully combated by the more natural influence of Jane’s attractions."}
{"doc_id": "gutenberg_1342", "para_id": 683, "text": "Miss Bennet accepted her aunt’s invitation with pleasure; and the Bingleys were no otherwise in her thoughts at the same time than as she hoped, by Caroline’s not living in the same house with her brother, she might occasionally spend a morning with her, without any danger of seeing him."}
{"doc_id": "gutenberg_1342", "para_id": 684, "text": "The Gardiners stayed a week at Longbourn; and what with the Philipses, the Lucases, and the officers, there was not a day without its engagement. Mrs. Bennet had so carefully provided for the entertainment of her brother and sister, that they did not once sit down to a family dinner. When the engagement was for home, some of the officers always made part of it, of which officers Mr. Wickham was sure to be one; and on these occasions Mrs. Gardiner, rendered suspicious by Elizabeth’s warm commendation of him, narrowly observed them both. Without supposing them, from what she saw, to be very seriously in love, their preference of each other was plain enough to make her a little uneasy; and she resolved to speak to Elizabeth on the subject before she left Hertfordshire, and represent to her the imprudence of encouraging such an attachment."}
{"doc_id": "gutenberg_1342", "para_id": 685, "text": "To Mrs. Gardiner, Wickham had one means of affording pleasure, unconnected with his general powers. About ten or a dozen years ago, before her marriage, she had spent a considerable time in that very part of Derbyshire to which he belonged. They had, therefore, many acquaintance in common; and, though Wickham had been little there since the death of Darcy’s father, five years before, it was yet in his power to give her fresher intelligence of her former friends than she had been in the way of procuring."}
{"doc_id": "gutenberg_1342", "para_id": 686, "text": "Mrs. Gardiner had seen Pemberley, and known the late Mr. Darcy by character perfectly well. Here, consequently, was an inexhaustible subject of discourse. In comparing her recollection of Pemberley with the minute description which Wickham could give, and in bestowing her tribute of praise on the character of its late possessor, she was delighting both him and herself. On being made acquainted with the present Mr. Darcy’s treatment of him, she tried to remember something of that gentleman’s reputed disposition, when quite a lad, which might agree with it; and was confident, at last, that she recollected having heard Mr. Fitzwilliam Darcy formerly spoken of as a very proud, ill-natured boy."}
{"doc_id": "gutenberg_1342", "para_id": 687, "text": "Mrs. Gardiner’s caution to Elizabeth was punctually and kindly given on the first favourable opportunity of speaking to her alone: after honestly telling her what she thought, she thus went on:--"}
{"doc_id": "gutenberg_1342", "para_id": 688, "text": "“You are too sensible a girl, Lizzy, to fall in love merely because you are warned against it; and, therefore, I am not afraid of speaking openly. Seriously, I would have you be on your guard. Do not involve yourself, or endeavour to involve him, in an affection which the want of fortune would make so very imprudent. I have nothing to say against _him_: he is a most interesting young man; and if he had the fortune he ought to have, I should think you could not do better. But as it is--you must not let your fancy run away with you. You have sense, and we all expect you to use it. Your father would depend on _your_ resolution and good conduct, I am sure. You must not disappoint your father.”"}
{"doc_id": "gutenberg_1342", "para_id": 689, "text": "“Well, then, you need not be under any alarm. I will take care of myself, and of Mr. Wickham too. He shall not be in love with me, if I can prevent it.”"}
{"doc_id": "gutenberg_1342", "para_id": 690, "text": "“I beg your pardon. I will try again. At present I am not in love with Mr. Wickham; no, I certainly am not. But he is, beyond all comparison, the most agreeable man I ever saw--and if he becomes really attached to me--I believe it will be better that he should not. I see the imprudence of it. Oh, _that_ abominable Mr. Darcy! My father’s opinion of me does me the greatest honour; and I should be miserable to forfeit it. My father, however, is partial to Mr. Wickham. In short, my dear aunt, I should be very sorry to be the means of making any of you unhappy; but since we see, every day, that where there is affection young people are seldom withheld, by immediate want of fortune, from entering into engagements with each other, how can I promise to be wiser than so many of my fellow-creatures, if I am tempted, or how am I even to know that it would be wiser to resist? All that I can promise you, therefore, is not to be in a hurry. I will not be in a hurry to believe myself his first object. When I am in company with him, I will not be wishing. In short, I will do my best.”"}
{"doc_id": "gutenberg_1342", "para_id": 691, "text": "“Perhaps it will be as well if you discourage his coming here so very often. At least you should not _remind_ your mother of inviting him.”"}
{"doc_id": "gutenberg_1342", "para_id": 692, "text": "“As I did the other day,” said Elizabeth, with a conscious smile; “very true, it will be wise in me to refrain from _that_. But do not imagine that he is always here so often. It is on your account that he has been so frequently invited this week. You know my mother’s ideas as to the necessity of constant company for her friends. But really, and upon my honour, I will try to do what I think to be wisest; and now I hope you are satisfied.”"}
{"doc_id": "gutenberg_1342", "para_id": 693, "text": "Her aunt assured her that she was; and Elizabeth, having thanked her for the kindness of her hints, they parted,--a wonderful instance of advice being given on such a point without being resented."}
{"doc_id": "gutenberg_1342", "para_id": 694, "text": "Mr. Collins returned into Hertfordshire soon after it had been quitted by the Gardiners and Jane; but, as he took up his abode with the Lucases, his arrival was no great inconvenience to Mrs. Bennet. His marriage was now fast approaching; and she was at length so far resigned as to think it inevitable, and even repeatedly to say, in an ill-natured tone, that she “_wished_ they might be happy.” Thursday was to be the wedding-day, and on Wednesday Miss Lucas paid her farewell visit; and when she rose to take leave, Elizabeth, ashamed of her mother’s ungracious and reluctant good wishes, and sincerely affected herself, accompanied her out of the room. As they went down stairs together, Charlotte said,--"}
{"doc_id": "gutenberg_1342", "para_id": 695, "text": "“I am not likely to leave Kent for some time. Promise me, therefore, to come to Hunsford.”"}
{"doc_id": "gutenberg_1342", "para_id": 696, "text": "“My father and Maria are to come to me in March,” added Charlotte, “and I hope you will consent to be of the party. Indeed, Eliza, you will be as welcome to me as either of them.”"}
{"doc_id": "gutenberg_1342", "para_id": 697, "text": "The wedding took place: the bride and bridegroom set off for Kent from the church door, and everybody had as much to say or to hear on the subject as usual. Elizabeth soon heard from her friend, and their correspondence was as regular and frequent as it ever had been: that it should be equally unreserved was impossible. Elizabeth could never address her without feeling that all the comfort of intimacy was over; and, though determined not to slacken as a correspondent, it was for the sake of what had been rather than what was. Charlotte’s first letters were received with a good deal of eagerness: there could not but be curiosity to know how she would speak of her new home, how she would like Lady Catherine, and how happy she would dare pronounce herself to be; though, when the letters were read, Elizabeth felt that Charlotte expressed herself on every point exactly as she might have foreseen. She wrote cheerfully, seemed surrounded with comforts, and mentioned nothing which she could not praise. The house, furniture, neighbourhood, and roads, were all to her taste, and Lady Catherine’s behaviour was most friendly and obliging. It was Mr. Collins’s picture of Hunsford and Rosings rationally softened; and Elizabeth perceived that she must wait for her own visit there, to know the rest."}
{"doc_id": "gutenberg_1342", "para_id": 698, "text": "Jane had already written a few lines to her sister, to announce their safe arrival in London; and when she wrote again, Elizabeth hoped it would be in her power to say something of the Bingleys."}
{"doc_id": "gutenberg_1342", "para_id": 699, "text": "Her impatience for this second letter was as well rewarded as impatience generally is. Jane had been a week in town, without either seeing or hearing from Caroline. She accounted for it, however, by supposing that her last letter to her friend from Longbourn had by some accident been lost."}
{"doc_id": "gutenberg_1342", "para_id": 700, "text": "“My aunt,” she continued, “is going to-morrow into that part of the town, and I shall take the opportunity of calling in Grosvenor Street.”"}
{"doc_id": "gutenberg_1342", "para_id": 701, "text": "She wrote again when the visit was paid, and she had seen Miss Bingley. “I did not think Caroline in spirits,” were her words, “but she was very glad to see me, and reproached me for giving her no notice of my coming to London. I was right, therefore; my last letter had never reached her. I inquired after their brother, of course. He was well, but so much engaged with Mr. Darcy that they scarcely ever saw him. I found that Miss Darcy was expected to dinner: I wish I could see her. My visit was not long, as Caroline and Mrs. Hurst were going out. I dare say I shall soon see them here.”"}
{"doc_id": "gutenberg_1342", "para_id": 702, "text": "Elizabeth shook her head over this letter. It convinced her that accident only could discover to Mr. Bingley her sister’s being in town."}
{"doc_id": "gutenberg_1342", "para_id": 703, "text": "Four weeks passed away, and Jane saw nothing of him. She endeavoured to persuade herself that she did not regret it; but she could no longer be blind to Miss Bingley’s inattention. After waiting at home every morning for a fortnight, and inventing every evening a fresh excuse for her, the visitor did at last appear; but the shortness of her stay, and, yet more, the alteration of her manner, would allow Jane to deceive herself no longer. The letter which she wrote on this occasion to her sister will prove what she felt:--"}
{"doc_id": "gutenberg_1342", "para_id": 704, "text": "“My dearest Lizzy will, I am sure, be incapable of triumphing in her better judgment, at my expense, when I confess myself to have been entirely deceived in Miss Bingley’s regard for me. But, my dear sister, though the event has proved you right, do not think me obstinate if I still assert that, considering what her behaviour was, my confidence was as natural as your suspicion. I do not at all comprehend her reason for wishing to be intimate with me; but, if the same circumstances were to happen again, I am sure I should be deceived again. Caroline did not return my visit till yesterday; and not a note, not a line, did I receive in the meantime. When she did come, it was very evident that she had no pleasure in it; she made a slight, formal apology for not calling before, said not a word of wishing to see me again, and was, in every respect, so altered a creature, that when she went away I was perfectly resolved to continue the acquaintance no longer. I pity, though I cannot help blaming, her. She was very wrong in singling me out as she did; I can safely say, that every advance to intimacy began on her side. But I pity her, because she must feel that she has been acting wrong, and because I am very sure that anxiety for her brother is the cause of it. I need not explain myself farther; and though _we_ know this anxiety to be quite needless, yet if she feels it, it will easily account for her behaviour to me; and so deservedly dear as he is to his sister, whatever anxiety she may feel on his behalf is natural and amiable. I cannot but wonder, however, at her having any such fears now, because if he had at all cared about me, we must have met long, long ago. He knows of my being in town, I am certain, from something she said herself; and yet it would seem, by her manner of talking, as if she wanted to persuade herself that he is really partial to Miss Darcy. I cannot understand it. If I were not afraid of judging harshly, I should be almost tempted to say, that there is a strong appearance of duplicity in all this. I will endeavour to banish every painful thought, and think only of what will make me happy, your affection, and the invariable kindness of my dear uncle and aunt. Let me hear from you very soon. Miss Bingley said something of his never returning to Netherfield again, of giving up the house, but not with any certainty. We had better not mention it. I am extremely glad that you have such pleasant accounts from our friends at Hunsford. Pray go to see them, with Sir William and Maria. I am sure you will be very comfortable there."}
{"doc_id": "gutenberg_1342", "para_id": 705, "text": "This letter gave Elizabeth some pain; but her spirits returned, as she considered that Jane would no longer be duped, by the sister at least. All expectation from the brother was now absolutely over. She would not even wish for any renewal of his attentions. His character sunk on every review of it; and, as a punishment for him, as well as a possible advantage to Jane, she seriously hoped he might really soon marry Mr. Darcy’s sister, as, by Wickham’s account, she would make him abundantly regret what he had thrown away."}
{"doc_id": "gutenberg_1342", "para_id": 706, "text": "Mrs. Gardiner about this time reminded Elizabeth of her promise concerning that gentleman, and required information; and Elizabeth had such to send as might rather give contentment to her aunt than to herself. His apparent partiality had subsided, his attentions were over, he was the admirer of some one else. Elizabeth was watchful enough to see it all, but she could see it and write of it without material pain. Her heart had been but slightly touched, and her vanity was satisfied with believing that _she_ would have been his only choice, had fortune permitted it. The sudden acquisition of ten thousand pounds was the most remarkable charm of the young lady to whom he was now rendering himself agreeable; but Elizabeth, less clear-sighted perhaps in this case than in Charlotte’s, did not quarrel with him for his wish of independence. Nothing, on the contrary, could be more natural; and, while able to suppose that it cost him a few struggles to relinquish her, she was ready to allow it a wise and desirable measure for both, and could very sincerely wish him happy."}
{"doc_id": "gutenberg_1342", "para_id": 707, "text": "All this was acknowledged to Mrs. Gardiner; and, after relating the circumstances, she thus went on:--“I am now convinced, my dear aunt, that I have never been much in love; for had I really experienced that pure and elevating passion, I should at present detest his very name, and wish him all manner of evil. But my feelings are not only cordial towards _him_, they are even impartial towards Miss King. I cannot find out that I hate her at all, or that I am in the least unwilling to think her a very good sort of girl. There can be no love in all this. My watchfulness has been effectual; and though I should certainly be a more interesting object to all my acquaintance, were I distractedly in love with him, I cannot say that I regret my comparative insignificance. Importance may sometimes be purchased too dearly. Kitty and Lydia take his defection much more to heart than I do. They are young in the ways of the world, and not yet open to the mortifying conviction that handsome young men must have something to live on as well as the plain.”"}
{"doc_id": "gutenberg_1342", "para_id": 708, "text": "With no greater events than these in the Longbourn family, and otherwise diversified by little beyond the walks to Meryton, sometimes dirty and sometimes cold, did January and February pass away. March was to take Elizabeth to Hunsford. She had not at first thought very seriously of going thither; but Charlotte, she soon found, was depending on the plan, and she gradually learned to consider it herself with greater pleasure as well as greater certainty. Absence had increased her desire of seeing Charlotte again, and weakened her disgust of Mr. Collins. There was novelty in the scheme; and as, with such a mother and such uncompanionable sisters, home could not be faultless, a little change was not unwelcome for its own sake. The journey would, moreover, give her a peep at Jane; and, in short, as the time drew near, she would have been very sorry for any delay. Everything, however, went on smoothly, and was finally settled according to Charlotte’s first sketch. She was to accompany Sir William and his second daughter. The improvement of spending a night in London was added in time, and the plan became as perfect as plan could be."}
{"doc_id": "gutenberg_1342", "para_id": 709, "text": "The only pain was in leaving her father, who would certainly miss her, and who, when it came to the point, so little liked her going, that he told her to write to him, and almost promised to answer her letter."}
{"doc_id": "gutenberg_1342", "para_id": 710, "text": "The farewell between herself and Mr. Wickham was perfectly friendly; on his side even more. His present pursuit could not make him forget that Elizabeth had been the first to excite and to deserve his attention, the first to listen and to pity, the first to be admired; and in his manner of bidding her adieu, wishing her every enjoyment, reminding her of what she was to expect in Lady Catherine de Bourgh, and trusting their opinion of her--their opinion of everybody--would always coincide, there was a solicitude, an interest, which she felt must ever attach her to him with a most sincere regard; and she parted from him convinced, that, whether married or single, he must always be her model of the amiable and pleasing."}
{"doc_id": "gutenberg_1342", "para_id": 711, "text": "Her fellow-travellers the next day were not of a kind to make her think him less agreeable. Sir William Lucas, and his daughter Maria, a good-humoured girl, but as empty-headed as himself, had nothing to say that could be worth hearing, and were listened to with about as much delight as the rattle of the chaise. Elizabeth loved absurdities, but she had known Sir William’s too long. He could tell her nothing new of the wonders of his presentation and knighthood; and his civilities were worn out, like his information."}
{"doc_id": "gutenberg_1342", "para_id": 712, "text": "It was a journey of only twenty-four miles, and they began it so early as to be in Gracechurch Street by noon. As they drove to Mr. Gardiner’s door, Jane was at a drawing-room window watching their arrival: when they entered the passage, she was there to welcome them, and Elizabeth, looking earnestly in her face, was pleased to see it healthful and lovely as ever. On the stairs were a troop of little boys and girls, whose eagerness for their cousin’s appearance would not allow them to wait in the drawing-room, and whose shyness, as they had not seen her for a twelvemonth, prevented their coming lower. All was joy and kindness. The day passed most pleasantly away; the morning in bustle and shopping, and the evening at one of the theatres."}
{"doc_id": "gutenberg_1342", "para_id": 713, "text": "Elizabeth then contrived to sit by her aunt. Their first subject was her sister; and she was more grieved than astonished to hear, in reply to her minute inquiries, that though Jane always struggled to support her spirits, there were periods of dejection. It was reasonable, however, to hope that they would not continue long. Mrs. Gardiner gave her the particulars also of Miss Bingley’s visit in Gracechurch Street, and repeated conversations occurring at different times between Jane and herself, which proved that the former had, from her heart, given up the acquaintance."}
{"doc_id": "gutenberg_1342", "para_id": 714, "text": "Mrs. Gardiner then rallied her niece on Wickham’s desertion, and complimented her on bearing it so well."}
{"doc_id": "gutenberg_1342", "para_id": 715, "text": "“But, my dear Elizabeth,” she added, “what sort of girl is Miss King? I should be sorry to think our friend mercenary.”"}
{"doc_id": "gutenberg_1342", "para_id": 716, "text": "“Pray, my dear aunt, what is the difference in matrimonial affairs, between the mercenary and the prudent motive? Where does discretion end, and avarice begin? Last Christmas you were afraid of his marrying me, because it would be imprudent; and now, because he is trying to get a girl with only ten thousand pounds, you want to find out that he is mercenary.”"}
{"doc_id": "gutenberg_1342", "para_id": 717, "text": "“If you will only tell me what sort of girl Miss King is, I shall know what to think.”"}
{"doc_id": "gutenberg_1342", "para_id": 718, "text": "“But he paid her not the smallest attention till her grandfather’s death made her mistress of this fortune?”"}
{"doc_id": "gutenberg_1342", "para_id": 719, "text": "“No--why should he? If it were not allowable for him to gain _my_ affections, because I had no money, what occasion could there be for making love to a girl whom he did not care about, and who was equally poor?”"}
{"doc_id": "gutenberg_1342", "para_id": 720, "text": "“But there seems indelicacy in directing his attentions towards her so soon after this event.”"}
{"doc_id": "gutenberg_1342", "para_id": 721, "text": "“A man in distressed circumstances has not time for all those elegant decorums which other people may observe. If _she_ does not object to it, why should _we_?”"}
{"doc_id": "gutenberg_1342", "para_id": 722, "text": "“_Her_ not objecting does not justify _him_. It only shows her being deficient in something herself--sense or feeling.”"}
{"doc_id": "gutenberg_1342", "para_id": 723, "text": "“Well,” cried Elizabeth, “have it as you choose. _He_ shall be mercenary, and _she_ shall be foolish.”"}
{"doc_id": "gutenberg_1342", "para_id": 724, "text": "“No, Lizzy, that is what I do _not_ choose. I should be sorry, you know, to think ill of a young man who has lived so long in Derbyshire.”"}
{"doc_id": "gutenberg_1342", "para_id": 725, "text": "“Oh, if that is all, I have a very poor opinion of young men who live in Derbyshire; and their intimate friends who live in Hertfordshire are not much better. I am sick of them all. Thank heaven! I am going to-morrow where I shall find a man who has not one agreeable quality, who has neither manners nor sense to recommend him. Stupid men are the only ones worth knowing, after all.”"}
{"doc_id": "gutenberg_1342", "para_id": 726, "text": "Before they were separated by the conclusion of the play, she had the unexpected happiness of an invitation to accompany her uncle and aunt in a tour of pleasure which they proposed taking in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 727, "text": "“We have not quite determined how far it shall carry us,” said Mrs. Gardiner; “but perhaps, to the Lakes.”"}
{"doc_id": "gutenberg_1342", "para_id": 728, "text": "No scheme could have been more agreeable to Elizabeth, and her acceptance of the invitation was most ready and grateful. “My dear, dear aunt,” she rapturously cried, “what delight! what felicity! You give me fresh life and vigour. Adieu to disappointment and spleen. What are men to rocks and mountains? Oh, what hours of transport we shall spend! And when we _do_ return, it shall not be like other travellers, without being able to give one accurate idea of anything. We _will_ know where we have gone--we _will_ recollect what we have seen. Lakes, mountains, and rivers, shall not be jumbled together in our imaginations; nor, when we attempt to describe any particular scene, will we begin quarrelling about its relative situation. Let _our_ first effusions be less insupportable than those of the generality of travellers.”"}
{"doc_id": "gutenberg_1342", "para_id": 729, "text": "Every object in the next day’s journey was new and interesting to Elizabeth; and her spirits were in a state of enjoyment; for she had seen her sister looking so well as to banish all fear for her health, and the prospect of her northern tour was a constant source of delight."}
{"doc_id": "gutenberg_1342", "para_id": 730, "text": "When they left the high road for the lane to Hunsford, every eye was in search of the Parsonage, and every turning expected to bring it in view. The paling of Rosings park was their boundary on one side. Elizabeth smiled at the recollection of all that she had heard of its inhabitants."}
{"doc_id": "gutenberg_1342", "para_id": 731, "text": "At length the Parsonage was discernible. The garden sloping to the road, the house standing in it, the green pales and the laurel hedge, everything declared they were arriving. Mr. Collins and Charlotte appeared at the door, and the carriage stopped at the small gate, which led by a short gravel walk to the house, amidst the nods and smiles of the whole party. In a moment they were all out of the chaise, rejoicing at the sight of each other. Mrs. Collins welcomed her friend with the liveliest pleasure, and Elizabeth was more and more satisfied with coming, when she found herself so affectionately received. She saw instantly that her cousin’s manners were not altered by his marriage: his formal civility was just what it had been; and he detained her some minutes at the gate to hear and satisfy his inquiries after all her family. They were then, with no other delay than his pointing out the neatness of the entrance, taken into the house; and as soon as they were in the parlour, he welcomed them a second time, with ostentatious formality, to his humble abode, and punctually repeated all his wife’s offers of refreshment."}
{"doc_id": "gutenberg_1342", "para_id": 732, "text": "Elizabeth was prepared to see him in his glory; and she could not help fancying that in displaying the good proportion of the room, its aspect, and its furniture, he addressed himself particularly to her, as if wishing to make her feel what she had lost in refusing him. But though everything seemed neat and comfortable, she was not able to gratify him by any sigh of repentance; and rather looked with wonder at her friend, that she could have so cheerful an air with such a companion. When Mr. Collins said anything of which his wife might reasonably be ashamed, which certainly was not seldom, she involuntarily turned her eye on Charlotte. Once or twice she could discern a faint blush; but in general Charlotte wisely did not hear. After sitting long enough to admire every article of furniture in the room, from the sideboard to the fender, to give an account of their journey, and of all that had happened in London, Mr. Collins invited them to take a stroll in the garden, which was large and well laid out, and to the cultivation of which he attended himself. To work in his garden was one of his most respectable pleasures; and Elizabeth admired the command of countenance with which Charlotte talked of the healthfulness of the exercise, and owned she encouraged it as much as possible. Here, leading the way through every walk and cross walk, and scarcely allowing them an interval to utter the praises he asked for, every view was pointed out with a minuteness which left beauty entirely behind. He could number the fields in every direction, and could tell how many trees there were in the most distant clump. But of all the views which his garden, or which the country or the kingdom could boast, none were to be compared with the prospect of Rosings, afforded by an opening in the trees that bordered the park nearly opposite the front of his house. It was a handsome modern building, well situated on rising ground."}
{"doc_id": "gutenberg_1342", "para_id": 733, "text": "From his garden, Mr. Collins would have led them round his two meadows; but the ladies, not having shoes to encounter the remains of a white frost, turned back; and while Sir William accompanied him, Charlotte took her sister and friend over the house, extremely well pleased, probably, to have the opportunity of showing it without her husband’s help. It was rather small, but well built and convenient; and everything was fitted up and arranged with a neatness and consistency, of which Elizabeth gave Charlotte all the credit. When Mr. Collins could be forgotten, there was really a great air of comfort throughout, and by Charlotte’s evident enjoyment of it, Elizabeth supposed he must be often forgotten."}
{"doc_id": "gutenberg_1342", "para_id": 734, "text": "She had already learnt that Lady Catherine was still in the country. It was spoken of again while they were at dinner, when Mr. Collins joining in, observed,--"}
{"doc_id": "gutenberg_1342", "para_id": 735, "text": "“Yes, Miss Elizabeth, you will have the honour of seeing Lady Catherine de Bourgh on the ensuing Sunday at church, and I need not say you will be delighted with her. She is all affability and condescension, and I doubt not but you will be honoured with some portion of her notice when service is over. I have scarcely any hesitation in saying that she will include you and my sister Maria in every invitation with which she honours us during your stay here. Her behaviour to my dear Charlotte is charming. We dine at Rosings twice every week, and are never allowed to walk home. Her Ladyship’s carriage is regularly ordered for us. I _should_ say, one of her Ladyship’s carriages, for she has several.”"}
{"doc_id": "gutenberg_1342", "para_id": 736, "text": "“Lady Catherine is a very respectable, sensible woman, indeed,” added Charlotte, “and a most attentive neighbour.”"}
{"doc_id": "gutenberg_1342", "para_id": 737, "text": "“Very true, my dear, that is exactly what I say. She is the sort of woman whom one cannot regard with too much deference.”"}
{"doc_id": "gutenberg_1342", "para_id": 738, "text": "The evening was spent chiefly in talking over Hertfordshire news, and telling again what had been already written; and when it closed, Elizabeth, in the solitude of her chamber, had to meditate upon Charlotte’s degree of contentment, to understand her address in guiding, and composure in bearing with, her husband, and to acknowledge that it was all done very well. She had also to anticipate how her visit would pass, the quiet tenour of their usual employments, the vexatious interruptions of Mr. Collins, and the gaieties of their intercourse with Rosings. A lively imagination soon settled it all."}
{"doc_id": "gutenberg_1342", "para_id": 739, "text": "About the middle of the next day, as she was in her room getting ready for a walk, a sudden noise below seemed to speak the whole house in confusion; and, after listening a moment, she heard somebody running upstairs in a violent hurry, and calling loudly after her. She opened the door, and met Maria in the landing-place, who, breathless with agitation, cried out,--"}
{"doc_id": "gutenberg_1342", "para_id": 740, "text": "“Oh, my dear Eliza! pray make haste and come into the dining-room, for there is such a sight to be seen! I will not tell you what it is. Make haste, and come down this moment.”"}
{"doc_id": "gutenberg_1342", "para_id": 741, "text": "Elizabeth asked questions in vain; Maria would tell her nothing more; and down they ran into the dining-room which fronted the lane, in quest of this wonder; it was two ladies, stopping in a low phaeton at the garden gate."}
{"doc_id": "gutenberg_1342", "para_id": 742, "text": "“And is this all?” cried Elizabeth. “I expected at least that the pigs were got into the garden, and here is nothing but Lady Catherine and her daughter!”"}
{"doc_id": "gutenberg_1342", "para_id": 743, "text": "“La! my dear,” said Maria, quite shocked at the mistake, “it is not Lady Catherine. The old lady is Mrs. Jenkinson, who lives with them. The other is Miss De Bourgh. Only look at her. She is quite a little creature. Who would have thought she could be so thin and small!”"}
{"doc_id": "gutenberg_1342", "para_id": 744, "text": "“She is abominably rude to keep Charlotte out of doors in all this wind. Why does she not come in?”"}
{"doc_id": "gutenberg_1342", "para_id": 745, "text": "“Oh, Charlotte says she hardly ever does. It is the greatest of favours when Miss De Bourgh comes in.”"}
{"doc_id": "gutenberg_1342", "para_id": 746, "text": "“I like her appearance,” said Elizabeth, struck with other ideas. “She looks sickly and cross. Yes, she will do for him very well. She will make him a very proper wife.”"}
{"doc_id": "gutenberg_1342", "para_id": 747, "text": "Mr. Collins and Charlotte were both standing at the gate in conversation with the ladies; and Sir William, to Elizabeth’s high diversion, was stationed in the doorway, in earnest contemplation of the greatness before him, and constantly bowing whenever Miss De Bourgh looked that way."}
{"doc_id": "gutenberg_1342", "para_id": 748, "text": "At length there was nothing more to be said; the ladies drove on, and the others returned into the house. Mr. Collins no sooner saw the two girls than he began to congratulate them on their good fortune, which Charlotte explained by letting them know that the whole party was asked to dine at Rosings the next day."}
{"doc_id": "gutenberg_1342", "para_id": 749, "text": "Mr. Collins’s triumph, in consequence of this invitation, was complete. The power of displaying the grandeur of his patroness to his wondering visitors, and of letting them see her civility towards himself and his wife, was exactly what he had wished for; and that an opportunity of doing it should be given so soon was such an instance of Lady Catherine’s condescension as he knew not how to admire enough."}
{"doc_id": "gutenberg_1342", "para_id": 750, "text": "“I confess,” said he, “that I should not have been at all surprised by her Ladyship’s asking us on Sunday to drink tea and spend the evening at Rosings. I rather expected, from my knowledge of her affability, that it would happen. But who could have foreseen such an attention as this? Who could have imagined that we should receive an invitation to dine there (an invitation, moreover, including the whole party) so immediately after your arrival?”"}
{"doc_id": "gutenberg_1342", "para_id": 751, "text": "“I am the less surprised at what has happened,” replied Sir William, “from that knowledge of what the manners of the great really are, which my situation in life has allowed me to acquire. About the court, such instances of elegant breeding are not uncommon.”"}
{"doc_id": "gutenberg_1342", "para_id": 752, "text": "Scarcely anything was talked of the whole day or next morning but their visit to Rosings. Mr. Collins was carefully instructing them in what they were to expect, that the sight of such rooms, so many servants, and so splendid a dinner, might not wholly overpower them."}
{"doc_id": "gutenberg_1342", "para_id": 753, "text": "“Do not make yourself uneasy, my dear cousin, about your apparel. Lady Catherine is far from requiring that elegance of dress in us which becomes herself and daughter. I would advise you merely to put on whatever of your clothes is superior to the rest--there is no occasion for anything more. Lady Catherine will not think the worse of you for being simply dressed. She likes to have the distinction of rank preserved.”"}
{"doc_id": "gutenberg_1342", "para_id": 754, "text": "While they were dressing, he came two or three times to their different doors, to recommend their being quick, as Lady Catherine very much objected to be kept waiting for her dinner. Such formidable accounts of her Ladyship, and her manner of living, quite frightened Maria Lucas, who had been little used to company; and she looked forward to her introduction at Rosings with as much apprehension as her father had done to his presentation at St. James’s."}
{"doc_id": "gutenberg_1342", "para_id": 755, "text": "As the weather was fine, they had a pleasant walk of about half a mile across the park. Every park has its beauty and its prospects; and Elizabeth saw much to be pleased with, though she could not be in such raptures as Mr. Collins expected the scene to inspire, and was but slightly affected by his enumeration of the windows in front of the house, and his relation of what the glazing altogether had originally cost Sir Lewis de Bourgh."}
{"doc_id": "gutenberg_1342", "para_id": 756, "text": "When they ascended the steps to the hall, Maria’s alarm was every moment increasing, and even Sir William did not look perfectly calm. Elizabeth’s courage did not fail her. She had heard nothing of Lady Catherine that spoke her awful from any extraordinary talents or miraculous virtue, and the mere stateliness of money and rank she thought she could witness without trepidation."}
{"doc_id": "gutenberg_1342", "para_id": 757, "text": "From the entrance hall, of which Mr. Collins pointed out, with a rapturous air, the fine proportion and finished ornaments, they followed the servants through an antechamber to the room where Lady Catherine, her daughter, and Mrs. Jenkinson were sitting. Her Ladyship, with great condescension, arose to receive them; and as Mrs. Collins had settled it with her husband that the office of introduction should be hers, it was performed in a proper manner, without any of those apologies and thanks which he would have thought necessary."}
{"doc_id": "gutenberg_1342", "para_id": 758, "text": "In spite of having been at St. James’s, Sir William was so completely awed by the grandeur surrounding him, that he had but just courage enough to make a very low bow, and take his seat without saying a word; and his daughter, frightened almost out of her senses, sat on the edge of her chair, not knowing which way to look. Elizabeth found herself quite equal to the scene, and could observe the three ladies before her composedly. Lady Catherine was a tall, large woman, with strongly-marked features, which might once have been handsome. Her air was not conciliating, nor was her manner of receiving them such as to make her visitors forget their inferior rank. She was not rendered formidable by silence: but whatever she said was spoken in so authoritative a tone as marked her self-importance, and brought Mr. Wickham immediately to Elizabeth’s mind; and, from the observation of the day altogether, she believed Lady Catherine to be exactly what he had represented."}
{"doc_id": "gutenberg_1342", "para_id": 759, "text": "When, after examining the mother, in whose countenance and deportment she soon found some resemblance of Mr. Darcy, she turned her eyes on the daughter, she could almost have joined in Maria’s astonishment at her being so thin and so small. There was neither in figure nor face any likeness between the ladies. Miss de Bourgh was pale and sickly: her features, though not plain, were insignificant; and she spoke very little, except in a low voice, to Mrs. Jenkinson, in whose appearance there was nothing remarkable, and who was entirely engaged in listening to what she said, and placing a screen in the proper direction before her eyes."}
{"doc_id": "gutenberg_1342", "para_id": 760, "text": "After sitting a few minutes, they were all sent to one of the windows to admire the view, Mr. Collins attending them to point out its beauties, and Lady Catherine kindly informing them that it was much better worth looking at in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 761, "text": "The dinner was exceedingly handsome, and there were all the servants, and all the articles of plate which Mr. Collins had promised; and, as he had likewise foretold, he took his seat at the bottom of the table, by her Ladyship’s desire, and looked as if he felt that life could furnish nothing greater. He carved and ate and praised with delighted alacrity; and every dish was commended first by him, and then by Sir William, who was now enough recovered to echo whatever his son-in-law said, in a manner which Elizabeth wondered Lady Catherine could bear. But Lady Catherine seemed gratified by their excessive admiration, and gave most gracious smiles, especially when any dish on the table proved a novelty to them. The party did not supply much conversation. Elizabeth was ready to speak whenever there was an opening, but she was seated between Charlotte and Miss de Bourgh--the former of whom was engaged in listening to Lady Catherine, and the latter said not a word to her all the dinnertime. Mrs. Jenkinson was chiefly employed in watching how little Miss de Bourgh ate, pressing her to try some other dish and fearing she was indisposed. Maria thought speaking out of the question, and the gentlemen did nothing but eat and admire."}
{"doc_id": "gutenberg_1342", "para_id": 762, "text": "When the ladies returned to the drawing-room, there was little to be done but to hear Lady Catherine talk, which she did without any intermission till coffee came in, delivering her opinion on every subject in so decisive a manner as proved that she was not used to have her judgment controverted. She inquired into Charlotte’s domestic concerns familiarly and minutely, and gave her a great deal of advice as to the management of them all; told her how everything ought to be regulated in so small a family as hers, and instructed her as to the care of her cows and her poultry. Elizabeth found that nothing was beneath this great lady’s attention which could furnish her with an occasion for dictating to others. In the intervals of her discourse with Mrs. Collins, she addressed a variety of questions to Maria and Elizabeth, but especially to the latter, of whose connections she knew the least, and who, she observed to Mrs. Collins, was a very genteel, pretty kind of girl. She asked her at different times how many sisters she had, whether they were older or younger than herself, whether any of them were likely to be married, whether they were handsome, where they had been educated, what carriage her father kept, and what had been her mother’s maiden name? Elizabeth felt all the impertinence of her questions, but answered them very composedly. Lady Catherine then observed,--"}
{"doc_id": "gutenberg_1342", "para_id": 763, "text": "“Your father’s estate is entailed on Mr. Collins, I think? For your sake,” turning to Charlotte, “I am glad of it; but otherwise I see no occasion for entailing estates from the female line. It was not thought necessary in Sir Lewis de Bourgh’s family. Do you play and sing, Miss Bennet?”"}
{"doc_id": "gutenberg_1342", "para_id": 764, "text": "“Oh then--some time or other we shall be happy to hear you. Our instrument is a capital one, probably superior to ---- you shall try it some day. Do your sisters play and sing?”"}
{"doc_id": "gutenberg_1342", "para_id": 765, "text": "“Why did not you all learn? You ought all to have learned. The Miss Webbs all play, and their father has not so good an income as yours. Do you draw?”"}
{"doc_id": "gutenberg_1342", "para_id": 766, "text": "“That is very strange. But I suppose you had no opportunity. Your mother should have taken you to town every spring for the benefit of masters.”"}
{"doc_id": "gutenberg_1342", "para_id": 767, "text": "“No governess! How was that possible? Five daughters brought up at home without a governess! I never heard of such a thing. Your mother must have been quite a slave to your education.”"}
{"doc_id": "gutenberg_1342", "para_id": 768, "text": "Elizabeth could hardly help smiling, as she assured her that had not been the case."}
{"doc_id": "gutenberg_1342", "para_id": 769, "text": "“Then who taught you? who attended to you? Without a governess, you must have been neglected.”"}
{"doc_id": "gutenberg_1342", "para_id": 770, "text": "“Compared with some families, I believe we were; but such of us as wished to learn never wanted the means. We were always encouraged to read, and had all the masters that were necessary. Those who chose to be idle certainly might.”"}
{"doc_id": "gutenberg_1342", "para_id": 771, "text": "“Ay, no doubt: but that is what a governess will prevent; and if I had known your mother, I should have advised her most strenuously to engage one. I always say that nothing is to be done in education without steady and regular instruction, and nobody but a governess can give it. It is wonderful how many families I have been the means of supplying in that way. I am always glad to get a young person well placed out. Four nieces of Mrs. Jenkinson are most delightfully situated through my means; and it was but the other day that I recommended another young person, who was merely accidentally mentioned to me, and the family are quite delighted with her. Mrs. Collins, did I tell you of Lady Metcalfe’s calling yesterday to thank me? She finds Miss Pope a treasure. ‘Lady Catherine,’ said she, ‘you have given me a treasure.’ Are any of your younger sisters out, Miss Bennet?”"}
{"doc_id": "gutenberg_1342", "para_id": 772, "text": "“All! What, all five out at once? Very odd! And you only the second. The younger ones out before the elder are married! Your younger sisters must be very young?”"}
{"doc_id": "gutenberg_1342", "para_id": 773, "text": "“Yes, my youngest is not sixteen. Perhaps _she_ is full young to be much in company. But really, ma’am, I think it would be very hard upon younger sisters that they should not have their share of society and amusement, because the elder may not have the means or inclination to marry early. The last born has as good a right to the pleasures of youth as the first. And to be kept back on _such_ a motive! I think it would not be very likely to promote sisterly affection or delicacy of mind.”"}
{"doc_id": "gutenberg_1342", "para_id": 774, "text": "“Upon my word,” said her Ladyship, “you give your opinion very decidedly for so young a person. Pray, what is your age?”"}
{"doc_id": "gutenberg_1342", "para_id": 775, "text": "“With three younger sisters grown up,” replied Elizabeth, smiling, “your Ladyship can hardly expect me to own it.”"}
{"doc_id": "gutenberg_1342", "para_id": 776, "text": "Lady Catherine seemed quite astonished at not receiving a direct answer; and Elizabeth suspected herself to be the first creature who had ever dared to trifle with so much dignified impertinence."}
{"doc_id": "gutenberg_1342", "para_id": 777, "text": "“You cannot be more than twenty, I am sure,--therefore you need not conceal your age.”"}
{"doc_id": "gutenberg_1342", "para_id": 778, "text": "When the gentlemen had joined them, and tea was over, the card tables were placed. Lady Catherine, Sir William, and Mr. and Mrs. Collins sat down to quadrille; and as Miss De Bourgh chose to play at cassino, the two girls had the honour of assisting Mrs. Jenkinson to make up her party. Their table was superlatively stupid. Scarcely a syllable was uttered that did not relate to the game, except when Mrs. Jenkinson expressed her fears of Miss De Bourgh’s being too hot or too cold, or having too much or too little light. A great deal more passed at the other table. Lady Catherine was generally speaking--stating the mistakes of the three others, or relating some anecdote of herself. Mr. Collins was employed in agreeing to everything her Ladyship said, thanking her for every fish he won, and apologizing if he thought he won too many. Sir William did not say much. He was storing his memory with anecdotes and noble names."}
{"doc_id": "gutenberg_1342", "para_id": 779, "text": "When Lady Catherine and her daughter had played as long as they chose, the tables were broken up, the carriage was offered to Mrs. Collins, gratefully accepted, and immediately ordered. The party then gathered round the fire to hear Lady Catherine determine what weather they were to have on the morrow. From these instructions they were summoned by the arrival of the coach; and with many speeches of thankfulness on Mr. Collins’s side, and as many bows on Sir William’s, they departed. As soon as they had driven from the door, Elizabeth was called on by her cousin to give her opinion of all that she had seen at Rosings, which, for Charlotte’s sake, she made more favourable than it really was. But her commendation, though costing her some trouble, could by no means satisfy Mr. Collins, and he was very soon obliged to take her Ladyship’s praise into his own hands."}
{"doc_id": "gutenberg_1342", "para_id": 780, "text": "Sir William stayed only a week at Hunsford; but his visit was long enough to convince him of his daughter’s being most comfortably settled, and of her possessing such a husband and such a neighbour as were not often met with. While Sir William was with them, Mr. Collins devoted his mornings to driving him out in his gig, and showing him the country: but when he went away, the whole family returned to their usual employments, and Elizabeth was thankful to find that they did not see more of her cousin by the alteration; for the chief of the time between breakfast and dinner was now passed by him either at work in the garden, or in reading and writing, and looking out of window in his own book room, which fronted the road. The room in which the ladies sat was backwards. Elizabeth at first had rather wondered that Charlotte should not prefer the dining parlour for common use; it was a better sized room, and had a pleasanter aspect: but she soon saw that her friend had an excellent reason for what she did, for Mr. Collins would undoubtedly have been much less in his own apartment had they sat in one equally lively; and she gave Charlotte credit for the arrangement."}
{"doc_id": "gutenberg_1342", "para_id": 781, "text": "From the drawing-room they could distinguish nothing in the lane, and were indebted to Mr. Collins for the knowledge of what carriages went along, and how often especially Miss De Bourgh drove by in her phaeton, which he never failed coming to inform them of, though it happened almost every day. She not unfrequently stopped at the Parsonage, and had a few minutes’ conversation with Charlotte, but was scarcely ever prevailed on to get out."}
{"doc_id": "gutenberg_1342", "para_id": 782, "text": "Very few days passed in which Mr. Collins did not walk to Rosings, and not many in which his wife did not think it necessary to go likewise; and till Elizabeth recollected that there might be other family livings to be disposed of, she could not understand the sacrifice of so many hours. Now and then they were honoured with a call from her Ladyship, and nothing escaped her observation that was passing in the room during these visits. She examined into their employments, looked at their work, and advised them to do it differently; found fault with the arrangement of the furniture, or detected the housemaid in negligence; and if she accepted any refreshment, seemed to do it only for the sake of finding out that Mrs. Collins’s joints of meat were too large for her family."}
{"doc_id": "gutenberg_1342", "para_id": 783, "text": "Elizabeth soon perceived, that though this great lady was not in the commission of the peace for the county, she was a most active magistrate in her own parish, the minutest concerns of which were carried to her by Mr. Collins; and whenever any of the cottagers were disposed to be quarrelsome, discontented, or too poor, she sallied forth into the village to settle their differences, silence their complaints, and scold them into harmony and plenty."}
{"doc_id": "gutenberg_1342", "para_id": 784, "text": "The entertainment of dining at Rosings was repeated about twice a week; and, allowing for the loss of Sir William, and there being only one card-table in the evening, every such entertainment was the counterpart of the first. Their other engagements were few, as the style of living of the neighbourhood in general was beyond the Collinses’ reach. This, however, was no evil to Elizabeth, and upon the whole she spent her time comfortably enough: there were half hours of pleasant conversation with Charlotte, and the weather was so fine for the time of year, that she had often great enjoyment out of doors. Her favourite walk, and where she frequently went while the others were calling on Lady Catherine, was along the open grove which edged that side of the park, where there was a nice sheltered path, which no one seemed to value but herself, and where she felt beyond the reach of Lady Catherine’s curiosity."}
{"doc_id": "gutenberg_1342", "para_id": 785, "text": "In this quiet way the first fortnight of her visit soon passed away. Easter was approaching, and the week preceding it was to bring an addition to the family at Rosings, which in so small a circle must be important. Elizabeth had heard, soon after her arrival, that Mr. Darcy was expected there in the course of a few weeks; and though there were not many of her acquaintance whom she did not prefer, his coming would furnish one comparatively new to look at in their Rosings parties, and she might be amused in seeing how hopeless Miss Bingley’s designs on him were, by his behaviour to his cousin, for whom he was evidently destined by Lady Catherine, who talked of his coming with the greatest satisfaction, spoke of him in terms of the highest admiration, and seemed almost angry to find that he had already been frequently seen by Miss Lucas and herself."}
{"doc_id": "gutenberg_1342", "para_id": 786, "text": "His arrival was soon known at the Parsonage; for Mr. Collins was walking the whole morning within view of the lodges opening into Hunsford Lane, in order to have"}
{"doc_id": "gutenberg_1342", "para_id": 787, "text": "the earliest assurance of it; and, after making his bow as the carriage turned into the park, hurried home with the great intelligence. On the following morning he hastened to Rosings to pay his respects. There were two nephews of Lady Catherine to require them, for Mr. Darcy had brought with him a Colonel Fitzwilliam, the younger son of his uncle, Lord ----; and, to the great surprise of all the party, when Mr. Collins returned, the gentlemen accompanied him. Charlotte had seen them from her husband’s room, crossing the road, and immediately running into the other, told the girls what an honour they might expect, adding,--"}
{"doc_id": "gutenberg_1342", "para_id": 788, "text": "“I may thank you, Eliza, for this piece of civility. Mr. Darcy would never have come so soon to wait upon me.”"}
{"doc_id": "gutenberg_1342", "para_id": 789, "text": "Elizabeth had scarcely time to disclaim all right to the compliment before their approach was announced by the door-bell, and shortly afterwards the three gentlemen entered the room. Colonel Fitzwilliam, who led the way, was about thirty, not handsome, but in person and address most truly the gentleman. Mr. Darcy looked just as he had been used to look in Hertfordshire, paid his compliments, with his usual reserve, to Mrs. Collins; and whatever might be his feelings towards her friend, met her with every appearance of composure. Elizabeth merely courtesied to him, without saying a word."}
{"doc_id": "gutenberg_1342", "para_id": 790, "text": "Colonel Fitzwilliam entered into conversation directly, with the readiness and ease of a well-bred man, and talked very pleasantly; but his cousin, after having addressed a slight observation on the house and garden to Mrs. Collins, sat for some time without speaking to anybody. At length, however, his civility was so far awakened as to inquire of Elizabeth after the health of her family. She answered him in the usual way; and, after a moment’s pause, added,--"}
{"doc_id": "gutenberg_1342", "para_id": 791, "text": "“My eldest sister has been in town these three months. Have you never happened to see her there?”"}
{"doc_id": "gutenberg_1342", "para_id": 792, "text": "She was perfectly sensible that he never had: but she wished to see whether he would betray any consciousness of what had passed between the Bingleys and Jane; and she thought he looked a little confused as he answered that he had never been so fortunate as to meet Miss Bennet. The subject was pursued no further, and the gentlemen soon afterwards went away."}
{"doc_id": "gutenberg_1342", "para_id": 793, "text": "Colonel Fitzwilliam’s manners were very much admired at the Parsonage, and the ladies all felt that he must add considerably to the pleasure of their engagements at Rosings. It was some days, however, before they received any invitation thither, for while there were visitors in the house they could not be necessary; and it was not till Easter-day, almost a week after the gentlemen’s arrival, that they were honoured by such an attention, and then they were merely asked on leaving church to come there in the evening. For the last week they had seen very little of either Lady Catherine or her daughter. Colonel Fitzwilliam had called at the Parsonage more than once during the time, but Mr. Darcy they had only seen at church."}
{"doc_id": "gutenberg_1342", "para_id": 794, "text": "The invitation was accepted, of course, and at a proper hour they joined the party in Lady Catherine’s drawing-room. Her Ladyship received them civilly, but it was plain that their company was by no means so acceptable as when she could get nobody else; and she was, in fact, almost engrossed by her nephews, speaking to them, especially to Darcy, much more than to any other person in the room."}
{"doc_id": "gutenberg_1342", "para_id": 795, "text": "Colonel Fitzwilliam seemed really glad to see them: anything was a welcome relief to him at Rosings; and Mrs. Collins’s pretty friend had, moreover, caught his fancy very much. He now seated himself by her, and talked so agreeably of Kent and Hertfordshire, of travelling and staying at home, of new books and music, that Elizabeth had never been half so well entertained in that room before; and they conversed with so much spirit and flow as to draw the attention of Lady Catherine herself, as well as of Mr. Darcy. _His_ eyes had been soon and repeatedly turned towards them with a look of curiosity; and that her Ladyship, after a while, shared the feeling, was more openly acknowledged, for she did not scruple to call out,--"}
{"doc_id": "gutenberg_1342", "para_id": 796, "text": "“What is that you are saying, Fitzwilliam? What is it you are talking of? What are you telling Miss Bennet? Let me hear what it is.”"}
{"doc_id": "gutenberg_1342", "para_id": 797, "text": "“We were talking of music, madam,” said he, when no longer able to avoid a reply."}
{"doc_id": "gutenberg_1342", "para_id": 798, "text": "“Of music! Then pray speak aloud. It is of all subjects my delight. I must have my share in the conversation, if you are speaking of music. There are few people in England, I suppose, who have more true enjoyment of music than myself, or a better natural taste. If I had ever learnt, I should have been a great proficient. And so would Anne, if her health had allowed her to apply. I am confident that she would have performed delightfully. How does Georgiana get on, Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 799, "text": "“I am very glad to hear such a good account of her,” said Lady Catherine; “and pray tell her from me, that she cannot expect to excel, if she does not practise a great deal.”"}
{"doc_id": "gutenberg_1342", "para_id": 800, "text": "“I assure you, madam,” he replied, “that she does not need such advice. She practises very constantly.”"}
{"doc_id": "gutenberg_1342", "para_id": 801, "text": "“So much the better. It cannot be done too much; and when I next write to her, I shall charge her not to neglect it on any account. I often tell young ladies, that no excellence in music is to be acquired without constant practice. I have told Miss Bennet several times, that she will never play really well, unless she practises more; and though Mrs. Collins has no instrument, she is very welcome, as I have often told her, to come to Rosings every day, and play on the pianoforte in Mrs. Jenkinson’s room. She would be in nobody’s way, you know, in that part of the house.”"}
{"doc_id": "gutenberg_1342", "para_id": 802, "text": "Mr. Darcy looked a little ashamed of his aunt’s ill-breeding, and made no answer."}
{"doc_id": "gutenberg_1342", "para_id": 803, "text": "When coffee was over, Colonel Fitzwilliam reminded Elizabeth of having promised to play to him; and she sat down directly to the instrument. He drew a chair near her. Lady Catherine listened to half a song, and then talked, as before, to her other nephew; till the latter walked away from her, and moving with his usual deliberation towards the pianoforte, stationed himself so as to command a full view of the fair performer’s countenance. Elizabeth saw what he was doing, and at the first convenient pause turned to him with an arch smile, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 804, "text": "“You mean to frighten me, Mr. Darcy, by coming in all this state to hear me. But I will not be alarmed, though your sister _does_ play so well. There is a stubbornness about me that never can bear to be frightened at the will of others. My courage always rises with every attempt to intimidate me.”"}
{"doc_id": "gutenberg_1342", "para_id": 805, "text": "“I shall not say that you are mistaken,” he replied, “because you could not really believe me to entertain any design of alarming you; and I have had the pleasure of your acquaintance long enough to know, that you find great enjoyment in occasionally professing opinions which, in fact, are not your own.”"}
{"doc_id": "gutenberg_1342", "para_id": 806, "text": "Elizabeth laughed heartily at this picture of herself, and said to Colonel Fitzwilliam, “Your cousin will give you a very pretty notion of me, and teach you not to believe a word I say. I am particularly unlucky in meeting with a person so well able to expose my real character, in a part of the world where I had hoped to pass myself off with some degree of credit. Indeed, Mr. Darcy, it is very ungenerous in you to mention all that you knew to my disadvantage in Hertfordshire--and, give me leave to say, very impolitic too--for it is provoking me to retaliate, and such things may come out as will shock your relations to hear.”"}
{"doc_id": "gutenberg_1342", "para_id": 807, "text": "“Pray let me hear what you have to accuse him of,” cried Colonel Fitzwilliam. “I should like to know how he behaves among strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 808, "text": "“You shall hear, then--but prepare for something very dreadful. The first time of my ever seeing him in Hertfordshire, you must know, was at a ball--and at this ball, what do you think he did? He danced only four dances! I am sorry to pain you, but so it was. He danced only four dances, though gentlemen were scarce; and, to my certain knowledge, more than one young lady was sitting down in want of a partner. Mr. Darcy, you cannot deny the fact.”"}
{"doc_id": "gutenberg_1342", "para_id": 809, "text": "“I had not at that time the honour of knowing any lady in the assembly beyond my own party.”"}
{"doc_id": "gutenberg_1342", "para_id": 810, "text": "“True; and nobody can ever be introduced in a ball-room. Well, Colonel Fitzwilliam, what do I play next? My fingers wait your orders.”"}
{"doc_id": "gutenberg_1342", "para_id": 811, "text": "“Perhaps,” said Darcy, “I should have judged better had I sought an introduction, but I am ill-qualified to recommend myself to strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 812, "text": "“Shall we ask your cousin the reason of this?” said Elizabeth, still addressing Colonel Fitzwilliam. “Shall we ask him why a man of sense and education, and who has lived in the world, is ill-qualified to recommend himself to strangers?”"}
{"doc_id": "gutenberg_1342", "para_id": 813, "text": "“I can answer your question,” said Fitzwilliam, “without applying to him. It is because he will not give himself the trouble.”"}
{"doc_id": "gutenberg_1342", "para_id": 814, "text": "“I certainly have not the talent which some people possess,” said Darcy, “of conversing easily with those I have never seen before. I cannot catch their tone of conversation, or appear interested in their concerns, as I often see done.”"}
{"doc_id": "gutenberg_1342", "para_id": 815, "text": "“My fingers,” said Elizabeth, “do not move over this instrument in the masterly manner which I see so many women’s do. They have not the same force or rapidity, and do not produce the same expression. But then I have always supposed it to be my own fault--because I would not take the trouble of practising. It is not that I do not believe _my_ fingers as capable as any other woman’s of superior execution.”"}
{"doc_id": "gutenberg_1342", "para_id": 816, "text": "Darcy smiled and said, “You are perfectly right. You have employed your time much better. No one admitted to the privilege of hearing you can think anything wanting. We neither of us perform to strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 817, "text": "Here they were interrupted by Lady Catherine, who called out to know what they were talking of. Elizabeth immediately began playing again. Lady Catherine approached, and, after listening for a few minutes, said to Darcy,--"}
{"doc_id": "gutenberg_1342", "para_id": 818, "text": "“Miss Bennet would not play at all amiss if she practised more, and could have the advantage of a London master. She has a very good notion of fingering, though her taste is not equal to Anne’s. Anne would have been a delightful performer, had her health allowed her to learn.”"}
{"doc_id": "gutenberg_1342", "para_id": 819, "text": "Elizabeth looked at Darcy, to see how cordially he assented to his cousin’s praise: but neither at that moment nor at any other could she discern any symptom of love; and from the whole of his behaviour to Miss De Bourgh she derived this comfort for Miss Bingley, that he might have been just as likely to marry _her_, had she been his relation."}
{"doc_id": "gutenberg_1342", "para_id": 820, "text": "Lady Catherine continued her remarks on Elizabeth’s performance, mixing with them many instructions on execution and taste. Elizabeth received them with all the forbearance of civility; and at the request of the gentlemen remained at the instrument till her Ladyship’s carriage was ready to take them all home."}
{"doc_id": "gutenberg_1342", "para_id": 821, "text": "Elizabeth was sitting by herself the next morning, and writing to Jane, while Mrs. Collins and Maria were gone on business into the village, when she was startled by a ring at the door, the certain signal of a visitor. As she had heard no carriage, she thought it not unlikely to be Lady Catherine; and under that apprehension was putting away her half-finished letter, that she might escape all impertinent questions, when the door opened, and to her very great surprise Mr. Darcy, and Mr. Darcy only, entered the room."}
{"doc_id": "gutenberg_1342", "para_id": 822, "text": "He seemed astonished too on finding her alone, and apologized for his intrusion, by letting her know that he had understood all the ladies to be within."}
{"doc_id": "gutenberg_1342", "para_id": 823, "text": "They then sat down, and when her inquiries after Rosings were made, seemed in danger of sinking into total silence. It was absolutely necessary, therefore, to think of something; and in this emergency recollecting _when_ she had seen him last in Hertfordshire, and feeling curious to know what he would say on the subject of their hasty departure, she observed,--"}
{"doc_id": "gutenberg_1342", "para_id": 824, "text": "“How very suddenly you all quitted Netherfield last November, Mr. Darcy! It must have been a most agreeable surprise to Mr. Bingley to see you all after him so soon; for, if I recollect right, he went but the day before. He and his sisters were well, I hope, when you left London?”"}
{"doc_id": "gutenberg_1342", "para_id": 825, "text": "She found that she was to receive no other answer; and, after a short pause, added,--"}
{"doc_id": "gutenberg_1342", "para_id": 826, "text": "“I think I have understood that Mr. Bingley has not much idea of ever returning to Netherfield again?”"}
{"doc_id": "gutenberg_1342", "para_id": 827, "text": "“I have never heard him say so; but it is probable that he may spend very little of his time there in future. He has many friends, and he is at a time of life when friends and engagements are continually increasing.”"}
{"doc_id": "gutenberg_1342", "para_id": 828, "text": "“If he means to be but little at Netherfield, it would be better for the neighbourhood that he should give up the place entirely, for then we might possibly get a settled family there. But, perhaps, Mr. Bingley did not take the house so much for the convenience of the neighbourhood as for his own, and we must expect him to keep or quit it on the same principle.”"}
{"doc_id": "gutenberg_1342", "para_id": 829, "text": "“I should not be surprised,” said Darcy, “if he were to give it up as soon as any eligible purchase offers.”"}
{"doc_id": "gutenberg_1342", "para_id": 830, "text": "Elizabeth made no answer. She was afraid of talking longer of his friend; and, having nothing else to say, was now determined to leave the trouble of finding a subject to him."}
{"doc_id": "gutenberg_1342", "para_id": 831, "text": "He took the hint and soon began with, “This seems a very comfortable house. Lady Catherine, I believe, did a great deal to it when Mr. Collins first came to Hunsford.”"}
{"doc_id": "gutenberg_1342", "para_id": 832, "text": "“I believe she did--and I am sure she could not have bestowed her kindness on a more grateful object.”"}
{"doc_id": "gutenberg_1342", "para_id": 833, "text": "“Yes, indeed; his friends may well rejoice in his having met with one of the very few sensible women who would have accepted him, or have made him happy if they had. My friend has an excellent understanding--though I am not certain that I consider her marrying Mr. Collins as the wisest thing she ever did. She seems perfectly happy, however; and, in a prudential light, it is certainly a very good match for her.”"}
{"doc_id": "gutenberg_1342", "para_id": 834, "text": "“It must be very agreeable to her to be settled within so easy a distance of her own family and friends.”"}
{"doc_id": "gutenberg_1342", "para_id": 835, "text": "“And what is fifty miles of good road? Little more than half a day’s journey. Yes, I call it a very easy distance.”"}
{"doc_id": "gutenberg_1342", "para_id": 836, "text": "“I should never have considered the distance as one of the _advantages_ of the match,” cried Elizabeth. “I should never have said Mrs. Collins was settled _near_ her family.”"}
{"doc_id": "gutenberg_1342", "para_id": 837, "text": "“It is a proof of your own attachment to Hertfordshire. Anything beyond the very neighbourhood of Longbourn, I suppose, would appear far.”"}
{"doc_id": "gutenberg_1342", "para_id": 838, "text": "As he spoke there was a sort of smile, which Elizabeth fancied she understood; he must be supposing her to be thinking of Jane and Netherfield, and she blushed as she answered,--"}
{"doc_id": "gutenberg_1342", "para_id": 839, "text": "“I do not mean to say that a woman may not be settled too near her family. The far and the near must be relative, and depend on many varying circumstances. Where there is fortune to make the expense of travelling unimportant, distance becomes no evil. But that is not the case _here_. Mr. and Mrs. Collins have a comfortable income, but not such a one as will allow of frequent journeys--and I am persuaded my friend would not call herself _near_ her family under less than _half_ the present distance.”"}
{"doc_id": "gutenberg_1342", "para_id": 840, "text": "Mr. Darcy drew his chair a little towards her, and said, “_You_ cannot have a right to such very strong local attachment. _You_ cannot have been always at Longbourn.”"}
{"doc_id": "gutenberg_1342", "para_id": 841, "text": "Elizabeth looked surprised. The gentleman experienced some change of feeling; he drew back his chair, took a newspaper from the table, and, glancing over it, said, in a colder voice,--"}
{"doc_id": "gutenberg_1342", "para_id": 842, "text": "A short dialogue on the subject of the country ensued, on either side calm and concise--and soon put an end to by the entrance of Charlotte and her sister, just returned from their walk. The _tête-à-tête_ surprised them. Mr. Darcy related the mistake which had occasioned his intruding on Miss Bennet, and, after sitting a few minutes longer, without saying much to anybody, went away."}
{"doc_id": "gutenberg_1342", "para_id": 843, "text": "“What can be the meaning of this?” said Charlotte, as soon as he was gone. “My dear Eliza, he must be in love with you, or he would never have called on us in this familiar way.”"}
{"doc_id": "gutenberg_1342", "para_id": 844, "text": "But when Elizabeth told of his silence, it did not seem very likely, even to Charlotte’s wishes, to be the case; and, after various conjectures, they could at last only suppose his visit to proceed from the difficulty of finding anything to do, which was the more probable from the time of year. All field sports were over. Within doors there was Lady Catherine, books, and a billiard table, but gentlemen cannot be always within doors; and in the nearness of the Parsonage, or the pleasantness of the walk to it, or of the people who lived in it, the two cousins found a temptation from this period of walking thither almost every day. They called at various times of the morning, sometimes separately, sometimes together, and now and then accompanied by their aunt. It was plain to them all that Colonel Fitzwilliam came because he had pleasure in their society, a persuasion which of course recommended him still more; and Elizabeth was reminded by her own satisfaction in being with him, as well as by his evident admiration, of her former favourite, George Wickham; and though, in comparing them, she saw there was less captivating softness in Colonel Fitzwilliam’s manners, she believed he might have the best informed mind."}
{"doc_id": "gutenberg_1342", "para_id": 845, "text": "But why Mr. Darcy came so often to the Parsonage it was more difficult to understand. It could not be for society, as he frequently sat there ten minutes together without opening his lips; and when he did speak, it seemed the effect of necessity rather than of choice--a sacrifice to propriety, not a pleasure to himself. He seldom appeared really animated. Mrs. Collins knew not what to make of him. Colonel Fitzwilliam’s occasionally laughing at his stupidity proved that he was generally different, which her own knowledge of him could not have told her; and as she would have liked to believe this change the effect of love, and the object of that love her friend Eliza, she set herself seriously to work to find it out: she watched him whenever they were at Rosings, and whenever he came to Hunsford; but without much success. He certainly looked at her friend a great deal, but the expression of that look was disputable. It was an earnest, steadfast gaze, but she often doubted whether there were much admiration in it, and sometimes it seemed nothing but absence of mind."}
{"doc_id": "gutenberg_1342", "para_id": 846, "text": "She had once or twice suggested to Elizabeth the possibility of his being partial to her, but Elizabeth always laughed at the idea; and Mrs. Collins did not think it right to press the subject, from the danger of raising expectations which might only end in disappointment; for in her opinion it admitted not of a doubt, that all her friend’s dislike would vanish, if she could suppose him to be in her power."}
{"doc_id": "gutenberg_1342", "para_id": 847, "text": "In her kind schemes for Elizabeth, she sometimes planned her marrying Colonel Fitzwilliam. He was, beyond comparison, the pleasantest man: he certainly admired her, and his situation in life was most eligible; but, to counterbalance these advantages, Mr. Darcy had considerable patronage in the church, and his cousin could have none at all."}
{"doc_id": "gutenberg_1342", "para_id": 848, "text": "More than once did Elizabeth, in her ramble within the park, unexpectedly meet Mr. Darcy. She felt all the perverseness of the mischance that should bring him where no one else was brought; and, to prevent its ever happening again, took care to inform him, at first, that it was a favourite haunt of hers. How it could occur a second time, therefore, was very odd! Yet it did, and even the third. It seemed like wilful ill-nature, or a voluntary penance; for on these occasions it was not merely a few formal inquiries and an awkward pause and then away, but he actually thought it necessary to turn back and walk with her. He never said a great deal, nor did she give herself the trouble of talking or of listening much; but it struck her in the course of their third encounter that he was asking some odd unconnected questions--about her pleasure in being at Hunsford, her love of solitary walks, and her opinion of Mr. and Mrs. Collins’s happiness; and that in speaking of Rosings, and her not perfectly understanding the house, he seemed to expect that whenever she came into Kent again she would be staying _there_ too. His words seemed to imply it. Could he have Colonel Fitzwilliam in his thoughts? She supposed, if he meant anything, he must mean an allusion to what might arise in that quarter. It distressed her a little, and she was quite glad to find herself at the gate in the pales opposite the Parsonage."}
{"doc_id": "gutenberg_1342", "para_id": 849, "text": "She was engaged one day, as she walked, in re-perusing Jane’s last letter, and dwelling on some passages which proved that Jane had not written in spirits, when, instead of being again surprised by Mr. Darcy, she saw, on looking up, that Colonel Fitzwilliam was meeting her. Putting away the letter immediately, and forcing a smile, she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 850, "text": "“I have been making the tour of the park,” he replied, “as I generally do every year, and intended to close it with a call at the Parsonage. Are you going much farther?”"}
{"doc_id": "gutenberg_1342", "para_id": 851, "text": "“Yes--if Darcy does not put it off again. But I am at his disposal. He arranges the business just as he pleases.”"}
{"doc_id": "gutenberg_1342", "para_id": 852, "text": "“And if not able to please himself in the arrangement, he has at least great pleasure in the power of choice. I do not know anybody who seems more to enjoy the power of doing what he likes than Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 853, "text": "“He likes to have his own way very well,” replied Colonel Fitzwilliam. “But so we all do. It is only that he has better means of having it than many others, because he is rich, and many others are poor. I speak feelingly. A younger son, you know, must be inured to self-denial and dependence.”"}
{"doc_id": "gutenberg_1342", "para_id": 854, "text": "“In my opinion, the younger son of an earl can know very little of either. Now, seriously, what have you ever known of self-denial and dependence? When have you been prevented by want of money from going wherever you chose or procuring anything you had a fancy for?”"}
{"doc_id": "gutenberg_1342", "para_id": 855, "text": "“These are home questions--and perhaps I cannot say that I have experienced many hardships of that nature. But in matters of greater weight, I may suffer from the want of money. Younger sons cannot marry where they like.”"}
{"doc_id": "gutenberg_1342", "para_id": 856, "text": "“Our habits of expense make us too dependent, and there are not many in my rank of life who can afford to marry without some attention to money.”"}
{"doc_id": "gutenberg_1342", "para_id": 857, "text": "“Is this,” thought Elizabeth, “meant for me?” and she coloured at the idea; but, recovering herself, said in a lively tone, “And pray, what is the usual price of an earl’s younger son? Unless the elder brother is very sickly, I suppose you would not ask above fifty thousand pounds.”"}
{"doc_id": "gutenberg_1342", "para_id": 858, "text": "He answered her in the same style, and the subject dropped. To interrupt a silence which might make him fancy her affected with what had passed, she soon afterwards said,--"}
{"doc_id": "gutenberg_1342", "para_id": 859, "text": "“I imagine your cousin brought you down with him chiefly for the sake of having somebody at his disposal. I wonder he does not marry, to secure a lasting convenience of that kind. But, perhaps, his sister does as well for the present; and, as she is under his sole care, he may do what he likes with her.”"}
{"doc_id": "gutenberg_1342", "para_id": 860, "text": "“No,” said Colonel Fitzwilliam, “that is an advantage which he must divide with me. I am joined with him in the guardianship of Miss Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 861, "text": "“Are you, indeed? And pray what sort of a guardian do you make? Does your charge give you much trouble? Young ladies of her age are sometimes a little difficult to manage; and if she has the true Darcy spirit, she may like to have her own way.”"}
{"doc_id": "gutenberg_1342", "para_id": 862, "text": "As she spoke, she observed him looking at her earnestly; and the manner in which he immediately asked her why she supposed Miss Darcy likely to give them any uneasiness, convinced her that she had somehow or other got pretty near the truth. She directly replied,--"}
{"doc_id": "gutenberg_1342", "para_id": 863, "text": "“You need not be frightened. I never heard any harm of her; and I dare say she is one of the most tractable creatures in the world. She is a very great favourite with some ladies of my acquaintance, Mrs. Hurst and Miss Bingley. I think I have heard you say that you know them.”"}
{"doc_id": "gutenberg_1342", "para_id": 864, "text": "“I know them a little. Their brother is a pleasant, gentlemanlike man--he is a great friend of Darcy’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 865, "text": "“Oh yes,” said Elizabeth drily--“Mr. Darcy is uncommonly kind to Mr. Bingley, and takes a prodigious deal of care of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 866, "text": "“Care of him! Yes, I really believe Darcy _does_ take care of him in those points where he most wants care. From something that he told me in our journey hither, I have reason to think Bingley very much indebted to him. But I ought to beg his pardon, for I have no right to suppose that Bingley was the person meant. It was all conjecture.”"}
{"doc_id": "gutenberg_1342", "para_id": 867, "text": "“It is a circumstance which Darcy of course could not wish to be generally known, because if it were to get round to the lady’s family it would be an unpleasant thing.”"}
{"doc_id": "gutenberg_1342", "para_id": 868, "text": "“And remember that I have not much reason for supposing it to be Bingley. What he told me was merely this: that he congratulated himself on having lately saved a friend from the inconveniences of a most imprudent marriage, but without mentioning names or any other particulars; and I only suspected it to be Bingley from believing him the kind of young man to get into a scrape of that sort, and from knowing them to have been together the whole of last summer.”"}
{"doc_id": "gutenberg_1342", "para_id": 869, "text": "“He did not talk to me of his own arts,” said Fitzwilliam, smiling. “He only told me what I have now told you.”"}
{"doc_id": "gutenberg_1342", "para_id": 870, "text": "Elizabeth made no answer, and walked on, her heart swelling with indignation. After watching her a little, Fitzwilliam asked her why she was so thoughtful."}
{"doc_id": "gutenberg_1342", "para_id": 871, "text": "“I am thinking of what you have been telling me,” said she. “Your cousin’s conduct does not suit my feelings. Why was he to be the judge?”"}
{"doc_id": "gutenberg_1342", "para_id": 872, "text": "“I do not see what right Mr. Darcy had to decide on the propriety of his friend’s inclination; or why, upon his own judgment alone, he was to determine and direct in what manner that friend was to be happy. But,” she continued, recollecting herself, “as we know none of the particulars, it is not fair to condemn him. It is not to be supposed that there was much affection in the case.”"}
{"doc_id": "gutenberg_1342", "para_id": 873, "text": "“That is not an unnatural surmise,” said Fitzwilliam; “but it is lessening the honour of my cousin’s triumph very sadly.”"}
{"doc_id": "gutenberg_1342", "para_id": 874, "text": "This was spoken jestingly, but it appeared to her so just a picture of Mr. Darcy, that she would not trust herself with an answer; and, therefore, abruptly changing the conversation, talked on indifferent matters till they reached the Parsonage. There, shut into her own room, as soon as their visitor left them, she could think without interruption of all that she had heard. It was not to be supposed that any other people could be meant than those with whom she was connected. There could not exist in the world _two_ men over whom Mr. Darcy could have such boundless influence. That he had been concerned in the measures taken to separate Mr. Bingley and Jane, she had never doubted; but she had always attributed to Miss Bingley the principal design and arrangement of them. If his own vanity, however, did not mislead him, _he_ was the cause--his pride and caprice were the cause--of all that Jane had suffered, and still continued to suffer. He had ruined for a while every hope of happiness for the most affectionate, generous heart in the world; and no one could say how lasting an evil he might have inflicted."}
{"doc_id": "gutenberg_1342", "para_id": 875, "text": "“There were some very strong objections against the lady,” were Colonel Fitzwilliam’s words; and these strong objections probably were, her having one uncle who was a country attorney, and another who was in business in London."}
{"doc_id": "gutenberg_1342", "para_id": 876, "text": "“To Jane herself,” she exclaimed, “there could be no possibility of objection,--all loveliness and goodness as she is! Her understanding excellent, her mind improved, and her manners captivating. Neither could anything be urged against my father, who, though with some peculiarities, has abilities which Mr. Darcy himself need not disdain, and respectability which he will probably never reach.” When she thought of her mother, indeed, her confidence gave way a little; but she would not allow that any objections _there_ had material weight with Mr. Darcy, whose pride, she was convinced, would receive a deeper wound from the want of importance in his friend’s connections than from their want of sense; and she was quite decided, at last, that he had been partly governed by this worst kind of pride, and partly by the wish of retaining Mr. Bingley for his sister."}
{"doc_id": "gutenberg_1342", "para_id": 877, "text": "The agitation and tears which the subject occasioned brought on a headache; and it grew so much worse towards the evening that, added to her unwillingness to see Mr. Darcy, it determined her not to attend her cousins to Rosings, where they were engaged to drink tea. Mrs. Collins, seeing that she was really unwell, did not press her to go, and as much as possible prevented her husband from pressing her; but Mr. Collins could not conceal his apprehension of Lady Catherine’s being rather displeased by her staying at home."}
{"doc_id": "gutenberg_1342", "para_id": 878, "text": "When they were gone, Elizabeth, as if intending to exasperate herself as much as possible against Mr. Darcy, chose for her employment the examination of all the letters which Jane had written to her since her being in Kent. They contained no actual complaint, nor was there any revival of past occurrences, or any communication of present suffering. But in all, and in almost every line of each, there was a want of that cheerfulness which had been used to characterize her style, and which, proceeding from the serenity of a mind at ease with itself, and kindly disposed towards everyone, had been scarcely ever clouded. Elizabeth noticed every sentence conveying the idea of uneasiness, with an attention which it had hardly received on the first perusal. Mr. Darcy’s shameful boast of what misery he had been able to inflict gave her a keener sense of her sister’s sufferings. It was some consolation to think that his visit to Rosings was to end on the day after the next, and a still greater that in less than a fortnight she should herself be with Jane again, and enabled to contribute to the recovery of her spirits, by all that affection could do."}
{"doc_id": "gutenberg_1342", "para_id": 879, "text": "She could not think of Darcy’s leaving Kent without remembering that his cousin was to go with him; but Colonel Fitzwilliam had made it clear that he had no intentions at all, and, agreeable as he was, she did not mean to be unhappy about him."}
{"doc_id": "gutenberg_1342", "para_id": 880, "text": "While settling this point, she was suddenly roused by the sound of the door-bell; and her spirits were a little fluttered by the idea of its being Colonel Fitzwilliam himself, who had once before called late in the evening, and might now come to inquire particularly after her. But this idea was soon banished, and her spirits were very differently affected, when, to her utter amazement, she saw Mr. Darcy walk into the room. In a hurried manner he immediately began an inquiry after her health, imputing his visit to a wish of hearing that she were better. She answered him with cold civility. He sat down for a few moments, and then getting up walked about the room. Elizabeth was surprised, but said not a word. After a silence of several minutes, he came towards her in an agitated manner, and thus began:--"}
{"doc_id": "gutenberg_1342", "para_id": 881, "text": "“In vain have I struggled. It will not do. My feelings will not be repressed. You must allow me to tell you how ardently I admire and love you.”"}
{"doc_id": "gutenberg_1342", "para_id": 882, "text": "Elizabeth’s astonishment was beyond expression. She stared, coloured, doubted, and was silent. This he considered sufficient encouragement, and the avowal of all that he felt and had long felt for her immediately followed. He spoke well; but there were feelings besides those of the heart to be detailed, and he was not more eloquent on the subject of tenderness than of pride. His sense of her inferiority, of its being a degradation, of the family obstacles which judgment had always opposed to inclination, were dwelt on with a warmth which seemed due to the consequence he was wounding, but was very unlikely to recommend his suit."}
{"doc_id": "gutenberg_1342", "para_id": 883, "text": "In spite of her deeply-rooted dislike, she could not be insensible to the compliment of such a man’s affection, and though her intentions did not vary for an instant, she was at first sorry for the pain he was to receive; till roused to resentment by his subsequent language, she lost all compassion in anger. She tried, however, to compose herself to answer him with patience, when he should have done. He concluded with representing to her the strength of that attachment which in spite of all his endeavours he had found impossible to conquer; and with expressing his hope that it would now be rewarded by her acceptance of his hand. As he said this she could easily see that he had no doubt of a favourable answer. He _spoke_ of apprehension and anxiety, but his countenance expressed real security. Such a circumstance could only exasperate farther; and when he ceased the colour rose into her cheeks and she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 884, "text": "“In such cases as this, it is, I believe, the established mode to express a sense of obligation for the sentiments avowed, however unequally they may be returned. It is natural that obligation should be felt, and if I could _feel_ gratitude, I would now thank you. But I cannot--I have never desired your good opinion, and you have certainly bestowed it most unwillingly. I am sorry to have occasioned pain to anyone. It has been most unconsciously done, however, and I hope will be of short duration. The feelings which you tell me have long prevented the acknowledgment of your regard can have little difficulty in overcoming it after this explanation.”"}
{"doc_id": "gutenberg_1342", "para_id": 885, "text": "Mr. Darcy, who was leaning against the mantel-piece with his eyes fixed on her face, seemed to catch her words with no less resentment than surprise. His complexion became pale with anger, and the disturbance of his mind was visible in every feature. He was struggling for the appearance of composure, and would not open his lips till he believed himself to have attained it. The pause was to Elizabeth’s feelings dreadful. At length, in a voice of forced calmness, he said,--"}
{"doc_id": "gutenberg_1342", "para_id": 886, "text": "“And this is all the reply which I am to have the honour of expecting! I might, perhaps, wish to be informed why, with so little _endeavour_ at civility, I am thus rejected. But it is of small importance.”"}
{"doc_id": "gutenberg_1342", "para_id": 887, "text": "“I might as well inquire,” replied she, “why, with so evident a design of offending and insulting me, you chose to tell me that you liked me against your will, against your reason, and even against your character? Was not this some excuse for incivility, if I _was_ uncivil? But I have other provocations. You know I have. Had not my own feelings decided against you, had they been indifferent, or had they even been favourable, do you think that any consideration would tempt me to accept the man who has been the means of ruining, perhaps for ever, the happiness of a most beloved sister?”"}
{"doc_id": "gutenberg_1342", "para_id": 888, "text": "As she pronounced these words, Mr. Darcy changed colour; but the emotion was short, and he listened without attempting to interrupt her while she continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 889, "text": "“I have every reason in the world to think ill of you. No motive can excuse the unjust and ungenerous part you acted _there_. You dare not, you cannot deny that you have been the principal, if not the only means of dividing them from each other, of exposing one to the censure of the world for caprice and instability, the other to its derision for disappointed hopes, and involving them both in misery of the acutest kind.”"}
{"doc_id": "gutenberg_1342", "para_id": 890, "text": "She paused, and saw with no slight indignation that he was listening with an air which proved him wholly unmoved by any feeling of remorse. He even looked at her with a smile of affected incredulity."}
{"doc_id": "gutenberg_1342", "para_id": 891, "text": "With assumed tranquillity he then replied, “I have no wish of denying that I did everything in my power to separate my friend from your sister, or that I rejoice in my success. Towards _him_ I have been kinder than towards myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 892, "text": "Elizabeth disdained the appearance of noticing this civil reflection, but its meaning did not escape, nor was it likely to conciliate her."}
{"doc_id": "gutenberg_1342", "para_id": 893, "text": "“But it is not merely this affair,” she continued, “on which my dislike is founded. Long before it had taken place, my opinion of you was decided. Your character was unfolded in the recital which I received many months ago from Mr. Wickham. On this subject, what can you have to say? In what imaginary act of friendship can you here defend yourself? or under what misrepresentation can you here impose upon others?”"}
{"doc_id": "gutenberg_1342", "para_id": 894, "text": "“You take an eager interest in that gentleman’s concerns,” said Darcy, in a less tranquil tone, and with a heightened colour."}
{"doc_id": "gutenberg_1342", "para_id": 895, "text": "“Who that knows what his misfortunes have been can help feeling an interest in him?”"}
{"doc_id": "gutenberg_1342", "para_id": 896, "text": "“His misfortunes!” repeated Darcy, contemptuously,--“yes, his misfortunes have been great indeed.”"}
{"doc_id": "gutenberg_1342", "para_id": 897, "text": "“And of your infliction,” cried Elizabeth, with energy; “You have reduced him to his present state of poverty--comparative poverty. You have withheld the advantages which you must know to have been designed for him. You have deprived the best years of his life of that independence which was no less his due than his desert. You have done all this! and yet you can treat the mention of his misfortunes with contempt and ridicule.”"}
{"doc_id": "gutenberg_1342", "para_id": 898, "text": "“And this,” cried Darcy, as he walked with quick steps across the room, “is your opinion of me! This is the estimation in which you hold me! I thank you for explaining it so fully. My faults, according to this calculation, are heavy indeed! But, perhaps,” added he, stopping in his walk, and turning towards her, “these offences might have been overlooked, had not your pride been hurt by my honest confession of the scruples that had long prevented my forming any serious design. These bitter accusations might have been suppressed, had I, with greater policy, concealed my struggles, and flattered you into the belief of my being impelled by unqualified, unalloyed inclination; by reason, by reflection, by everything. But disguise of every sort is my abhorrence. Nor am I ashamed of the feelings I related. They were natural and just. Could you expect me to rejoice in the inferiority of your connections?--to congratulate myself on the hope of relations whose condition in life is so decidedly beneath my own?”"}
{"doc_id": "gutenberg_1342", "para_id": 899, "text": "Elizabeth felt herself growing more angry every moment; yet she tried to the utmost to speak with composure when she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 900, "text": "“You are mistaken, Mr. Darcy, if you suppose that the mode of your declaration affected me in any other way than as it spared me the concern which I might have felt in refusing you, had you behaved in a more gentlemanlike manner.”"}
{"doc_id": "gutenberg_1342", "para_id": 901, "text": "“You could not have made me the offer of your hand in any possible way that would have tempted me to accept it.”"}
{"doc_id": "gutenberg_1342", "para_id": 902, "text": "Again his astonishment was obvious; and he looked at her with an expression of mingled incredulity and mortification. She went on,--"}
{"doc_id": "gutenberg_1342", "para_id": 903, "text": "“From the very beginning, from the first moment, I may almost say, of my acquaintance with you, your manners impressing me with the fullest belief of your arrogance, your conceit, and your selfish disdain of the feelings of others, were such as to form that groundwork of disapprobation, on which succeeding events have built so immovable a dislike; and I had not known you a month before I felt that you were the last man in the world whom I could ever be prevailed on to marry.”"}
{"doc_id": "gutenberg_1342", "para_id": 904, "text": "“You have said quite enough, madam. I perfectly comprehend your feelings, and have now only to be ashamed of what my own have been. Forgive me for having taken up so much of your time, and accept my best wishes for your health and happiness.”"}
{"doc_id": "gutenberg_1342", "para_id": 905, "text": "And with these words he hastily left the room, and Elizabeth heard him the next moment open the front door and quit the house. The tumult of her mind was now painfully great. She knew not how to support herself, and, from actual weakness, sat down and cried for half an hour. Her astonishment, as she reflected on what had passed, was increased by every review of it. That she should receive an offer of marriage from Mr. Darcy! that he should have been in love with her for so many months! so much in love as to wish to marry her in spite of all the objections which had made him prevent his friend’s marrying her sister, and which must appear at least with equal force in his own case, was almost incredible! it was gratifying to have inspired unconsciously so strong an affection. But his pride, his abominable pride, his shameless avowal of what he had done with respect to Jane, his unpardonable assurance in acknowledging, though he could not justify it, and the unfeeling manner which he had mentioned Mr. Wickham, his cruelty towards whom he had not attempted to deny, soon overcame the pity which the consideration of his attachment had for a moment excited."}
{"doc_id": "gutenberg_1342", "para_id": 906, "text": "She continued in very agitating reflections till the sound of Lady Catherine’s carriage made her feel how unequal she was to encounter Charlotte’s observation, and hurried her away to her room."}
{"doc_id": "gutenberg_1342", "para_id": 907, "text": "Elizabeth awoke the next morning to the same thoughts and meditations which had at length closed her eyes. She could not yet recover from the surprise of what had happened: it was impossible to think of anything else; and, totally indisposed for employment, she resolved soon after breakfast to indulge herself in air and exercise. She was proceeding directly to her favourite walk, when the recollection of Mr. Darcy’s sometimes coming there stopped her, and instead of entering the park, she turned up the lane which led her farther from the turnpike road. The park paling was still the boundary on one side, and she soon passed one of the gates into the ground."}
{"doc_id": "gutenberg_1342", "para_id": 908, "text": "After walking two or three times along that part of the lane, she was tempted, by the pleasantness of the morning, to stop at the gates and look into the park. The five weeks which she had now passed in Kent had made a great difference in the country, and every day was adding to the verdure of the early trees. She was on the point of continuing her walk, when she caught a glimpse of a gentleman within the sort of grove which edged the park: he was moving that way; and fearful of its being Mr. Darcy, she was directly retreating. But the person who advanced was now near enough to see her, and stepping forward with eagerness, pronounced her name. She had turned away; but on hearing herself called, though in a voice which proved it to be Mr. Darcy, she moved again towards the gate. He had by that time reached it also; and, holding out a letter, which she instinctively took, said, with a look of haughty composure, “I have been walking in the grove some time, in the hope of meeting you. Will you do me the honour of reading that letter?” and then, with a slight bow, turned again into the plantation, and was soon out of sight."}
{"doc_id": "gutenberg_1342", "para_id": 909, "text": "With no expectation of pleasure, but with the strongest curiosity, Elizabeth opened the letter, and to her still increasing wonder, perceived an envelope containing two sheets of letter paper, written quite through, in a very close hand. The envelope itself was likewise full. Pursuing her way along the lane, she then began it. It was dated from Rosings, at eight o’clock in the morning, and was as follows:--"}
{"doc_id": "gutenberg_1342", "para_id": 910, "text": "“Be not alarmed, madam, on receiving this letter, by the apprehension of its containing any repetition of those sentiments, or renewal of those offers, which were last night so disgusting to you. I write without any intention of paining you, or humbling myself, by dwelling on wishes, which, for the happiness of both, cannot be too soon forgotten; and the effort which the formation and the perusal of this letter must occasion, should have been spared, had not my character required it to be written and read. You must, therefore, pardon the freedom with which I demand your attention; your feelings, I know, will bestow it unwillingly, but I demand it of your justice."}
{"doc_id": "gutenberg_1342", "para_id": 911, "text": "“Two offences of a very different nature, and by no means of equal magnitude, you last night laid to my charge. The first mentioned was, that, regardless of the sentiments of either, I had detached Mr. Bingley from your sister,--and the other, that I had, in defiance of various claims, in defiance of honour and humanity, ruined the immediate prosperity and blasted the prospects of Mr. Wickham. Wilfully and wantonly to have thrown off the companion of my youth, the acknowledged favourite of my father, a young man who had scarcely any other dependence than on our patronage, and who had been brought up to expect its exertion, would be a depravity, to which the separation of two young persons whose affection could be the growth of only a few weeks, could bear no comparison. But from the severity of that blame which was last night so liberally bestowed, respecting each circumstance, I shall hope to be in future secured, when the following account of my actions and their motives has been read. If, in the explanation of them which is due to myself, I am under the necessity of relating feelings which may be offensive to yours, I can only say that I am sorry. The necessity must be obeyed, and further apology would be absurd. I had not been long in Hertfordshire before I saw, in common with others, that Bingley preferred your elder sister to any other young woman in the country. But it was not till the evening of the dance at Netherfield that I had any apprehension of his feeling a serious attachment. I had often seen him in love before. At that ball, while I had the honour of dancing with you, I was first made acquainted, by Sir William Lucas’s accidental information, that Bingley’s attentions to your sister had given rise to a general expectation of their marriage. He spoke of it as a certain event, of which the time alone could be undecided. From that moment I observed my friend’s behaviour attentively; and I could then perceive that his partiality for Miss Bennet was beyond what I had ever witnessed in him. Your sister I also watched. Her look and manners were open, cheerful, and engaging as ever, but without any symptom of peculiar regard; and I remained convinced, from the evening’s scrutiny, that though she received his attentions with pleasure, she did not invite them by any participation of sentiment. If _you_ have not been mistaken here, _I_ must have been in an error. Your superior knowledge of your sister must make the latter probable. If it be so, if I have been misled by such error to inflict pain on her, your resentment has not been unreasonable. But I shall not scruple to assert, that the serenity of your sister’s countenance and air was such as might have given the most acute observer a conviction that, however amiable her temper, her heart was not likely to be easily touched. That I was desirous of believing her indifferent is certain; but I will venture to say that my investigations and decisions are not usually influenced by my hopes or fears. I did not believe her to be indifferent because I wished it; I believed it on impartial conviction, as truly as I wished it in reason. My objections to the marriage were not merely those which I last night acknowledged to have required the utmost force of passion to put aside in my own case; the want of connection could not be so great an evil to my friend as to me. But there were other causes of repugnance; causes which, though still existing, and existing to an equal degree in both instances, I had myself endeavoured to forget, because they were not immediately before me. These causes must be stated, though briefly. The situation of your mother’s family, though objectionable, was nothing in comparison of that total want of propriety so frequently, so almost uniformly betrayed by herself, by your three younger sisters, and occasionally even by your father:--pardon me,--it pains me to offend you. But amidst your concern for the defects of your nearest relations, and your displeasure at this representation of them, let it give you consolation to consider that to have conducted yourselves so as to avoid any share of the like censure is praise no less generally bestowed on you and your eldest sister than it is honourable to the sense and disposition of both. I will only say, farther, that from what passed that evening my opinion of all parties was confirmed, and every inducement heightened, which could have led me before to preserve my friend from what I esteemed a most unhappy connection. He left Netherfield for London on the day following, as you, I am certain, remember, with the design of soon returning. The part which I acted is now to be explained. His sisters’ uneasiness had been equally excited with my own: our coincidence of feeling was soon discovered; and, alike sensible that no time was to be lost in detaching their brother, we shortly resolved on joining him directly in London. We accordingly went--and there I readily engaged in the office of pointing out to my friend the certain evils of such a choice. I described and enforced them earnestly. But however this remonstrance might have staggered or delayed his determination, I do not suppose that it would ultimately have prevented the marriage, had it not been seconded by the assurance, which I hesitated not in giving, of your sister’s indifference. He had before believed her to return his affection with sincere, if not with equal, regard. But Bingley has great natural modesty, with a stronger dependence on my judgment than on his own. To convince him, therefore, that he had deceived himself was no very difficult point. To persuade him against returning into Hertfordshire, when that conviction had been given, was scarcely the work of a moment. I cannot blame myself for having done thus much. There is but one part of my conduct, in the whole affair, on which I do not reflect with satisfaction; it is that I condescended to adopt the measures of art so far as to conceal from him your sister’s being in town. I knew it myself, as it was known to Miss Bingley; but her brother is even yet ignorant of it. That they might have met without ill consequence is, perhaps, probable; but his regard did not appear to me enough extinguished for him to see her without some danger. Perhaps this concealment, this disguise, was beneath me. It is done, however, and it was done for the best. On this subject I have nothing more to say, no other apology to offer. If I have wounded your sister’s feelings, it was unknowingly done; and though the motives which governed me may to you very naturally appear insufficient, I have not yet learnt to condemn them.--With respect to that other, more weighty accusation, of having injured Mr. Wickham, I can only refute it by laying before you the whole of his connection with my family. Of what he has _particularly_ accused me I am ignorant; but of the truth of what I shall relate I can summon more than one witness of undoubted veracity. Mr. Wickham is the son of a very respectable man, who had for many years the management of all the Pemberley estates, and whose good conduct in the discharge of his trust naturally inclined my father to be of service to him; and on George Wickham, who was his godson, his kindness was therefore liberally bestowed. My father supported him at school, and afterwards at Cambridge; most important assistance, as his own father, always poor from the extravagance of his wife, would have been unable to give him a gentleman’s education. My father was not only fond of this young man’s society, whose manners were always engaging, he had also the highest opinion of him, and hoping the church would be his profession, intended to provide for him in it. As for myself, it is many, many years since I first began to think of him in a very different manner. The vicious propensities, the want of principle, which he was careful to guard from the knowledge of his best friend, could not escape the observation of a young man of nearly the same age with himself, and who had opportunities of seeing him in unguarded moments, which Mr. Darcy could not have. Here again I shall give you pain--to what degree you only can tell. But whatever may be the sentiments which Mr. Wickham has created, a suspicion of their nature shall not prevent me from unfolding his real character. It adds even another motive. My excellent father died about five years ago; and his attachment to Mr. Wickham was to the last so steady, that in his will he particularly recommended it to me to promote his advancement in the best manner that his profession might allow, and if he took orders, desired that a valuable family living might be his as soon as it became vacant. There was also a legacy of one thousand pounds. His own father did not long survive mine; and within half a year from these events Mr. Wickham wrote to inform me that, having finally resolved against taking orders, he hoped I should not think it unreasonable for him to expect some more immediate pecuniary advantage, in lieu of the preferment, by which he could not be benefited. He had some intention, he added, of studying the law, and I must be aware that the interest of one thousand pounds would be a very insufficient support therein. I rather wished than believed him to be sincere; but, at any rate, was perfectly ready to accede to his proposal. I knew that Mr. Wickham ought not to be a clergyman. The business was therefore soon settled. He resigned all claim to assistance in the church, were it possible that he could ever be in a situation to receive it, and accepted in return three thousand pounds. All connection between us seemed now dissolved. I thought too ill of him to invite him to Pemberley, or admit his society in town. In town, I believe, he chiefly lived, but his studying the law was a mere pretence; and being now free from all restraint, his life was a life of idleness and dissipation. For about three years I heard little of him; but on the decease of the incumbent of the living which had been designed for him, he applied to me again by letter for the presentation. His circumstances, he assured me, and I had no difficulty in believing it, were exceedingly bad. He had found the law a most unprofitable study, and was now absolutely resolved on being ordained, if I would present him to the living in question--of which he trusted there could be little doubt, as he was well assured that I had no other person to provide for, and I could not have forgotten my revered father’s intentions. You will hardly blame me for refusing to comply with this entreaty, or for resisting every repetition of it. His resentment was in proportion to the distress of his circumstances--and he was doubtless as violent in his abuse of me to others as in his reproaches to myself. After this period, every appearance of acquaintance was dropped. How he lived, I know not. But last summer he was again most painfully obtruded on my notice. I must now mention a circumstance which I would wish to forget myself, and which no obligation less than the present should induce me to unfold to any human being. Having said thus much, I feel no doubt of your secrecy. My sister, who is more than ten years my junior, was left to the guardianship of my mother’s nephew, Colonel Fitzwilliam, and myself. About a year ago, she was taken from school, and an establishment formed for her in London; and last summer she went with the lady who presided over it to Ramsgate; and thither also went Mr. Wickham, undoubtedly by design; for there proved to have been a prior acquaintance between him and Mrs. Younge, in whose character we were most unhappily deceived; and by her connivance and aid he so far recommended himself to Georgiana, whose affectionate heart retained a strong impression of his kindness to her as a child, that she was persuaded to believe herself in love and to consent to an elopement. She was then but fifteen, which must be her excuse; and after stating her imprudence, I am happy to add, that I owed the knowledge of it to herself. I joined them unexpectedly a day or two before the intended elopement; and then Georgiana, unable to support the idea of grieving and offending a brother whom she almost looked up to as a father, acknowledged the whole to me. You may imagine what I felt and how I acted. Regard for my sister’s credit and feelings prevented any public exposure; but I wrote to Mr. Wickham, who left the place immediately, and Mrs. Younge was of course removed from her charge. Mr. Wickham’s chief object was unquestionably my sister’s fortune, which is thirty thousand pounds; but I cannot help supposing that the hope of revenging himself on me was a strong inducement. His revenge would have been complete indeed. This, madam, is a faithful narrative of every event in which we have been concerned together; and if you do not absolutely reject it as false, you will, I hope, acquit me henceforth of cruelty towards Mr. Wickham. I know not in what manner, under what form of falsehood, he has imposed on you; but his success is not perhaps to be wondered at, ignorant as you previously were of everything concerning either. Detection could not be in your power, and suspicion certainly not in your inclination. You may possibly wonder why all this was not told you last night. But I was not then master enough of myself to know what could or ought to be revealed. For the truth of everything here related, I can appeal more particularly to the testimony of Colonel Fitzwilliam, who, from our near relationship and constant intimacy, and still more as one of the executors of my father’s will, has been unavoidably acquainted with every particular of these transactions. If your abhorrence of _me_ should make _my_ assertions valueless, you cannot be prevented by the same cause from confiding in my cousin; and that there may be the possibility of consulting him, I shall endeavour to find some opportunity of putting this letter in your hands in the course of the morning. I will only add, God bless you."}
{"doc_id": "gutenberg_1342", "para_id": 912, "text": "Elizabeth, when Mr. Darcy gave her the letter, did not expect it to contain a renewal of his offers, she had formed no expectation at all of its contents. But such as they were, it may be well supposed how eagerly she went through them, and what a contrariety of emotion they excited. Her feelings as she read were scarcely to be defined. With amazement did she first understand that he believed any apology to be in his power; and steadfastly was she persuaded, that he could have no explanation to give, which a just sense of shame would not conceal. With a strong prejudice against everything he might say, she began his account of what had happened at Netherfield. She read with an eagerness which hardly left her power of comprehension; and from impatience of knowing what the next sentence might bring, was incapable of attending to the sense of the one before her eyes. His belief of her sister’s insensibility she instantly resolved to be false; and his account of the real, the worst objections to the match, made her too angry to have any wish of doing him justice. He expressed no regret for what he had done which satisfied her; his style was not penitent, but haughty. It was all pride and insolence."}
{"doc_id": "gutenberg_1342", "para_id": 913, "text": "But when this subject was succeeded by his account of Mr. Wickham--when she read, with somewhat clearer attention, a relation of events which, if true, must overthrow every cherished opinion of his worth, and which bore so alarming an affinity to his own history of himself--her feelings were yet more acutely painful and more difficult of definition. Astonishment, apprehension, and even horror, oppressed her. She wished to discredit it entirely, repeatedly exclaiming, “This must be false! This cannot be! This must be the grossest falsehood!”--and when she had gone through the whole letter, though scarcely knowing anything of the last page or two, put it hastily away, protesting that she would not regard it, that she would never look in it again."}
{"doc_id": "gutenberg_1342", "para_id": 914, "text": "In this perturbed state of mind, with thoughts that could rest on nothing, she walked on; but it would not do: in half a minute the letter was unfolded again; and collecting herself as well as she could, she again began the mortifying perusal of all that related to Wickham, and commanded herself so far as to examine the meaning of every sentence. The account of his connection with the Pemberley family was exactly what he had related himself; and the kindness of the late Mr. Darcy, though she had not before known its extent, agreed equally well with his own words. So far each recital confirmed the other; but when she came to the will, the difference was great. What Wickham had said of the living was fresh in her memory; and as she recalled his very words, it was impossible not to feel that there was gross duplicity on one side or the other, and, for a few moments, she flattered herself that her wishes did not err. But when she read and re-read, with the closest attention, the particulars immediately following of Wickham’s resigning all pretensions to the living, of his receiving in lieu so considerable a sum as three thousand pounds, again was she forced to hesitate. She put down the letter, weighed every circumstance with what she meant to be impartiality--deliberated on the probability of each statement--but with little success. On both sides it was only assertion. Again she read on. But every line proved more clearly that the affair, which she had believed it impossible that any contrivance could so represent as to render Mr. Darcy’s conduct in it less than infamous, was capable of a turn which must make him entirely blameless throughout the whole."}
{"doc_id": "gutenberg_1342", "para_id": 915, "text": "The extravagance and general profligacy which he scrupled not to lay to Mr. Wickham’s charge exceedingly shocked her; the more so, as she could bring no proof of its injustice. She had never heard of him before his entrance into the ----shire militia, in which he had engaged at the persuasion of the young man, who, on meeting him accidentally in town, had there renewed a slight acquaintance. Of his former way of life, nothing had been known in Hertfordshire but what he told"}
{"doc_id": "gutenberg_1342", "para_id": 916, "text": "himself. As to his real character, had information been in her power, she had never felt a wish of inquiring. His countenance, voice, and manner, had established him at once in the possession of every virtue. She tried to recollect some instance of goodness, some distinguished trait of integrity or benevolence, that might rescue him from the attacks of Mr. Darcy; or at least, by the predominance of virtue, atone for those casual errors, under which she would endeavour to class what Mr. Darcy had described as the idleness and vice of many years’ continuance. But no such recollection befriended her. She could see him instantly before her, in every charm of air and address, but she could remember no more substantial good than the general approbation of the neighbourhood, and the regard which his social powers had gained him in the mess. After pausing on this point a considerable while, she once more continued to read. But, alas! the story which followed, of his designs on Miss Darcy, received some confirmation from what had passed between Colonel Fitzwilliam and herself only the morning before; and at last she was referred for the truth of every particular to Colonel Fitzwilliam himself--from whom she had previously received the information of his near concern in all his cousin’s affairs and whose character she had no reason to question. At one time she had almost resolved on applying to him, but the idea was checked by the awkwardness of the application, and at length wholly banished by the conviction that Mr. Darcy would never have hazarded such a proposal, if he had not been well assured of his cousin’s corroboration."}
{"doc_id": "gutenberg_1342", "para_id": 917, "text": "She perfectly remembered everything that had passed in conversation between Wickham and herself in their first evening at Mr. Philips’s. Many of his expressions were still fresh in her memory. She was _now_ struck with the impropriety of such communications to a stranger, and wondered it had escaped her before. She saw the indelicacy of putting himself forward as he had done, and the inconsistency of his professions with his conduct. She remembered that he had boasted of having no fear of seeing Mr. Darcy--that Mr. Darcy might leave the country, but that _he_ should stand his ground; yet he had avoided the Netherfield ball the very next week. She remembered, also, that till the Netherfield family had quitted the country, he had told his story to no one but herself; but that after their removal, it had been everywhere discussed; that he had then no reserves, no scruples in sinking Mr. Darcy’s character, though he had assured her that respect for the father would always prevent his exposing the son."}
{"doc_id": "gutenberg_1342", "para_id": 918, "text": "How differently did everything now appear in which he was concerned! His attentions to Miss King were now the consequence of views solely and hatefully mercenary; and the mediocrity of her fortune proved no longer the moderation of his wishes, but his eagerness to grasp at anything. His behaviour to herself could now have had no tolerable motive: he had either been deceived with regard to her fortune, or had been gratifying his vanity by encouraging the preference which she believed she had most incautiously shown. Every lingering struggle in his favour grew fainter and fainter; and in further justification of Mr. Darcy, she could not but allow that Mr. Bingley, when questioned by Jane, had long ago asserted his blamelessness in the affair;--that, proud and repulsive as were his manners, she had never, in the whole course of their acquaintance--an acquaintance which had latterly brought them much together, and given her a sort of intimacy with his ways--seen anything that betrayed him to be unprincipled or unjust--anything that spoke him of irreligious or immoral habits;--that among his own connections he was esteemed and valued;--that even Wickham had allowed him merit as a brother, and that she had often heard him speak so affectionately of his sister as to prove him capable of some amiable feeling;--that had his actions been what Wickham represented them, so gross a violation of everything right could hardly have been concealed from the world; and that friendship between a person capable of it and such an amiable man as Mr. Bingley was incomprehensible."}
{"doc_id": "gutenberg_1342", "para_id": 919, "text": "She grew absolutely ashamed of herself. Of neither Darcy nor Wickham could she think, without feeling that she had been blind, partial, prejudiced, absurd."}
{"doc_id": "gutenberg_1342", "para_id": 920, "text": "“How despicably have I acted!” she cried. “I, who have prided myself on my discernment! I, who have valued myself on my abilities! who have often disdained the generous candour of my sister, and gratified my vanity in useless or blameless distrust. How humiliating is this discovery! Yet, how just a humiliation! Had I been in love, I could not have been more wretchedly blind. But vanity, not love, has been my folly. Pleased with the preference of one, and offended by the neglect of the other, on the very beginning of our acquaintance, I have courted prepossession and ignorance, and driven reason away where either were concerned. Till this moment, I never knew myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 921, "text": "From herself to Jane, from Jane to Bingley, her thoughts were in a line which soon brought to her recollection that Mr. Darcy’s explanation _there_ had appeared very insufficient; and she read it again. Widely different was the effect of a second perusal. How could she deny that credit to his assertions, in one instance, which she had been obliged to give in the other? He declared himself to have been totally unsuspicious of her sister’s attachment; and she could not help remembering what Charlotte’s opinion had always been. Neither could she deny the justice of his description of Jane. She felt that Jane’s feelings, though fervent, were little displayed, and that there was a constant complacency in her air and manner, not often united with great sensibility."}
{"doc_id": "gutenberg_1342", "para_id": 922, "text": "When she came to that part of the letter in which her family were mentioned, in tones of such mortifying, yet merited, reproach, her sense of shame was severe. The justice of the charge struck her too forcibly for denial; and the circumstances to which he particularly alluded, as having passed at the Netherfield ball, and as confirming all his first disapprobation, could not have made a stronger impression on his mind than on hers."}
{"doc_id": "gutenberg_1342", "para_id": 923, "text": "The compliment to herself and her sister was not unfelt. It soothed, but it could not console her for the contempt which had been thus self-attracted by the rest of her family; and as she considered that Jane’s disappointment had, in fact, been the work of her nearest relations, and reflected how materially the credit of both must be hurt by such impropriety of conduct, she felt depressed beyond anything she had ever known before."}
{"doc_id": "gutenberg_1342", "para_id": 924, "text": "After wandering along the lane for two hours, giving way to every variety of thought, reconsidering events, determining probabilities, and reconciling herself, as well as she could, to a change so sudden and so important, fatigue, and a recollection of her long absence, made her at length return home; and she entered the house with the wish of appearing cheerful as usual, and the resolution of repressing such reflections as must make her unfit for conversation."}
{"doc_id": "gutenberg_1342", "para_id": 925, "text": "She was immediately told, that the two gentlemen from Rosings had each called during her absence; Mr. Darcy, only for a few minutes, to take leave, but that Colonel Fitzwilliam had been sitting with them at least an hour, hoping for her return, and almost resolving to walk after her till she could be found. Elizabeth could but just _affect_ concern in missing him; she really rejoiced at it. Colonel Fitzwilliam was no longer an object. She could think only of her letter."}
{"doc_id": "gutenberg_1342", "para_id": 926, "text": "The two gentlemen left Rosings the next morning; and Mr. Collins having been in waiting near the lodges, to make them his parting obeisance, was able to bring home the pleasing intelligence of their appearing in very good health, and in as tolerable spirits as could be expected, after the melancholy scene so lately gone through at Rosings. To Rosings he then hastened to console Lady Catherine and her daughter; and on his return brought back, with great satisfaction, a message from her Ladyship, importing that she felt herself so dull as to make her very desirous of having them all to dine with her."}
{"doc_id": "gutenberg_1342", "para_id": 927, "text": "Elizabeth could not see Lady Catherine without recollecting that, had she chosen it, she might by this time have been presented to her as her future niece; nor could she think, without a smile, of what her Ladyship’s indignation would have been. “What would she have said? how would she have behaved?” were the questions with which she amused herself."}
{"doc_id": "gutenberg_1342", "para_id": 928, "text": "Their first subject was the diminution of the Rosings’ party. “I assure you, I feel it exceedingly,” said Lady Catherine; “I believe nobody feels the loss of friends so much as I do. But I am particularly attached to these young men; and know them to be so much attached to me! They were excessively sorry to go! But so they always are. The dear Colonel rallied his spirits tolerably till just at last; but Darcy seemed to feel it most acutely--more, I think, than last year. His attachment to Rosings certainly increases.”"}
{"doc_id": "gutenberg_1342", "para_id": 929, "text": "Mr. Collins had a compliment and an allusion to throw in here, which were kindly smiled on by the mother and daughter."}
{"doc_id": "gutenberg_1342", "para_id": 930, "text": "Lady Catherine observed, after dinner, that Miss Bennet seemed out of spirits; and immediately accounting for it herself, by supposing that she did not like to go home again so soon, she added,--"}
{"doc_id": "gutenberg_1342", "para_id": 931, "text": "“But if that is the case, you must write to your mother to beg that you may stay a little longer. Mrs. Collins will be very glad of your company, I am sure.”"}
{"doc_id": "gutenberg_1342", "para_id": 932, "text": "“I am much obliged to your Ladyship for your kind invitation,” replied Elizabeth; “but it is not in my power to accept it. I must be in town next Saturday.”"}
{"doc_id": "gutenberg_1342", "para_id": 933, "text": "“Why, at that rate, you will have been here only six weeks. I expected you to stay two months. I told Mrs. Collins so before you came. There can be no occasion for your going so soon. Mrs. Bennet could certainly spare you for another fortnight.”"}
{"doc_id": "gutenberg_1342", "para_id": 934, "text": "“Oh, your father, of course, may spare you, if your mother can. Daughters are never of so much consequence to a father. And if you will stay another _month_ complete, it will be in my power to take one of you as far as London, for I am going there early in June, for a week; and as Dawson does not object to the barouche-box, there will be very good room for one of you--and, indeed, if the weather should happen to be cool, I should not object to taking you both, as you are neither of you large.”"}
{"doc_id": "gutenberg_1342", "para_id": 935, "text": "“You are all kindness, madam; but I believe we must abide by our original plan.”"}
{"doc_id": "gutenberg_1342", "para_id": 936, "text": "Lady Catherine seemed resigned. “Mrs. Collins, you must send a servant with them. You know I always speak my mind, and I cannot bear the idea of two young women travelling post by themselves. It is highly improper. You must contrive to send somebody. I have the greatest dislike in the world to that sort of thing. Young women should always be properly guarded and attended, according to their situation in life. When my niece Georgiana went to Ramsgate last summer, I made a point of her having two men-servants go with her. Miss Darcy, the daughter of Mr. Darcy of Pemberley, and Lady Anne, could not have appeared with propriety in a different manner. I am excessively attentive to all those things. You must send John with the young ladies, Mrs. Collins. I am glad it occurred to me to mention it; for it would really be discreditable to _you_ to let them go alone.”"}
{"doc_id": "gutenberg_1342", "para_id": 937, "text": "“Oh! Your uncle! He keeps a man-servant, does he? I am very glad you have somebody who thinks of those things. Where shall you change horses? Oh, Bromley, of course. If you mention my name at the Bell, you will be attended to.”"}
{"doc_id": "gutenberg_1342", "para_id": 938, "text": "Lady Catherine had many other questions to ask respecting their journey; and as she did not answer them all herself attention was necessary--which Elizabeth believed to be lucky for her; or, with a mind so occupied, she might have forgotten where she was. Reflection must be reserved for solitary hours: whenever she was alone, she gave way to it as the greatest relief; and not a day went by without a solitary walk, in which she might indulge in all the delight of unpleasant recollections."}
{"doc_id": "gutenberg_1342", "para_id": 939, "text": "Mr. Darcy’s letter she was in a fair way of soon knowing by heart. She studied every sentence; and her feelings towards its writer were at times widely different. When she remembered the style of his address, she was still full of indignation: but when she considered how unjustly she had condemned and upbraided him, her anger was turned against herself; and his disappointed feelings became the object of compassion. His attachment excited gratitude, his general character respect: but she could not approve him; nor could she for a moment repent her refusal, or feel the slightest inclination ever to see him again. In her own past behaviour, there was a constant source of vexation and regret: and in the unhappy defects of her family, a subject of yet heavier chagrin. They were hopeless of remedy. Her father, contented with laughing at them, would never exert himself to restrain the wild giddiness of his youngest daughters; and her mother, with manners so far from right herself, was entirely insensible of the evil. Elizabeth had frequently united with Jane in an endeavour to check the imprudence of Catherine and Lydia; but while they were supported by their mother’s indulgence, what chance could there be of improvement? Catherine, weak-spirited, irritable, and completely under Lydia’s guidance, had been always affronted by their advice; and Lydia, self-willed and careless, would scarcely give them a hearing. They were ignorant, idle, and vain. While there was an officer in Meryton, they would flirt with him; and while Meryton was within a walk of Longbourn, they would be going there for ever."}
{"doc_id": "gutenberg_1342", "para_id": 940, "text": "Anxiety on Jane’s behalf was another prevailing concern; and Mr. Darcy’s explanation, by restoring Bingley to all her former good opinion, heightened the sense of what Jane had lost. His affection was proved to have been sincere, and his conduct cleared of all blame, unless any could attach to the implicitness of his confidence in his friend. How grievous then was the thought that, of a situation so desirable in every respect, so replete with advantage, so promising for happiness, Jane had been deprived, by the folly and indecorum of her own family!"}
{"doc_id": "gutenberg_1342", "para_id": 941, "text": "When to these recollections was added the development of Wickham’s character, it may be easily believed that the happy spirits which had seldom been depressed before were now so much affected as to make it almost impossible for her to appear tolerably cheerful."}
{"doc_id": "gutenberg_1342", "para_id": 942, "text": "Their engagements at Rosings were as frequent during the last week of her stay as they had been at first. The very last evening was spent there; and her Ladyship again inquired minutely into the particulars of their journey, gave them directions as to the best method of packing, and was so urgent on the necessity of placing gowns in the only right way, that Maria thought herself obliged, on her return, to undo all the work of the morning, and pack her trunk afresh."}
{"doc_id": "gutenberg_1342", "para_id": 943, "text": "When they parted, Lady Catherine, with great condescension, wished them a good journey, and invited them to come to Hunsford again next year; and Miss de Bourgh exerted herself so far as to courtesy and hold out her hand to both."}
{"doc_id": "gutenberg_1342", "para_id": 944, "text": "On Saturday morning Elizabeth and Mr. Collins met for breakfast a few minutes before the others appeared; and he took the opportunity of paying the parting civilities which he deemed indispensably necessary."}
{"doc_id": "gutenberg_1342", "para_id": 945, "text": "“I know not, Miss Elizabeth,” said he, “whether Mrs. Collins has yet expressed her sense of your kindness in coming to us; but I am very certain you will not leave the house without receiving her thanks for it. The favour of your company has been much felt, I assure you. We know how little there is to tempt anyone to our humble abode. Our plain manner of living, our small rooms, and few domestics, and the little we see of the world, must make Hunsford extremely dull to a young lady like yourself; but I hope you will believe us grateful for the condescension, and that we have done everything in our power to prevent you spending your time unpleasantly.”"}
{"doc_id": "gutenberg_1342", "para_id": 946, "text": "Elizabeth was eager with her thanks and assurances of happiness. She had spent six weeks with great enjoyment; and the pleasure of being with Charlotte, and the kind attention she had received, must make _her_ feel the obliged. Mr. Collins was gratified; and with a more smiling solemnity replied,--"}
{"doc_id": "gutenberg_1342", "para_id": 947, "text": "“It gives me the greatest pleasure to hear that you have passed your time not disagreeably. We have certainly done our best; and most fortunately having it in our power to introduce you to very superior society, and from our connection with Rosings, the frequent means of varying the humble home scene, I think we may flatter ourselves that your Hunsford visit cannot have been entirely irksome. Our situation with regard to Lady Catherine’s family is, indeed, the sort of extraordinary advantage and blessing which few can boast. You see on what a footing we are. You see how continually we are engaged there. In truth, I must acknowledge, that, with all the disadvantages of this humble parsonage, I should not think anyone abiding in it an object of compassion, while they are sharers of our intimacy at Rosings.”"}
{"doc_id": "gutenberg_1342", "para_id": 948, "text": "Words were insufficient for the elevation of his feelings; and he was obliged to walk about the room, while Elizabeth tried to unite civility and truth in a few short sentences."}
{"doc_id": "gutenberg_1342", "para_id": 949, "text": "“You may, in fact, carry a very favourable report of us into Hertfordshire, my dear cousin. I flatter myself, at least, that you will be able to do so. Lady Catherine’s great attentions to Mrs. Collins you have been a daily witness of; and altogether I trust it does not appear that your friend has drawn an unfortunate--but on this point it will be as well to be silent. Only let me assure you, my dear Miss Elizabeth, that I can from my heart most cordially wish you equal felicity in marriage. My dear Charlotte and I have but one mind and one way of thinking. There is in everything a most remarkable resemblance of character and ideas between us. We seem to have been designed for each other.”"}
{"doc_id": "gutenberg_1342", "para_id": 950, "text": "Elizabeth could safely say that it was a great happiness where that was the case, and with equal sincerity could add, that she firmly believed and rejoiced in his domestic comforts. She was not sorry, however, to have the recital of them interrupted by the entrance of the lady from whom they sprang. Poor Charlotte! it was melancholy to leave her to such society! But she had chosen it with her eyes open; and though evidently regretting that her visitors were to go, she did not seem to ask for compassion. Her home and her housekeeping, her parish and her poultry, and all their dependent concerns, had not yet lost their charms."}
{"doc_id": "gutenberg_1342", "para_id": 951, "text": "At length the chaise arrived, the trunks were fastened on, the parcels placed within, and it was pronounced to be ready. After an affectionate parting between the friends, Elizabeth was attended to the carriage by Mr. Collins; and as they walked down the garden, he was commissioning her with his best respects to all her family, not forgetting his thanks for the kindness he had received at Longbourn in the winter, and his compliments to Mr. and Mrs. Gardiner, though unknown. He then handed her in, Maria followed, and the door was on the point of being closed, when he suddenly reminded them, with some consternation, that they had hitherto forgotten to leave any message for the ladies of Rosings."}
{"doc_id": "gutenberg_1342", "para_id": 952, "text": "“But,” he added, “you will of course wish to have your humble respects delivered to them, with your grateful thanks for their kindness to you while you have been here.”"}
{"doc_id": "gutenberg_1342", "para_id": 953, "text": "Elizabeth made no objection: the door was then allowed to be shut, and the carriage drove off."}
{"doc_id": "gutenberg_1342", "para_id": 954, "text": "“Good gracious!” cried Maria, after a few minutes’ silence, “it seems but a day or two since we first came! and yet how many things have happened!”"}
{"doc_id": "gutenberg_1342", "para_id": 955, "text": "“We have dined nine times at Rosings, besides drinking tea there twice! How much I shall have to tell!”"}
{"doc_id": "gutenberg_1342", "para_id": 956, "text": "Their journey was performed without much conversation, or any alarm; and within four hours of their leaving Hunsford they reached Mr. Gardiner’s house, where they were to remain a few days."}
{"doc_id": "gutenberg_1342", "para_id": 957, "text": "Jane looked well, and Elizabeth had little opportunity of studying her spirits, amidst the various engagements which the kindness of her aunt had reserved for them. But Jane was to go home with her, and at Longbourn there would be leisure enough for observation."}
{"doc_id": "gutenberg_1342", "para_id": 958, "text": "It was not without an effort, meanwhile, that she could wait even for Longbourn, before she told her sister of Mr. Darcy’s proposals. To know that she had the power of revealing what would so exceedingly astonish Jane, and must, at the same time, so highly gratify whatever of her own vanity she had not yet been able to reason away, was such a temptation to openness as nothing could have conquered, but the state of indecision in which she remained as to the extent of what she should communicate, and her fear, if she once entered on the subject, of being hurried into repeating something of Bingley, which might only grieve her sister further."}
{"doc_id": "gutenberg_1342", "para_id": 959, "text": "It was the second week in May, in which the three young ladies set out together from Gracechurch Street for the town of ----, in Hertfordshire; and, as they drew near the appointed inn where Mr. Bennet’s carriage was to meet them, they quickly perceived, in token of the coachman’s punctuality, both Kitty and Lydia looking out of a dining-room upstairs. These two girls had been above an hour in the place, happily employed in visiting an opposite milliner, watching the sentinel on guard, and dressing a salad and cucumber."}
{"doc_id": "gutenberg_1342", "para_id": 960, "text": "After welcoming their sisters, they triumphantly displayed a table set out with such cold meat as an inn larder usually affords, exclaiming, “Is not this nice? is not this an agreeable surprise?”"}
{"doc_id": "gutenberg_1342", "para_id": 961, "text": "“And we mean to treat you all,” added Lydia; “but you must lend us the money, for we have just spent ours at the shop out there.” Then showing her purchases,--“Look here, I have bought this bonnet. I do not think it is very pretty; but I thought I might as well buy it as not. I shall pull it to pieces as soon as I get home, and see if I can make it up any better.”"}
{"doc_id": "gutenberg_1342", "para_id": 962, "text": "And when her sisters abused it as ugly, she added, with perfect unconcern, “Oh, but there were two or three much uglier in the shop; and when I have bought some prettier-coloured satin to trim it with fresh, I think it will be very tolerable. Besides, it will not much signify what one wears this summer, after the ----shire have left Meryton, and they are going in a fortnight.”"}
{"doc_id": "gutenberg_1342", "para_id": 963, "text": "“They are going to be encamped near Brighton; and I do so want papa to take us all there for the summer! It would be such a delicious scheme, and I dare say would hardly cost anything at all. Mamma would like to go, too, of all things! Only think what a miserable summer else we shall have!”"}
{"doc_id": "gutenberg_1342", "para_id": 964, "text": "“Yes,” thought Elizabeth; “_that_ would be a delightful scheme, indeed, and completely do for us at once. Good Heaven! Brighton and a whole campful of soldiers, to us, who have been overset already by one poor regiment of militia, and the monthly balls of Meryton!”"}
{"doc_id": "gutenberg_1342", "para_id": 965, "text": "“Now I have got some news for you,” said Lydia, as they sat down to table. “What do you think? It is excellent news, capital news, and about a certain person that we all like.”"}
{"doc_id": "gutenberg_1342", "para_id": 966, "text": "Jane and Elizabeth looked at each other, and the waiter was told that he need not stay. Lydia laughed, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 967, "text": "“Ay, that is just like your formality and discretion. You thought the waiter must not hear, as if he cared! I dare say he often hears worse things said than I am going to say. But he is an ugly fellow! I am glad he is gone. I never saw such a long chin in my life. Well, but now for my news: it is about dear Wickham; too good for the waiter, is not it? There is no danger of Wickham’s marrying Mary King--there’s for you! She is gone down to her uncle at Liverpool; gone to stay. Wickham is safe.”"}
{"doc_id": "gutenberg_1342", "para_id": 968, "text": "“And Mary King is safe!” added Elizabeth; “safe from a connection imprudent as to fortune.”"}
{"doc_id": "gutenberg_1342", "para_id": 969, "text": "“I am sure there is not on _his_. I will answer for it, he never cared three straws about her. Who _could_ about such a nasty little freckled thing?”"}
{"doc_id": "gutenberg_1342", "para_id": 970, "text": "Elizabeth was shocked to think that, however incapable of such coarseness of _expression_ herself, the coarseness of the _sentiment_ was little other than her own breast had formerly harboured and fancied liberal!"}
{"doc_id": "gutenberg_1342", "para_id": 971, "text": "As soon as all had ate, and the elder ones paid, the carriage was ordered; and, after some contrivance, the whole party, with all their boxes, workbags, and parcels, and the unwelcome addition of Kitty’s and Lydia’s purchases, were seated in it."}
{"doc_id": "gutenberg_1342", "para_id": 972, "text": "“How nicely we are crammed in!” cried Lydia. “I am glad I brought my bonnet, if it is only for the fun of having another band-box! Well, now let us be quite comfortable and snug, and talk and laugh all the way home. And in the first place, let us hear what has happened to you all since you went away. Have you seen any pleasant men? Have you had any flirting? I was in great hopes that one of you would have got a husband before you came back. Jane will be quite an old maid soon, I declare. She is almost three-and-twenty! Lord! how ashamed I should be of not being married before three-and-twenty! My aunt Philips wants you so to get husbands you can’t think. She says Lizzy had better have taken Mr. Collins; but _I_ do not think there would have been any fun in it. Lord! how I should like to be married before any of you! and then I would _chaperon_ you about to all the balls. Dear me! we had such a good piece of fun the other day at Colonel Forster’s! Kitty and me were to spend the day there, and Mrs. Forster promised to have a little dance in the evening; (by-the-bye, Mrs. Forster and me are _such_ friends!) and so she asked the two Harringtons to come: but Harriet was ill, and so Pen was forced to come by herself; and then, what do you think we did? We dressed up Chamberlayne in woman’s clothes, on purpose to pass for a lady,--only think what fun! Not a soul knew of it, but Colonel and Mrs. Forster, and Kitty and me, except my aunt, for we were forced to borrow one of her gowns; and you cannot imagine how well he looked! When Denny, and Wickham, and Pratt, and two or three more of the men came in, they did not know him in the least. Lord! how I laughed! and so did Mrs. Forster. I thought I should have died. And _that_ made the men suspect something, and then they soon found out what was the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 973, "text": "With such kind of histories of their parties and good jokes did Lydia, assisted by Kitty’s hints and additions, endeavour to amuse her companions all the way to Longbourn. Elizabeth listened as little as she could, but there was no escaping the frequent mention of Wickham’s name."}
{"doc_id": "gutenberg_1342", "para_id": 974, "text": "Their reception at home was most kind. Mrs. Bennet rejoiced to see Jane in undiminished beauty; and more than once during dinner did Mr. Bennet say voluntarily to Elizabeth,----"}
{"doc_id": "gutenberg_1342", "para_id": 975, "text": "Their party in the dining-room was large, for almost all the Lucases came to meet Maria and hear the news; and various were the subjects which occupied them: Lady Lucas was inquiring of Maria, across the table, after the welfare and poultry of her eldest daughter; Mrs. Bennet was doubly engaged, on one hand collecting an account of the present fashions from Jane, who sat some way below her, and on the other, retailing them all to the younger Miss Lucases; and Lydia, in a voice rather louder than any other person’s, was enumerating the various pleasures of the morning to anybody who would hear her."}
{"doc_id": "gutenberg_1342", "para_id": 976, "text": "“Oh, Mary,” said she, “I wish you had gone with us, for we had such fun! as we went along Kitty and me drew up all the blinds, and pretended there was nobody in the coach; and I should have gone so all the way, if Kitty had not been sick; and when we got to the George, I do think we behaved very handsomely, for we treated the other three with the nicest cold luncheon in the world, and if you would have gone, we would have treated you too. And then when we came away it was such fun! I thought we never should have got into the coach. I was ready to die of laughter. And then we were so merry all the way home! we talked and laughed so loud, that anybody might have heard us ten miles off!”"}
{"doc_id": "gutenberg_1342", "para_id": 977, "text": "To this, Mary very gravely replied, “Far be it from me, my dear sister, to depreciate such pleasures. They would doubtless be congenial with the generality of female minds. But I confess they would have no charms for _me_. I should infinitely prefer a book.”"}
{"doc_id": "gutenberg_1342", "para_id": 978, "text": "But of this answer Lydia heard not a word. She seldom listened to anybody for more than half a minute, and never attended to Mary at all."}
{"doc_id": "gutenberg_1342", "para_id": 979, "text": "In the afternoon Lydia was urgent with the rest of the girls to walk to Meryton, and see how everybody went on; but Elizabeth steadily opposed the scheme. It should not be said, that the Miss Bennets could not be at home half a day before they were in pursuit of the officers. There was another reason, too, for her opposition. She dreaded seeing Wickham again, and was resolved to avoid it as long as possible. The comfort to _her_, of the regiment’s approaching removal, was indeed beyond expression. In a fortnight they were to go, and once gone, she hoped there could be nothing more to plague her on his account."}
{"doc_id": "gutenberg_1342", "para_id": 980, "text": "She had not been many hours at home, before she found that the Brighton scheme, of which Lydia had given them a hint at the inn, was under frequent discussion between her parents. Elizabeth saw directly that her father had not the smallest intention of yielding; but his answers were at the same time so vague and equivocal, that her mother, though often disheartened, had never yet despaired of succeeding at last."}
{"doc_id": "gutenberg_1342", "para_id": 981, "text": "Elizabeth’s impatience to acquaint Jane with what had happened could no longer be overcome; and at length resolving to suppress every particular in which her sister was concerned, and preparing her to be surprised, she related to her the next morning the chief of the scene between Mr. Darcy and herself."}
{"doc_id": "gutenberg_1342", "para_id": 982, "text": "Miss Bennet’s astonishment was soon lessened by the strong sisterly partiality which made any admiration of Elizabeth appear perfectly natural; and all surprise was shortly lost in other feelings. She was sorry that Mr. Darcy should have delivered his sentiments in a manner so little suited to recommend them; but still more was she grieved for the unhappiness which her sister’s refusal must have given him."}
{"doc_id": "gutenberg_1342", "para_id": 983, "text": "“His being so sure of succeeding was wrong,” said she, “and certainly ought not to have appeared; but consider how much it must increase his disappointment.”"}
{"doc_id": "gutenberg_1342", "para_id": 984, "text": "“Indeed,” replied Elizabeth, “I am heartily sorry for him; but he has other feelings which will probably soon drive away his regard for me. You do not blame me, however, for refusing him?”"}
{"doc_id": "gutenberg_1342", "para_id": 985, "text": "She then spoke of the letter, repeating the whole of its contents as far as they concerned George Wickham. What a stroke was this for poor Jane, who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind as was here collected in one individual! Nor was Darcy’s vindication, though grateful to her feelings, capable of consoling her for such discovery. Most earnestly did she labour to prove the probability of error, and seek to clear one, without involving the other."}
{"doc_id": "gutenberg_1342", "para_id": 986, "text": "“This will not do,” said Elizabeth; “you never will be able to make both of them good for anything. Take your choice, but you must be satisfied with only one. There is but such a quantity of merit between them; just enough to make one good sort of man; and of late it has been shifting about pretty much. For my part, I am inclined to believe it all Mr. Darcy’s, but you shall do as you choose.”"}
{"doc_id": "gutenberg_1342", "para_id": 987, "text": "“I do not know when I have been more shocked,” said she. “Wickham so very bad! It is almost past belief. And poor Mr. Darcy! dear Lizzy, only consider what he must have suffered. Such a disappointment! and with the knowledge of your ill opinion too! and having to relate such a thing of his sister! It is really too distressing, I am sure you must feel it so.”"}
{"doc_id": "gutenberg_1342", "para_id": 988, "text": "“Oh no, my regret and compassion are all done away by seeing you so full of both. I know you will do him such ample justice, that I am growing every moment more unconcerned and indifferent. Your profusion makes me saving; and if you lament over him much longer, my heart will be as light as a feather.”"}
{"doc_id": "gutenberg_1342", "para_id": 989, "text": "“Poor Wickham! there is such an expression of goodness in his countenance! such an openness and gentleness in his manner.”"}
{"doc_id": "gutenberg_1342", "para_id": 990, "text": "“There certainly was some great mismanagement in the education of those two young men. One has got all the goodness, and the other all the appearance of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 991, "text": "“I never thought Mr. Darcy so deficient in the _appearance_ of it as you used to do.”"}
{"doc_id": "gutenberg_1342", "para_id": 992, "text": "“And yet I meant to be uncommonly clever in taking so decided a dislike to him, without any reason. It is such a spur to one’s genius, such an opening for wit, to have a dislike of that kind. One may be continually abusive without saying anything just; but one cannot be always laughing at a man without now and then stumbling on something witty.”"}
{"doc_id": "gutenberg_1342", "para_id": 993, "text": "“Lizzy, when you first read that letter, I am sure you could not treat the matter as you do now.”"}
{"doc_id": "gutenberg_1342", "para_id": 994, "text": "“Indeed, I could not. I was uncomfortable enough, I was very uncomfortable--I may say unhappy. And with no one to speak to of what I felt, no Jane to comfort me, and say that I had not been so very weak, and vain, and nonsensical, as I knew I had! Oh, how I wanted you!”"}
{"doc_id": "gutenberg_1342", "para_id": 995, "text": "“How unfortunate that you should have used such very strong expressions in speaking of Wickham to Mr. Darcy, for now they _do_ appear wholly undeserved.”"}
{"doc_id": "gutenberg_1342", "para_id": 996, "text": "“Certainly. But the misfortune of speaking with bitterness is a most natural consequence of the prejudices I had been encouraging. There is one point on which I want your advice. I want to be told whether I ought, or ought not, to make our acquaintance in general understand Wickham’s character.”"}
{"doc_id": "gutenberg_1342", "para_id": 997, "text": "Miss Bennet paused a little, and then replied, “Surely there can be no occasion for exposing him so dreadfully. What is your own opinion?”"}
{"doc_id": "gutenberg_1342", "para_id": 998, "text": "“That it ought not to be attempted. Mr. Darcy has not authorized me to make his communication public. On the contrary, every particular relative to his sister was meant to be kept as much as possible to myself; and if I endeavour to undeceive people as to the rest of his conduct, who will believe me? The general prejudice against Mr. Darcy is so violent, that it would be the death of half the good people in Meryton, to attempt to place him in an amiable light. I am not equal to it. Wickham will soon be gone; and, therefore, it will not signify to anybody here what he really is. Some time hence it will be all found out, and then we may laugh at their stupidity in not knowing it before. At present I will say nothing about it.”"}
{"doc_id": "gutenberg_1342", "para_id": 999, "text": "“You are quite right. To have his errors made public might ruin him for ever. He is now, perhaps, sorry for what he has done, and anxious to re-establish a character. We must not make him desperate.”"}
{"doc_id": "gutenberg_1342", "para_id": 1000, "text": "The tumult of Elizabeth’s mind was allayed by this conversation. She had got rid of two of the secrets which had weighed on her for a fortnight, and was certain of a willing listener in Jane, whenever she might wish to talk again of either. But there was still something lurking behind, of which prudence forbade the disclosure. She dared not relate the other half of Mr. Darcy’s letter, nor explain to her sister how sincerely she had been valued by his friend. Here was knowledge in which no one could partake; and she was sensible that nothing less than a perfect understanding between the parties could justify her in throwing off this last encumbrance of mystery. “And then,” said she, “if that very improbable event should ever take place, I shall merely be able to tell what Bingley may tell in a much more agreeable manner himself. The liberty of communication cannot be mine till it has lost all its value!”"}
{"doc_id": "gutenberg_1342", "para_id": 1001, "text": "She was now, on being settled at home, at leisure to observe the real state of her sister’s spirits. Jane was not happy. She still cherished a very tender affection for Bingley. Having never even fancied herself in love before, her regard had all the warmth of first attachment, and from her age and disposition, greater steadiness than first attachments often boast; and so fervently did she value his remembrance, and prefer him to every other man, that all her good sense, and all her attention to the feelings of her friends, were requisite to check the indulgence of those regrets which must have been injurious to her own health and their tranquillity."}
{"doc_id": "gutenberg_1342", "para_id": 1002, "text": "“Well, Lizzy,” said Mrs. Bennet, one day, “what is your opinion _now_ of this sad business of Jane’s? For my part, I am determined never to speak of it again to anybody. I told my sister Philips so the other day. But I cannot find out that Jane saw anything of him in London. Well, he is a very undeserving young man--and I do not suppose there is the least chance in the world of her ever getting him now. There is no talk of his coming to Netherfield again in the summer; and I have inquired of everybody, too, who is likely to know.”"}
{"doc_id": "gutenberg_1342", "para_id": 1003, "text": "“Oh, well! it is just as he chooses. Nobody wants him to come; though I shall always say that he used my daughter extremely ill; and, if I was her, I would not have put up with it. Well, my comfort is, I am sure Jane will die of a broken heart, and then he will be sorry for what he has done.”"}
{"doc_id": "gutenberg_1342", "para_id": 1004, "text": "But as Elizabeth could not receive comfort from any such expectation she made no answer."}
{"doc_id": "gutenberg_1342", "para_id": 1005, "text": "“Well, Lizzy,” continued her mother, soon afterwards, “and so the Collinses live very comfortable, do they? Well, well, I only hope it will last. And what sort of table do they keep? Charlotte is an excellent manager, I dare say. If she is half as sharp as her mother, she is saving enough. There is nothing extravagant in _their_ housekeeping, I dare say.”"}
{"doc_id": "gutenberg_1342", "para_id": 1006, "text": "“A great deal of good management, depend upon it. Yes, yes. _They_ will take care not to outrun their income. _They_ will never be distressed for money. Well, much good may it do them! And so, I suppose, they often talk of having Longbourn when your father is dead. They look upon it quite as their own, I dare say, whenever that happens.”"}
{"doc_id": "gutenberg_1342", "para_id": 1007, "text": "“No; it would have been strange if they had. But I make no doubt they often talk of it between themselves. Well, if they can be easy with an estate that is not lawfully their own, so much the better. _I_ should be ashamed of having one that was only entailed on me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1008, "text": "The first week of their return was soon gone. The second began. It was the last of the regiment’s stay in Meryton, and all the young ladies in the neighbourhood were drooping apace. The dejection was almost universal. The elder Miss Bennets alone were still able to eat, drink, and sleep, and pursue the usual course of their employments. Very frequently were they reproached for this insensibility by Kitty and Lydia, whose own misery was extreme, and who could not comprehend such hard-heartedness in any of the family."}
{"doc_id": "gutenberg_1342", "para_id": 1009, "text": "“Good Heaven! What is to become of us? What are we to do?” would they often exclaim in the bitterness of woe. “How can you be smiling so, Lizzy?”"}
{"doc_id": "gutenberg_1342", "para_id": 1010, "text": "Their affectionate mother shared all their grief; she remembered what she had herself endured on a similar occasion five-and-twenty years ago."}
{"doc_id": "gutenberg_1342", "para_id": 1011, "text": "“I am sure,” said she, “I cried for two days together when Colonel Miller’s regiment went away. I thought I should have broke my heart.”"}
{"doc_id": "gutenberg_1342", "para_id": 1012, "text": "“And my aunt Philips is sure it would do _me_ a great deal of good,” added Kitty."}
{"doc_id": "gutenberg_1342", "para_id": 1013, "text": "Such were the kind of lamentations resounding perpetually through Longbourn House. Elizabeth tried to be diverted by them; but all sense of pleasure was lost in shame. She felt anew the justice of Mr. Darcy’s objections; and never had she before been so much disposed to pardon his interference in the views of his friend."}
{"doc_id": "gutenberg_1342", "para_id": 1014, "text": "But the gloom of Lydia’s prospect was shortly cleared away; for she received an invitation from Mrs. Forster, the wife of the colonel of the regiment, to accompany her to Brighton. This invaluable friend was a very young woman, and very lately married. A resemblance in good-humour and good spirits had recommended her and Lydia to each other, and out of their _three_ months’ acquaintance they had been intimate _two_."}
{"doc_id": "gutenberg_1342", "para_id": 1015, "text": "The rapture of Lydia on this occasion, her adoration of Mrs. Forster, the delight of Mrs. Bennet, and the mortification of Kitty, are scarcely to be described. Wholly inattentive to her sister’s feelings, Lydia flew about the house in restless ecstasy, calling for everyone’s congratulations, and laughing and talking with more violence than ever; whilst the luckless Kitty continued in the parlour repining at her fate in terms as unreasonable as her accent was peevish."}
{"doc_id": "gutenberg_1342", "para_id": 1016, "text": "“I cannot see why Mrs. Forster should not ask _me_ as well as Lydia,” said she, “though I am _not_ her particular friend. I have just as much right to be asked as she has, and more too, for I am two years older.”"}
{"doc_id": "gutenberg_1342", "para_id": 1017, "text": "In vain did Elizabeth attempt to make her reasonable, and Jane to make her resigned. As for Elizabeth herself, this invitation was so far from exciting in her the same feelings as in her mother and Lydia, that she considered it as the death-warrant of all possibility of common sense for the latter; and detestable as such a step must make her, were it known, she could not help secretly advising her father not to let her go. She represented to him all the improprieties of Lydia’s general behaviour, the little advantage she could derive from the friendship of such a woman as Mrs. Forster, and the probability of her being yet more imprudent with such a companion at Brighton, where the temptations must be greater than at home. He heard her attentively, and then said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1018, "text": "“Lydia will never be easy till she has exposed herself in some public place or other, and we can never expect her to do it with so little expense or inconvenience to her family as under the present circumstances.”"}
{"doc_id": "gutenberg_1342", "para_id": 1019, "text": "“If you were aware,” said Elizabeth, “of the very great disadvantage to us all, which must arise from the public notice of Lydia’s unguarded and imprudent manner, nay, which has already arisen from it, I am sure you would judge differently in the affair.”"}
{"doc_id": "gutenberg_1342", "para_id": 1020, "text": "“Already arisen!” repeated Mr. Bennet. “What! has she frightened away some of your lovers? Poor little Lizzy! But do not be cast down. Such squeamish youths as cannot bear to be connected with a little absurdity are not worth a regret. Come, let me see the list of the pitiful fellows who have been kept aloof by Lydia’s folly.”"}
{"doc_id": "gutenberg_1342", "para_id": 1021, "text": "“Indeed, you are mistaken. I have no such injuries to resent. It is not of peculiar, but of general evils, which I am now complaining. Our importance, our respectability in the world, must be affected by the wild volatility, the assurance and disdain of all restraint which mark Lydia’s character. Excuse me,--for I must speak plainly. If you, my dear father, will not take the trouble of checking her exuberant spirits, and of teaching her that her present pursuits are not to be the business of her life, she will soon be beyond the reach of amendment. Her character will be fixed; and she will, at sixteen, be the most determined flirt that ever made herself and her family ridiculous;--a flirt, too, in the worst and meanest degree of flirtation; without any attraction beyond youth and a tolerable person; and, from the ignorance and emptiness of her mind, wholly unable to ward off any portion of that universal contempt which her rage for admiration will excite. In this danger Kitty is also comprehended. She will follow wherever Lydia leads. Vain, ignorant, idle, and absolutely uncontrolled! Oh, my dear father, can you suppose it possible that they will not be censured and despised wherever they are known, and that their sisters will not be often involved in the disgrace?”"}
{"doc_id": "gutenberg_1342", "para_id": 1022, "text": "Mr. Bennet saw that her whole heart was in the subject; and, affectionately taking her hand, said, in reply,--"}
{"doc_id": "gutenberg_1342", "para_id": 1023, "text": "“Do not make yourself uneasy, my love. Wherever you and Jane are known, you must be respected and valued; and you will not appear to less advantage for having a couple of--or I may say, three--very silly sisters. We shall have no peace at Longbourn if Lydia does not go to Brighton. Let her go, then. Colonel Forster is a sensible man, and will keep her out of any real mischief; and she is luckily too poor to be an object of prey to anybody. At Brighton she will be of less importance even as a common flirt than she has been here. The officers will find women better worth their notice. Let us hope, therefore, that her being there may teach her her own insignificance. At any rate, she cannot grow many degrees worse, without authorizing us to lock her up for the rest of her life.”"}
{"doc_id": "gutenberg_1342", "para_id": 1024, "text": "With this answer Elizabeth was forced to be content; but her own opinion continued the same, and she left him disappointed and sorry. It was not in her nature, however, to increase her vexations by dwelling on them. She was confident of having performed her duty; and to fret over unavoidable evils, or augment them by anxiety, was no part of her disposition."}
{"doc_id": "gutenberg_1342", "para_id": 1025, "text": "Had Lydia and her mother known the substance of her conference with her father, their indignation would hardly have found expression in their united volubility. In Lydia’s imagination, a visit to Brighton comprised every possibility of earthly happiness. She saw, with the creative eye of fancy, the streets of that gay bathing-place covered with officers. She saw herself the object of attention to tens and to scores of them at present unknown. She saw all the glories of the camp: its tents stretched forth in beauteous uniformity of lines, crowded with the young and the gay, and dazzling with scarlet; and, to complete the view, she saw herself seated beneath a tent, tenderly flirting with at least six officers at once."}
{"doc_id": "gutenberg_1342", "para_id": 1026, "text": "Had she known that her sister sought to tear her from such prospects and such realities as these, what would have been her sensations? They could have been understood only by her mother, who might have felt nearly the same. Lydia’s going to Brighton was all that consoled her for the melancholy conviction of her husband’s never intending to go there himself."}
{"doc_id": "gutenberg_1342", "para_id": 1027, "text": "But they were entirely ignorant of what had passed; and their raptures continued, with little intermission, to the very day of Lydia’s leaving home."}
{"doc_id": "gutenberg_1342", "para_id": 1028, "text": "Elizabeth was now to see Mr. Wickham for the last time. Having been frequently in company with him since her return, agitation was pretty well over; the agitations of former partiality entirely so. She had even learnt to detect, in the very gentleness which had first delighted her, an affectation and a sameness to disgust and weary. In his present behaviour to herself, moreover, she had a fresh source of displeasure; for the inclination he soon testified of renewing those attentions which had marked the early part of their acquaintance could only serve, after what had since passed, to provoke her. She lost all concern for him in finding herself thus selected as the object of such idle and frivolous gallantry; and while she steadily repressed it, could not but feel the reproof contained in his believing, that however long, and for whatever cause, his attentions had been withdrawn, her vanity would be gratified, and her preference secured, at any time, by their renewal."}
{"doc_id": "gutenberg_1342", "para_id": 1029, "text": "On the very last day of the regiment’s remaining in Meryton, he dined, with others of the officers, at Longbourn; and so little was Elizabeth disposed to part from him in good-humour, that, on his making some inquiry as to the manner in which her time had passed at Hunsford, she mentioned Colonel Fitzwilliam’s and Mr. Darcy’s having both spent three weeks at Rosings, and asked him if he were acquainted with the former."}
{"doc_id": "gutenberg_1342", "para_id": 1030, "text": "He looked surprised, displeased, alarmed; but, with a moment’s recollection, and a returning smile, replied, that he had formerly seen him often; and, after observing that he was a very gentlemanlike man, asked her how she had liked him. Her answer was warmly in his favour. With an air of indifference, he soon afterwards added, “How long did you say that he was at Rosings?”"}
{"doc_id": "gutenberg_1342", "para_id": 1031, "text": "“Indeed!” cried Wickham, with a look which did not escape her. “And pray may I ask--” but checking himself, he added, in a gayer tone, “Is it in address that he improves? Has he deigned to add aught of civility to his ordinary style? for I dare not hope,” he continued, in a lower and more serious tone, “that he is improved in essentials.”"}
{"doc_id": "gutenberg_1342", "para_id": 1032, "text": "“Oh, no!” said Elizabeth. “In essentials, I believe, he is very much what he ever was.”"}
{"doc_id": "gutenberg_1342", "para_id": 1033, "text": "While she spoke, Wickham looked as if scarcely knowing whether to rejoice over her words or to distrust their meaning. There was a something in her countenance which made him listen with an apprehensive and anxious attention, while she added,--"}
{"doc_id": "gutenberg_1342", "para_id": 1034, "text": "“When I said that he improved on acquaintance, I did not mean that either his mind or manners were in a state of improvement; but that, from knowing him better, his disposition was better understood.”"}
{"doc_id": "gutenberg_1342", "para_id": 1035, "text": "Wickham’s alarm now appeared in a heightened complexion and agitated look; for a few minutes he was silent; till, shaking off his embarrassment, he turned to her again, and said in the gentlest of accents,--"}
{"doc_id": "gutenberg_1342", "para_id": 1036, "text": "“You, who so well know my feelings towards Mr. Darcy, will readily comprehend how sincerely I must rejoice that he is wise enough to assume even the _appearance_ of what is right. His pride, in that direction, may be of service, if not to himself, to many others, for it must deter him from such foul misconduct as I have suffered by. I only fear that the sort of cautiousness to which you, I imagine, have been alluding, is merely adopted on his visits to his aunt, of whose good opinion and judgment he stands much in awe. His fear of her has always operated, I know, when they were together; and a good deal is to be imputed to his wish of forwarding the match with Miss de Bourgh, which I am certain he has very much at heart.”"}
{"doc_id": "gutenberg_1342", "para_id": 1037, "text": "Elizabeth could not repress a smile at this, but she answered only by a slight inclination of the head. She saw that he wanted to engage her on the old subject of his grievances, and she was in no humour to indulge him. The rest of the evening passed with the _appearance_, on his side, of usual cheerfulness, but with no further attempt to distinguish Elizabeth; and they parted at last with mutual civility, and possibly a mutual desire of never meeting again."}
{"doc_id": "gutenberg_1342", "para_id": 1038, "text": "When the party broke up, Lydia returned with Mrs. Forster to Meryton, from whence they were to set out early the next morning. The separation between her and her family was rather noisy than pathetic. Kitty was the only one who shed tears; but she did weep from vexation and envy. Mrs. Bennet was diffuse in her good wishes for the felicity of her daughter, and impressive in her injunctions that she would not miss the opportunity of enjoying herself as much as possible,--advice which there was every reason to believe would be attended to; and, in the clamorous happiness of Lydia herself in bidding farewell, the more gentle adieus of her sisters were uttered without being heard."}
{"doc_id": "gutenberg_1342", "para_id": 1039, "text": "Had Elizabeth’s opinion been all drawn from her own family, she could not have formed a very pleasing picture of conjugal felicity or domestic comfort. Her father, captivated by youth and beauty, and that appearance of good-humour which youth and beauty generally give, had married a woman whose weak understanding and illiberal mind had very early in their marriage put an end to all real affection for her. Respect, esteem, and confidence had vanished for ever; and all his views of domestic happiness were overthrown. But Mr. Bennet was not of a disposition to seek comfort for the disappointment which his own imprudence had brought on in any of those pleasures which too often console the unfortunate for their folly or their vice. He was fond of the country and of books; and from these tastes had arisen his principal enjoyments. To his wife he was very little otherwise indebted than as her ignorance and folly had contributed to his amusement. This is not the sort of happiness which a man would in general wish to owe to his wife; but where other powers of entertainment are wanting, the true philosopher will derive benefit from such as are given."}
{"doc_id": "gutenberg_1342", "para_id": 1040, "text": "Elizabeth, however, had never been blind to the impropriety of her father’s behaviour as a husband. She had always seen it with pain; but respecting his abilities, and grateful for his affectionate treatment of herself, she endeavoured to forget what she could not overlook, and to banish from her thoughts that continual breach of conjugal obligation and decorum which, in exposing his wife to the contempt of her own children, was so highly reprehensible. But she had never felt so strongly as now the disadvantages which must attend the children of so unsuitable a marriage, nor ever been so fully aware of the evils arising from so ill-judged a direction of talents--talents which, rightly used, might at least have preserved the respectability of his daughters, even if incapable of enlarging the mind of his wife."}
{"doc_id": "gutenberg_1342", "para_id": 1041, "text": "When Elizabeth had rejoiced over Wickham’s departure, she found little other cause for satisfaction in the loss of the regiment. Their parties abroad were less varied than before; and at home she had a mother and sister, whose constant repinings at the dulness of everything around them threw a real gloom over their domestic circle; and, though Kitty might in time regain her natural degree of sense, since the disturbers of her brain were removed, her other sister, from whose disposition greater evil might be apprehended, was likely to be hardened in all her folly and assurance, by a situation of such double danger as a watering-place and a camp. Upon the whole, therefore, she found, what has been sometimes found before, that an event to which she had looked forward with impatient desire, did not, in taking place, bring all the satisfaction she had promised herself. It was consequently necessary to name some other period for the commencement of actual felicity; to have some other point on which her wishes and hopes might be fixed, and by again enjoying the pleasure of anticipation, console herself for the present, and prepare for another disappointment. Her tour to the Lakes was now the object of her happiest thoughts: it was her best consolation for all the uncomfortable hours which the discontentedness of her mother and Kitty made inevitable; and could she have included Jane in the scheme, every part of it would have been perfect."}
{"doc_id": "gutenberg_1342", "para_id": 1042, "text": "“But it is fortunate,” thought she, “that I have something to wish for. Were the whole arrangement complete, my disappointment would be certain. But here, by carrying with me one ceaseless source of regret in my sister’s absence, I may reasonably hope to have all my expectations of pleasure realized. A scheme of which every part promises delight can never be successful; and general disappointment is only warded off by the defence of some little peculiar vexation.”"}
{"doc_id": "gutenberg_1342", "para_id": 1043, "text": "When Lydia went away she promised to write very often and very minutely to her mother and Kitty; but her letters were always long expected, and always very short. Those to her mother contained little else than that they were just returned from the library, where such and such officers had attended them, and where she had seen such beautiful ornaments as made her quite wild; that she had a new gown, or a new parasol, which she would have described more fully, but was obliged to leave off in a violent hurry, as Mrs. Forster called her, and they were going to the camp; and from her correspondence with her sister there was still less to be learnt, for her letters to Kitty, though rather longer, were much too full of lines under the words to be made public."}
{"doc_id": "gutenberg_1342", "para_id": 1044, "text": "After the first fortnight or three weeks of her absence, health, good-humour, and cheerfulness began to reappear at Longbourn. Everything wore a happier aspect. The families who had been in town for the winter came back again, and summer finery and summer engagements arose. Mrs. Bennet was restored to her usual querulous serenity; and by the middle of June Kitty was so much recovered as to be able to enter Meryton without tears,--an event of such happy promise as to make Elizabeth hope, that by the following Christmas she might be so tolerably reasonable as not to mention an officer above once a day, unless, by some cruel and malicious arrangement at the War Office, another regiment should be quartered in Meryton."}
{"doc_id": "gutenberg_1342", "para_id": 1045, "text": "The time fixed for the beginning of their northern tour was now fast approaching; and a fortnight only was wanting of it, when a letter arrived from Mrs. Gardiner, which at once delayed its commencement and curtailed its extent. Mr. Gardiner would be prevented by business from setting out till a fortnight later in July, and must be in London again within a month; and as that left too short a period for them to go so far, and see so much as they had proposed, or at least to see it with the leisure and comfort they had built on, they were obliged to give up the Lakes, and substitute a more contracted tour; and, according to the present plan, were to go no farther northward than Derbyshire. In that county there was enough to be seen to occupy the chief of their three weeks; and to Mrs. Gardiner it had a peculiarly strong attraction. The town where she had formerly passed some years of her life, and where they were now to spend a few days, was probably as great an object of her curiosity as all the celebrated beauties of Matlock, Chatsworth, Dovedale, or the Peak."}
{"doc_id": "gutenberg_1342", "para_id": 1046, "text": "Elizabeth was excessively disappointed: she had set her heart on seeing the Lakes; and still thought there might have been time enough. But it was her business to be satisfied--and certainly her temper to be happy; and all was soon right again."}
{"doc_id": "gutenberg_1342", "para_id": 1047, "text": "With the mention of Derbyshire, there were many ideas connected. It was impossible for her to see the word without thinking of Pemberley and its owner. “But surely,” said she, “I may enter his county with impunity, and rob it of a few petrified spars, without his perceiving me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1048, "text": "The period of expectation was now doubled. Four weeks were to pass away before her uncle and aunt’s arrival. But they did pass away, and Mr. and Mrs. Gardiner, with their four children, did at length appear at Longbourn. The children, two girls of six and eight years old, and two younger boys, were to be left under the particular care of their cousin Jane, who was the general favourite, and whose steady sense and sweetness of temper exactly adapted her for attending to them in every way--teaching them, playing with them, and loving them."}
{"doc_id": "gutenberg_1342", "para_id": 1049, "text": "The Gardiners stayed only one night at Longbourn, and set off the next morning with Elizabeth in pursuit of novelty and amusement. One enjoyment was certain--that of suitableness as companions; a suitableness which comprehended health and temper to bear inconveniences--cheerfulness to enhance every pleasure--and affection and intelligence, which might supply it among themselves if there were disappointments abroad."}
{"doc_id": "gutenberg_1342", "para_id": 1050, "text": "It is not the object of this work to give a description of Derbyshire, nor of any of the remarkable places through which their route thither lay--Oxford, Blenheim, Warwick, Kenilworth, Birmingham, etc., are sufficiently known. A small part of Derbyshire is all the present concern. To the little town of Lambton, the scene of Mrs. Gardiner’s former residence, and where she had lately learned that some acquaintance still remained, they bent their steps, after having seen all the principal wonders of the country; and within five miles of Lambton, Elizabeth found, from her aunt, that Pemberley was situated. It was not in their direct road; nor more than a mile or two out of it. In talking over their route the evening before, Mrs. Gardiner expressed an inclination to see the place again. Mr. Gardiner declared his willingness, and Elizabeth was applied to for her approbation."}
{"doc_id": "gutenberg_1342", "para_id": 1051, "text": "“My love, should not you like to see a place of which you have heard so much?” said her aunt. “A place, too, with which so many of your acquaintance are connected. Wickham passed all his youth there, you know.”"}
{"doc_id": "gutenberg_1342", "para_id": 1052, "text": "Elizabeth was distressed. She felt that she had no business at Pemberley, and was obliged to assume a disinclination for seeing it. She must own that she was tired of great houses: after going over so many, she really had no pleasure in fine carpets or satin curtains."}
{"doc_id": "gutenberg_1342", "para_id": 1053, "text": "Mrs. Gardiner abused her stupidity. “If it were merely a fine house richly furnished,” said she, “I should not care about it myself; but the grounds are delightful. They have some of the finest woods in the country.”"}
{"doc_id": "gutenberg_1342", "para_id": 1054, "text": "Elizabeth said no more; but her mind could not acquiesce. The possibility of meeting Mr. Darcy, while viewing the place, instantly occurred. It would be dreadful! She blushed at the very idea; and thought it would be better to speak openly to her aunt, than to run such a risk. But against this there were objections; and she finally resolved that it could be the last resource, if her private inquiries as to the absence of the family were unfavourably answered."}
{"doc_id": "gutenberg_1342", "para_id": 1055, "text": "Accordingly, when she retired at night, she asked the chambermaid whether Pemberley were not a very fine place, what was the name of its proprietor, and, with no little alarm, whether the family were down for the summer? A most welcome negative followed the last question; and her alarms being now removed, she was at leisure to feel a great deal of curiosity to see the house herself; and when the subject was revived the next morning, and she was again applied to, could readily answer, and with a proper air of indifference, that she had not really any dislike to the scheme."}
{"doc_id": "gutenberg_1342", "para_id": 1056, "text": "Elizabeth, as they drove along, watched for the first appearance of Pemberley Woods with some perturbation; and when at length they turned in at the lodge, her spirits were in a high flutter."}
{"doc_id": "gutenberg_1342", "para_id": 1057, "text": "The park was very large, and contained great variety of ground. They entered it in one of its lowest points, and drove for some time through a beautiful wood stretching over a wide extent."}
{"doc_id": "gutenberg_1342", "para_id": 1058, "text": "Elizabeth’s mind was too full for conversation, but she saw and admired every remarkable spot and point of view. They gradually ascended for half a mile, and then found themselves at the top of a considerable eminence, where the wood ceased, and the eye was instantly caught by Pemberley House, situated on the opposite side of the valley, into which the road with some abruptness wound. It was a large, handsome stone building, standing well on rising ground, and backed by a ridge of high woody hills; and in front a stream of some natural importance was swelled into greater, but without any artificial appearance. Its banks were neither formal nor falsely adorned. Elizabeth was delighted. She had never seen a place for which nature had done more, or where natural beauty had been so little counteracted by an awkward taste. They were all of them warm in their admiration; and at that moment she felt that to be mistress of Pemberley might be something!"}
{"doc_id": "gutenberg_1342", "para_id": 1059, "text": "They descended the hill, crossed the bridge, and drove to the door; and, while examining the nearer aspect of the house, all her apprehension of meeting its owner returned. She dreaded lest the chambermaid had been mistaken. On applying to see the place, they were admitted into the hall; and Elizabeth, as they waited for the housekeeper, had leisure to wonder at her being where she was."}
{"doc_id": "gutenberg_1342", "para_id": 1060, "text": "The housekeeper came; a respectable looking elderly woman, much less fine, and more civil, than she had any notion of finding her. They followed her into the dining-parlour. It was a large, well-proportioned room, handsomely fitted up. Elizabeth, after slightly surveying it, went to a window to enjoy its prospect. The hill, crowned with wood, from which they had descended, receiving increased abruptness from the distance, was a beautiful object. Every disposition of the ground was good; and she looked on the whole scene, the river, the trees scattered on its banks, and the winding of the valley, as far as she could trace it, with delight. As they passed into other rooms, these objects were taking different positions; but from every window there were beauties to be seen. The rooms were lofty and handsome, and their furniture suitable to the fortune of their proprietor; but Elizabeth saw, with admiration of his taste, that it was neither gaudy nor uselessly fine,--with less of splendour, and more real elegance, than the furniture of Rosings."}
{"doc_id": "gutenberg_1342", "para_id": 1061, "text": "“And of this place,” thought she, “I might have been mistress! With these rooms I might have now been familiarly acquainted! Instead of viewing them as a stranger, I might have rejoiced in them as my own, and welcomed to them as visitors my uncle and aunt. But, no,” recollecting herself, “that could never be; my uncle and aunt would have been lost to me; I should not have been allowed to invite them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1062, "text": "She longed to inquire of the housekeeper whether her master were really absent, but had not courage for it. At length, however, the question was asked by her uncle; and she turned away with alarm, while Mrs. Reynolds replied, that he was; adding, “But we expect him to-morrow, with a large party of friends.” How rejoiced was Elizabeth that their own journey had not by any circumstance been delayed a day!"}
{"doc_id": "gutenberg_1342", "para_id": 1063, "text": "Her aunt now called her to look at a picture. She approached, and saw the likeness of Mr. Wickham, suspended, amongst several other miniatures, over the mantel-piece. Her aunt asked her, smilingly, how she liked it. The housekeeper came forward, and told them it was the picture of a young gentleman, the son of her late master’s steward, who had been brought up by him at his own expense. “He is now gone into the army,” she added; “but I am afraid he has turned out very wild.”"}
{"doc_id": "gutenberg_1342", "para_id": 1064, "text": "Mrs. Gardiner looked at her niece with a smile, but Elizabeth could not return it."}
{"doc_id": "gutenberg_1342", "para_id": 1065, "text": "“And that,” said Mrs. Reynolds, pointing to another of the miniatures, “is my master--and very like him. It was drawn at the same time as the other--about eight years ago.”"}
{"doc_id": "gutenberg_1342", "para_id": 1066, "text": "“I have heard much of your master’s fine person,” said Mrs. Gardiner, looking at the picture; “it is a handsome face. But, Lizzy, you can tell us whether it is like or not.”"}
{"doc_id": "gutenberg_1342", "para_id": 1067, "text": "Mrs. Reynolds’ respect for Elizabeth seemed to increase on this intimation of her knowing her master."}
{"doc_id": "gutenberg_1342", "para_id": 1068, "text": "“I am sure _I_ know none so handsome; but in the gallery upstairs you will see a finer, larger picture of him than this. This room was my late master’s favourite room, and these miniatures are just as they used to be then. He was very fond of them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1069, "text": "Mrs. Reynolds then directed their attention to one of Miss Darcy, drawn when she was only eight years old."}
{"doc_id": "gutenberg_1342", "para_id": 1070, "text": "“Oh, yes--the handsomest young lady that ever was seen; and so accomplished! She plays and sings all day long. In the next room is a new instrument just come down for her--a present from my master: she comes here to-morrow with him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1071, "text": "Mr. Gardiner, whose manners were easy and pleasant, encouraged her communicativeness by his questions and remarks: Mrs. Reynolds, either from pride or attachment, had evidently great pleasure in talking of her master and his sister."}
{"doc_id": "gutenberg_1342", "para_id": 1072, "text": "“Not so much as I could wish, sir: but I dare say he may spend half his time here; and Miss Darcy is always down for the summer months.”"}
{"doc_id": "gutenberg_1342", "para_id": 1073, "text": "“Yes, sir; but I do not know when _that_ will be. I do not know who is good enough for him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1074, "text": "Mr. and Mrs. Gardiner smiled. Elizabeth could not help saying, “It is very much to his credit, I am sure, that you should think so.”"}
{"doc_id": "gutenberg_1342", "para_id": 1075, "text": "“I say no more than the truth, and what everybody will say that knows him,” replied the other. Elizabeth thought this was going pretty far; and she listened with increasing astonishment as the housekeeper added, “I have never had a cross word from him in my life, and I have known him ever since he was four years old.”"}
{"doc_id": "gutenberg_1342", "para_id": 1076, "text": "This was praise of all others most extraordinary, most opposite to her ideas. That he was not a good-tempered man had been her firmest opinion. Her keenest attention was awakened: she longed to hear more; and was grateful to her uncle for saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 1077, "text": "“There are very few people of whom so much can be said. You are lucky in having such a master.”"}
{"doc_id": "gutenberg_1342", "para_id": 1078, "text": "“Yes, sir, I know I am. If I were to go through the world, I could not meet with a better. But I have always observed, that they who are good-natured when children, are good-natured when they grow up; and he was always the sweetest tempered, most generous-hearted boy in the world.”"}
{"doc_id": "gutenberg_1342", "para_id": 1079, "text": "“Yes, ma’am, that he was indeed; and his son will be just like him--just as affable to the poor.”"}
{"doc_id": "gutenberg_1342", "para_id": 1080, "text": "Elizabeth listened, wondered, doubted, and was impatient for more. Mrs. Reynolds could interest her on no other point. She related the subjects of the pictures, the dimensions of the rooms, and the price of the furniture in vain. Mr. Gardiner, highly amused by the kind of family prejudice, to which he attributed her excessive commendation of her master, soon led again to the subject; and she dwelt with energy on his many merits, as they proceeded together up the great staircase."}
{"doc_id": "gutenberg_1342", "para_id": 1081, "text": "“He is the best landlord, and the best master,” said she, “that ever lived. Not like the wild young men now-a-days, who think of nothing but themselves. There is not one of his tenants or servants but what will give him a good name. Some people call him proud; but I am sure I never saw anything of it. To my fancy, it is only because he does not rattle away like other young men.”"}
{"doc_id": "gutenberg_1342", "para_id": 1082, "text": "“This fine account of him,” whispered her aunt as they walked, “is not quite consistent with his behaviour to our poor friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 1083, "text": "On reaching the spacious lobby above, they were shown into a very pretty sitting-room, lately fitted up with greater elegance and lightness than the apartments below; and were informed that it was but just done to give pleasure to Miss Darcy, who had taken a liking to the room, when last at Pemberley."}
{"doc_id": "gutenberg_1342", "para_id": 1084, "text": "“He is certainly a good brother,” said Elizabeth, as she walked towards one of the windows."}
{"doc_id": "gutenberg_1342", "para_id": 1085, "text": "Mrs. Reynolds anticipated Miss Darcy’s delight, when she should enter the room. “And this is always the way with him,” she added. “Whatever can give his sister any pleasure, is sure to be done in a moment. There is nothing he would not do for her.”"}
{"doc_id": "gutenberg_1342", "para_id": 1086, "text": "The picture gallery, and two or three of the principal bed-rooms, were all that remained to be shown. In the former were many good paintings: but Elizabeth knew nothing of the art; and from such as had been already visible below, she had willingly turned to look at some drawings of Miss Darcy’s, in crayons, whose subjects were usually more interesting, and also more intelligible."}
{"doc_id": "gutenberg_1342", "para_id": 1087, "text": "In the gallery there were many family portraits, but they could have little to fix the attention of a stranger. Elizabeth walked on in quest of the only face whose features would be known to her. At last it arrested her--and she beheld a striking resemblance of Mr. Darcy, with such a smile over the face, as she remembered to have sometimes seen, when he looked at her. She stood several minutes before the picture, in earnest contemplation, and returned to it again before they quitted the gallery. Mrs. Reynolds informed them, that it had been taken in his father’s lifetime."}
{"doc_id": "gutenberg_1342", "para_id": 1088, "text": "There was certainly at this moment, in Elizabeth’s mind, a more gentle sensation towards the original than she had ever felt in the height of their acquaintance. The commendation bestowed on him by Mrs. Reynolds was of no trifling nature. What praise is more valuable than the praise of an intelligent servant? As a brother, a landlord, a master, she considered how many people’s happiness were in his guardianship! How much of pleasure or pain it was in his power to bestow! How much of good or evil must be done by him! Every idea that had been brought forward by the housekeeper was favourable to his character; and as she stood before the canvas, on which he was represented, and fixed his eyes upon herself, she thought of his regard with a deeper sentiment of gratitude than it had ever raised before: she remembered its warmth, and softened its impropriety of expression."}
{"doc_id": "gutenberg_1342", "para_id": 1089, "text": "When all of the house that was open to general inspection had been seen, they returned down stairs; and, taking leave of the housekeeper, were consigned over to the gardener, who met them at the hall door."}
{"doc_id": "gutenberg_1342", "para_id": 1090, "text": "As they walked across the lawn towards the river, Elizabeth turned back to look again; her uncle and aunt stopped also; and while the former was conjecturing as to the date of the building, the owner of it himself suddenly came forward from the road which led behind it to the stables."}
{"doc_id": "gutenberg_1342", "para_id": 1091, "text": "They were within twenty yards of each other; and so abrupt was his appearance, that it was impossible to avoid his sight. Their eyes instantly met, and the cheeks of each were overspread with the deepest blush. He absolutely started, and for a moment seemed immovable from surprise; but shortly recovering himself, advanced towards the party, and spoke to Elizabeth, if not in terms of perfect composure, at least of perfect civility."}
{"doc_id": "gutenberg_1342", "para_id": 1092, "text": "She had instinctively turned away; but stopping on his approach, received his compliments with an embarrassment impossible to be overcome. Had his first appearance, or his resemblance to the picture they had just been examining, been insufficient to assure the other two that they now saw Mr. Darcy, the gardener’s expression of surprise, on beholding his master, must immediately have told it. They stood a little aloof while he was talking to their niece, who, astonished and confused, scarcely dared lift her eyes to his face, and knew not what answer she returned to his civil inquiries after her family. Amazed at the alteration of his manner since they last parted, every sentence that he uttered was increasing her embarrassment; and every idea of the impropriety of her being found there recurring to her mind, the few minutes in which they continued together were some of the most uncomfortable of her life. Nor did he seem much more at ease; when he spoke, his accent had none of its usual sedateness; and he repeated his inquiries as to the time of her having left Longbourn, and of her stay in Derbyshire, so often, and in so hurried a way, as plainly spoke the distraction of his thoughts."}
{"doc_id": "gutenberg_1342", "para_id": 1093, "text": "At length, every idea seemed to fail him; and after standing a few moments without saying a word, he suddenly recollected himself, and took leave."}
{"doc_id": "gutenberg_1342", "para_id": 1094, "text": "The others then joined her, and expressed their admiration of his figure; but Elizabeth heard not a word, and, wholly engrossed by her own feelings, followed them in silence. She was overpowered by shame and vexation. Her coming there was the most unfortunate, the most ill-judged thing in the world! How strange must it appear to him! In what a disgraceful light might it not strike so vain a man! It might seem as if she had purposely thrown herself in his way again! Oh! why did she come? or, why did he thus come a day before he was expected? Had they been only ten minutes sooner, they should have been beyond the reach of his discrimination; for it was plain that he was that moment arrived, that moment alighted from his horse or his carriage. She blushed again and again over the perverseness of the meeting. And his behaviour, so strikingly altered,--what could it mean? That he should even speak to her was amazing!--but to speak with such civility, to inquire after her family! Never in her life had she seen his manners so little dignified, never had he spoken with such gentleness as on this unexpected meeting. What a contrast did it offer to his last address in Rosings Park, when he put his letter into her hand! She knew not what to think, or how to account for it."}
{"doc_id": "gutenberg_1342", "para_id": 1095, "text": "They had now entered a beautiful walk by the side of the water, and every step was bringing forward a nobler fall of ground, or a finer reach of the woods to which they were approaching: but it was some time before Elizabeth was sensible of any of it; and, though she answered mechanically to the repeated appeals of her uncle and aunt, and seemed to direct her eyes to such objects as they pointed out, she distinguished no part of the scene. Her thoughts were all fixed on that one spot of Pemberley House, whichever it might be, where Mr. Darcy then was. She longed to know what at that moment was passing in his mind; in what manner he thought of her, and whether, in defiance of everything, she was still dear to him. Perhaps he had been civil only because he felt himself at ease; yet there had been _that_ in his voice, which was not like ease. Whether he had felt more of pain or of pleasure in seeing her, she could not tell, but he certainly had not seen her with composure."}
{"doc_id": "gutenberg_1342", "para_id": 1096, "text": "At length, however, the remarks of her companions on her absence of mind roused her, and she felt the necessity of appearing more like herself."}
{"doc_id": "gutenberg_1342", "para_id": 1097, "text": "They entered the woods, and, bidding adieu to the river for a while, ascended some of the higher grounds; whence, in spots where the opening of the trees gave the eye power to wander, were many charming views of the valley, the opposite hills, with the long range of woods overspreading many, and occasionally part of the stream. Mr. Gardiner expressed a wish of going round the whole park, but feared it might be beyond a walk. With a triumphant smile, they were told, that it was ten miles round. It settled the matter; and they pursued the accustomed circuit; which brought them again, after some time, in a descent among hanging woods, to the edge of the water, and one of its narrowest parts. They crossed it by a simple bridge, in character with the general air of the scene: it was a spot less adorned than any they had yet visited; and the valley, here contracted into a glen, allowed room only for the stream, and a narrow walk amidst the rough coppice-wood which bordered it. Elizabeth longed to explore its windings; but when they had crossed the bridge, and perceived their distance from the house, Mrs. Gardiner, who was not a great walker, could go no farther, and thought only of returning to the carriage as quickly as possible. Her niece was, therefore, obliged to submit, and they took their way towards the house on the opposite side of the river, in the nearest direction; but their progress was slow, for Mr. Gardiner, though seldom able to indulge the taste, was very fond of fishing, and was so much engaged in watching the occasional appearance of some trout in the water, and talking to the man about them, that he advanced but little. Whilst wandering on in this slow manner, they were again surprised, and Elizabeth’s astonishment was quite equal to what it had been at first, by the sight of Mr. Darcy approaching them, and at no great distance. The walk being here less sheltered than on the other side, allowed them to see him before they met. Elizabeth, however astonished, was at least more prepared for an interview than before, and resolved to appear and to speak with calmness, if he really intended to meet them. For a few moments, indeed, she felt that he would probably strike into some other path. The idea lasted while a turning in the walk concealed him from their view; the turning past, he was immediately before them. With a glance she saw that he had lost none of his recent civility; and, to imitate his politeness, she began as they met to admire the beauty of the place; but she had not got beyond the words “delightful,” and “charming,” when some unlucky recollections obtruded, and she fancied that praise of Pemberley from her might be mischievously construed. Her colour changed, and she said no more."}
{"doc_id": "gutenberg_1342", "para_id": 1098, "text": "Mrs. Gardiner was standing a little behind; and on her pausing, he asked her if she would do him the honour of introducing him to her friends. This was a stroke of civility for which she was quite unprepared; and she could hardly suppress a smile at his being now seeking the acquaintance of some of those very people, against whom his pride had revolted, in his offer to herself. “What will be his surprise,” thought she, “when he knows who they are! He takes them now for people of fashion.”"}
{"doc_id": "gutenberg_1342", "para_id": 1099, "text": "The introduction, however, was immediately made; and as she named their relationship to herself, she stole a sly look at him, to see how he bore it; and was not without the expectation of his decamping as fast as he could from such disgraceful companions. That he was _surprised_ by the connection was evident: he sustained it, however, with fortitude: and, so far from going away, turned back with them, and entered into conversation with Mr. Gardiner. Elizabeth could not but be pleased, could not but triumph. It was consoling that he should know she had some relations for whom there was no need to blush. She listened most attentively to all that passed between them, and gloried in every expression, every sentence of her uncle, which marked his intelligence, his taste, or his good manners."}
{"doc_id": "gutenberg_1342", "para_id": 1100, "text": "The conversation soon turned upon fishing; and she heard Mr. Darcy invite him, with the greatest civility, to fish there as often as he chose, while he continued in the neighbourhood, offering at the same time to supply him with fishing tackle, and pointing out those parts of the stream where there was usually most sport. Mrs. Gardiner, who was walking arm in arm with Elizabeth, gave her a look expressive of her wonder. Elizabeth said nothing, but it gratified her exceedingly; the compliment must be all for herself. Her astonishment, however, was extreme; and continually was she repeating, “Why is he so altered? From what can it proceed? It cannot be for _me_, it cannot be for _my_ sake that his manners are thus softened. My reproofs at Hunsford could not work such a change as this. It is impossible that he should still love me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1101, "text": "After walking some time in this way, the two ladies in front, the two gentlemen behind, on resuming their places, after descending to the brink of the river for the better inspection of some curious water-plant, there chanced to be a little alteration. It originated in Mrs. Gardiner, who, fatigued by the exercise of the morning, found Elizabeth’s arm inadequate to her support, and consequently preferred her husband’s. Mr. Darcy took her place by her niece, and they walked on together. After a short silence the lady first spoke. She wished him to know that she had been assured of his absence before she came to the place, and accordingly began by observing, that his arrival had been very unexpected--“for your housekeeper,” she added, “informed us that you would certainly not be here till to-morrow; and, indeed, before we left Bakewell, we understood that you were not immediately expected in the country.” He acknowledged the truth of it all; and said that business with his steward had occasioned his coming forward a few hours before the rest of the party with whom he had been travelling. “They will join me early to-morrow,” he continued, “and among them are some who will claim an acquaintance with you,--Mr. Bingley and his sisters.”"}
{"doc_id": "gutenberg_1342", "para_id": 1102, "text": "Elizabeth answered only by a slight bow. Her thoughts were instantly driven back to the time when Mr. Bingley’s name had been last mentioned between them; and if she might judge from his complexion, _his_ mind was not very differently engaged."}
{"doc_id": "gutenberg_1342", "para_id": 1103, "text": "“There is also one other person in the party,” he continued after a pause, “who more particularly wishes to be known to you. Will you allow me, or do I ask too much, to introduce my sister to your acquaintance during your stay at Lambton?”"}
{"doc_id": "gutenberg_1342", "para_id": 1104, "text": "The surprise of such an application was great indeed; it was too great for her to know in what manner she acceded to it. She immediately felt that whatever desire Miss Darcy might have of being acquainted with her, must be the work of her brother, and without looking farther, it was satisfactory; it was gratifying to know that his resentment had not made him think really ill of her."}
{"doc_id": "gutenberg_1342", "para_id": 1105, "text": "They now walked on in silence; each of them deep in thought. Elizabeth was not comfortable; that was impossible; but she was flattered and pleased. His wish of introducing his sister to her was a compliment of the highest kind. They soon outstripped the others; and when they had reached the carriage, Mr. and Mrs. Gardiner were half a quarter of a mile behind."}
{"doc_id": "gutenberg_1342", "para_id": 1106, "text": "He then asked her to walk into the house--but she declared herself not tired, and they stood together on the lawn. At such a time much might have been said, and silence was very awkward. She wanted to talk, but there seemed an embargo on every subject. At last she recollected that she had been travelling, and they talked of Matlock and Dovedale with great perseverance. Yet time and her aunt moved slowly--and her patience and her ideas were nearly worn out before the _tête-à-tête_ was over."}
{"doc_id": "gutenberg_1342", "para_id": 1107, "text": "On Mr. and Mrs. Gardiner’s coming up they were all pressed to go into the house and take some refreshment; but this was declined, and they parted on each side with the utmost politeness. Mr. Darcy handed the ladies into the carriage; and when it drove off, Elizabeth saw him walking slowly towards the house."}
{"doc_id": "gutenberg_1342", "para_id": 1108, "text": "The observations of her uncle and aunt now began; and each of them pronounced him to be infinitely superior to anything they had expected."}
{"doc_id": "gutenberg_1342", "para_id": 1109, "text": "“There _is_ something a little stately in him, to be sure,” replied her aunt; “but it is confined to his air, and is not unbecoming. I can now say with the housekeeper, that though some people may call him proud, _I_ have seen nothing of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1110, "text": "“I was never more surprised than by his behaviour to us. It was more than civil; it was really attentive; and there was no necessity for such attention. His acquaintance with Elizabeth was very trifling.”"}
{"doc_id": "gutenberg_1342", "para_id": 1111, "text": "“To be sure, Lizzy,” said her aunt, “he is not so handsome as Wickham; or rather he has not Wickham’s countenance, for his features are perfectly good. But how came you to tell us that he was so disagreeable?”"}
{"doc_id": "gutenberg_1342", "para_id": 1112, "text": "Elizabeth excused herself as well as she could: said that she had liked him better when they met in Kent than before, and that she had never seen him so pleasant as this morning."}
{"doc_id": "gutenberg_1342", "para_id": 1113, "text": "“But perhaps he may be a little whimsical in his civilities,” replied her uncle. “Your great men often are; and therefore I shall not take him at his word about fishing, as he might change his mind another day, and warn me off his grounds.”"}
{"doc_id": "gutenberg_1342", "para_id": 1114, "text": "“From what we have seen of him,” continued Mrs. Gardiner, “I really should not have thought that he could have behaved in so cruel a way by anybody as he has done by poor Wickham. He has not an ill-natured look. On the contrary, there is something pleasing about his mouth when he speaks. And there is something of dignity in his countenance, that would not give one an unfavourable idea of his heart. But, to be sure, the good lady who showed us the house did give him a most flaming character! I could hardly help laughing aloud sometimes. But he is a liberal master, I suppose, and _that_, in the eye of a servant, comprehends every virtue.”"}
{"doc_id": "gutenberg_1342", "para_id": 1115, "text": "Elizabeth here felt herself called on to say something in vindication of his behaviour to Wickham; and, therefore, gave them to understand, in as guarded a manner as she could, that by what she had heard from his relations in Kent, his actions were capable of a very different construction; and that his character was by no means so faulty, nor Wickham’s so amiable, as they had been considered in Hertfordshire. In confirmation of this, she related the particulars of all the pecuniary transactions in which they had been connected, without actually naming her authority, but stating it to be such as might be relied on."}
{"doc_id": "gutenberg_1342", "para_id": 1116, "text": "Mrs. Gardiner was surprised and concerned: but as they were now approaching the scene of her former pleasures, every idea gave way to the charm of recollection; and she was too much engaged in pointing out to her husband all the interesting spots in its environs, to think of anything else. Fatigued as she had been by the morning’s walk, they had no sooner dined than she set off again in quest of her former acquaintance, and the evening was spent in the satisfactions of an intercourse renewed after many years’ discontinuance."}
{"doc_id": "gutenberg_1342", "para_id": 1117, "text": "The occurrences of the day were too full of interest to leave Elizabeth much attention for any of these new friends; and she could do nothing but think, and think with wonder, of Mr. Darcy’s civility, and, above all, of his wishing her to be acquainted with his sister."}
{"doc_id": "gutenberg_1342", "para_id": 1118, "text": "Elizabeth had settled it that Mr. Darcy would bring his sister to visit her the very day after her reaching Pemberley; and was, consequently, resolved not to be out of sight of the inn the whole of that morning. But her conclusion was false; for on the very morning after their own arrival at Lambton these visitors came. They had been walking about the place with some of their new friends, and were just returned to the inn to dress themselves for dining with the same family, when the sound of a carriage drew them to a window, and they saw a gentleman and lady in a curricle driving up the street. Elizabeth, immediately recognizing the livery, guessed what it meant, and imparted no small degree of surprise to her relations, by acquainting them with the honour which she expected. Her uncle and aunt were all amazement; and the embarrassment of her manner as she spoke, joined to the circumstance itself, and many of the circumstances of the preceding day, opened to them a new idea on the business. Nothing had ever suggested it before, but they now felt that there was no other way of accounting for such attentions from such a quarter than by supposing a partiality for their niece. While these newly-born notions were passing in their heads, the perturbation of Elizabeth’s feelings was every moment increasing. She was quite amazed at her own discomposure; but, amongst other causes of disquiet, she dreaded lest the partiality of the brother should have said too much in her favour; and, more than commonly anxious to please, she naturally suspected that every power of pleasing would fail her."}
{"doc_id": "gutenberg_1342", "para_id": 1119, "text": "She retreated from the window, fearful of being seen; and as she walked up and down the room, endeavouring to compose herself, saw such looks of inquiring surprise in her uncle and aunt as made everything worse."}
{"doc_id": "gutenberg_1342", "para_id": 1120, "text": "Miss Darcy and her brother appeared, and this formidable introduction took place. With astonishment did Elizabeth see that her new acquaintance was at least as much embarrassed as herself. Since her being at Lambton, she had heard that Miss Darcy was exceedingly proud; but the observation of a very few minutes convinced her that she was only exceedingly shy. She found it difficult to obtain even a word from her beyond a monosyllable."}
{"doc_id": "gutenberg_1342", "para_id": 1121, "text": "Miss Darcy was tall, and on a larger scale than Elizabeth; and, though little more than sixteen, her figure was formed, and her appearance womanly and graceful. She was less handsome than her brother, but there was sense and good-humour in her face, and her manners were perfectly unassuming and gentle. Elizabeth, who had expected to find in her as acute and unembarrassed an observer as ever Mr. Darcy had been, was much relieved by discerning such different feelings."}
{"doc_id": "gutenberg_1342", "para_id": 1122, "text": "They had not been long together before Darcy told her that Bingley was also coming to wait on her; and she had barely time to express her satisfaction, and prepare for such a visitor, when Bingley’s quick step was heard on the stairs, and in a moment he entered the room. All Elizabeth’s anger against him had been long done away; but had she still felt any, it could hardly have stood its ground against the unaffected cordiality with which he expressed himself on seeing her again. He inquired in a friendly, though general, way, after her family, and looked and spoke with the same good-humoured ease that he had ever done."}
{"doc_id": "gutenberg_1342", "para_id": 1123, "text": "To Mr. and Mrs. Gardiner he was scarcely a less interesting personage than to herself. They had long wished to see him. The whole party before them, indeed, excited a lively attention. The suspicions which had just arisen of Mr. Darcy and their niece, directed their observation towards each with an earnest, though guarded, inquiry; and they soon drew from those inquiries the full conviction that one of them at least knew what it was to love. Of the lady’s sensations they remained a little in doubt; but that the gentleman was overflowing with admiration was evident enough."}
{"doc_id": "gutenberg_1342", "para_id": 1124, "text": "Elizabeth, on her side, had much to do. She wanted to ascertain the feelings of each of her visitors, she wanted to compose her own, and to make herself agreeable to all; and in the latter object, where she feared most to fail, she was most sure of success, for those to whom she endeavoured to give pleasure were pre-possessed in her favour. Bingley was ready, Georgiana was eager, and Darcy determined, to be pleased."}
{"doc_id": "gutenberg_1342", "para_id": 1125, "text": "In seeing Bingley, her thoughts naturally flew to her sister; and oh! how ardently did she long to know whether any of his were directed in a like manner. Sometimes she could fancy that he talked less than on former occasions, and once or twice pleased herself with the notion that, as he looked at her, he was trying to trace a resemblance. But, though this might be imaginary, she could not be deceived as to his behaviour to Miss Darcy, who had been set up as a rival to Jane. No look appeared on either side that spoke particular regard. Nothing occurred between them that could justify the hopes of his sister. On this point she was soon satisfied; and two or three little circumstances occurred ere they parted, which, in her anxious interpretation, denoted a recollection of Jane, not untinctured by tenderness, and a wish of saying more that might lead to the mention of her, had he dared. He observed to her, at a moment when the others were talking together, and in a tone which had something of real regret, that it “was a very long time since he had had the pleasure of seeing her;” and, before she could reply, he added, “It is above eight months. We have not met since the 26th of November, when we were all dancing together at Netherfield.”"}
{"doc_id": "gutenberg_1342", "para_id": 1126, "text": "Elizabeth was pleased to find his memory so exact; and he afterwards took occasion to ask her, when unattended to by any of the rest, whether _all_ her sisters were at Longbourn. There was not much in the question, nor in the preceding remark; but there was a look and a manner which gave them meaning."}
{"doc_id": "gutenberg_1342", "para_id": 1127, "text": "It was not often that she could turn her eyes on Mr. Darcy himself; but whenever she did catch a glimpse she saw an expression of general complaisance, and in all that he said, she heard an accent so far removed from _hauteur_ or disdain of his companions, as convinced her that the improvement of manners which she had yesterday witnessed, however temporary its existence might prove, had at least outlived one day. When she saw him thus seeking the acquaintance, and courting the good opinion of people with whom any intercourse a few months ago would have been a disgrace; when she saw him thus civil, not only to herself, but to the very relations whom he had openly disdained, and recollected their last lively scene in Hunsford Parsonage, the difference, the change was so great, and struck so forcibly on her mind, that she could hardly restrain her astonishment from being visible. Never, even in the company of his dear friends at Netherfield, or his dignified relations at Rosings, had she seen him so desirous to please, so free from self-consequence or unbending reserve, as now, when no importance could result from the success of his endeavours, and when even the acquaintance of those to whom his attentions were addressed, would draw down the ridicule and censure of the ladies both of Netherfield and Rosings."}
{"doc_id": "gutenberg_1342", "para_id": 1128, "text": "Their visitors stayed with them above half an hour; and when they arose to depart, Mr. Darcy called on his sister to join him in expressing their wish of seeing Mr. and Mrs. Gardiner, and Miss Bennet, to dinner at Pemberley, before they left the country. Miss Darcy, though with a diffidence which marked her little in the habit of giving invitations, readily obeyed. Mrs. Gardiner looked at her niece, desirous of knowing how _she_, whom the invitation most concerned, felt disposed as to its acceptance, but Elizabeth had turned away her head. Presuming, however, that this studied avoidance spoke rather a momentary embarrassment than any dislike of the proposal, and seeing in her husband, who was fond of society, a perfect willingness to accept it, she ventured to engage for her attendance, and the day after the next was fixed on."}
{"doc_id": "gutenberg_1342", "para_id": 1129, "text": "Bingley expressed great pleasure in the certainty of seeing Elizabeth again, having still a great deal to say to her, and many inquiries to make after all their Hertfordshire friends. Elizabeth, construing all this into a wish of hearing her speak of her sister, was pleased; and on this account, as well as some others, found herself, when their visitors left them, capable of considering the last half hour with some satisfaction, though while it was passing the enjoyment of it had been little. Eager to be alone, and fearful of inquiries or hints from her uncle and aunt, she stayed with them only long enough to hear their favourable opinion of Bingley, and then hurried away to dress."}
{"doc_id": "gutenberg_1342", "para_id": 1130, "text": "But she had no reason to fear Mr. and Mrs. Gardiner’s curiosity; it was not their wish to force her communication. It was evident that she was much better acquainted with Mr. Darcy than they had before any idea of; it was evident that he was very much in love with her. They saw much to interest, but nothing to justify inquiry."}
{"doc_id": "gutenberg_1342", "para_id": 1131, "text": "Of Mr. Darcy it was now a matter of anxiety to think well; and, as far as their acquaintance reached, there was no fault to find. They could not be untouched by his politeness; and had they drawn his character from their own feelings and his servant’s report, without any reference to any other account, the circle in Hertfordshire to which he was known would not have recognized it for Mr. Darcy. There was now an interest, however, in believing the housekeeper; and they soon became sensible that the authority of a servant, who had known him since he was four years old, and whose own manners indicated respectability, was not to be hastily rejected. Neither had anything occurred in the intelligence of their Lambton friends that could materially lessen its weight. They had nothing to accuse him of but pride; pride he probably had, and if not, it would certainly be imputed by the inhabitants of a small market town where the family did not visit. It was acknowledged, however, that he was a liberal man, and did much good among the poor."}
{"doc_id": "gutenberg_1342", "para_id": 1132, "text": "With respect to Wickham, the travellers soon found that he was not held there in much estimation; for though the chief of his concerns with the son of his patron were imperfectly understood, it was yet a well-known fact that, on his quitting Derbyshire, he had left many debts behind him, which Mr. Darcy afterwards discharged."}
{"doc_id": "gutenberg_1342", "para_id": 1133, "text": "As for Elizabeth, her thoughts were at Pemberley this evening more than the last; and the evening, though as it passed it seemed long, was not long enough to determine her feelings towards _one_ in that mansion; and she lay awake two whole hours, endeavouring to make them out. She certainly did not hate him. No; hatred had vanished long ago, and she had almost as long been ashamed of ever feeling a dislike against him, that could be so called. The respect created by the conviction of his valuable qualities, though at first unwillingly admitted, had for some time ceased to be repugnant to her feelings; and it was now heightened into somewhat of a friendlier nature by the testimony so highly in his favour, and bringing forward his disposition in so amiable a light, which yesterday had produced. But above all, above respect and esteem, there was a motive within her of good-will which could not be overlooked. It was gratitude;--gratitude, not merely for having once loved her, but for loving her still well enough to forgive all the petulance and acrimony of her manner in rejecting him, and all the unjust accusations accompanying her rejection. He who, she had been persuaded, would avoid her as his greatest enemy, seemed, on this accidental meeting, most eager to preserve the acquaintance; and without any indelicate display of regard, or any peculiarity of manner, where their two selves only were concerned, was soliciting the good opinion of her friends, and bent on making her known to his sister. Such a change in a man of so much pride excited not only astonishment but gratitude--for to love, ardent love, it must be attributed; and, as such, its impression on her was of a sort to be encouraged, as by no means unpleasing, though it could not be exactly defined. She respected, she esteemed, she was grateful to him, she felt a real interest in his welfare; and she only wanted to know how far she wished that welfare to depend upon herself, and how far it would be for the happiness of both that she should employ the power, which her fancy told her she still possessed, of bringing on the renewal of his addresses."}
{"doc_id": "gutenberg_1342", "para_id": 1134, "text": "It had been settled in the evening, between the aunt and niece, that such a striking civility as Miss Darcy’s, in coming to them on the very day of her arrival at Pemberley--for she had reached it only to a late breakfast--ought to be imitated, though it could not be equalled, by some exertion of politeness on their side; and, consequently, that it would be highly expedient to wait on her at Pemberley the following morning. They were, therefore, to go. Elizabeth was pleased; though when she asked herself the reason, she had very little to say in reply."}
{"doc_id": "gutenberg_1342", "para_id": 1135, "text": "Mr. Gardiner left them soon after breakfast. The fishing scheme had been renewed the day before, and a positive engagement made of his meeting some of the gentlemen at Pemberley by noon."}
{"doc_id": "gutenberg_1342", "para_id": 1136, "text": "Convinced as Elizabeth now was that Miss Bingley’s dislike of her had originated in jealousy, she could not help feeling how very unwelcome her appearance at Pemberley must be to her, and was curious to know with how much civility on that lady’s side the acquaintance would now be renewed."}
{"doc_id": "gutenberg_1342", "para_id": 1137, "text": "On reaching the house, they were shown through the hall into the saloon, whose northern aspect rendered it delightful for summer. Its windows, opening to the ground, admitted a most refreshing view of the high woody hills behind the house, and of the beautiful oaks and Spanish chestnuts which were scattered over the intermediate lawn."}
{"doc_id": "gutenberg_1342", "para_id": 1138, "text": "In this room they were received by Miss Darcy, who was sitting there with Mrs. Hurst and Miss Bingley, and the lady with whom she lived in London. Georgiana’s reception of them was very civil, but attended with all that embarrassment which, though proceeding from shyness and the fear of doing wrong, would easily give to those who felt themselves inferior the belief of her being proud and reserved. Mrs. Gardiner and her niece, however, did her justice, and pitied her."}
{"doc_id": "gutenberg_1342", "para_id": 1139, "text": "By Mrs. Hurst and Miss Bingley they were noticed only by a courtesy; and on their being seated, a pause, awkward as such pauses must always be, succeeded for a few moments. It was first broken by Mrs. Annesley, a genteel, agreeable-looking woman, whose endeavour to introduce some kind of discourse proved her to be more truly well-bred than either of the others; and between her and Mrs. Gardiner, with occasional help from Elizabeth, the conversation was carried on. Miss Darcy looked as if she wished for courage enough to join in it; and sometimes did venture a short sentence, when there was least danger of its being heard."}
{"doc_id": "gutenberg_1342", "para_id": 1140, "text": "Elizabeth soon saw that she was herself closely watched by Miss Bingley, and that she could not speak a word, especially to Miss Darcy, without calling her attention. This observation would not have prevented her from trying to talk to the latter, had they not been seated at an inconvenient distance; but she was not sorry to be spared the necessity of saying much: her own thoughts were employing her. She expected every moment that some of the gentlemen would enter the room: she wished, she feared, that the master of the house might be amongst them; and whether she wished or feared it most, she could scarcely determine. After sitting in this manner a quarter of an hour, without hearing Miss Bingley’s voice, Elizabeth was roused by receiving from her a cold inquiry after the health of her family. She answered with equal indifference and brevity, and the other said no more."}
{"doc_id": "gutenberg_1342", "para_id": 1141, "text": "The next variation which their visit afforded was produced by the entrance of servants with cold meat, cake, and a variety of all the finest fruits in season; but this did not take place till after many a significant look and smile from Mrs. Annesley to Miss Darcy had been given, to remind her of her post. There was now employment for the whole party; for though they could not all talk, they could all eat; and the beautiful pyramids of grapes, nectarines, and peaches, soon collected them round the table."}
{"doc_id": "gutenberg_1342", "para_id": 1142, "text": "While thus engaged, Elizabeth had a fair opportunity of deciding whether she most feared or wished for the appearance of Mr. Darcy, by the feelings which prevailed on his entering the room; and then, though but a moment before she had believed her wishes to predominate, she began to regret that he came."}
{"doc_id": "gutenberg_1342", "para_id": 1143, "text": "He had been some time with Mr. Gardiner, who, with two or three other gentlemen from the house, was engaged by the river; and had left him only on learning that the ladies of the family intended a visit to Georgiana that morning. No sooner did he appear, than Elizabeth wisely resolved to be perfectly easy and unembarrassed;--a resolution the more necessary to be made, but perhaps not the more easily kept, because she saw that the suspicions of the whole party were awakened against them, and that there was scarcely an eye which did not watch his behaviour when he first came into the room. In no countenance was attentive curiosity so strongly marked as in Miss Bingley’s, in spite of the smiles which overspread her face whenever she spoke to one of its objects; for jealousy had not yet made her desperate, and her attentions to Mr. Darcy were by no means over. Miss Darcy, on her brother’s entrance, exerted herself much more to talk; and Elizabeth saw that he was anxious for his sister and herself to get acquainted, and forwarded, as much as possible, every attempt at conversation on either side. Miss Bingley saw all this likewise; and, in the imprudence of anger, took the first opportunity of saying, with sneering civility,--"}
{"doc_id": "gutenberg_1342", "para_id": 1144, "text": "“Pray, Miss Eliza, are not the ----shire militia removed from Meryton? They must be a great loss to _your_ family.”"}
{"doc_id": "gutenberg_1342", "para_id": 1145, "text": "In Darcy’s presence she dared not mention Wickham’s name: but Elizabeth instantly comprehended that he was uppermost in her thoughts; and the various recollections connected with him gave her a moment’s distress; but, exerting herself vigorously to repel the ill-natured attack, she presently answered the question in a tolerably disengaged tone. While she spoke, an involuntary glance showed her Darcy with a heightened complexion, earnestly looking at her, and his sister overcome with confusion, and unable to lift up her eyes. Had Miss Bingley known what pain she was then giving her beloved friend, she undoubtedly would have refrained from the hint; but she had merely intended to discompose Elizabeth, by bringing forward the idea of a man to whom she believed her partial, to make her betray a sensibility which might injure her in Darcy’s opinion, and, perhaps, to remind the latter of all the follies and absurdities by which some part of her family were connected with that corps. Not a syllable had ever reached her of Miss Darcy’s meditated elopement. To no creature had it been revealed, where secrecy was possible, except to Elizabeth; and from all Bingley’s connections her brother was particularly anxious to conceal it, from that very wish which Elizabeth had long ago attributed to him, of their becoming hereafter her own. He had certainly formed such a plan; and without meaning that it should affect his endeavour to separate him from Miss Bennet, it is probable that it might add something to his lively concern for the welfare of his friend."}
{"doc_id": "gutenberg_1342", "para_id": 1146, "text": "Elizabeth’s collected behaviour, however, soon quieted his emotion; and as Miss Bingley, vexed and disappointed, dared not approach nearer to Wickham, Georgiana also recovered in time, though not enough to be able to speak any more. Her brother, whose eye she feared to meet, scarcely recollected her interest in the affair; and the very circumstance which had been designed to turn his thoughts from Elizabeth, seemed to have fixed them on her more and more cheerfully."}
{"doc_id": "gutenberg_1342", "para_id": 1147, "text": "Their visit did not continue long after the question and answer above mentioned; and while Mr. Darcy was attending them to their carriage, Miss Bingley was venting her feelings in criticisms on Elizabeth’s person, behaviour, and dress. But Georgiana would not join her. Her brother’s recommendation was enough to insure her favour: his judgment could not err; and he had spoken in such terms of Elizabeth, as to leave Georgiana without the power of finding her otherwise than lovely and amiable. When Darcy returned to the saloon, Miss Bingley could not help repeating to him some part of what she had been saying to his sister."}
{"doc_id": "gutenberg_1342", "para_id": 1148, "text": "“How very ill Eliza Bennet looks this morning, Mr. Darcy,” she cried: “I never in my life saw anyone so much altered as she is since the winter. She is grown so brown and coarse! Louisa and I were agreeing that we should not have known her again.”"}
{"doc_id": "gutenberg_1342", "para_id": 1149, "text": "However little Mr. Darcy might have liked such an address, he contented himself with coolly replying, that he perceived no other alteration than her being rather tanned,--no miraculous consequence of travelling in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 1150, "text": "“For my own part,” she rejoined, “I must confess that I never could see any beauty in her. Her face is too thin; her complexion has no brilliancy; and her features are not at all handsome. Her nose wants character; there is nothing marked in its lines. Her teeth are tolerable, but not out of the common way; and as for her eyes, which have sometimes been called so fine, I never could perceive anything extraordinary in them. They have a sharp, shrewish look, which I do not like at all; and in her air altogether, there is a self-sufficiency without fashion, which is intolerable.”"}
{"doc_id": "gutenberg_1342", "para_id": 1151, "text": "Persuaded as Miss Bingley was that Darcy admired Elizabeth, this was not the best method of recommending herself; but angry people are not always wise; and in seeing him at last look somewhat nettled, she had all the success she expected. He was resolutely silent, however; and, from a determination of making him speak, she continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 1152, "text": "“I remember, when we first knew her in Hertfordshire, how amazed we all were to find that she was a reputed beauty; and I particularly recollect your saying one night, after they had been dining at Netherfield, ‘_She_ a beauty! I should as soon call her mother a wit.’ But afterwards she seemed to improve on you, and I believe you thought her rather pretty at one time.”"}
{"doc_id": "gutenberg_1342", "para_id": 1153, "text": "“Yes,” replied Darcy, who could contain himself no longer, “but _that_ was only when I first knew her; for it is many months since I have considered her as one of the handsomest women of my acquaintance.”"}
{"doc_id": "gutenberg_1342", "para_id": 1154, "text": "He then went away, and Miss Bingley was left to all the satisfaction of having forced him to say what gave no one any pain but herself."}
{"doc_id": "gutenberg_1342", "para_id": 1155, "text": "Mrs. Gardiner and Elizabeth talked of all that had occurred during their visit, as they returned, except what had particularly interested them both. The looks and behaviour of everybody they had seen were discussed, except of the person who had mostly engaged their attention. They talked of his sister, his friends, his house, his fruit, of everything but himself; yet Elizabeth was longing to know what Mrs. Gardiner thought of him, and Mrs. Gardiner would have been highly gratified by her niece’s beginning the subject."}
{"doc_id": "gutenberg_1342", "para_id": 1156, "text": "Elizabeth had been a good deal disappointed in not finding a letter from Jane on their first arrival at Lambton; and this disappointment had been renewed on each of the mornings that had now been spent there; but on the third her repining was over, and her sister justified, by the receipt of two letters from her at once, on one of which was marked that it had been mis-sent elsewhere. Elizabeth was not surprised at it, as Jane had written the direction remarkably ill."}
{"doc_id": "gutenberg_1342", "para_id": 1157, "text": "They had just been preparing to walk as the letters came in; and her uncle and aunt, leaving her to enjoy them in quiet, set off by themselves. The one mis-sent must be first attended to; it had been written five days ago. The beginning contained an account of all their little parties and engagements, with such news as the country afforded; but the latter half, which was dated a day later, and written in evident agitation, gave more important intelligence. It was to this effect:--"}
{"doc_id": "gutenberg_1342", "para_id": 1158, "text": "“Since writing the above, dearest Lizzy, something has occurred of a most unexpected and serious nature; but I am afraid of alarming you--be assured that we are all well. What I have to say relates to poor Lydia. An express came at twelve last night, just as we were all gone to bed, from Colonel Forster, to inform us that she was gone off to Scotland with one of his officers; to own the truth, with Wickham! Imagine our surprise. To Kitty, however, it does not seem so wholly unexpected. I am very, very sorry. So imprudent a match on both sides! But I am willing to hope the best, and that his character has been misunderstood. Thoughtless and indiscreet I can easily believe him, but this step (and let us rejoice over it) marks nothing bad at heart. His choice is disinterested at least, for he must know my father can give her nothing. Our poor mother is sadly grieved. My father bears it better. How thankful am I, that we never let them know what has been said against him; we must forget it ourselves. They were off Saturday night about twelve, as is conjectured, but were not missed till yesterday morning at eight. The express was sent off directly. My dear Lizzy, they must have passed within ten miles of us. Colonel Forster gives us reason to expect him here soon. Lydia left a few lines for his wife, informing her of their intention. I must conclude, for I cannot be long from my poor mother. I am afraid you will not be able to make it out, but I hardly know what I have written.”"}
{"doc_id": "gutenberg_1342", "para_id": 1159, "text": "Without allowing herself time for consideration, and scarcely knowing what she felt, Elizabeth, on finishing this letter, instantly seized the other, and opening it with the utmost impatience, read as follows: it had been written a day later than the conclusion of the first."}
{"doc_id": "gutenberg_1342", "para_id": 1160, "text": "“By this time, my dearest sister, you have received my hurried letter; I wish this may be more intelligible, but though not confined for time, my head is so bewildered that I cannot answer for being coherent. Dearest Lizzy, I hardly know what I would write, but I have bad news for you, and it cannot be delayed. Imprudent as a marriage between Mr. Wickham and our poor Lydia would be, we are now anxious to be assured it has taken place, for there is but too much reason to fear they are not gone to Scotland. Colonel Forster came yesterday, having left Brighton the day before, not many hours after the express. Though Lydia’s short letter to Mrs. F. gave them to understand that they were going to Gretna Green, something was dropped by Denny expressing his belief that W. never intended to go there, or to marry Lydia at all, which was repeated to Colonel F., who, instantly taking the alarm, set off from B., intending to trace their route. He did trace them easily to Clapham, but no farther; for on entering that place, they removed into a hackney-coach, and dismissed the chaise that brought them from Epsom. All that is known after this is, that they were seen to continue the London road. I know not what to think. After making every possible inquiry on that side of London, Colonel F. came on into Hertfordshire, anxiously renewing them at all the turnpikes, and at the inns in Barnet and Hatfield, but without any success,--no such people had been seen to pass through. With the kindest concern he came on to Longbourn, and broke his apprehensions to us in a manner most creditable to his heart. I am sincerely grieved for him and Mrs. F.; but no one can throw any blame on them. Our distress, my dear Lizzy, is very great. My father and mother believe the worst, but I cannot think so ill of him. Many circumstances might make it more eligible for them to be married privately in town than to pursue their first plan; and even if _he_ could form such a design against a young woman of Lydia’s connections, which is not likely, can I suppose her so lost to everything? Impossible! I grieve to find, however, that Colonel F. is not disposed to depend upon their marriage: he shook his head when I expressed my hopes, and said he feared W. was not a man to be trusted. My poor mother is really ill, and keeps her room. Could she exert herself, it would be better, but this is not to be expected; and as to my father, I never in my life saw him so affected. Poor Kitty has anger for having concealed their attachment; but as it was a matter of confidence, one cannot wonder. I am truly glad, dearest Lizzy, that you have been spared something of these distressing scenes; but now, as the first shock is over, shall I own that I long for your return? I am not so selfish, however, as to press for it, if inconvenient. Adieu! I take up my pen again to do, what I have just told you I would not; but circumstances are such, that I cannot help earnestly begging you all to come here as soon as possible. I know my dear uncle and aunt so well, that I am not afraid of requesting it, though I have still something more to ask of the former. My father is going to London with Colonel Forster instantly, to try to discover her. What he means to do, I am sure I know not; but his excessive distress will not allow him to pursue any measure in the best and safest way, and Colonel Forster is obliged to be at Brighton again to-morrow evening. In such an exigence my uncle’s advice and assistance would be everything in the world; he will immediately comprehend what I must feel, and I rely upon his goodness.”"}
{"doc_id": "gutenberg_1342", "para_id": 1161, "text": "“Oh! where, where is my uncle?” cried Elizabeth, darting from her seat as she finished the letter, in eagerness to follow him, without losing a moment of the time so precious; but as she reached the door, it was opened by a servant, and Mr. Darcy appeared. Her pale face and impetuous manner made him start, and before he could recover himself enough to speak, she, in whose mind every idea was superseded by Lydia’s situation, hastily exclaimed, “I beg your pardon, but I must leave you. I must find Mr. Gardiner this moment on business that cannot be delayed; I have not an instant to lose.”"}
{"doc_id": "gutenberg_1342", "para_id": 1162, "text": "“Good God! what is the matter?” cried he, with more feeling than politeness; then recollecting himself, “I will not detain you a minute; but let me, or let the servant, go after Mr. and Mrs. Gardiner. You are not well enough; you cannot go yourself.”"}
{"doc_id": "gutenberg_1342", "para_id": 1163, "text": "Elizabeth hesitated; but her knees trembled under her, and she felt how little would be gained by her attempting to pursue them. Calling back the servant, therefore, she commissioned him, though in so breathless an accent as made her almost unintelligible, to fetch his master and mistress home instantly."}
{"doc_id": "gutenberg_1342", "para_id": 1164, "text": "On his quitting the room, she sat down, unable to support herself, and looking so miserably ill, that it was impossible for Darcy to leave her, or to refrain from saying, in a tone of gentleness and commiseration, “Let me call your maid. Is there nothing you could take to give you present relief? A glass of wine; shall I get you one? You are very ill.”"}
{"doc_id": "gutenberg_1342", "para_id": 1165, "text": "“No, I thank you,” she replied, endeavouring to recover herself. “There is nothing the matter with me. I am quite well, I am only distressed by some dreadful news which I have just received from Longbourn.”"}
{"doc_id": "gutenberg_1342", "para_id": 1166, "text": "She burst into tears as she alluded to it, and for a few minutes could not speak another word. Darcy, in wretched suspense, could only say something indistinctly of his"}
{"doc_id": "gutenberg_1342", "para_id": 1167, "text": "concern, and observe her in compassionate silence. At length she spoke again. “I have just had a letter from Jane, with such dreadful news. It cannot be concealed from anyone. My youngest sister has left all her friends--has eloped; has thrown herself into the power of--of Mr. Wickham. They are gone off together from Brighton. _You_ know him too well to doubt the rest. She has no money, no connections, nothing that can tempt him to--she is lost for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 1168, "text": "“When I consider,” she added, in a yet more agitated voice, “that _I_ might have prevented it! _I_ who knew what he was. Had I but explained some part of it only--some part of what I learnt, to my own family! Had his character been known, this could not have happened. But it is all, all too late now.”"}
{"doc_id": "gutenberg_1342", "para_id": 1169, "text": "“I am grieved, indeed,” cried Darcy: “grieved--shocked. But is it certain, absolutely certain?”"}
{"doc_id": "gutenberg_1342", "para_id": 1170, "text": "“Oh, yes! They left Brighton together on Sunday night, and were traced almost to London, but not beyond: they are certainly not gone to Scotland.”"}
{"doc_id": "gutenberg_1342", "para_id": 1171, "text": "“My father has gone to London, and Jane has written to beg my uncle’s immediate assistance, and we shall be off, I hope, in half an hour. But nothing can be done; I know very well that nothing can be done. How is such a man to be worked on? How are they even to be discovered? I have not the smallest hope. It is every way horrible!”"}
{"doc_id": "gutenberg_1342", "para_id": 1172, "text": "“When _my_ eyes were opened to his real character, oh! had I known what I ought, what I dared to do! But I knew not--I was afraid of doing too much. Wretched, wretched mistake!”"}
{"doc_id": "gutenberg_1342", "para_id": 1173, "text": "Darcy made no answer. He seemed scarcely to hear her, and was walking up and down the room in earnest meditation; his brow contracted, his air gloomy. Elizabeth soon observed, and instantly understood it. Her power was sinking; everything _must_ sink under such a proof of family weakness, such an assurance of the deepest disgrace. She could neither wonder nor condemn; but the belief of his self-conquest brought nothing consolatory to her bosom, afforded no palliation of her distress. It was, on the contrary, exactly calculated to make her understand her own wishes; and never had she so honestly felt that she could have loved him, as now, when all love must be vain."}
{"doc_id": "gutenberg_1342", "para_id": 1174, "text": "But self, though it would intrude, could not engross her. Lydia--the humiliation, the misery she was bringing on them all--soon swallowed up every private care; and covering her face with her handkerchief, Elizabeth was soon lost to everything else; and, after a pause of several minutes, was only recalled to a sense of her situation by the voice of her companion, who, in a manner which, though it spoke compassion, spoke likewise restraint, said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1175, "text": "“I am afraid you have been long desiring my absence, nor have I anything to plead in excuse of my stay, but real, though unavailing concern. Would to Heaven that anything could be either said or done on my part, that might offer consolation to such distress! But I will not torment you with vain wishes, which may seem purposely to ask for your thanks. This unfortunate affair will, I fear, prevent my sister’s having the pleasure of seeing you at Pemberley to-day.”"}
{"doc_id": "gutenberg_1342", "para_id": 1176, "text": "“Oh, yes! Be so kind as to apologize for us to Miss Darcy. Say that urgent business calls us home immediately. Conceal the unhappy truth as long as it is possible. I know it cannot be long.”"}
{"doc_id": "gutenberg_1342", "para_id": 1177, "text": "He readily assured her of his secrecy, again expressed his sorrow for her distress, wished it a happier conclusion than there was at present reason to hope, and, leaving his compliments for her relations, with only one serious parting look, went away."}
{"doc_id": "gutenberg_1342", "para_id": 1178, "text": "As he quitted the room, Elizabeth felt how improbable it was that they should ever see each other again on such terms of cordiality as had marked their several meetings in Derbyshire; and as she threw a retrospective glance over the whole of their acquaintance, so full of contradictions and varieties, sighed at the perverseness of those feelings which would now have promoted its continuance, and would formerly have rejoiced in its termination."}
{"doc_id": "gutenberg_1342", "para_id": 1179, "text": "If gratitude and esteem are good foundations of affection, Elizabeth’s change of sentiment will be neither improbable nor faulty. But if otherwise, if the regard springing from such sources is unreasonable or unnatural, in comparison of what is so often described as arising on a first interview with its object, and even before two words have been exchanged, nothing can be said in her defence, except that she had given somewhat of a trial to the latter method, in her partiality for Wickham, and that its ill success might, perhaps, authorize her to seek the other less interesting mode of attachment. Be that as it may, she saw him go with regret; and in this early example of what Lydia’s infamy must produce, found additional anguish as she reflected on that wretched business. Never since reading Jane’s second letter had she entertained a hope of Wickham’s meaning to marry her. No one but Jane, she thought, could flatter herself with such an expectation. Surprise was the least of all her feelings on this development. While the contents of the first letter remained on her mind, she was all surprise, all astonishment, that Wickham should marry a girl whom it was impossible he could marry for money; and how Lydia could ever have attached him had appeared incomprehensible. But now it was all too natural. For such an attachment as this, she might have sufficient charms; and though she did not suppose Lydia to be deliberately engaging in an elopement, without the intention of marriage, she had no difficulty in believing that neither her virtue nor her understanding would preserve her from falling an easy prey."}
{"doc_id": "gutenberg_1342", "para_id": 1180, "text": "She had never perceived, while the regiment was in Hertfordshire, that Lydia had any partiality for him; but she was convinced that Lydia had wanted only encouragement to attach herself to anybody. Sometimes one officer, sometimes another, had been her favourite, as their attentions raised them in her opinion. Her affections had been continually fluctuating, but never without an object. The mischief of neglect and mistaken indulgence towards such a girl--oh! how acutely did she now feel it!"}
{"doc_id": "gutenberg_1342", "para_id": 1181, "text": "She was wild to be at home--to hear, to see, to be upon the spot to share with Jane in the cares that must now fall wholly upon her, in a family so deranged; a father absent, a mother incapable of exertion, and requiring constant attendance; and though almost persuaded that nothing could be done for Lydia, her uncle’s interference seemed of the utmost importance, and till he entered the room the misery of her impatience was severe. Mr. and Mrs. Gardiner had hurried back in alarm, supposing, by the servant’s account, that their niece was taken suddenly ill; but satisfying them instantly on that head, she eagerly communicated the cause of their summons, reading the two letters aloud, and dwelling on the postscript of the last with trembling energy. Though Lydia had never been a favourite with them, Mr. and Mrs. Gardiner could not but be deeply affected. Not Lydia only, but all were concerned in it; and after the first exclamations of surprise and horror, Mr. Gardiner readily promised every assistance in his power. Elizabeth, though expecting no less, thanked him with tears of gratitude; and all three being actuated by one spirit, everything relating to their journey was speedily settled. They were to be off as soon as possible. “But what is to be done about Pemberley?” cried Mrs. Gardiner. “John told us Mr. Darcy was here when you sent for us;--was it so?”"}
{"doc_id": "gutenberg_1342", "para_id": 1182, "text": "“Yes; and I told him we should not be able to keep our engagement. _That_ is all settled.”"}
{"doc_id": "gutenberg_1342", "para_id": 1183, "text": "“What is all settled?” repeated the other, as she ran into her room to prepare. “And are they upon such terms as for her to disclose the real truth? Oh, that I knew how it was!”"}
{"doc_id": "gutenberg_1342", "para_id": 1184, "text": "But wishes were vain; or, at best, could serve only to amuse her in the hurry and confusion of the following hour. Had Elizabeth been at leisure to be idle, she would have remained certain that all employment was impossible to one so wretched as herself; but she had her share of business as well as her aunt, and amongst the rest there were notes to be written to all their friends at Lambton, with false excuses for their sudden departure. An hour, however, saw the whole completed; and Mr. Gardiner, meanwhile, having settled his account at the inn, nothing remained to be done but to go; and Elizabeth, after all the misery of the morning, found herself, in a shorter space of time than she could have supposed, seated in the carriage, and on the road to Longbourn."}
{"doc_id": "gutenberg_1342", "para_id": 1185, "text": "“I have been thinking it over again, Elizabeth,” said her uncle, as they drove from the town; “and really, upon serious consideration, I am much more inclined than I was to judge as your eldest sister does of the matter. It appears to me so very unlikely that any young man should form such a design against a girl who is by no means unprotected or friendless, and who was actually staying in his Colonel’s family, that I am strongly inclined to hope the best. Could he expect that her friends would not step forward? Could he expect to be noticed again by the regiment, after such an affront to Colonel Forster? His temptation is not adequate to the risk.”"}
{"doc_id": "gutenberg_1342", "para_id": 1186, "text": "“Upon my word,” said Mrs. Gardiner, “I begin to be of your uncle’s opinion. It is really too great a violation of decency, honour, and interest, for him to be guilty of it. I cannot think so very ill of Wickham. Can you, yourself, Lizzie, so wholly give him up, as to believe him capable of it?”"}
{"doc_id": "gutenberg_1342", "para_id": 1187, "text": "“Not perhaps of neglecting his own interest. But of every other neglect I can believe him capable. If, indeed, it should be so! But I dare not hope it. Why should they not go on to Scotland, if that had been the case?”"}
{"doc_id": "gutenberg_1342", "para_id": 1188, "text": "“In the first place,” replied Mr. Gardiner, “there is no absolute proof that they are not gone to Scotland.”"}
{"doc_id": "gutenberg_1342", "para_id": 1189, "text": "“Oh, but their removing from the chaise into a hackney coach is such a presumption! And, besides, no traces of them were to be found on the Barnet road.”"}
{"doc_id": "gutenberg_1342", "para_id": 1190, "text": "“Well, then,--supposing them to be in London--they may be there, though for the purpose of concealment, for no more exceptionable purpose. It is not likely that money should be very abundant on either side; and it might strike them that they could be more economically, though less expeditiously, married in London, than in Scotland.”"}
{"doc_id": "gutenberg_1342", "para_id": 1191, "text": "“But why all this secrecy? Why any fear of detection? Why must their marriage be private? Oh, no, no--this is not likely. His most particular friend, you see by Jane’s account, was persuaded of his never intending to marry her. Wickham will never marry a woman without some money. He cannot afford it. And what claims has Lydia, what attractions has she beyond youth, health, and good humour, that could make him for her sake forego every chance of benefiting himself by marrying well? As to what restraint the apprehensions of disgrace in the corps might throw on a dishonourable elopement with her, I am not able to judge; for I know nothing of the effects that such a step might produce. But as to your other objection, I am afraid it will hardly hold good. Lydia has no brothers to step forward; and he might imagine, from my father’s behaviour, from his indolence and the little attention he has ever seemed to give to what was going forward in his family, that _he_ would do as little and think as little about it, as any father could do, in such a matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 1192, "text": "“But can you think that Lydia is so lost to everything but love of him, as to consent to live with him on any other terms than marriage?”"}
{"doc_id": "gutenberg_1342", "para_id": 1193, "text": "“It does seem, and it is most shocking, indeed,” replied Elizabeth, with tears in her eyes, “that a sister’s sense of decency and virtue in such a point should admit of doubt. But, really, I know not what to say. Perhaps I am not doing her justice. But she is very young: she has never been taught to think on serious subjects; and for the last half year, nay, for a twelvemonth, she has been given up to nothing but amusement and vanity. She has been allowed to dispose of her time in the most idle and frivolous manner, and to adopt any opinions that came in her way. Since the ----shire were first quartered in Meryton, nothing but love, flirtation, and officers, have been in her head. She has been doing everything in her power, by thinking and talking on the subject, to give greater--what shall I call it?--susceptibility to her feelings; which are naturally lively enough. And we all know that Wickham has every charm of person and address that can captivate a woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 1194, "text": "“But you see that Jane,” said her aunt, “does not think so ill of Wickham, as to believe him capable of the attempt.”"}
{"doc_id": "gutenberg_1342", "para_id": 1195, "text": "“Of whom does Jane ever think ill? And who is there, whatever might be their former conduct, that she would believe capable of such an attempt, till it were proved against them? But Jane knows, as well as I do, what Wickham really is. We both know that he has been profligate in every sense of the word; that he has neither integrity nor honour; that he is as false and deceitful as he is insinuating.”"}
{"doc_id": "gutenberg_1342", "para_id": 1196, "text": "“And do you really know all this?” cried Mrs. Gardiner, whose curiosity as to the mode of her intelligence was all alive."}
{"doc_id": "gutenberg_1342", "para_id": 1197, "text": "“I do, indeed,” replied Elizabeth, colouring. “I told you the other day of his infamous behaviour to Mr. Darcy; and you, yourself, when last at Longbourn, heard in what manner he spoke of the man who had behaved with such forbearance and liberality towards him. And there are other circumstances which I am not at liberty--which it is not worth while to relate; but his lies about the whole Pemberley family are endless. From what he said of Miss Darcy, I was thoroughly prepared to see a proud, reserved, disagreeable girl. Yet he knew to the contrary himself. He must know that she was as amiable and unpretending as we have found her.”"}
{"doc_id": "gutenberg_1342", "para_id": 1198, "text": "“But does Lydia know nothing of this? can she be ignorant of what you and Jane seem so well to understand?”"}
{"doc_id": "gutenberg_1342", "para_id": 1199, "text": "“Oh, yes!--that, that is the worst of all. Till I was in Kent, and saw so much both of Mr. Darcy and his relation Colonel Fitzwilliam, I was ignorant of the truth myself. And when I returned home the ----shire was to leave Meryton in a week or fortnight’s time. As that was the case, neither Jane, to whom I related the whole, nor I, thought it necessary to make our knowledge public; for of what use could it apparently be to anyone, that the good opinion, which all the neighbourhood had of him, should then be overthrown? And even when it was settled that Lydia should go with Mrs. Forster, the necessity of opening her eyes to his character never occurred to me. That _she_ could be in any danger from the deception never entered my head. That such a consequence as _this_ should ensue, you may easily believe was far enough from my thoughts.”"}
{"doc_id": "gutenberg_1342", "para_id": 1200, "text": "“When they all removed to Brighton, therefore, you had no reason, I suppose, to believe them fond of each other?”"}
{"doc_id": "gutenberg_1342", "para_id": 1201, "text": "“Not the slightest. I can remember no symptom of affection on either side; and had anything of the kind been perceptible, you must be aware that ours is not a family on which it could be thrown away. When first he entered the corps, she was ready enough to admire him; but so we all were. Every girl in or near Meryton was out of her senses about him for the first two months: but he never distinguished _her_ by any particular attention; and, consequently, after a moderate period of extravagant and wild admiration, her fancy for him gave way, and others of the regiment, who treated her with more distinction, again became her favourites.”"}
{"doc_id": "gutenberg_1342", "para_id": 1202, "text": "It may be easily believed, that however little of novelty could be added to their fears, hopes, and conjectures, on this interesting subject by its repeated discussion, no other could detain them from it long, during the whole of the journey. From Elizabeth’s thoughts it was never absent. Fixed there by the keenest of all anguish, self-reproach, she could find no interval of ease or forgetfulness."}
{"doc_id": "gutenberg_1342", "para_id": 1203, "text": "They travelled as expeditiously as possible; and sleeping one night on the road, reached Longbourn by dinnertime the next day. It was a comfort to Elizabeth to consider that Jane could not have been wearied by long expectations."}
{"doc_id": "gutenberg_1342", "para_id": 1204, "text": "The little Gardiners, attracted by the sight of a chaise, were standing on the steps of the house, as they entered the paddock; and when the carriage drove up to the door, the joyful surprise that lighted up their faces and displayed itself over their whole bodies, in a variety of capers and frisks, was the first pleasing earnest of their welcome."}
{"doc_id": "gutenberg_1342", "para_id": 1205, "text": "Elizabeth jumped out; and after giving each of them a hasty kiss, hurried into the vestibule, where Jane, who came running downstairs from her mother’s apartment, immediately met her."}
{"doc_id": "gutenberg_1342", "para_id": 1206, "text": "Elizabeth, as she affectionately embraced her, whilst tears filled the eyes of both, lost not a moment in asking whether anything had been heard of the fugitives."}
{"doc_id": "gutenberg_1342", "para_id": 1207, "text": "“Not yet,” replied Jane. “But now that my dear uncle is come, I hope everything will be well.”"}
{"doc_id": "gutenberg_1342", "para_id": 1208, "text": "“We have heard only once. He wrote me a few lines on Wednesday, to say that he had arrived in safety, and to give me his directions, which I particularly begged him to do. He merely added, that he should not write again, till he had something of importance to mention.”"}
{"doc_id": "gutenberg_1342", "para_id": 1209, "text": "“My mother is tolerably well, I trust; though her spirits are greatly shaken. She is upstairs, and will have great satisfaction in seeing you all. She does not yet leave her dressing-room. Mary and Kitty, thank Heaven! are quite well.”"}
{"doc_id": "gutenberg_1342", "para_id": 1210, "text": "“But you--how are you?” cried Elizabeth. “You look pale. How much you must have gone through!”"}
{"doc_id": "gutenberg_1342", "para_id": 1211, "text": "Her sister, however, assured her of her being perfectly well; and their conversation, which had been passing while Mr. and Mrs. Gardiner were engaged with their children, was now put an end to by the approach of the whole party. Jane ran to her uncle and aunt, and welcomed and thanked them both, with alternate smiles and tears."}
{"doc_id": "gutenberg_1342", "para_id": 1212, "text": "When they were all in the drawing-room, the questions which Elizabeth had already asked were of course repeated by the others, and they soon found that Jane had no intelligence to give. The sanguine hope of good, however, which the benevolence of her heart suggested, had not yet deserted her; she still expected that it would all end well, and that every morning would bring some letter, either from Lydia or her father, to explain their proceedings, and, perhaps, announce the marriage."}
{"doc_id": "gutenberg_1342", "para_id": 1213, "text": "Mrs. Bennet, to whose apartment they all repaired, after a few minutes’ conversation together, received them exactly as might be expected; with tears and lamentations of regret, invectives against the villainous conduct of Wickham, and complaints of her own sufferings and ill-usage; blaming everybody but the person to whose ill-judging indulgence the errors of her daughter must be principally owing."}
{"doc_id": "gutenberg_1342", "para_id": 1214, "text": "“If I had been able,” said she, “to carry my point in going to Brighton with all my family, _this_ would not have happened: but poor dear Lydia had nobody to take care of her. Why did the Forsters ever let her go out of their sight? I am sure there was some great neglect or other on their side, for she is not the kind of girl to do such a thing, if she had been well looked after. I always thought they were very unfit to have the charge of her; but I was over-ruled, as I always am. Poor, dear child! And now here’s Mr. Bennet gone away, and I know he will fight Wickham, wherever he meets him, and then he will be killed, and what is to become of us all? The Collinses will turn us out, before he is cold in his grave; and if you are not kind to us, brother, I do not know what we shall do.”"}
{"doc_id": "gutenberg_1342", "para_id": 1215, "text": "They all exclaimed against such terrific ideas; and Mr. Gardiner, after general assurances of his affection for her and all her family, told her that he meant to be in London the very next day, and would assist Mr. Bennet in every endeavour for recovering Lydia."}
{"doc_id": "gutenberg_1342", "para_id": 1216, "text": "“Do not give way to useless alarm,” added he: “though it is right to be prepared for the worst, there is no occasion to look on it as certain. It is not quite a week since they left Brighton. In a few days more, we may gain some news of them; and till we know that they are not married, and have no design of marrying, do not let us give the matter over as lost. As soon as I get to town, I shall go to my brother, and make him come home with me to Gracechurch Street, and then we may consult together as to what is to be done.”"}
{"doc_id": "gutenberg_1342", "para_id": 1217, "text": "“Oh, my dear brother,” replied Mrs. Bennet, “that is exactly what I could most wish for. And now do, when you get to town, find them out, wherever they may be; and if they are not married already, _make_ them marry. And as for wedding clothes, do not let them wait for that, but tell Lydia she shall have as much money as she chooses to buy them, after they are married. And, above all things, keep Mr. Bennet from fighting. Tell him what a dreadful state I am in--that I am frightened out of my wits; and have such tremblings, such flutterings all over me, such spasms in my side, and pains in my head, and such beatings at my heart, that I can get no rest by night nor by day. And tell my dear Lydia not to give any directions about her clothes till she has seen me, for she does not know which are the best warehouses. Oh, brother, how kind you are! I know you will contrive it all.”"}
{"doc_id": "gutenberg_1342", "para_id": 1218, "text": "But Mr. Gardiner, though he assured her again of his earnest endeavours in the cause, could not avoid recommending moderation to her, as well in her hopes as her fears; and after talking with her in this manner till dinner was on table, they left her to vent all her feelings on the housekeeper, who attended in the absence of her daughters."}
{"doc_id": "gutenberg_1342", "para_id": 1219, "text": "Though her brother and sister were persuaded that there was no real occasion for such a seclusion from the family, they did not attempt to oppose it; for they knew that she had not prudence enough to hold her tongue before the servants, while they waited at table, and judged it better that _one_ only of the household, and the one whom they could most trust, should comprehend all her fears and solicitude on the subject."}
{"doc_id": "gutenberg_1342", "para_id": 1220, "text": "In the dining-room they were soon joined by Mary and Kitty, who had been too busily engaged in their separate apartments to make their appearance before. One came from her books, and the other from her toilette. The faces of both, however, were tolerably calm; and no change was visible in either, except that the loss of her favourite sister, or the anger which she had herself incurred in the business, had given something more of fretfulness than usual to the accents of Kitty. As for Mary, she was mistress enough of herself to whisper to Elizabeth, with a countenance of grave reflection, soon after they were seated at table,--"}
{"doc_id": "gutenberg_1342", "para_id": 1221, "text": "“This is a most unfortunate affair, and will probably be much talked of. But we must stem the tide of malice, and pour into the wounded bosoms of each other the balm of sisterly consolation.”"}
{"doc_id": "gutenberg_1342", "para_id": 1222, "text": "Then perceiving in Elizabeth no inclination of replying, she added, “Unhappy as the event must be for Lydia, we may draw from it this useful lesson:--that loss of virtue in a female is irretrievable, that one false step involves her in endless ruin, that her reputation is no less brittle than it is beautiful, and that she cannot be too much guarded in her behaviour towards the undeserving of the other sex.”"}
{"doc_id": "gutenberg_1342", "para_id": 1223, "text": "Elizabeth lifted up her eyes in amazement, but was too much oppressed to make any reply. Mary, however, continued to console herself with such kind of moral extractions from the evil before them."}
{"doc_id": "gutenberg_1342", "para_id": 1224, "text": "In the afternoon, the two elder Miss Bennets were able to be for half an hour by themselves; and Elizabeth instantly availed herself of the opportunity of making any inquiries which Jane was equally eager to satisfy. After joining in general lamentations over the dreadful sequel of this event, which Elizabeth considered as all but certain, and Miss Bennet could not assert to be wholly impossible, the former continued the subject by saying, “But tell me all and everything about it which I have not already heard. Give me further particulars. What did Colonel Forster say? Had they no apprehension of anything before the elopement took place? They must have seen them together for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 1225, "text": "“Colonel Forster did own that he had often suspected some partiality, especially on Lydia’s side, but nothing to give him any alarm. I am so grieved for him. His behaviour was attentive and kind to the utmost. He _was_ coming to us, in order to assure us of his concern, before he had any idea of their not being gone to Scotland: when that apprehension first got abroad, it hastened his journey.”"}
{"doc_id": "gutenberg_1342", "para_id": 1226, "text": "“And was Denny convinced that Wickham would not marry? Did he know of their intending to go off? Had Colonel Forster seen Denny himself?”"}
{"doc_id": "gutenberg_1342", "para_id": 1227, "text": "“Yes; but when questioned by _him_, Denny denied knowing anything of their plan, and would not give his real opinion about it. He did not repeat his persuasion of their not marrying, and from _that_ I am inclined to hope he might have been misunderstood before.”"}
{"doc_id": "gutenberg_1342", "para_id": 1228, "text": "“And till Colonel Forster came himself, not one of you entertained a doubt, I suppose, of their being really married?”"}
{"doc_id": "gutenberg_1342", "para_id": 1229, "text": "“How was it possible that such an idea should enter our brains? I felt a little uneasy--a little fearful of my sister’s happiness with him in marriage, because I knew that his conduct had not been always quite right. My father and mother knew nothing of that; they only felt how imprudent a match it must be. Kitty then owned, with a very natural triumph on knowing more than the rest of us, that in Lydia’s last letter she had prepared her for such a step. She had known, it seems, of their being in love with each other many weeks.”"}
{"doc_id": "gutenberg_1342", "para_id": 1230, "text": "“And did Colonel Forster appear to think ill of Wickham himself? Does he know his real character?”"}
{"doc_id": "gutenberg_1342", "para_id": 1231, "text": "“I must confess that he did not speak so well of Wickham as he formerly did. He believed him to be imprudent and extravagant; and since this sad affair has taken place, it is said that he left Meryton greatly in debt: but I hope this may be false.”"}
{"doc_id": "gutenberg_1342", "para_id": 1232, "text": "“Oh, Jane, had we been less secret, had we told what we knew of him, this could not have happened!”"}
{"doc_id": "gutenberg_1342", "para_id": 1233, "text": "“But to expose the former faults of any person, without knowing what their present feelings were, seemed unjustifiable.”"}
{"doc_id": "gutenberg_1342", "para_id": 1234, "text": "Jane then took it from her pocket-book, and gave it to Elizabeth. These were the contents:--"}
{"doc_id": "gutenberg_1342", "para_id": 1235, "text": "“You will laugh when you know where I am gone, and I cannot help laughing myself at your surprise to-morrow morning, as soon as I am missed. I am going to Gretna Green, and if you cannot guess with who, I shall think you a simpleton, for there is but one man in the world I love, and he is an angel. I should never be happy without him, so think it no harm to be off. You need not send them word at Longbourn of my going, if you do not like it, for it will make the surprise the greater when I write to them, and sign my name Lydia Wickham. What a good joke it will be! I can hardly write for laughing. Pray make my excuses to Pratt for not keeping my engagement, and dancing with him to-night. Tell him I hope he will excuse me when he knows all, and tell him I will dance with him at the next ball we meet with great pleasure. I shall send for my clothes when I get to Longbourn; but I wish you would tell Sally to mend a great slit in my worked muslin gown before they are packed up. Good-bye. Give my love to Colonel Forster. I hope you will drink to our good journey."}
{"doc_id": "gutenberg_1342", "para_id": 1236, "text": "“Oh, thoughtless, thoughtless Lydia!” cried Elizabeth when she had finished it. “What a letter is this, to be written at such a moment! But at least it shows that _she_ was serious in the object of her journey. Whatever he might afterwards persuade her to, it was not on her side a _scheme_ of infamy. My poor father! how he must have felt it!”"}
{"doc_id": "gutenberg_1342", "para_id": 1237, "text": "“I never saw anyone so shocked. He could not speak a word for full ten minutes. My mother was taken ill immediately, and the whole house in such confusion!”"}
{"doc_id": "gutenberg_1342", "para_id": 1238, "text": "“Oh, Jane,” cried Elizabeth, “was there a servant belonging to it who did not know the whole story before the end of the day?”"}
{"doc_id": "gutenberg_1342", "para_id": 1239, "text": "“I do not know: I hope there was. But to be guarded at such a time is very difficult. My mother was in hysterics; and though I endeavoured to give her every assistance in my power, I am afraid I did not do so much as I might have done. But the horror of what might possibly happen almost took from me my faculties.”"}
{"doc_id": "gutenberg_1342", "para_id": 1240, "text": "“Your attendance upon her has been too much for you. You do not look well. Oh that I had been with you! you have had every care and anxiety upon yourself alone.”"}
{"doc_id": "gutenberg_1342", "para_id": 1241, "text": "“Mary and Kitty have been very kind, and would have shared in every fatigue, I am sure, but I did not think it right for either of them. Kitty is slight and delicate, and Mary studies so much that her hours of repose should not be broken in on. My aunt Philips came to Longbourn on Tuesday, after my father went away; and was so good as to stay till Thursday with me. She was of great use and comfort to us all, and Lady Lucas has been very kind: she walked here on Wednesday morning to condole with us, and offered her services, or any of her daughters, if they could be of use to us.”"}
{"doc_id": "gutenberg_1342", "para_id": 1242, "text": "“She had better have stayed at home,” cried Elizabeth: “perhaps she _meant_ well, but, under such a misfortune as this, one cannot see too little of one’s neighbours. Assistance is impossible; condolence, insufferable. Let them triumph over us at a distance, and be satisfied.”"}
{"doc_id": "gutenberg_1342", "para_id": 1243, "text": "She then proceeded to inquire into the measures which her father had intended to pursue, while in town, for the recovery of his daughter."}
{"doc_id": "gutenberg_1342", "para_id": 1244, "text": "“He meant, I believe,” replied Jane, “to go to Epsom, the place where they last changed horses, see the postilions, and try if anything could be made out from them. His principal object must be to discover the number of the hackney coach which took them from Clapham. It had come with a fare from London; and as he thought the circumstance of a gentleman and lady’s removing from one carriage into another might be remarked, he meant to make inquiries at Clapham. If he could anyhow discover at what house the coachman had before set down his fare, he determined to make inquiries there, and hoped it might not be impossible to find out the stand and number of the coach. I do not know of any other designs that he had formed; but he was in such a hurry to be gone, and his spirits so greatly discomposed, that I had difficulty in finding out even so much as this.”"}
{"doc_id": "gutenberg_1342", "para_id": 1245, "text": "The whole party were in hopes of a letter from Mr. Bennet the next morning, but the post came in without bringing a single line from him. His family knew him to be, on all common occasions, a most negligent and dilatory correspondent; but at such a time they had hoped for exertion. They were forced to conclude, that he had no pleasing intelligence to send; but even of _that_ they would have been glad to be certain. Mr. Gardiner had waited only for the letters before he set off."}
{"doc_id": "gutenberg_1342", "para_id": 1246, "text": "When he was gone, they were certain at least of receiving constant information of what was going on; and their uncle promised, at parting, to prevail on Mr. Bennet to return to Longbourn as soon as he could, to the great consolation of his sister, who considered it as the only security for her husband’s not being killed in a duel."}
{"doc_id": "gutenberg_1342", "para_id": 1247, "text": "Mrs. Gardiner and the children were to remain in Hertfordshire a few days longer, as the former thought her presence might be serviceable to her nieces. She shared in their attendance on Mrs. Bennet, and was a great comfort to them in their hours of freedom. Their other aunt also visited them frequently, and always, as she said, with the design of cheering and heartening them up--though, as she never came without reporting some fresh instance of Wickham’s extravagance or irregularity, she seldom went away without leaving them more dispirited than she found them."}
{"doc_id": "gutenberg_1342", "para_id": 1248, "text": "All Meryton seemed striving to blacken the man who, but three months before, had been almost an angel of light. He was declared to be in debt to every tradesman in the place, and his intrigues, all honoured with the title of seduction, had been extended into every tradesman’s family. Everybody declared that he was the wickedest young man in the world; and everybody began to find out that they had always distrusted the appearance of his goodness. Elizabeth, though she did not credit above half of what was said, believed enough to make her former assurance of her sister’s ruin still more certain; and even Jane, who believed still less of it, became almost hopeless, more especially as the time was now come, when, if they had gone to Scotland, which she had never before entirely despaired of, they must in all probability have gained some news of them."}
{"doc_id": "gutenberg_1342", "para_id": 1249, "text": "Mr. Gardiner left Longbourn on Sunday; on Tuesday, his wife received a letter from him: it told them, that on his arrival he had immediately found out his brother, and persuaded him to come to Gracechurch Street. That Mr. Bennet had been to Epsom and Clapham, before his arrival, but without gaining any satisfactory information; and that he was now determined to inquire at all the principal hotels in town, as Mr. Bennet thought it possible they might have gone to one of them, on their first coming to London, before they procured lodgings. Mr. Gardiner himself did not expect any success from this measure; but as his brother was eager in it, he meant to assist him in pursuing it. He added, that Mr. Bennet seemed wholly disinclined at present to leave London, and promised to write again very soon. There was also a postscript to this effect:--"}
{"doc_id": "gutenberg_1342", "para_id": 1250, "text": "“I have written to Colonel Forster to desire him to find out, if possible, from some of the young man’s intimates in the regiment, whether Wickham has any relations or connections who would be likely to know in what part of the town he has now concealed himself. If there were anyone that one could apply to, with a probability of gaining such a clue as that, it might be of essential consequence. At present we have nothing to guide us. Colonel Forster will, I dare say, do everything in his power to satisfy us on this head. But, on second thoughts, perhaps Lizzy could tell us what relations he has now living better than any other person.”"}
{"doc_id": "gutenberg_1342", "para_id": 1251, "text": "Elizabeth was at no loss to understand from whence this deference for her authority proceeded; but it was not in her power to give any information of so satisfactory a nature as the compliment deserved."}
{"doc_id": "gutenberg_1342", "para_id": 1252, "text": "She had never heard of his having had any relations, except a father and mother, both of whom had been dead many years. It was possible, however, that some of his companions in the ----shire might be able to give more information; and though she was not very sanguine in expecting it, the application was a something to look forward to."}
{"doc_id": "gutenberg_1342", "para_id": 1253, "text": "Every day at Longbourn was now a day of anxiety; but the most anxious part of each was when the post was expected. The arrival of letters was the first grand object of every morning’s impatience. Through letters, whatever of good or bad was to be told would be communicated; and every succeeding day was expected to bring some news of importance."}
{"doc_id": "gutenberg_1342", "para_id": 1254, "text": "But before they heard again from Mr. Gardiner, a letter arrived for their father, from a different quarter, from Mr. Collins; which, as Jane had received directions to open all that came for him in his absence, she accordingly read; and Elizabeth, who knew what curiosities his letters always were, looked over her, and read it likewise. It was as follows:--"}
{"doc_id": "gutenberg_1342", "para_id": 1255, "text": "“I feel myself called upon, by our relationship, and my situation in life, to condole with you on the grievous affliction you are now suffering under, of which we were yesterday informed by a letter from Hertfordshire. Be assured, my dear sir, that Mrs. Collins and myself sincerely sympathize with you, and all your respectable family, in your present distress, which must be of the bitterest kind, because proceeding from a cause which no time can remove. No arguments shall be wanting on my part, that can alleviate so severe a misfortune; or that may comfort you, under a circumstance that must be, of all others, most afflicting to a parent’s mind. The death of your daughter would have been a blessing in comparison of this. And it is the more to be lamented, because there is reason to suppose, as my dear Charlotte informs me, that this licentiousness of behaviour in your"}
{"doc_id": "gutenberg_1342", "para_id": 1256, "text": "daughter has proceeded from a faulty degree of indulgence; though, at the same time, for the consolation of yourself and Mrs. Bennet, I am inclined to think that her own disposition must be naturally bad, or she could not be guilty of such an enormity, at so early an age. Howsoever that may be, you are grievously to be pitied; in which opinion I am not only joined by Mrs. Collins, but likewise by Lady Catherine and her daughter, to whom I have related the affair. They agree with me in apprehending that this false step in one daughter will be injurious to the fortunes of all the others: for who, as Lady Catherine herself condescendingly says, will connect themselves with such a family? And this consideration leads me, moreover, to reflect, with augmented satisfaction, on a certain event of last November; for had it been otherwise, I must have been involved in all your sorrow and disgrace. Let me advise you, then, my dear sir, to console yourself as much as possible, to throw off your unworthy child from your affection for ever, and leave her to reap the fruits of her own heinous offence."}
{"doc_id": "gutenberg_1342", "para_id": 1257, "text": "Mr. Gardiner did not write again, till he had received an answer from Colonel Forster; and then he had nothing of a pleasant nature to send. It was not known that Wickham had a single relation with whom he kept up any connection, and it was certain that he had no near one living. His former acquaintance had been numerous; but since he had been in the militia, it did not appear that he was on terms of particular friendship with any of them. There was no one, therefore, who could be pointed out as likely to give any news of him. And in the wretched state of his own finances, there was a very powerful motive for secrecy, in addition to his fear of discovery by Lydia’s relations; for it had just transpired that he had left gaming debts behind him to a very considerable amount. Colonel Forster believed that more than a thousand pounds would be necessary to clear his expenses at Brighton. He owed a good deal in the town, but his debts of honour were still more formidable. Mr. Gardiner did not attempt to conceal these particulars from the Longbourn family; Jane heard them with horror. “A gamester!” she cried. “This is wholly unexpected; I had not an idea of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1258, "text": "Mr. Gardiner added, in his letter, that they might expect to see their father at home on the following day, which was Saturday. Rendered spiritless by the ill success of all their endeavours, he had yielded to his brother-in-law’s entreaty that he would return to his family and leave it to him to do whatever occasion might suggest to be advisable for continuing their pursuit. When Mrs. Bennet was told of this, she did not express so much satisfaction as her children expected, considering what her anxiety for his life had been before."}
{"doc_id": "gutenberg_1342", "para_id": 1259, "text": "“What! is he coming home, and without poor Lydia?” she cried. “Sure he will not leave London before he has found them. Who is to fight Wickham, and make him marry her, if he comes away?”"}
{"doc_id": "gutenberg_1342", "para_id": 1260, "text": "As Mrs. Gardiner began to wish to be at home, it was settled that she and her children should go to London at the same time that Mr. Bennet came from it. The coach, therefore, took them the first stage of their journey, and brought its master back to Longbourn."}
{"doc_id": "gutenberg_1342", "para_id": 1261, "text": "Mrs. Gardiner went away in all the perplexity about Elizabeth and her Derbyshire friend, that had attended her from that part of the world. His name had never been voluntarily mentioned before them by her niece; and the kind of half-expectation which Mrs. Gardiner had formed, of their being followed by a letter from him, had ended in nothing. Elizabeth had received none since her return, that could come from Pemberley."}
{"doc_id": "gutenberg_1342", "para_id": 1262, "text": "The present unhappy state of the family rendered any other excuse for the lowness of her spirits unnecessary; nothing, therefore, could be fairly conjectured from _that_,--though Elizabeth, who was by this time tolerably well acquainted with her own feelings, was perfectly aware that, had she known nothing of Darcy, she could have borne the dread of Lydia’s infamy somewhat better. It would have spared her, she thought, one sleepless night out of two."}
{"doc_id": "gutenberg_1342", "para_id": 1263, "text": "When Mr. Bennet arrived, he had all the appearance of his usual philosophic composure. He said as little as he had ever been in the habit of saying; made no mention of the business that had taken him away; and it was some time before his daughters had courage to speak of it."}
{"doc_id": "gutenberg_1342", "para_id": 1264, "text": "It was not till the afternoon, when he joined them at tea, that Elizabeth ventured to introduce the subject; and then, on her briefly expressing her sorrow for what he must have endured, he replied, “Say nothing of that. Who should suffer but myself? It has been my own doing, and I ought to feel it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1265, "text": "“You may well warn me against such an evil. Human nature is so prone to fall into it! No, Lizzy, let me once in my life feel how much I have been to blame. I am not afraid of being overpowered by the impression. It will pass away soon enough.”"}
{"doc_id": "gutenberg_1342", "para_id": 1266, "text": "“She is happy, then,” said her father, drily; “and her residence there will probably be of some duration.”"}
{"doc_id": "gutenberg_1342", "para_id": 1267, "text": "Then, after a short silence, he continued, “Lizzy, I bear you no ill-will for being justified in your advice to me last May, which, considering the event, shows some greatness of mind.”"}
{"doc_id": "gutenberg_1342", "para_id": 1268, "text": "“This is a parade,” cried he, “which does one good; it gives such an elegance to misfortune! Another day I will do the same; I will sit in my library, in my nightcap and powdering gown, and give as much trouble as I can,--or perhaps I may defer it till Kitty runs away.”"}
{"doc_id": "gutenberg_1342", "para_id": 1269, "text": "“I am not going to run away, papa,” said Kitty, fretfully. “If _I_ should ever go to Brighton, I would behave better than Lydia.”"}
{"doc_id": "gutenberg_1342", "para_id": 1270, "text": "“_You_ go to Brighton! I would not trust you so near it as Eastbourne, for fifty pounds! No, Kitty, I have at least learnt to be cautious, and you will feel the effects of it. No officer is ever to enter my house again, nor even to pass through the village. Balls will be absolutely prohibited, unless you stand up with one of your sisters. And you are never to stir out of doors, till you can prove that you have spent ten minutes of every day in a rational manner.”"}
{"doc_id": "gutenberg_1342", "para_id": 1271, "text": "“Well, well,” said he, “do not make yourself unhappy. If you are a good girl for the next ten years, I will take you to a review at the end of them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1272, "text": "Two days after Mr. Bennet’s return, as Jane and Elizabeth were walking together in the shrubbery behind the house, they saw the housekeeper coming towards them, and concluding that she came to call them to their mother, went forward to meet her; but instead of the expected summons, when they approached her, she said to Miss Bennet, “I beg your pardon, madam, for interrupting you, but I was in hopes you might have got some good news from town, so I took the liberty of coming to ask.”"}
{"doc_id": "gutenberg_1342", "para_id": 1273, "text": "“Dear madam,” cried Mrs. Hill, in great astonishment, “don’t you know there is an express come for master from Mr. Gardiner? He has been here this half hour, and master has had a letter.”"}
{"doc_id": "gutenberg_1342", "para_id": 1274, "text": "Away ran the girls, too eager to get in to have time for speech. They ran through the vestibule into the breakfast-room; from thence to the library;--their father was in neither; and they were on the point of seeking him upstairs with their mother, when they were met by the butler, who said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1275, "text": "“If you are looking for my master, ma’am, he is walking towards the little copse.”"}
{"doc_id": "gutenberg_1342", "para_id": 1276, "text": "Upon this information, they instantly passed through the hall once more, and ran across the lawn after their father, who was deliberately pursuing his way towards a small wood on one side of the paddock."}
{"doc_id": "gutenberg_1342", "para_id": 1277, "text": "Jane, who was not so light, nor so much in the habit of running as Elizabeth, soon lagged behind, while her sister, panting for breath, came up with him, and eagerly cried out,--"}
{"doc_id": "gutenberg_1342", "para_id": 1278, "text": "“What is there of good to be expected?” said he, taking the letter from his pocket; “but perhaps you would like to read it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1279, "text": "“Read it aloud,” said their father, “for I hardly know myself what it is about.”"}
{"doc_id": "gutenberg_1342", "para_id": 1280, "text": "“At last I am able to send you some tidings of my niece, and such as, upon the whole, I hope will give you satisfaction. Soon after you left me on Saturday, I was fortunate enough to find out in what part of London they were. The particulars I reserve till we meet. It is enough to know they are discovered: I have seen them both----”"}
{"doc_id": "gutenberg_1342", "para_id": 1281, "text": "Elizabeth read on: “I have seen them both. They are not married, nor can I find there was any intention of being so; but if you are willing to perform the engagements which I have ventured to make on your side, I hope it will not be long before they are. All that is required of you is, to assure to your daughter, by settlement, her equal share of the five thousand pounds, secured among your children after the decease of yourself and my sister; and, moreover, to enter into an engagement of allowing her, during your life, one hundred pounds per annum. These are conditions which, considering everything, I had no hesitation in complying with, as far as I thought myself privileged, for you. I shall send this by express, that no time may be lost in bringing me your answer. You will easily comprehend, from these particulars, that Mr. Wickham’s circumstances are not so hopeless as they are generally believed to be. The world has been deceived in that respect; and I am happy to say, there will be some little money, even when all his debts are discharged, to settle on my niece, in addition to her own fortune. If, as I conclude will be the case, you send me full powers to act in your name throughout the whole of this business, I will immediately give directions to Haggerston for preparing a proper settlement. There will not be the smallest occasion for your coming to town again; therefore stay quietly at Longbourn, and depend on my diligence and care. Send back your answer as soon as you can, and be careful to write explicitly. We have judged it best that my niece should be married from this house, of which I hope you will approve. She comes to us to-day. I shall write again as soon as anything more is determined on. Yours, etc."}
{"doc_id": "gutenberg_1342", "para_id": 1282, "text": "“Is it possible?” cried Elizabeth, when she had finished. “Can it be possible that he will marry her?”"}
{"doc_id": "gutenberg_1342", "para_id": 1283, "text": "“Wickham is not so undeserving, then, as we have thought him,” said her sister. “My dear father, I congratulate you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1284, "text": "“Oh! my dear father,” she cried, “come back and write immediately. Consider how important every moment is in such a case.”"}
{"doc_id": "gutenberg_1342", "para_id": 1285, "text": "“And--may I ask?” said Elizabeth; “but the terms, I suppose, must be complied with.”"}
{"doc_id": "gutenberg_1342", "para_id": 1286, "text": "“Yes, yes, they must marry. There is nothing else to be done. But there are two things that I want very much to know:--one is, how much money your uncle has laid down to bring it about; and the other, how I am ever to pay him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1287, "text": "“I mean that no man in his proper senses would marry Lydia on so slight a temptation as one hundred a year during my life, and fifty after I am gone.”"}
{"doc_id": "gutenberg_1342", "para_id": 1288, "text": "“That is very true,” said Elizabeth; “though it had not occurred to me before. His debts to be discharged, and something still to remain! Oh, it must be my uncle’s doings! Generous, good man, I am afraid he has distressed himself. A small sum could not do all this.”"}
{"doc_id": "gutenberg_1342", "para_id": 1289, "text": "“No,” said her father. “Wickham’s a fool if he takes her with a farthing less than ten thousand pounds: I should be sorry to think so ill of him, in the very beginning of our relationship.”"}
{"doc_id": "gutenberg_1342", "para_id": 1290, "text": "Mr. Bennet made no answer; and each of them, deep in thought, continued silent till they reached the house. Their father then went to the library to write, and the girls walked into the breakfast-room."}
{"doc_id": "gutenberg_1342", "para_id": 1291, "text": "“And they are really to be married!” cried Elizabeth, as soon as they were by themselves. “How strange this is! and for _this_ we are to be thankful. That they should marry, small as is their chance of happiness, and wretched as is his character, we are forced to rejoice! Oh, Lydia!”"}
{"doc_id": "gutenberg_1342", "para_id": 1292, "text": "“I comfort myself with thinking,” replied Jane, “that he certainly would not marry Lydia, if he had not a real regard for her. Though our kind uncle has done something towards clearing him, I cannot believe that ten thousand pounds, or anything like it, has been advanced. He has children of his own, and may have more. How could he spare half ten thousand pounds?”"}
{"doc_id": "gutenberg_1342", "para_id": 1293, "text": "“If we are ever able to learn what Wickham’s debts have been,” said Elizabeth, “and how much is settled on his side on our sister, we shall exactly know what Mr. Gardiner has done for them, because Wickham has not sixpence of his own. The kindness of my uncle and aunt can never be requited. Their taking her home, and affording her their personal protection and countenance, is such a sacrifice to her advantage as years of gratitude cannot enough acknowledge. By this time she is actually with them! If such goodness does not make her miserable now, she will never deserve to be happy! What a meeting for her, when she first sees my aunt!”"}
{"doc_id": "gutenberg_1342", "para_id": 1294, "text": "“We must endeavour to forget all that has passed on either side,” said Jane: “I hope and trust they will yet be happy. His consenting to marry her is a proof, I will believe, that he is come to a right way of thinking. Their mutual affection will steady them; and I flatter myself they will settle so quietly, and live in so rational a manner, as may in time make their past imprudence forgotten.”"}
{"doc_id": "gutenberg_1342", "para_id": 1295, "text": "“Their conduct has been such,” replied Elizabeth, “as neither you, nor I, nor anybody, can ever forget. It is useless to talk of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1296, "text": "It now occurred to the girls that their mother was in all likelihood perfectly ignorant of what had happened. They went to the library, therefore, and asked their father whether he would not wish them to make it known to her. He was writing, and, without raising his head, coolly replied,--"}
{"doc_id": "gutenberg_1342", "para_id": 1297, "text": "Elizabeth took the letter from his writing-table, and they went upstairs together. Mary and Kitty were both with Mrs. Bennet: one communication would, therefore, do for all. After a slight preparation for good news, the letter was read aloud. Mrs. Bennet could hardly contain herself. As soon as Jane had read Mr. Gardiner’s hope of Lydia’s being soon married, her joy burst forth, and every following sentence added to its exuberance. She was now in an irritation as violent from delight as she had ever been fidgety from alarm and vexation. To know that her daughter would be married was enough. She was disturbed by no fear for her felicity, nor humbled by any remembrance of her misconduct."}
{"doc_id": "gutenberg_1342", "para_id": 1298, "text": "“My dear, dear Lydia!” she cried: “this is delightful indeed! She will be married! I shall see her again! She will be married at sixteen! My good, kind brother! I knew how it would be--I knew he would manage everything. How I long to see her! and to see dear Wickham too! But the clothes, the wedding clothes! I will write to my sister Gardiner about them directly. Lizzy, my dear, run down to your father, and ask him how much he will give her. Stay, stay, I will go myself. Ring the bell, Kitty, for Hill. I will put on my things in a moment. My dear, dear Lydia! How merry we shall be together when we meet!”"}
{"doc_id": "gutenberg_1342", "para_id": 1299, "text": "Her eldest daughter endeavoured to give some relief to the violence of these transports, by leading her thoughts to the obligations which Mr. Gardiner’s behaviour laid them all under."}
{"doc_id": "gutenberg_1342", "para_id": 1300, "text": "“For we must attribute this happy conclusion,” she added, “in a great measure to his kindness. We are persuaded that he has pledged himself to assist Mr. Wickham with money.”"}
{"doc_id": "gutenberg_1342", "para_id": 1301, "text": "“Well,” cried her mother, “it is all very right; who should do it but her own uncle? If he had not had a family of his own, I and my children must have had all his money, you know; and it is the first time we have ever had anything from him except a few presents. Well! I am so happy. In a short time, I shall have a daughter married. Mrs. Wickham! How well it sounds! And she was only sixteen last June. My dear Jane, I am in such a flutter, that I am sure I can’t write; so I will dictate, and you write for me. We will settle with your father about the money afterwards; but the things should be ordered immediately.”"}
{"doc_id": "gutenberg_1342", "para_id": 1302, "text": "She was then proceeding to all the particulars of calico, muslin, and cambric, and would shortly have dictated some very plentiful orders, had not Jane, though with some difficulty, persuaded her to wait till her father was at leisure to be consulted. One day’s delay, she observed, would be of small importance; and her mother was too happy to be quite so obstinate as usual. Other schemes, too, came into her head."}
{"doc_id": "gutenberg_1342", "para_id": 1303, "text": "“I will go to Meryton,” said she, “as soon as I am dressed, and tell the good, good news to my sister Philips. And as I come back, I can call on Lady Lucas and Mrs. Long. Kitty, run down and order the carriage. An airing would do me a great deal of good, I am sure. Girls, can I do anything for you in Meryton? Oh! here comes Hill. My dear Hill, have you heard the good news? Miss Lydia is going to be married; and you shall all have a bowl of punch to make merry at her wedding.”"}
{"doc_id": "gutenberg_1342", "para_id": 1304, "text": "Mrs. Hill began instantly to express her joy. Elizabeth received her congratulations amongst the rest, and then, sick of this folly, took refuge in her own room, that she might think with freedom. Poor Lydia’s situation must, at best, be bad enough; but that it was no worse, she had need to be thankful. She felt it so; and though, in looking forward, neither rational happiness, nor worldly prosperity could be justly expected for her sister, in looking back to what they had feared, only two hours ago, she felt all the advantages of what they had gained."}
{"doc_id": "gutenberg_1342", "para_id": 1305, "text": "Mr. Bennet had very often wished, before this period of his life, that, instead of spending his whole income, he had laid by an annual sum, for the better provision of his children, and of his wife, if she survived him. He now wished it more than ever. Had he done his duty in that respect, Lydia need not have been indebted to her uncle for whatever of honour or credit could now be purchased for her. The satisfaction of prevailing on one of the most worthless young men in Great Britain to be her husband might then have rested in its proper place."}
{"doc_id": "gutenberg_1342", "para_id": 1306, "text": "He was seriously concerned that a cause of so little advantage to anyone should be forwarded at the sole expense of his brother-in-law; and he was determined, if possible, to find out the extent of his assistance, and to discharge the obligation as soon as he could."}
{"doc_id": "gutenberg_1342", "para_id": 1307, "text": "When first Mr. Bennet had married, economy was held to be perfectly useless; for, of course, they were to have a son. This son was to join in cutting off the entail, as soon as he should be of age, and the widow and younger children would by that means be provided for. Five daughters successively entered the world, but yet the son was to come; and Mrs. Bennet, for many years after Lydia’s birth, had been certain that he would. This event had at last been despaired of, but it was then too late to be saving. Mrs. Bennet had no turn for economy; and her husband’s love of independence had alone prevented their exceeding their income."}
{"doc_id": "gutenberg_1342", "para_id": 1308, "text": "Five thousand pounds was settled by marriage articles on Mrs. Bennet and the children. But in what proportions it should be divided amongst the latter depended on the will of the parents. This was one point, with regard to Lydia at least, which was now to be settled, and Mr. Bennet could have no hesitation in acceding to the proposal before him. In terms of grateful acknowledgment for the kindness of his brother, though expressed most concisely, he then delivered on paper his perfect approbation of all that was done, and his willingness to fulfil the engagements that had been made for him. He had never before supposed that, could Wickham be prevailed on to marry his daughter, it would be done with so little inconvenience to himself as by the present arrangement. He would scarcely be ten pounds a year the loser, by the hundred that was to be paid them; for, what with her board and pocket allowance, and the continual presents in money which passed to her through her mother’s hands, Lydia’s expenses had been very little within that sum."}
{"doc_id": "gutenberg_1342", "para_id": 1309, "text": "That it would be done with such trifling exertion on his side, too, was another very welcome surprise; for his chief wish at present was to have as little trouble in the business as possible. When the first transports of rage which had produced his activity in seeking her were over, he naturally returned to all his former indolence. His letter was soon despatched; for though dilatory in undertaking business, he was quick in its execution. He begged to know further particulars of what he was indebted to his brother; but was too angry with Lydia to send any message to her."}
{"doc_id": "gutenberg_1342", "para_id": 1310, "text": "The good news quickly spread through the house; and with proportionate speed through the neighbourhood. It was borne in the latter with decent philosophy. To be sure, it would have been more for the advantage of conversation, had Miss Lydia Bennet come upon the town; or, as the happiest alternative, been secluded from the world in some distant farm-house. But there was much to be talked of, in marrying her; and the good-natured wishes for her well-doing, which had proceeded before from all the spiteful old ladies in Meryton, lost but little of their spirit in this change of circumstances, because with such a husband her misery was considered certain."}
{"doc_id": "gutenberg_1342", "para_id": 1311, "text": "It was a fortnight since Mrs. Bennet had been down stairs, but on this happy day she again took her seat at the head of her table, and in spirits oppressively high. No sentiment of shame gave a damp to her triumph. The marriage of a daughter, which had been the first object of her wishes since Jane was sixteen, was now on the point of accomplishment, and her thoughts and her words ran wholly on those attendants of elegant nuptials, fine muslins, new carriages, and servants. She was busily searching through the neighbourhood for a proper situation for her daughter; and, without knowing or considering what their income might be, rejected many as deficient in size and importance."}
{"doc_id": "gutenberg_1342", "para_id": 1312, "text": "“Haye Park might do,” said she, “if the Gouldings would quit it, or the great house at Stoke, if the drawing-room were larger; but Ashworth is too far off. I could not bear to have her ten miles from me; and as for Purvis Lodge, the attics are dreadful.”"}
{"doc_id": "gutenberg_1342", "para_id": 1313, "text": "Her husband allowed her to talk on without interruption while the servants remained. But when they had withdrawn, he said to her, “Mrs. Bennet, before you take any, or all of these houses, for your son and daughter, let us come to a right understanding. Into _one_ house in this neighbourhood they shall never have admittance. I will not encourage the imprudence of either, by receiving them at Longbourn.”"}
{"doc_id": "gutenberg_1342", "para_id": 1314, "text": "A long dispute followed this declaration; but Mr. Bennet was firm: it soon led to another; and Mrs. Bennet found, with amazement and horror, that her husband would not advance a guinea to buy clothes for his daughter. He protested that she should receive from him no mark of affection whatever on the occasion. Mrs. Bennet could hardly comprehend it. That his anger could be carried to such a point of inconceivable resentment as to refuse his daughter a privilege, without which her marriage would scarcely seem valid, exceeded all that she could believe possible. She was more alive to the disgrace, which her want of new clothes must reflect on her daughter’s nuptials, than to any sense of shame at her eloping and living with Wickham a fortnight before they took place."}
{"doc_id": "gutenberg_1342", "para_id": 1315, "text": "Elizabeth was now most heartily sorry that she had, from the distress of the moment, been led to make Mr. Darcy acquainted with their fears for her sister; for since her marriage would so shortly give the proper termination to the elopement, they might hope to conceal its unfavourable beginning from all those who were not immediately on the spot."}
{"doc_id": "gutenberg_1342", "para_id": 1316, "text": "She had no fear of its spreading farther, through his means. There were few people on whose secrecy she would have more confidently depended; but at the same time there was no one whose knowledge of a sister’s frailty would have mortified her so much. Not, however, from any fear of disadvantage from it individually to herself; for at any rate there seemed a gulf impassable between them. Had Lydia’s marriage been concluded on the most honourable terms, it was not to be supposed that Mr. Darcy would connect himself with a family, where to every other objection would now be added an alliance and relationship of the nearest kind with the man whom he so justly scorned."}
{"doc_id": "gutenberg_1342", "para_id": 1317, "text": "From such a connection she could not wonder that he should shrink. The wish of procuring her regard, which she had assured herself of his feeling in Derbyshire, could not in rational expectation survive such a blow as this. She was humbled, she was grieved; she repented, though she hardly knew of what. She became jealous of his esteem, when she could no longer hope to be benefited by it. She wanted to hear of him, when there seemed the least chance of gaining intelligence. She was convinced that she could have been happy with him, when it was no longer likely they should meet."}
{"doc_id": "gutenberg_1342", "para_id": 1318, "text": "What a triumph for him, as she often thought, could he know that the proposals which she had proudly spurned only four months ago would now have been gladly and gratefully received! He was as generous, she doubted not, as the most generous of his sex. But while he was mortal, there must be a triumph."}
{"doc_id": "gutenberg_1342", "para_id": 1319, "text": "She began now to comprehend that he was exactly the man who, in disposition and talents, would most suit her. His understanding and temper, though unlike her own, would have answered all her wishes. It was an union that must have been to the advantage of both: by her ease and liveliness, his mind might have been softened, his manners improved; and from his judgment, information, and knowledge of the world, she must have received benefit of greater importance."}
{"doc_id": "gutenberg_1342", "para_id": 1320, "text": "But no such happy marriage could now teach the admiring multitude what connubial felicity really was. An union of a different tendency, and precluding the possibility of the other, was soon to be formed in their family."}
{"doc_id": "gutenberg_1342", "para_id": 1321, "text": "How Wickham and Lydia were to be supported in tolerable independence she could not imagine. But how little of permanent happiness could belong to a couple who were only brought together because their passions were stronger than their virtue, she could easily conjecture."}
{"doc_id": "gutenberg_1342", "para_id": 1322, "text": "Mr. Gardiner soon wrote again to his brother. To Mr. Bennet’s acknowledgments he briefly replied, with assurances of his eagerness to promote the welfare of any of his family; and concluded with entreaties that the subject might never be mentioned to him again. The principal purport of his letter was to inform them, that Mr. Wickham had resolved on quitting the militia."}
{"doc_id": "gutenberg_1342", "para_id": 1323, "text": "“It was greatly my wish that he should do so,” he added, “as soon as his marriage was fixed on. And I think you will agree with me, in considering a removal from that corps as highly advisable, both on his account and my niece’s. It is Mr. Wickham’s intention to go into the Regulars; and, among his former friends, there are still some who are able and willing to assist him in the army. He has the promise of an ensigncy in General----’s regiment, now quartered in the north. It is an advantage to have it so far from this part of the kingdom. He promises fairly; and I hope among different people, where they may each have a character to preserve, they will both be more prudent. I have written to Colonel Forster, to inform him of our present arrangements, and to request that he will satisfy the various creditors of Mr. Wickham in and near Brighton with assurances of speedy payment, for which I have pledged myself. And will you give yourself the trouble of carrying similar assurances to his creditors in Meryton, of whom I shall subjoin a list, according to his information? He has given in all his debts; I hope at least he has not deceived us. Haggerston has our directions, and all will be completed in a week. They will then join his regiment, unless they are first invited to Longbourn; and I understand from Mrs. Gardiner that my niece is very desirous of seeing you all before she leaves the south. She is well, and begs to be dutifully remembered to you and her mother.--Yours, etc."}
{"doc_id": "gutenberg_1342", "para_id": 1324, "text": "Mr. Bennet and his daughters saw all the advantages of Wickham’s removal from the ----shire, as clearly as Mr. Gardiner could do. But Mrs. Bennet was not so well pleased with it. Lydia’s being settled in the north, just when she had expected most pleasure and pride in her company, for she had by no means given up her plan of their residing in Hertfordshire, was a severe disappointment; and, besides, it was such a pity that Lydia should be taken from a regiment where she was acquainted with everybody, and had so many favourites."}
{"doc_id": "gutenberg_1342", "para_id": 1325, "text": "“She is so fond of Mrs. Forster,” said she, “it will be quite shocking to send her away! And there are several of the young men, too, that she likes very much. The officers may not be so pleasant in General----’s regiment.”"}
{"doc_id": "gutenberg_1342", "para_id": 1326, "text": "His daughter’s request, for such it might be considered, of being admitted into her family again, before she set off for the north, received at first an absolute negative. But Jane and Elizabeth, who agreed in wishing, for the sake of their sister’s feelings and consequence, that she should be noticed on her marriage by her parents, urged him so earnestly, yet so rationally and so mildly, to receive her and her husband at Longbourn, as soon as they were married, that he was prevailed on to think as they thought, and act as they wished. And their mother had the satisfaction of knowing, that she should be able to show her married daughter in the neighbourhood, before she was banished to the north. When Mr. Bennet wrote again to his brother, therefore, he sent his permission for them to come; and it was settled, that, as soon as the ceremony was over, they should proceed to Longbourn. Elizabeth was surprised, however, that Wickham should consent to such a scheme; and, had she consulted only her own inclination, any meeting with him would have been the last object of her wishes."}
{"doc_id": "gutenberg_1342", "para_id": 1327, "text": "Their sister’s wedding-day arrived; and Jane and Elizabeth felt for her probably more than she felt for herself. The carriage was sent to meet them at----, and they were to return in it by dinnertime. Their arrival was dreaded by the elder Miss Bennets--and Jane more especially, who gave Lydia the feelings which would have attended herself, had _she_ been the culprit, and was wretched in the thought of what her sister must endure."}
{"doc_id": "gutenberg_1342", "para_id": 1328, "text": "They came. The family were assembled in the breakfast-room to receive them. Smiles decked the face of Mrs. Bennet, as the carriage drove up to the door; her husband looked impenetrably grave; her daughters, alarmed, anxious, uneasy."}
{"doc_id": "gutenberg_1342", "para_id": 1329, "text": "Lydia’s voice was heard in the vestibule; the door was thrown open, and she ran into the room. Her mother stepped forwards, embraced her, and welcomed her with rapture; gave her hand with an affectionate smile to Wickham, who followed his lady; and wished them both joy, with an alacrity which showed no doubt of their happiness."}
{"doc_id": "gutenberg_1342", "para_id": 1330, "text": "Their reception from Mr. Bennet, to whom they then turned, was not quite so cordial. His countenance rather gained in austerity; and he scarcely opened his lips. The easy assurance of the young couple, indeed, was enough to provoke him."}
{"doc_id": "gutenberg_1342", "para_id": 1331, "text": "Elizabeth was disgusted, and even Miss Bennet was shocked. Lydia was Lydia still; untamed, unabashed, wild, noisy, and fearless. She turned from sister to sister, demanding their congratulations; and when at length they all sat down, looked eagerly round the room, took notice of some little alteration in it, and observed, with a laugh, that it was a great while since she had been there."}
{"doc_id": "gutenberg_1342", "para_id": 1332, "text": "Wickham was not at all more distressed than herself; but his manners were always so pleasing, that, had his character and his marriage been exactly what they ought, his smiles and his easy address, while he claimed their relationship, would have delighted them all. Elizabeth had not before believed him quite equal to such assurance; but she sat down, resolving within herself to draw no limits in future to the impudence of an impudent man. _She_ blushed, and Jane blushed; but the cheeks of the two who caused their confusion suffered no variation of colour."}
{"doc_id": "gutenberg_1342", "para_id": 1333, "text": "There was no want of discourse. The bride and her mother could neither of them talk fast enough; and Wickham, who happened to sit near Elizabeth, began inquiring after his acquaintance in that neighbourhood, with a good-humoured ease, which she felt very unable to equal in her replies. They seemed each of them to have the happiest memories in the world. Nothing of the past was recollected with pain; and Lydia led voluntarily to subjects which her sisters would not have alluded to for the world."}
{"doc_id": "gutenberg_1342", "para_id": 1334, "text": "“Only think of its being three months,” she cried, “since I went away: it seems but a fortnight, I declare; and yet there have been things enough happened in the time. Good gracious! when I went away, I am sure I had no more idea of being married till I came back again! though I thought it would be very good fun if I was.”"}
{"doc_id": "gutenberg_1342", "para_id": 1335, "text": "Her father lifted up his eyes, Jane was distressed, Elizabeth looked expressively at Lydia; but she, who never heard nor saw anything of which she chose to be insensible, gaily continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 1336, "text": "“Oh, mamma, do the people hereabouts know I am married to-day? I was afraid they might not; and we overtook William Goulding in his curricle, so I was determined he should know it, and so I let down the side glass next to him, and took off my glove and let my hand just rest upon the window frame, so that he might see the ring, and then I bowed and smiled like anything.”"}
{"doc_id": "gutenberg_1342", "para_id": 1337, "text": "Elizabeth could bear it no longer. She got up and ran out of the room; and returned no more, till she heard them passing through the hall to the dining-parlour. She then joined them soon enough to see Lydia, with anxious parade, walk up to her mother’s right hand, and hear her say to her eldest sister,--"}
{"doc_id": "gutenberg_1342", "para_id": 1338, "text": "“Ah, Jane, I take your place now, and you must go lower, because I am a married woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 1339, "text": "It was not to be supposed that time would give Lydia that embarrassment from which she had been so wholly free at first. Her ease and good spirits increased. She longed to see Mrs. Philips, the Lucases, and all their other neighbours, and to hear herself called “Mrs. Wickham” by each of them; and in the meantime she went after dinner to show her ring and boast of being married to Mrs. Hill and the two housemaids."}
{"doc_id": "gutenberg_1342", "para_id": 1340, "text": "“Well, mamma,” said she, when they were all returned to the breakfast-room, “and what do you think of my husband? Is not he a charming man? I am sure my sisters must all envy me. I only hope they may have half my good luck. They must all go to Brighton. That is the place to get husbands. What a pity it is, mamma, we did not all go!”"}
{"doc_id": "gutenberg_1342", "para_id": 1341, "text": "“Very true; and if I had my will we should. But, my dear Lydia, I don’t at all like your going such a way off. Must it be so?”"}
{"doc_id": "gutenberg_1342", "para_id": 1342, "text": "“Oh, Lord! yes; there is nothing in that. I shall like it of all things. You and papa, and my sisters, must come down and see us. We shall be at Newcastle all the winter, and I dare say there will be some balls, and I will take care to get good partners for them all.”"}
{"doc_id": "gutenberg_1342", "para_id": 1343, "text": "“And then when you go away, you may leave one or two of my sisters behind you; and I dare say I shall get husbands for them before the winter is over.”"}
{"doc_id": "gutenberg_1342", "para_id": 1344, "text": "“I thank you for my share of the favour,” said Elizabeth; “but I do not particularly like your way of getting husbands.”"}
{"doc_id": "gutenberg_1342", "para_id": 1345, "text": "Their visitors were not to remain above ten days with them. Mr. Wickham had received his commission before he left London, and he was to join his regiment at the end of a fortnight."}
{"doc_id": "gutenberg_1342", "para_id": 1346, "text": "No one but Mrs. Bennet regretted that their stay would be so short; and she made the most of the time by visiting about with her daughter, and having very frequent parties at home. These parties were acceptable to all; to avoid a family circle was even more desirable to such as did think than such as did not."}
{"doc_id": "gutenberg_1342", "para_id": 1347, "text": "Wickham’s affection for Lydia was just what Elizabeth had expected to find it; not equal to Lydia’s for him. She had scarcely needed her present observation to be satisfied, from the reason of things, that their elopement had been brought on by the strength of her love rather than by his; and she would have wondered why, without violently caring for her, he chose to elope with her at all, had she not felt certain that his flight was rendered necessary by distress of circumstances; and if that were the case, he was not the young man to resist an opportunity of having a companion."}
{"doc_id": "gutenberg_1342", "para_id": 1348, "text": "Lydia was exceedingly fond of him. He was her dear Wickham on every occasion; no one was to be put in competition with him. He did everything best in the world; and she was sure he would kill more birds on the first of September than anybody else in the country."}
{"doc_id": "gutenberg_1342", "para_id": 1349, "text": "One morning, soon after their arrival, as she was sitting with her two elder sisters, she said to Elizabeth,--"}
{"doc_id": "gutenberg_1342", "para_id": 1350, "text": "“Lizzy, I never gave _you_ an account of my wedding, I believe. You were not by, when I told mamma, and the others, all about it. Are not you curious to hear how it was managed?”"}
{"doc_id": "gutenberg_1342", "para_id": 1351, "text": "“No, really,” replied Elizabeth; “I think there cannot be too little said on the subject.”"}
{"doc_id": "gutenberg_1342", "para_id": 1352, "text": "“La! You are so strange! But I must tell you how it went off. We were married, you know, at St. Clement’s, because Wickham’s lodgings were in that parish. And it was settled that we should all be there by eleven o’clock. My uncle and aunt and I were to go together; and the others were to meet us at the church."}
{"doc_id": "gutenberg_1342", "para_id": 1353, "text": "“Well, Monday morning came, and I was in such a fuss! I was so afraid, you know, that something would happen to put it off, and then I should have gone quite distracted. And there was my aunt, all the time I was dressing, preaching and talking away just as if she was reading a sermon. However, I did not hear above one word in ten, for I was thinking, you may suppose, of my dear Wickham. I longed to know whether he would be married in his blue coat."}
{"doc_id": "gutenberg_1342", "para_id": 1354, "text": "“Well, and so we breakfasted at ten as usual: I thought it would never be over; for, by the bye, you are to understand that my uncle and aunt were horrid unpleasant all the time I was with them. If you’ll believe me, I did not once put my foot out of doors, though I was there a fortnight. Not one party, or scheme, or anything! To be sure, London was rather thin, but, however, the Little Theatre was open."}
{"doc_id": "gutenberg_1342", "para_id": 1355, "text": "“Well, and so, just as the carriage came to the door, my uncle was called away upon business to that horrid man Mr. Stone. And then, you know, when once they get together, there is no end of it. Well, I was so frightened I did not know what to do, for my uncle was to give me away; and if we were beyond the hour we could not be married all day. But, luckily, he came back again in ten minutes’ time, and then we all set out. However, I recollected afterwards, that if he _had_ been prevented going, the wedding need not be put off, for Mr. Darcy might have done as well.”"}
{"doc_id": "gutenberg_1342", "para_id": 1356, "text": "“Oh, yes! he was to come there with Wickham, you know. But, gracious me! I quite forgot! I ought not to have said a word about it. I promised them so faithfully! What will Wickham say? It was to be such a secret!”"}
{"doc_id": "gutenberg_1342", "para_id": 1357, "text": "“If it was to be a secret,” said Jane, “say not another word on the subject. You may depend upon my seeking no further.”"}
{"doc_id": "gutenberg_1342", "para_id": 1358, "text": "“Oh, certainly,” said Elizabeth, though burning with curiosity; “we will ask you no questions.”"}
{"doc_id": "gutenberg_1342", "para_id": 1359, "text": "“Thank you,” said Lydia; “for if you did, I should certainly tell you all, and then Wickham would be so angry.”"}
{"doc_id": "gutenberg_1342", "para_id": 1360, "text": "On such encouragement to ask, Elizabeth was forced to put it out of her power, by running away."}
{"doc_id": "gutenberg_1342", "para_id": 1361, "text": "But to live in ignorance on such a point was impossible; or at least it was impossible not to try for information. Mr. Darcy had been at her sister’s wedding. It was exactly a scene, and exactly among people, where he had apparently least to do, and least temptation to go. Conjectures as to the meaning of it, rapid and wild, hurried into her brain; but she was satisfied with none. Those that best pleased her, as placing his conduct in the noblest light, seemed most improbable. She could not bear such suspense; and hastily seizing a sheet of paper, wrote a short letter to her aunt, to request an explanation of what Lydia had dropped, if it were compatible with the secrecy which had been intended."}
{"doc_id": "gutenberg_1342", "para_id": 1362, "text": "“You may readily comprehend,” she added, “what my curiosity must be to know how a person unconnected with any of us, and, comparatively speaking, a stranger to our family, should have been amongst you at such a time. Pray write instantly, and let me understand it--unless it is, for very cogent reasons, to remain in the secrecy which Lydia seems to think necessary; and then I must endeavour to be satisfied with ignorance.”"}
{"doc_id": "gutenberg_1342", "para_id": 1363, "text": "“Not that I _shall_, though,” she added to herself, and she finished the letter; “and, my dear aunt, if you do not tell me in an honourable manner, I shall certainly be reduced to tricks and stratagems to find it out.”"}
{"doc_id": "gutenberg_1342", "para_id": 1364, "text": "Jane’s delicate sense of honour would not allow her to speak to Elizabeth privately of what Lydia had let fall; Elizabeth was glad of it:--till it appeared whether her inquiries would receive any satisfaction, she had rather be without a confidante."}
{"doc_id": "gutenberg_1342", "para_id": 1365, "text": "Elizabeth had the satisfaction of receiving an answer to her letter as soon as she possibly could. She was no sooner in possession of it, than hurrying into the little copse, where she was least likely to be interrupted, she sat down on one of the benches, and prepared to be happy; for the length of the letter convinced her that it did not contain a denial."}
{"doc_id": "gutenberg_1342", "para_id": 1366, "text": "“I have just received your letter, and shall devote this whole morning to answering it, as I foresee that a _little_ writing will not comprise what I have to tell you. I must confess myself surprised by your application; I did not expect it from _you_. Don’t think me angry, however, for I only mean to let you know, that I had not imagined such inquiries to be necessary on _your_ side. If you do not choose to understand me, forgive my impertinence. Your uncle is as much surprised as I am; and nothing but the belief of your being a party concerned would have allowed him to act as he has done. But if you are really innocent and ignorant, I must be more explicit. On the very day of my coming home from Longbourn, your uncle had a most unexpected visitor. Mr. Darcy called, and was shut up with him several hours. It was all over before I arrived; so my curiosity was not so dreadfully racked as _yours_ seems to have been. He came to tell Mr. Gardiner that he had found out where your sister and Mr. Wickham were, and that he had seen and talked with them both--Wickham repeatedly, Lydia once. From what I can collect, he left Derbyshire only one day after ourselves, and came to town with the resolution of hunting for them. The motive professed was his conviction of its being owing to himself that Wickham’s worthlessness had not been so well known as to make it impossible for any young woman of character to love or confide in him. He generously imputed the whole to his mistaken pride, and confessed that he had before thought it beneath him to lay his private actions open to the world. His character was to speak for itself. He called it, therefore, his duty to step forward, and endeavour to remedy an evil which had been brought on by himself. If he _had another_ motive, I am sure it would never disgrace him. He had been some days in town before he was able to discover them; but he had something to direct his search, which was more than _we_ had; and the consciousness of this was another reason for his resolving to follow us. There is a lady, it seems, a Mrs. Younge, who was some time ago governess to Miss Darcy, and was dismissed from her charge on some cause of disapprobation, though he did not say what. She then took a large house in Edward Street, and has since maintained herself by letting lodgings. This Mrs. Younge was, he knew, intimately acquainted with Wickham; and he went to her for intelligence of him, as soon as he got to town. But it was two or three days before he could get from her what he wanted. She would not betray her trust, I suppose, without bribery and corruption, for she really did know where her friend was to be found. Wickham, indeed, had gone to her on their first arrival in London; and had she been able to receive them into her house, they would have taken up their abode with her. At length, however, our kind friend procured the wished-for direction. They were in ---- Street. He saw Wickham, and afterwards insisted on seeing Lydia. His first object with her, he acknowledged, had been to persuade her to quit her present disgraceful situation, and return to her friends as soon as they could be prevailed on to receive her, offering his assistance as far as it would go. But he found Lydia absolutely resolved on remaining where she was. She cared for none of her friends; she wanted no help of his; she would not hear of leaving Wickham. She was sure they should be married some time or other, and it did not much signify when. Since such were her feelings, it only remained, he thought, to secure and expedite a marriage, which, in his very first conversation with Wickham, he easily learnt had never been _his_ design. He confessed himself obliged to leave the regiment on account of some debts of honour which were very pressing; and scrupled not to lay all the ill consequences of Lydia’s flight on her own folly alone. He meant to resign his commission immediately; and as to his future situation, he could conjecture very little about it. He must go somewhere, but he did not know where, and he knew he should have nothing to live on. Mr. Darcy asked why he did not marry your sister at once. Though Mr. Bennet was not imagined to be very rich, he would have been able to do something for him, and his situation must have been benefited by marriage. But he found, in reply to this question, that Wickham still cherished the hope of more effectually making his fortune by marriage, in some other country. Under such circumstances, however, he was not likely to be proof against the temptation of immediate relief. They met several times, for there was much to be discussed. Wickham, of course, wanted more than he could get; but at length was reduced to be reasonable. Everything being settled between _them_, Mr. Darcy’s next step was to make your uncle acquainted with it, and he first called in Gracechurch Street the evening before I came home. But Mr. Gardiner could not be seen; and Mr. Darcy found, on further inquiry, that your father was still with him, but would quit town the next morning. He did not judge your father to be a person whom he could so properly consult as your uncle, and therefore readily postponed seeing him till after the departure of the former. He did not leave his name, and till the next day it was only known that a gentleman had called on business. On Saturday he came again. Your father was gone, your uncle at home, and, as I said before, they had a great deal of talk together. They met again on Sunday, and then _I_ saw him too. It was not all settled before Monday: as soon as it was, the express was sent off to Longbourn. But our visitor was very obstinate. I fancy, Lizzy, that obstinacy is the real defect of his character, after all. He has been accused of many faults at different times; but _this_ is the true one. Nothing was to be done that he did not do himself; though I am sure (and I do not speak it to be thanked, therefore say nothing about it) your uncle would most readily have settled the whole. They battled it together for a long time, which was more than either the gentleman or lady concerned in it deserved. But at last your uncle was forced to yield, and instead of being allowed to be of use to his niece, was forced to put up with only having the probable credit of it, which went sorely against the grain; and I really believe your letter this morning gave him great pleasure, because it required an explanation that would rob him of his borrowed feathers, and give the praise where it was due. But, Lizzy, this must go no further than yourself, or Jane at most. You know pretty well, I suppose, what has been done for the young people. His debts are to be paid, amounting, I believe, to considerably more than a thousand pounds, another thousand in addition to her own settled upon _her_, and his commission purchased. The reason why all this was to be done by him alone, was such as I have given above. It was owing to him, to his reserve and want of proper consideration, that Wickham’s character had been so misunderstood, and consequently that he had been received and noticed as he was. Perhaps there was some truth in _this_; though I doubt whether _his_ reserve, or _anybody’s_ reserve can be answerable for the event. But in spite of all this fine talking, my dear Lizzy, you may rest perfectly assured that your uncle would never have yielded, if we had not given him credit for _another interest_ in the affair. When all this was resolved on, he returned again to his friends, who were still staying at Pemberley; but it was agreed that he should be in London once more when the wedding took place, and all money matters were then to receive the last finish. I believe I have now told you everything. It is a relation which you tell me is to give you great surprise; I hope at least it will not afford you any displeasure. Lydia came to us, and Wickham had constant admission to the house. _He_ was exactly what he had been when I knew him in Hertfordshire; but I would not tell you how little I was satisfied with _her_ behaviour while she stayed with us, if I had not perceived, by Jane’s letter last Wednesday, that her conduct on coming home was exactly of a piece with it, and therefore what I now tell you can give you no fresh pain. I talked to her repeatedly in the most serious manner, representing to her the wickedness of what she had done, and all the unhappiness she had brought on her family. If she heard me, it was by good luck, for I am sure she did not listen. I was sometimes quite provoked; but then I recollected my dear Elizabeth and Jane, and for their sakes had patience with her. Mr. Darcy was punctual in his return, and, as Lydia informed you, attended the wedding. He dined with us the next day, and was to leave town again on Wednesday or Thursday. Will you be very angry with me, my dear Lizzy, if I take this opportunity of saying (what I was never bold enough to say before) how much I like him? His behaviour to us has, in every respect, been as pleasing as when we were in Derbyshire. His understanding and opinions all please me; he wants nothing but a little more liveliness, and _that_, if he marry _prudently_, his wife may teach him. I thought him very sly; he hardly ever mentioned your name. But slyness seems the fashion. Pray forgive me, if I have been very presuming, or at least do not punish me so far as to exclude me from P. I shall never be quite happy till I have been all round the park. A low phaeton with a nice little pair of ponies would be the very thing. But I must write no more. The children have been wanting me this half hour."}
{"doc_id": "gutenberg_1342", "para_id": 1367, "text": "The contents of this letter threw Elizabeth into a flutter of spirits, in which it was difficult to determine whether pleasure or pain bore the greatest share. The vague and unsettled suspicions which uncertainty had produced, of what Mr. Darcy might have been doing to forward her sister’s match--which she had feared to encourage, as an exertion of goodness too great to be probable, and at the same time dreaded to be just, from the pain of obligation--were proved beyond their greatest extent to be true! He had followed them purposely to town, he had taken on himself all the trouble and mortification attendant on such a research; in which supplication had been necessary to a woman whom he must abominate and despise, and where he was reduced to meet, frequently meet, reason with, persuade, and finally bribe the man whom he always most wished to avoid, and whose very name it was punishment to him to pronounce. He had done all this for a girl whom he could neither regard nor esteem. Her heart did whisper that he had done it for her. But it was a hope shortly checked by other considerations; and she soon felt that even her vanity was insufficient, when required to depend on his affection for her, for a woman who had already refused him, as able to overcome a sentiment so natural as abhorrence against relationship with Wickham. Brother-in-law of Wickham! Every kind of pride must revolt from the connection. He had, to be sure, done much. She was ashamed to think how much. But he had given a reason for his interference, which asked no extraordinary stretch of belief. It was reasonable that he should feel he had been wrong; he had liberality, and he had the means of exercising it; and though she would not place herself as his principal inducement, she could perhaps believe, that remaining partiality for her might assist his endeavours in a cause where her peace of mind must be materially concerned. It was painful, exceedingly painful, to know that they were under obligations to a person who could never receive a return. They owed the restoration of Lydia, her character, everything to him. Oh, how heartily did she grieve over every ungracious sensation she had ever encouraged, every saucy speech she had ever directed towards him! For herself she was humbled; but she was proud of him,--proud that in a cause of compassion and honour he had been able to get the better of himself. She read over her aunt’s commendation of him again and again. It was hardly enough; but it pleased her. She was even sensible of some pleasure, though mixed with regret, on finding how steadfastly both she and her uncle had been persuaded that affection and confidence subsisted between Mr. Darcy and herself."}
{"doc_id": "gutenberg_1342", "para_id": 1368, "text": "She was roused from her seat and her reflections, by someone’s approach; and, before she could strike into another path, she was overtaken by Wickham."}
{"doc_id": "gutenberg_1342", "para_id": 1369, "text": "“I am afraid I interrupt your solitary ramble, my dear sister?” said he, as he joined her."}
{"doc_id": "gutenberg_1342", "para_id": 1370, "text": "“You certainly do,” she replied with a smile; “but it does not follow that the interruption must be unwelcome.”"}
{"doc_id": "gutenberg_1342", "para_id": 1371, "text": "“I should be sorry, indeed, if it were. _We_ were always good friends, and now we are better.”"}
{"doc_id": "gutenberg_1342", "para_id": 1372, "text": "“I do not know. Mrs. Bennet and Lydia are going in the carriage to Meryton. And so, my dear sister, I find, from our uncle and aunt, that you have actually seen Pemberley.”"}
{"doc_id": "gutenberg_1342", "para_id": 1373, "text": "“I almost envy you the pleasure, and yet I believe it would be too much for me, or else I could take it in my way to Newcastle. And you saw the old housekeeper, I suppose? Poor Reynolds, she was always very fond of me. But of course she did not mention my name to you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1374, "text": "“That you were gone into the army, and she was afraid had--not turned out well. At such a distance as _that_, you know, things are strangely misrepresented.”"}
{"doc_id": "gutenberg_1342", "para_id": 1375, "text": "“Certainly,” he replied, biting his lips. Elizabeth hoped she had silenced him; but he soon afterwards said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1376, "text": "“I was surprised to see Darcy in town last month. We passed each other several times. I wonder what he can be doing there.”"}
{"doc_id": "gutenberg_1342", "para_id": 1377, "text": "“Perhaps preparing for his marriage with Miss de Bourgh,” said Elizabeth. “It must be something particular to take him there at this time of year.”"}
{"doc_id": "gutenberg_1342", "para_id": 1378, "text": "“Undoubtedly. Did you see him while you were at Lambton? I thought I understood from the Gardiners that you had.”"}
{"doc_id": "gutenberg_1342", "para_id": 1379, "text": "“I have heard, indeed, that she is uncommonly improved within this year or two. When I last saw her, she was not very promising. I am very glad you liked her. I hope she will turn out well.”"}
{"doc_id": "gutenberg_1342", "para_id": 1380, "text": "“I mention it because it is the living which I ought to have had. A most delightful place! Excellent parsonage-house! It would have suited me in every respect.”"}
{"doc_id": "gutenberg_1342", "para_id": 1381, "text": "“Exceedingly well. I should have considered it as part of my duty, and the exertion would soon have been nothing. One ought not to repine; but, to be sure, it would have been such a thing for me! The quiet, the retirement of such a life, would have answered all my ideas of happiness! But it was not to be. Did you ever hear Darcy mention the circumstance when you were in Kent?”"}
{"doc_id": "gutenberg_1342", "para_id": 1382, "text": "“I _have_ heard from authority, which I thought _as good_, that it was left you conditionally only, and at the will of the present patron.”"}
{"doc_id": "gutenberg_1342", "para_id": 1383, "text": "“You have! Yes, there was something in _that_; I told you so from the first, you may remember.”"}
{"doc_id": "gutenberg_1342", "para_id": 1384, "text": "“I _did_ hear, too, that there was a time when sermon-making was not so palatable to you as it seems to be at present; that you actually declared your resolution of never taking orders, and that the business had been compromised accordingly.”"}
{"doc_id": "gutenberg_1342", "para_id": 1385, "text": "“You did! and it was not wholly without foundation. You may remember what I told you on that point, when first we talked of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1386, "text": "They were now almost at the door of the house, for she had walked fast to get rid of him; and unwilling, for her sister’s sake, to provoke him, she only said in reply, with a good-humoured smile,--"}
{"doc_id": "gutenberg_1342", "para_id": 1387, "text": "“Come, Mr. Wickham, we are brother and sister, you know. Do not let us quarrel about the past. In future, I hope we shall be always of one mind.”"}
{"doc_id": "gutenberg_1342", "para_id": 1388, "text": "She held out her hand: he kissed it with affectionate gallantry, though he hardly knew how to look, and they entered the house."}
{"doc_id": "gutenberg_1342", "para_id": 1389, "text": "Mr. Wickham was so perfectly satisfied with this conversation, that he never again distressed himself, or provoked his dear sister Elizabeth, by introducing the subject of it; and she was pleased to find that she had said enough to keep him quiet."}
{"doc_id": "gutenberg_1342", "para_id": 1390, "text": "The day of his and Lydia’s departure soon came; and Mrs. Bennet was forced to submit to a separation, which, as her husband by no means entered into her scheme of their all going to Newcastle, was likely to continue at least a twelvemonth."}
{"doc_id": "gutenberg_1342", "para_id": 1391, "text": "“As often as I can. But you know married women have never much time for writing. My sisters may write to _me_. They will have nothing else to do.”"}
{"doc_id": "gutenberg_1342", "para_id": 1392, "text": "Mr. Wickham’s adieus were much more affectionate than his wife’s. He smiled, looked handsome, and said many pretty things."}
{"doc_id": "gutenberg_1342", "para_id": 1393, "text": "“He is as fine a fellow,” said Mr. Bennet, as soon as they were out of the house, “as ever I saw. He simpers, and smirks, and makes love to us all. I am prodigiously proud of him. I defy even Sir William Lucas himself to produce a more valuable son-in-law.”"}
{"doc_id": "gutenberg_1342", "para_id": 1394, "text": "“I often think,” said she, “that there is nothing so bad as parting with one’s friends. One seems so forlorn without them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1395, "text": "“This is the consequence, you see, madam, of marrying a daughter,” said Elizabeth. “It must make you better satisfied that your other four are single.”"}
{"doc_id": "gutenberg_1342", "para_id": 1396, "text": "“It is no such thing. Lydia does not leave me because she is married; but only because her husband’s regiment happens to be so far off. If that had been nearer, she would not have gone so soon.”"}
{"doc_id": "gutenberg_1342", "para_id": 1397, "text": "But the spiritless condition which this event threw her into was shortly relieved, and her mind opened again to the agitation of hope, by an article of news which then began to be in circulation. The housekeeper at Netherfield had received orders to prepare for the arrival of her master, who was coming down in a day or two, to shoot there for several weeks. Mrs. Bennet was quite in the fidgets. She looked at Jane, and smiled, and shook her head, by turns."}
{"doc_id": "gutenberg_1342", "para_id": 1398, "text": "“Well, well, and so Mr. Bingley is coming down, sister,” (for Mrs. Philips first brought her the news). “Well, so much the better. Not that I care about it, though. He is nothing to us, you know, and I am sure I never want to see him again. But, however, he is very welcome to come to Netherfield, if he likes it. And who knows what _may_ happen? But that is nothing to us. You know, sister, we agreed long ago never to mention a word about it. And so, it is quite certain he is coming?”"}
{"doc_id": "gutenberg_1342", "para_id": 1399, "text": "“You may depend on it,” replied the other, “for Mrs. Nichols was in Meryton last night: I saw her passing by, and went out myself on purpose to know the truth of it; and she told me that it was certainly true. He comes down on Thursday, at the latest, very likely on Wednesday. She was going to the butcher’s, she told me, on purpose to order in some meat on Wednesday, and she has got three couple of ducks just fit to be killed.”"}
{"doc_id": "gutenberg_1342", "para_id": 1400, "text": "Miss Bennet had not been able to hear of his coming without changing colour. It was many months since she had mentioned his name to Elizabeth; but now, as soon as they were alone together, she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1401, "text": "“I saw you look at me to-day, Lizzy, when my aunt told us of the present report; and I know I appeared distressed; but don’t imagine it was from any silly cause. I was only confused for the moment, because I felt that I _should_ be looked at. I do assure you that the news does not affect me either with pleasure or pain. I am glad of one thing, that he comes alone; because we shall see the less of him. Not that I am afraid of _myself_, but I dread other people’s remarks.”"}
{"doc_id": "gutenberg_1342", "para_id": 1402, "text": "Elizabeth did not know what to make of it. Had she not seen him in Derbyshire, she might have supposed him capable of coming there with no other view than what was acknowledged; but she still thought him partial to Jane, and she wavered as to the greater probability of his coming there _with_ his friend’s permission, or being bold enough to come without it."}
{"doc_id": "gutenberg_1342", "para_id": 1403, "text": "“Yet it is hard,” she sometimes thought, “that this poor man cannot come to a house, which he has legally hired, without raising all this speculation! I _will_ leave him to himself.”"}
{"doc_id": "gutenberg_1342", "para_id": 1404, "text": "In spite of what her sister declared, and really believed to be her feelings, in the expectation of his arrival, Elizabeth could easily perceive that her spirits were affected by it. They were more disturbed, more unequal, than she had often seen them."}
{"doc_id": "gutenberg_1342", "para_id": 1405, "text": "The subject which had been so warmly canvassed between their parents, about a twelvemonth ago, was now brought forward again."}
{"doc_id": "gutenberg_1342", "para_id": 1406, "text": "“As soon as ever Mr. Bingley comes, my dear,” said Mrs. Bennet, “you will wait on him, of course.”"}
{"doc_id": "gutenberg_1342", "para_id": 1407, "text": "“No, no. You forced me into visiting him last year, and promised, if I went to see him, he should marry one of my daughters. But it ended in nothing, and I will not be sent on a fool’s errand again.”"}
{"doc_id": "gutenberg_1342", "para_id": 1408, "text": "His wife represented to him how absolutely necessary such an attention would be from all the neighbouring gentlemen, on his returning to Netherfield."}
{"doc_id": "gutenberg_1342", "para_id": 1409, "text": "“’Tis an _etiquette_ I despise,” said he. “If he wants our society, let him seek it. He knows where we live. I will not spend _my_ hours in running after my neighbours every time they go away and come back again.”"}
{"doc_id": "gutenberg_1342", "para_id": 1410, "text": "“Well, all I know is, that it will be abominably rude if you do not wait on him. But, however, that shan’t prevent my asking him to dine here, I am determined. We must have Mrs. Long and the Gouldings soon. That will make thirteen with ourselves, so there will be just room at table for him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1411, "text": "Consoled by this resolution, she was the better able to bear her husband’s incivility; though it was very mortifying to know that her neighbours might all see Mr. Bingley, in consequence of it, before _they_ did. As the day of his arrival drew near,--"}
{"doc_id": "gutenberg_1342", "para_id": 1412, "text": "“I begin to be sorry that he comes at all,” said Jane to her sister. “It would be nothing; I could see him with perfect indifference; but I can hardly bear to hear it thus perpetually talked of. My mother means well; but she does not know, no one can know, how much I suffer from what she says. Happy shall I be when his stay at Netherfield is over!”"}
{"doc_id": "gutenberg_1342", "para_id": 1413, "text": "“I wish I could say anything to comfort you,” replied Elizabeth; “but it is wholly out of my power. You must feel it; and the usual satisfaction of preaching patience to a sufferer is denied me, because you have always so much.”"}
{"doc_id": "gutenberg_1342", "para_id": 1414, "text": "Mr. Bingley arrived. Mrs. Bennet, through the assistance of servants, contrived to have the earliest tidings of it, that the period of anxiety and fretfulness on her side be as long as it could. She counted the days that must intervene before their invitation could be sent--hopeless of seeing him before. But on the third morning after his arrival in Hertfordshire, she saw him from her dressing-room window enter the paddock, and ride towards the house."}
{"doc_id": "gutenberg_1342", "para_id": 1415, "text": "Her daughters were eagerly called to partake of her joy. Jane resolutely kept her place at the table; but Elizabeth, to satisfy her mother, went to the window--she looked--she saw Mr. Darcy with him, and sat down again by her sister."}
{"doc_id": "gutenberg_1342", "para_id": 1416, "text": "“La!” replied Kitty, “it looks just like that man that used to be with him before. Mr. what’s his name--that tall, proud man.”"}
{"doc_id": "gutenberg_1342", "para_id": 1417, "text": "“Good gracious! Mr. Darcy!--and so it does, I vow. Well, any friend of Mr. Bingley’s will always be welcome here, to be sure; but else I must say that I hate the very sight of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1418, "text": "Jane looked at Elizabeth with surprise and concern. She knew but little of their meeting in Derbyshire, and therefore felt for the awkwardness which must attend her sister, in seeing him almost for the first time after receiving his explanatory letter. Both sisters were uncomfortable enough. Each felt for the other, and of course for themselves; and their mother talked on of her dislike of Mr. Darcy, and her resolution to be civil to him only as Mr. Bingley’s friend, without being heard by either of them. But Elizabeth had sources of uneasiness which could not yet be suspected by Jane, to whom she had never yet had courage to show Mrs. Gardiner’s letter, or to relate her own change of sentiment towards him. To Jane, he could be only a man whose proposals she had refused, and whose merits she had undervalued; but to her own more extensive information, he was the person to whom the whole family were indebted for the first of benefits, and whom she regarded herself with an interest, if not quite so tender, at least as reasonable and just, as what Jane felt for Bingley. Her astonishment at his coming--at his coming to Netherfield, to Longbourn, and voluntarily seeking her again, was almost equal to what she had known on first witnessing his altered behaviour in Derbyshire."}
{"doc_id": "gutenberg_1342", "para_id": 1419, "text": "The colour which had been driven from her face returned for half a minute with an additional glow, and a smile of delight added lustre to her eyes, as she thought for that space of time that his affection and wishes must still be unshaken; but she would not be secure."}
{"doc_id": "gutenberg_1342", "para_id": 1420, "text": "“Let me first see how he behaves,” said she; “it will then be early enough for expectation.”"}
{"doc_id": "gutenberg_1342", "para_id": 1421, "text": "She sat intently at work, striving to be composed, and without daring to lift up her eyes, till anxious curiosity carried them to the face of her sister as the servant was approaching the door. Jane looked a little paler than usual, but more sedate than Elizabeth had expected. On the gentlemen’s appearing, her colour increased; yet she received them with tolerable ease, and with a propriety of behaviour equally free from any symptom of resentment, or any unnecessary complaisance."}
{"doc_id": "gutenberg_1342", "para_id": 1422, "text": "Elizabeth said as little to either as civility would allow, and sat down again to her work, with an eagerness which it did not often command. She had ventured only one glance at Darcy. He looked serious as usual; and, she thought, more as he had been used to look in Hertfordshire, than as she had seen him at Pemberley. But, perhaps, he could not in her mother’s presence be what he was before her uncle and aunt. It was a painful, but not an improbable, conjecture."}
{"doc_id": "gutenberg_1342", "para_id": 1423, "text": "Bingley she had likewise seen for an instant, and in that short period saw him looking both pleased and embarrassed. He was received by Mrs. Bennet with a degree of civility which made her two daughters ashamed, especially when contrasted with the cold and ceremonious politeness of her courtesy and address of his friend."}
{"doc_id": "gutenberg_1342", "para_id": 1424, "text": "Elizabeth particularly, who knew that her mother owed to the latter the preservation of her favourite daughter from irremediable infamy, was hurt and distressed to a most painful degree by a distinction so ill applied."}
{"doc_id": "gutenberg_1342", "para_id": 1425, "text": "Darcy, after inquiring of her how Mr. and Mrs. Gardiner did--a question which she could not answer without confusion--said scarcely anything. He was not seated by her: perhaps that was the reason of his silence; but it had not been so in Derbyshire. There he had talked to her friends when he could not to herself. But now several minutes elapsed, without bringing the sound of his voice; and when occasionally, unable to resist the impulse of curiosity, she raised her eyes to his face, she as often found him looking at Jane as at herself, and frequently on no object but the ground. More thoughtfulness and less anxiety to please, than when they last met, were plainly expressed. She was disappointed, and angry with herself for being so."}
{"doc_id": "gutenberg_1342", "para_id": 1426, "text": "She was in no humour for conversation with anyone but himself; and to him she had hardly courage to speak."}
{"doc_id": "gutenberg_1342", "para_id": 1427, "text": "“I began to be afraid you would never come back again. People _did_ say, you meant to quit the place entirely at Michaelmas; but, however, I hope it is not true. A great many changes have happened in the neighbourhood since you went away. Miss Lucas is married and settled: and one of my own daughters. I suppose you have heard of it; indeed, you must have seen it in the papers. It was in the ‘Times’ and the ‘Courier,’ I know; though it was not put in as it ought to be. It was only said, ‘Lately, George Wickham, Esq., to Miss Lydia Bennet,’ without there being a syllable said of her father, or the place where she lived, or anything. It was my brother Gardiner’s drawing up, too, and I wonder how he came to make such an awkward business of it. Did you see it?”"}
{"doc_id": "gutenberg_1342", "para_id": 1428, "text": "Bingley replied that he did, and made his congratulations. Elizabeth dared not lift up her eyes. How Mr. Darcy looked, therefore, she could not tell."}
{"doc_id": "gutenberg_1342", "para_id": 1429, "text": "“It is a delightful thing, to be sure, to have a daughter well married,” continued her mother; “but at the same time, Mr. Bingley, it is very hard to have her taken away from me. They are gone down to Newcastle, a place quite northward it seems, and there they are to stay, I do not know how long. His regiment is there; for I suppose you have heard of his leaving the ----shire, and of his being gone into the Regulars. Thank heaven! he has _some_ friends, though, perhaps, not so many as he deserves.”"}
{"doc_id": "gutenberg_1342", "para_id": 1430, "text": "Elizabeth, who knew this to be levelled at Mr. Darcy, was in such misery of shame that she could hardly keep her seat. It drew from her, however, the exertion of speaking, which nothing else had so effectually done before; and she asked Bingley whether he meant to make any stay in the country at present. A few weeks, he believed."}
{"doc_id": "gutenberg_1342", "para_id": 1431, "text": "“When you have killed all your own birds, Mr. Bingley,” said her mother, “I beg you will come here and shoot as many as you please on Mr. Bennet’s manor. I am sure he will be vastly happy to oblige you, and will save all the best of the coveys for you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1432, "text": "Elizabeth’s misery increased at such unnecessary, such officious attention! Were the same fair prospect to arise at present, as had flattered them a year ago, everything, she was persuaded, would be hastening to the same vexatious conclusion. At that instant she felt, that years of happiness could not make Jane or herself amends for moments of such painful confusion."}
{"doc_id": "gutenberg_1342", "para_id": 1433, "text": "“The first wish of my heart,” said she to herself, “is never more to be in company with either of them. Their society can afford no pleasure that will atone for such wretchedness as this! Let me never see either one or the other again!”"}
{"doc_id": "gutenberg_1342", "para_id": 1434, "text": "Yet the misery, for which years of happiness were to offer no compensation, received soon afterwards material relief, from observing how much the beauty of her sister rekindled the admiration of her former lover. When first he came in, he had spoken to her but little, but every five minutes seemed to be giving her more of his attention. He found her as handsome as she had been last year; as good-natured, and as unaffected, though not quite so chatty. Jane was anxious that no difference should be perceived in her at all, and was really persuaded that she talked as much as ever; but her mind was so busily engaged, that she did not always know when she was silent."}
{"doc_id": "gutenberg_1342", "para_id": 1435, "text": "When the gentlemen rose to go away, Mrs. Bennet was mindful of her intended civility, and they were invited and engaged to dine at Longbourn in a few days’ time."}
{"doc_id": "gutenberg_1342", "para_id": 1436, "text": "“You are quite a visit in my debt, Mr. Bingley,” she added; “for when you went to town last winter, you promised to take a family dinner with us as soon as you returned. I have not forgot, you see; and I assure you I was very much disappointed that you did not come back and keep your engagement.”"}
{"doc_id": "gutenberg_1342", "para_id": 1437, "text": "Bingley looked a little silly at this reflection, and said something of his concern at having been prevented by business. They then went away."}
{"doc_id": "gutenberg_1342", "para_id": 1438, "text": "Mrs. Bennet had been strongly inclined to ask them to stay and dine there that day; but, though she always kept a very good table, she did not think anything less than two courses could be good enough for a man on whom she had such anxious designs, or satisfy the appetite and pride of one who had ten thousand a year."}
{"doc_id": "gutenberg_1342", "para_id": 1439, "text": "As soon as they were gone, Elizabeth walked out to recover her spirits; or, in other words, to dwell without interruption on those subjects which must deaden them more. Mr. Darcy’s behaviour astonished and vexed her."}
{"doc_id": "gutenberg_1342", "para_id": 1440, "text": "“Why, if he came only to be silent, grave, and indifferent,” said she, “did he come at all?”"}
{"doc_id": "gutenberg_1342", "para_id": 1441, "text": "“He could be still amiable, still pleasing to my uncle and aunt, when he was in town; and why not to me? If he fears me, why come hither? If he no longer cares for me, why silent? Teasing, teasing man! I will think no more about him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1442, "text": "Her resolution was for a short time involuntarily kept by the approach of her sister, who joined her with a cheerful look which showed her better satisfied with their visitors than Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 1443, "text": "“Now,” said she, “that this first meeting is over, I feel perfectly easy. I know my own strength, and I shall never be embarrassed again by his coming. I am glad he dines here on Tuesday. It will then be publicly seen, that on both sides we meet only as common and indifferent acquaintance.”"}
{"doc_id": "gutenberg_1342", "para_id": 1444, "text": "“Yes, very indifferent, indeed,” said Elizabeth, laughingly. “Oh, Jane! take care.”"}
{"doc_id": "gutenberg_1342", "para_id": 1445, "text": "“I think you are in very great danger of making him as much in love with you as ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 1446, "text": "They did not see the gentlemen again till Tuesday; and Mrs. Bennet, in the meanwhile, was giving way to all the happy schemes which the good-humour and common politeness of Bingley, in half an hour’s visit, had revived."}
{"doc_id": "gutenberg_1342", "para_id": 1447, "text": "On Tuesday there was a large party assembled at Longbourn; and the two who were most anxiously expected, to the credit of their punctuality as sportsmen, were in very good time. When they repaired to the dining-room, Elizabeth eagerly watched to see whether Bingley would take the place which, in all their former parties, had belonged to him, by her sister. Her prudent mother, occupied by the same ideas, forbore to invite him to sit by herself. On entering the room, he seemed to hesitate; but Jane happened to look round, and happened to smile: it was decided. He placed himself by her."}
{"doc_id": "gutenberg_1342", "para_id": 1448, "text": "Elizabeth, with a triumphant sensation, looked towards his friend. He bore it with noble indifference; and she would have imagined that Bingley had received his sanction to be happy, had she not seen his eyes likewise turned towards Mr. Darcy, with an expression of half-laughing alarm."}
{"doc_id": "gutenberg_1342", "para_id": 1449, "text": "His behaviour to her sister was such during dinnertime as showed an admiration of her, which, though more guarded than formerly, persuaded Elizabeth, that, if left wholly to himself, Jane’s happiness, and his own, would be speedily secured. Though she dared not depend upon the consequence, she yet received pleasure from observing his behaviour. It gave her all the animation that her spirits could boast; for she was in no cheerful humour. Mr. Darcy was almost as far from her as the table could divide them. He was on one side of her mother. She knew how little such a situation would give pleasure to either, or make either appear to advantage. She was not near enough to hear any of their discourse; but she could see how seldom they spoke to each other, and how formal and cold was their manner whenever they did. Her mother’s ungraciousness made the sense of what they owed him more painful to Elizabeth’s mind; and she would, at times, have given anything to be privileged to tell him, that his kindness was neither unknown nor unfelt by the whole of the family."}
{"doc_id": "gutenberg_1342", "para_id": 1450, "text": "She was in hopes that the evening would afford some opportunity of bringing them together; that the whole of the visit would not pass away without enabling them to enter into something more of conversation, than the mere ceremonious salutation attending his entrance. Anxious and uneasy, the period which passed in the drawing-room before the gentlemen came, was wearisome and dull to a degree that almost made her uncivil. She looked forward to their entrance as the point on which all her chance of pleasure for the evening must depend."}
{"doc_id": "gutenberg_1342", "para_id": 1451, "text": "The gentlemen came; and she thought he looked as if he would have answered her hopes; but, alas! the ladies had crowded round the table, where Miss Bennet was making tea, and Elizabeth pouring out the coffee, in so close a confederacy, that there was not a single vacancy near her which would admit of a chair. And on the gentlemen’s approaching, one of the girls moved closer to her than ever, and said, in a whisper,--"}
{"doc_id": "gutenberg_1342", "para_id": 1452, "text": "“The men shan’t come and part us, I am determined. We want none of them; do we?”"}
{"doc_id": "gutenberg_1342", "para_id": 1453, "text": "Darcy had walked away to another part of the room. She followed him with her eyes, envied everyone to whom he spoke, had scarcely patience enough to help anybody to coffee, and then was enraged against herself for being so silly!"}
{"doc_id": "gutenberg_1342", "para_id": 1454, "text": "“A man who has once been refused! How could I ever be foolish enough to expect a renewal of his love? Is there one among the sex who would not protest against such a weakness as a second proposal to the same woman? There is no indignity so abhorrent to their feelings.”"}
{"doc_id": "gutenberg_1342", "para_id": 1455, "text": "She was a little revived, however, by his bringing back his coffee-cup himself; and she seized the opportunity of saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 1456, "text": "“Mrs. Annesley is with her. The others have been gone on to Scarborough these three weeks.”"}
{"doc_id": "gutenberg_1342", "para_id": 1457, "text": "She could think of nothing more to say; but if he wished to converse with her, he might have better success. He stood by her, however, for some minutes, in silence; and, at last, on the young lady’s whispering to Elizabeth again, he walked away."}
{"doc_id": "gutenberg_1342", "para_id": 1458, "text": "When the tea things were removed, and the card tables placed, the ladies all rose; and Elizabeth was then hoping to be soon joined by him, when all her views were overthrown, by seeing him fall a victim to her mother’s rapacity for whist players, and in a few moments after seated with the rest of the party. She now lost every expectation of pleasure. They were confined for the evening at different tables; and she had nothing to hope, but that his eyes were so often turned towards her side of the room, as to make him play as unsuccessfully as herself."}
{"doc_id": "gutenberg_1342", "para_id": 1459, "text": "Mrs. Bennet had designed to keep the two Netherfield gentlemen to supper; but their carriage was, unluckily, ordered before any of the others, and she had no opportunity of detaining them."}
{"doc_id": "gutenberg_1342", "para_id": 1460, "text": "“Well, girls,” said she, as soon as they were left to themselves, “what say you to the day? I think everything has passed off uncommonly well, I assure you. The dinner was as well dressed as any I ever saw. The venison was roasted to a turn--and everybody said, they never saw so fat a haunch. The soup was fifty times better than what we had at the Lucases’ last week; and even Mr. Darcy acknowledged that the partridges were remarkably well done; and I suppose he has two or three French cooks at least. And, my dear Jane, I never saw you look in greater beauty. Mrs. Long said so too, for I asked her whether you did not. And what do you think she said besides? ‘Ah! Mrs. Bennet, we shall have her at Netherfield at last!’ She did, indeed. I do think Mrs. Long is as good a creature as ever lived--and her nieces are very pretty behaved girls, and not at all handsome: I like them prodigiously.”"}
{"doc_id": "gutenberg_1342", "para_id": 1461, "text": "Mrs. Bennet, in short, was in very great spirits: she had seen enough of Bingley’s behaviour to Jane to be convinced that she would get him at last; and her expectations of advantage to her family, when in a happy humour, were so far beyond reason, that she was quite disappointed at not seeing him there again the next day, to make his proposals."}
{"doc_id": "gutenberg_1342", "para_id": 1462, "text": "“It has been a very agreeable day,” said Miss Bennet to Elizabeth. “The party seemed so well selected, so suitable one with the other. I hope we may often meet again.”"}
{"doc_id": "gutenberg_1342", "para_id": 1463, "text": "“Lizzy, you must not do so. You must not suspect me. It mortifies me. I assure you that I have now learnt to enjoy his conversation as an agreeable and sensible young man without having a wish beyond it. I am perfectly satisfied, from what his manners now are, that he never had any design of engaging my affection. It is only that he is blessed with greater sweetness of address, and a stronger desire of generally pleasing, than any other man.”"}
{"doc_id": "gutenberg_1342", "para_id": 1464, "text": "“You are very cruel,” said her sister, “you will not let me smile, and are provoking me to it every moment.”"}
{"doc_id": "gutenberg_1342", "para_id": 1465, "text": "“How hard it is in some cases to be believed! And how impossible in others! But why should you wish to persuade me that I feel more than I acknowledge?”"}
{"doc_id": "gutenberg_1342", "para_id": 1466, "text": "“That is a question which I hardly know how to answer. We all love to instruct, though we can teach only what is not worth knowing. Forgive me; and if you persist in indifference, do not make _me_ your confidante.”"}
{"doc_id": "gutenberg_1342", "para_id": 1467, "text": "A few days after this visit, Mr. Bingley called again, and alone. His friend had left him that morning for London, but was to return home in ten days’ time. He sat with them above an hour, and was in remarkably good spirits. Mrs. Bennet invited him to dine with them; but, with many expressions of concern, he confessed himself engaged elsewhere."}
{"doc_id": "gutenberg_1342", "para_id": 1468, "text": "He should be particularly happy at any time, etc., etc.; and if she would give him leave, would take an early opportunity of waiting on them."}
{"doc_id": "gutenberg_1342", "para_id": 1469, "text": "Yes, he had no engagement at all for to-morrow; and her invitation was accepted with alacrity."}
{"doc_id": "gutenberg_1342", "para_id": 1470, "text": "He came, and in such very good time, that the ladies were none of them dressed. In ran Mrs. Bennet to her daughters’ room, in her dressing-gown, and with her hair half finished, crying out,--"}
{"doc_id": "gutenberg_1342", "para_id": 1471, "text": "“My dear Jane, make haste and hurry down. He is come--Mr. Bingley is come. He is, indeed. Make haste, make haste. Here, Sarah, come to Miss Bennet this moment, and help her on with her gown. Never mind Miss Lizzy’s hair.”"}
{"doc_id": "gutenberg_1342", "para_id": 1472, "text": "“We will be down as soon as we can,” said Jane; “but I dare say Kitty is forwarder than either of us, for she went upstairs half an hour ago.”"}
{"doc_id": "gutenberg_1342", "para_id": 1473, "text": "“Oh! hang Kitty! what has she to do with it? Come, be quick, be quick! where is your sash, my dear?”"}
{"doc_id": "gutenberg_1342", "para_id": 1474, "text": "But when her mother was gone, Jane would not be prevailed on to go down without one of her sisters."}
{"doc_id": "gutenberg_1342", "para_id": 1475, "text": "The same anxiety to get them by themselves was visible again in the evening. After tea, Mr. Bennet retired to the library, as was his custom, and Mary went upstairs to her instrument. Two obstacles of the five being thus removed, Mrs. Bennet sat looking and winking at Elizabeth and Catherine for a considerable time, without making any impression on them. Elizabeth would not observe her; and when at last Kitty did, she very innocently said, “What is the matter, mamma? What do you keep winking at me for? What am I to do?”"}
{"doc_id": "gutenberg_1342", "para_id": 1476, "text": "“Nothing, child, nothing. I did not wink at you.” She then sat still five minutes longer; but unable to waste such a precious occasion, she suddenly got up, and saying to Kitty,--"}
{"doc_id": "gutenberg_1342", "para_id": 1477, "text": "“Come here, my love, I want to speak to you,” took her out of the room. Jane instantly gave a look at Elizabeth which spoke her distress at such premeditation, and her entreaty that _she_ would not give in to it. In a few minutes, Mrs. Bennet half opened the door and called out,--"}
{"doc_id": "gutenberg_1342", "para_id": 1478, "text": "“We may as well leave them by themselves, you know,” said her mother as soon as she was in the hall. “Kitty and I are going upstairs to sit in my dressing-room.”"}
{"doc_id": "gutenberg_1342", "para_id": 1479, "text": "Elizabeth made no attempt to reason with her mother, but remained quietly in the hall till she and Kitty were out of sight, then returned into the drawing-room."}
{"doc_id": "gutenberg_1342", "para_id": 1480, "text": "Mrs. Bennet’s schemes for this day were ineffectual. Bingley was everything that was charming, except the professed lover of her daughter. His ease and cheerfulness rendered him a most agreeable addition to their evening party; and he bore with the ill-judged officiousness of the mother, and heard all her silly remarks with a forbearance and command of countenance particularly grateful to the daughter."}
{"doc_id": "gutenberg_1342", "para_id": 1481, "text": "He scarcely needed an invitation to stay supper; and before he went away an engagement was formed, chiefly through his own and Mrs. Bennet’s means, for his coming next morning to shoot with her husband."}
{"doc_id": "gutenberg_1342", "para_id": 1482, "text": "After this day, Jane said no more of her indifference. Not a word passed between the sisters concerning Bingley; but Elizabeth went to bed in the happy belief that all must speedily be concluded, unless Mr. Darcy returned within the stated time. Seriously, however, she felt tolerably persuaded that all this must have taken place with that gentleman’s concurrence."}
{"doc_id": "gutenberg_1342", "para_id": 1483, "text": "Bingley was punctual to his appointment; and he and Mr. Bennet spent the morning together, as had been agreed on. The latter was much more agreeable than his companion expected. There was nothing of presumption or folly in Bingley that could provoke his ridicule, or disgust him into silence; and he was more communicative, and less eccentric, than the other had ever seen him. Bingley of course returned with him to dinner; and in the evening Mrs. Bennet’s invention was again at work to get everybody away from him and her daughter. Elizabeth, who had a letter to write, went into the breakfast-room for that purpose soon after tea; for as the others were all going to sit down to cards, she could not be wanted to counteract her mother’s schemes."}
{"doc_id": "gutenberg_1342", "para_id": 1484, "text": "But on her returning to the drawing-room, when her letter was finished, she saw, to her infinite surprise, there was reason to fear that her mother had been too ingenious for her. On opening the door, she perceived her sister and Bingley standing together over the hearth, as if engaged in earnest conversation; and had this led to no suspicion, the faces of both, as they hastily turned round and moved away from each other, would have told it all. _Their_ situation was awkward enough; but _hers_ she thought was still worse. Not a syllable was uttered by either; and Elizabeth was on the point of going away again, when Bingley, who as well as the other had sat down, suddenly rose, and, whispering a few words to her sister, ran out of the room."}
{"doc_id": "gutenberg_1342", "para_id": 1485, "text": "Jane could have no reserves from Elizabeth, where confidence would give pleasure; and, instantly embracing her, acknowledged, with the liveliest emotion, that she was the happiest creature in the world."}
{"doc_id": "gutenberg_1342", "para_id": 1486, "text": "“’Tis too much!” she added, “by far too much. I do not deserve it. Oh, why is not everybody as happy?”"}
{"doc_id": "gutenberg_1342", "para_id": 1487, "text": "Elizabeth’s congratulations were given with a sincerity, a warmth, a delight, which words could but poorly express. Every sentence of kindness was a fresh source of happiness to Jane. But she would not allow herself to stay with her sister, or say half that remained to be said, for the present."}
{"doc_id": "gutenberg_1342", "para_id": 1488, "text": "“I must go instantly to my mother,” she cried. “I would not on any account trifle with her affectionate solicitude, or allow her to hear it from anyone but myself. He is gone to my father already. Oh, Lizzy, to know that what I have to relate will give such pleasure to all my dear family! how shall I bear so much happiness?”"}
{"doc_id": "gutenberg_1342", "para_id": 1489, "text": "She then hastened away to her mother, who had purposely broken up the card-party, and was sitting upstairs with Kitty."}
{"doc_id": "gutenberg_1342", "para_id": 1490, "text": "Elizabeth, who was left by herself, now smiled at the rapidity and ease with which an affair was finally settled, that had given them so many previous months of suspense and vexation."}
{"doc_id": "gutenberg_1342", "para_id": 1491, "text": "“And this,” said she, “is the end of all his friend’s anxious circumspection! of all his sister’s falsehood and contrivance! the happiest, wisest, and most reasonable end!”"}
{"doc_id": "gutenberg_1342", "para_id": 1492, "text": "In a few minutes she was joined by Bingley, whose conference with her father had been short and to the purpose."}
{"doc_id": "gutenberg_1342", "para_id": 1493, "text": "He then shut the door, and, coming up to her, claimed the good wishes and affection of a sister. Elizabeth honestly and heartily expressed her delight in the prospect of their relationship. They shook hands with great cordiality; and then, till her sister came down, she had to listen to all he had to say of his own happiness, and of Jane’s perfections; and in spite of his being a lover, Elizabeth really believed all his expectations of felicity to be rationally founded, because they had for basis the excellent understanding and super-excellent disposition of Jane, and a general similarity of feeling and taste between her and himself."}
{"doc_id": "gutenberg_1342", "para_id": 1494, "text": "It was an evening of no common delight to them all; the satisfaction of Miss Bennet’s mind gave such a glow of sweet animation to her face, as made her look handsomer than ever. Kitty simpered and smiled, and hoped her turn was coming soon. Mrs. Bennet could not give her consent, or speak her approbation in terms warm enough to satisfy her feelings, though she talked to Bingley of nothing else, for half an hour; and when Mr. Bennet joined them at supper, his voice and manner plainly showed how really happy he was."}
{"doc_id": "gutenberg_1342", "para_id": 1495, "text": "Not a word, however, passed his lips in allusion to it, till their visitor took his leave for the night; but as soon as he was gone, he turned to his daughter and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1496, "text": "“You are a good girl,” he replied, “and I have great pleasure in thinking you will be so happily settled. I have not a doubt of your doing very well together. Your tempers are by no means unlike. You are each of you so complying, that nothing will ever be resolved on; so easy, that every servant will cheat you; and so generous, that you will always exceed your income.”"}
{"doc_id": "gutenberg_1342", "para_id": 1497, "text": "“I hope not so. Imprudence or thoughtlessness in money matters would be unpardonable in _me_.”"}
{"doc_id": "gutenberg_1342", "para_id": 1498, "text": "“Exceed their income! My dear Mr. Bennet,” cried his wife, “what are you talking of? Why, he has four or five thousand a year, and very likely more.” Then addressing her daughter, “Oh, my dear, dear Jane, I am so happy! I am sure I shan’t get a wink of sleep all night. I knew how it would be. I always said it must be so, at last. I was sure you could not be so beautiful for nothing! I remember, as soon as ever I saw him, when he first came into Hertfordshire last year, I thought how likely it was that you should come together. Oh, he is the handsomest young man that ever was seen!”"}
{"doc_id": "gutenberg_1342", "para_id": 1499, "text": "Wickham, Lydia, were all forgotten. Jane was beyond competition her favourite child. At that moment she cared for no other. Her younger sisters soon began to make interest with her for objects of happiness which she might in future be able to dispense."}
{"doc_id": "gutenberg_1342", "para_id": 1500, "text": "Mary petitioned for the use of the library at Netherfield; and Kitty begged very hard for a few balls there every winter."}
{"doc_id": "gutenberg_1342", "para_id": 1501, "text": "Bingley, from this time, was of course a daily visitor at Longbourn; coming frequently before breakfast, and always remaining till after supper; unless when some barbarous neighbour, who could not be enough detested, had given him an invitation to dinner, which he thought himself obliged to accept."}
{"doc_id": "gutenberg_1342", "para_id": 1502, "text": "Elizabeth had now but little time for conversation with her sister; for while he was present Jane had no attention to bestow on anyone else: but she found herself considerably useful to both of them, in those hours of separation that must sometimes occur. In the absence of Jane, he always attached himself to Elizabeth for the pleasure of talking of her; and when Bingley was gone, Jane constantly sought the same means of relief."}
{"doc_id": "gutenberg_1342", "para_id": 1503, "text": "“He has made me so happy,” said she, one evening, “by telling me that he was totally ignorant of my being in town last spring! I had not believed it possible.”"}
{"doc_id": "gutenberg_1342", "para_id": 1504, "text": "“It must have been his sisters’ doing. They were certainly no friends to his acquaintance with me, which I cannot wonder at, since he might have chosen so much more advantageously in many respects. But when they see, as I trust they will, that their brother is happy with me, they will learn to be contented, and we shall be on good terms again: though we can never be what we once were to each other.”"}
{"doc_id": "gutenberg_1342", "para_id": 1505, "text": "“That is the most unforgiving speech,” said Elizabeth, “that I ever heard you utter. Good girl! It would vex me, indeed, to see you again the dupe of Miss Bingley’s pretended regard.”"}
{"doc_id": "gutenberg_1342", "para_id": 1506, "text": "“Would you believe it, Lizzy, that when he went to town last November he really loved me, and nothing but a persuasion of _my_ being indifferent would have prevented his coming down again?”"}
{"doc_id": "gutenberg_1342", "para_id": 1507, "text": "This naturally introduced a panegyric from Jane on his diffidence, and the little value he put on his own good qualities."}
{"doc_id": "gutenberg_1342", "para_id": 1508, "text": "Elizabeth was pleased to find that he had not betrayed the interference of his friend; for, though Jane had the most generous and forgiving heart in the world, she knew it was a circumstance which must prejudice her against him."}
{"doc_id": "gutenberg_1342", "para_id": 1509, "text": "“I am certainly the most fortunate creature that ever existed!” cried Jane. “Oh, Lizzy, why am I thus singled from my family, and blessed above them all? If I could but see you as happy! If there were but such another man for you!”"}
{"doc_id": "gutenberg_1342", "para_id": 1510, "text": "“If you were to give me forty such men I never could be so happy as you. Till I have your disposition, your goodness, I never can have your happiness. No, no, let me shift for myself; and, perhaps, if I have very good luck, I may meet with another Mr. Collins in time.”"}
{"doc_id": "gutenberg_1342", "para_id": 1511, "text": "The situation of affairs in the Longbourn family could not be long a secret. Mrs. Bennet was privileged to whisper it to Mrs. Philips, and she ventured, without any permission, to do the same by all her neighbours in Meryton."}
{"doc_id": "gutenberg_1342", "para_id": 1512, "text": "The Bennets were speedily pronounced to be the luckiest family in the world; though only a few weeks before, when Lydia had first run away, they had been generally proved to be marked out for misfortune."}
{"doc_id": "gutenberg_1342", "para_id": 1513, "text": "One morning, about a week after Bingley’s engagement with Jane had been formed, as he and the females of the family were sitting together in the dining-room, their attention was suddenly drawn to the window by the sound of a carriage; and they perceived a chaise and four driving up the lawn. It was too early in the morning for visitors; and besides, the equipage did not answer to that of any of their neighbours. The horses were post; and neither the carriage, nor the livery of the servant who preceded it, were familiar to them. As it was certain, however, that somebody was coming, Bingley instantly prevailed on Miss Bennet to avoid the confinement of such an intrusion, and walk away with him into the shrubbery. They both set off; and the conjectures of the remaining three continued, though with little satisfaction, till the door was thrown open, and their visitor entered. It was Lady Catherine de Bourgh."}
{"doc_id": "gutenberg_1342", "para_id": 1514, "text": "They were of course all intending to be surprised: but their astonishment was beyond their expectation; and on the part of Mrs. Bennet and Kitty, though she was perfectly unknown to them, even inferior to what Elizabeth felt."}
{"doc_id": "gutenberg_1342", "para_id": 1515, "text": "She entered the room with an air more than usually ungracious, made no other reply to Elizabeth’s salutation than a slight inclination of the head, and sat down without saying a word. Elizabeth had mentioned her name to her mother on her Ladyship’s entrance, though no request of introduction had been made."}
{"doc_id": "gutenberg_1342", "para_id": 1516, "text": "Mrs. Bennet, all amazement, though flattered by having a guest of such high importance, received her with the utmost politeness. After sitting for a moment in silence, she said, very stiffly, to Elizabeth,--"}
{"doc_id": "gutenberg_1342", "para_id": 1517, "text": "“Yes, madam,” said Mrs. Bennet, delighted to speak to a Lady Catherine. “She is my youngest girl but one. My youngest of all is lately married, and my eldest is somewhere about the ground, walking with a young man, who, I believe, will soon become a part of the family.”"}
{"doc_id": "gutenberg_1342", "para_id": 1518, "text": "“You have a very small park here,” returned Lady Catherine, after a short silence."}
{"doc_id": "gutenberg_1342", "para_id": 1519, "text": "“It is nothing in comparison of Rosings, my Lady, I dare say; but, I assure you, it is much larger than Sir William Lucas’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 1520, "text": "“This must be a most inconvenient sitting-room for the evening in summer: the windows are full west.”"}
{"doc_id": "gutenberg_1342", "para_id": 1521, "text": "Mrs. Bennet assured her that they never sat there after dinner; and then added,--"}
{"doc_id": "gutenberg_1342", "para_id": 1522, "text": "“May I take the liberty of asking your Ladyship whether you left Mr. and Mrs. Collins well?”"}
{"doc_id": "gutenberg_1342", "para_id": 1523, "text": "Elizabeth now expected that she would produce a letter for her from Charlotte, as it seemed the only probable motive for her calling. But no letter appeared, and she was completely puzzled."}
{"doc_id": "gutenberg_1342", "para_id": 1524, "text": "Mrs. Bennet, with great civility, begged her Ladyship to take some refreshment: but Lady Catherine very resolutely, and not very politely, declined eating anything; and then, rising up, said to Elizabeth,--"}
{"doc_id": "gutenberg_1342", "para_id": 1525, "text": "“Miss Bennet, there seemed to be a prettyish kind of a little wilderness on one side of your lawn. I should be glad to take a turn in it, if you will favour me with your company.”"}
{"doc_id": "gutenberg_1342", "para_id": 1526, "text": "“Go, my dear,” cried her mother, “and show her Ladyship about the different walks. I think she will be pleased with the hermitage.”"}
{"doc_id": "gutenberg_1342", "para_id": 1527, "text": "Elizabeth obeyed; and, running into her own room for her parasol, attended her noble guest downstairs. As they passed through the hall, Lady Catherine opened the doors into the dining-parlour and drawing-room, and pronouncing them, after a short survey, to be decent-looking rooms, walked on."}
{"doc_id": "gutenberg_1342", "para_id": 1528, "text": "Her carriage remained at the door, and Elizabeth saw that her waiting-woman was in it. They proceeded in silence along the gravel walk that led to the copse; Elizabeth was determined to make no effort for conversation with a woman who was now more than usually insolent and disagreeable."}
{"doc_id": "gutenberg_1342", "para_id": 1529, "text": "“How could I ever think her like her nephew?” said she, as she looked in her face."}
{"doc_id": "gutenberg_1342", "para_id": 1530, "text": "As soon as they entered the copse, Lady Catherine began in the following manner:--"}
{"doc_id": "gutenberg_1342", "para_id": 1531, "text": "“You can be at no loss, Miss Bennet, to understand the reason of my journey hither. Your own heart, your own conscience, must tell you why I come.”"}
{"doc_id": "gutenberg_1342", "para_id": 1532, "text": "“Indeed, you are mistaken, madam; I have not been at all able to account for the honour of seeing you here.”"}
{"doc_id": "gutenberg_1342", "para_id": 1533, "text": "“Miss Bennet,” replied her Ladyship, in an angry tone, “you ought to know that I am not to be trifled with. But however insincere _you_ may choose to be, you shall not find _me_ so. My character has ever been celebrated for its sincerity and frankness; and in a cause of such moment as this, I shall certainly not depart from it. A report of a most alarming nature reached me two days ago. I was told, that not only your sister was on the point of being most advantageously married, but that _you_--that Miss Elizabeth Bennet would, in all likelihood, be soon afterwards united to my nephew--my own nephew, Mr. Darcy. Though I _know_ it must be a scandalous falsehood, though I would not injure him so much as to suppose the truth of it possible, I instantly resolved on setting off for this place, that I might make my sentiments known to you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1534, "text": "“If you believed it impossible to be true,” said Elizabeth, colouring with astonishment and disdain, “I wonder you took the trouble of coming so far. What could your Ladyship propose by it?”"}
{"doc_id": "gutenberg_1342", "para_id": 1535, "text": "“Your coming to Longbourn, to see me and my family,” said Elizabeth coolly, “will be rather a confirmation of it--if, indeed, such a report is in existence.”"}
{"doc_id": "gutenberg_1342", "para_id": 1536, "text": "“If! do you then pretend to be ignorant of it? Has it not been industriously circulated by yourselves? Do you not know that such a report is spread abroad?”"}
{"doc_id": "gutenberg_1342", "para_id": 1537, "text": "“I do not pretend to possess equal frankness with your Ladyship. _You_ may ask questions which _I_ shall not choose to answer.”"}
{"doc_id": "gutenberg_1342", "para_id": 1538, "text": "“This is not to be borne. Miss Bennet, I insist on being satisfied. Has he, has my nephew, made you an offer of marriage?”"}
{"doc_id": "gutenberg_1342", "para_id": 1539, "text": "“It ought to be so; it must be so, while he retains the use of his reason. But _your_ arts and allurements may, in a moment of infatuation, have made him forget what he owes to himself and to all his family. You may have drawn him in.”"}
{"doc_id": "gutenberg_1342", "para_id": 1540, "text": "“Miss Bennet, do you know who I am? I have not been accustomed to such language as this. I am almost the nearest relation he has in the world, and am entitled to know all his dearest concerns.”"}
{"doc_id": "gutenberg_1342", "para_id": 1541, "text": "“But you are not entitled to know _mine_; nor will such behaviour as this ever induce me to be explicit.”"}
{"doc_id": "gutenberg_1342", "para_id": 1542, "text": "“Let me be rightly understood. This match, to which you have the presumption to aspire, can never take place. No, never. Mr. Darcy is engaged to _my daughter_. Now, what have you to say?”"}
{"doc_id": "gutenberg_1342", "para_id": 1543, "text": "“Only this,--that if he is so, you can have no reason to suppose he will make an offer to me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1544, "text": "“The engagement between them is of a peculiar kind. From their infancy, they have been intended for each other. It was the favourite wish of _his_ mother, as well as of hers. While in their cradles we planned the union; and now, at the moment when the wishes of both sisters would be accomplished, is their marriage to be prevented by a young woman of inferior birth, of no importance in the world, and wholly unallied to the family? Do you pay no regard to the wishes of his friends--to his tacit engagement with Miss de Bourgh? Are you lost to every feeling of propriety and delicacy? Have you not heard me say, that from his earliest hours he was destined for his cousin?”"}
{"doc_id": "gutenberg_1342", "para_id": 1545, "text": "“Yes; and I had heard it before. But what is that to me? If there is no other objection to my marrying your nephew, I shall certainly not be kept from it by knowing that his mother and aunt wished him to marry Miss de Bourgh. You both did as much as you could in planning the marriage. Its completion depended on others. If Mr. Darcy is neither by honour nor inclination confined to his cousin, why is not he to make another choice? And if I am that choice, why may not I accept him?”"}
{"doc_id": "gutenberg_1342", "para_id": 1546, "text": "“Because honour, decorum, prudence--nay, interest--forbid it. Yes, Miss Bennet, interest; for do not expect to be noticed by his family or friends, if you wilfully act against the inclinations of all. You will be censured, slighted, and despised, by everyone connected with him. Your alliance will be a disgrace; your name will never even be mentioned by any of us.”"}
{"doc_id": "gutenberg_1342", "para_id": 1547, "text": "“These are heavy misfortunes,” replied Elizabeth. “But the wife of Mr. Darcy must have such extraordinary sources of happiness necessarily attached to her situation, that she could, upon the whole, have no cause to repine.”"}
{"doc_id": "gutenberg_1342", "para_id": 1548, "text": "“Obstinate, headstrong girl! I am ashamed of you! Is this your gratitude for my attentions to you last spring? Is nothing due to me on that score? Let us sit down. You are to understand, Miss Bennet, that I came here with the determined resolution of carrying my purpose; nor will I be dissuaded from it. I have not been used to submit to any person’s whims. I have not been in the habit of brooking disappointment.”"}
{"doc_id": "gutenberg_1342", "para_id": 1549, "text": "“_That_ will make your Ladyship’s situation at present more pitiable; but it will have no effect on _me_.”"}
{"doc_id": "gutenberg_1342", "para_id": 1550, "text": "“I will not be interrupted! Hear me in silence. My daughter and my nephew are formed for each other. They are descended, on the maternal side, from the same noble line; and, on the father’s, from respectable, honourable, and ancient, though untitled, families. Their fortune on both sides is splendid. They are destined for each other by the voice of every member of their respective houses; and what is to divide them?--the upstart pretensions of a young woman without family, connections, or fortune! Is this to be endured? But it must not, shall not be! If you were sensible of your own good, you would not wish to quit the sphere in which you have been brought up.”"}
{"doc_id": "gutenberg_1342", "para_id": 1551, "text": "“In marrying your nephew, I should not consider myself as quitting that sphere. He is a gentleman; I am a gentleman’s daughter; so far we are equal.”"}
{"doc_id": "gutenberg_1342", "para_id": 1552, "text": "“True. You _are_ a gentleman’s daughter. But what was your mother? Who are your uncles and aunts? Do not imagine me ignorant of their condition.”"}
{"doc_id": "gutenberg_1342", "para_id": 1553, "text": "“Whatever my connections may be,” said Elizabeth, “if your nephew does not object to them, they can be nothing to _you_.”"}
{"doc_id": "gutenberg_1342", "para_id": 1554, "text": "Though Elizabeth would not, for the mere purpose of obliging Lady Catherine, have answered this question, she could not but say, after a moment’s deliberation,--"}
{"doc_id": "gutenberg_1342", "para_id": 1555, "text": "“Miss Bennet, I am shocked and astonished. I expected to find a more reasonable young woman. But do not deceive yourself into a belief that I will ever recede. I shall not go away till you have given me the assurance I require.”"}
{"doc_id": "gutenberg_1342", "para_id": 1556, "text": "“And I certainly _never_ shall give it. I am not to be intimidated into anything so wholly unreasonable. Your Ladyship wants Mr. Darcy to marry your daughter; but would my giving you the wished-for promise make _their_ marriage at all more probable? Supposing him to be attached to me, would _my_ refusing to accept his hand make him wish to bestow it on his cousin? Allow me to say, Lady Catherine, that the arguments with which you have supported this extraordinary application have been as frivolous as the application was ill-judged. You have widely mistaken my character, if you think I can be worked on by such persuasions as these. How far your nephew might approve of your interference in _his_ affairs, I cannot tell; but you have certainly no right to concern yourself in mine. I must beg, therefore, to be importuned no further on the subject.”"}
{"doc_id": "gutenberg_1342", "para_id": 1557, "text": "“Not so hasty, if you please. I have by no means done. To all the objections I have already urged I have still another to add. I am no stranger to the particulars of your youngest sister’s infamous elopement. I know it all; that the young man’s marrying her was a patched-up business, at the expense of your father and uncle. And is _such_ a girl to be my nephew’s sister? Is _her_ husband, who is the son of his late father’s steward, to be his brother? Heaven and earth!--of what are you thinking? Are the shades of Pemberley to be thus polluted?”"}
{"doc_id": "gutenberg_1342", "para_id": 1558, "text": "“You can _now_ have nothing further to say,” she resentfully answered. “You have insulted me, in every possible method. I must beg to return to the house.”"}
{"doc_id": "gutenberg_1342", "para_id": 1559, "text": "And she rose as she spoke. Lady Catherine rose also, and they turned back. Her Ladyship was highly incensed."}
{"doc_id": "gutenberg_1342", "para_id": 1560, "text": "“You have no regard, then, for the honour and credit of my nephew! Unfeeling, selfish girl! Do you not consider that a connection with you must disgrace him in the eyes of everybody?”"}
{"doc_id": "gutenberg_1342", "para_id": 1561, "text": "“I have said no such thing. I am only resolved to act in that manner, which will, in my own opinion, constitute my happiness, without reference to _you_, or to any person so wholly unconnected with me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1562, "text": "“It is well. You refuse, then, to oblige me. You refuse to obey the claims of duty, honour, and gratitude. You are determined to ruin him in the opinion of all his friends, and make him the contempt of the world.”"}
{"doc_id": "gutenberg_1342", "para_id": 1563, "text": "“Neither duty, nor honour, nor gratitude,” replied Elizabeth, “has any possible claim on me, in the present instance. No principle of either would be violated by my marriage with Mr. Darcy. And with regard to the resentment of his family, or the indignation of the world, if the former _were_ excited by his marrying me, it would not give me one moment’s concern--and the world in general would have too much sense to join in the scorn.”"}
{"doc_id": "gutenberg_1342", "para_id": 1564, "text": "“And this is your real opinion! This is your final resolve! Very well. I shall now know how to act. Do not imagine, Miss Bennet, that your ambition will ever be gratified. I came to try you. I hoped to find you reasonable; but depend upon it I will carry my point.”"}
{"doc_id": "gutenberg_1342", "para_id": 1565, "text": "In this manner Lady Catherine talked on till they were at the door of the carriage, when, turning hastily round, she added,--"}
{"doc_id": "gutenberg_1342", "para_id": 1566, "text": "“I take no leave of you, Miss Bennet. I send no compliments to your mother. You deserve no such attention. I am most seriously displeased.”"}
{"doc_id": "gutenberg_1342", "para_id": 1567, "text": "Elizabeth made no answer; and without attempting to persuade her Ladyship to return into the house, walked quietly into it herself. She heard the carriage drive away as she proceeded upstairs. Her mother impatiently met her at the door of her dressing-room, to ask why Lady Catherine would not come in again and rest herself."}
{"doc_id": "gutenberg_1342", "para_id": 1568, "text": "“She is a very fine-looking woman! and her calling here was prodigiously civil! for she only came, I suppose, to tell us the Collinses were well. She is on her road somewhere, I dare say; and so, passing through Meryton, thought she might as well call on you. I suppose she had nothing particular to say to you, Lizzy?”"}
{"doc_id": "gutenberg_1342", "para_id": 1569, "text": "Elizabeth was forced to give in to a little falsehood here; for to acknowledge the substance of their conversation was impossible."}
{"doc_id": "gutenberg_1342", "para_id": 1570, "text": "The discomposure of spirits which this extraordinary visit threw Elizabeth into could not be easily overcome; nor could she for many hours learn to think of it less than incessantly. Lady Catherine, it appeared, had actually taken the trouble of this journey from Rosings for the sole purpose of breaking off her supposed engagement with Mr. Darcy. It was a rational scheme, to be sure! but from what the report of their engagement could originate, Elizabeth was at a loss to imagine; till she recollected that _his_ being the intimate friend of Bingley, and _her_ being the sister of Jane, was enough, at a time when the expectation of one wedding made everybody eager for another, to supply the idea. She had not herself forgotten to feel that the marriage of her sister must bring them more frequently together. And her neighbours at Lucas Lodge, therefore, (for through their communication with the Collinses, the report, she concluded, had reached Lady Catherine,) had only set _that_ down as almost certain and immediate which _she_ had looked forward to as possible at some future time."}
{"doc_id": "gutenberg_1342", "para_id": 1571, "text": "In revolving Lady Catherine’s expressions, however, she could not help feeling some uneasiness as to the possible consequence of her persisting in this interference. From what she had said of her resolution to prevent the marriage, it occurred to Elizabeth that she must meditate an application to her nephew; and how he might take a similar representation of the evils attached to a connection with her she dared not pronounce. She knew not the exact degree of his affection for his aunt, or his dependence on her judgment, but it was natural to suppose that he thought much higher of her Ladyship than _she_ could do; and it was certain, that in enumerating the miseries of a marriage with _one_ whose immediate connections were so unequal to his own, his aunt would address him on his weakest side. With his notions of dignity, he would probably feel that the arguments, which to Elizabeth had appeared weak and ridiculous, contained much good sense and solid reasoning."}
{"doc_id": "gutenberg_1342", "para_id": 1572, "text": "If he had been wavering before, as to what he should do, which had often seemed likely, the advice and entreaty of so near a relation might settle every doubt, and determine him at once to be as happy as dignity unblemished could make him. In that case he would return no more. Lady Catherine might see him in her way through town; and his engagement to Bingley of coming again to Netherfield must give way."}
{"doc_id": "gutenberg_1342", "para_id": 1573, "text": "“If, therefore, an excuse for not keeping his promise should come to his friend within a few days,” she added, “I shall know how to understand it. I shall then give over every expectation, every wish of his constancy. If he is satisfied with only regretting me, when he might have obtained my affections and hand, I shall soon cease to regret him at all.”"}
{"doc_id": "gutenberg_1342", "para_id": 1574, "text": "The surprise of the rest of the family, on hearing who their visitor had been, was very great: but they obligingly satisfied it with the same kind of supposition which had appeased Mrs. Bennet’s curiosity; and Elizabeth was spared from much teasing on the subject."}
{"doc_id": "gutenberg_1342", "para_id": 1575, "text": "The next morning, as she was going down stairs, she was met by her father, who came out of his library with a letter in his hand."}
{"doc_id": "gutenberg_1342", "para_id": 1576, "text": "She followed him thither; and her curiosity to know what he had to tell her was heightened by the supposition of its being in some manner connected with the letter he held. It suddenly struck her that it might be from Lady Catherine, and she anticipated with dismay all the consequent explanations."}
{"doc_id": "gutenberg_1342", "para_id": 1577, "text": "She followed her father to the fireplace, and they both sat down. He then said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1578, "text": "“I have received a letter this morning that has astonished me exceedingly. As it principally concerns yourself, you ought to know its contents. I did not know before that I had _two_ daughters on the brink of matrimony. Let me congratulate you on a very important conquest.”"}
{"doc_id": "gutenberg_1342", "para_id": 1579, "text": "The colour now rushed into Elizabeth’s cheeks in the instantaneous conviction of its being a letter from the nephew, instead of the aunt; and she was undetermined whether most to be pleased that he explained himself at all, or offended that his letter was not rather addressed to herself, when her father continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 1580, "text": "“You look conscious. Young ladies have great penetration in such matters as these; but I think I may defy even _your_ sagacity to discover the name of your admirer. This letter is from Mr. Collins.”"}
{"doc_id": "gutenberg_1342", "para_id": 1581, "text": "“Something very much to the purpose, of course. He begins with congratulations on the approaching nuptials of my eldest daughter, of which, it seems, he has been told by some of the good-natured, gossiping Lucases. I shall not sport with your impatience by reading what he says on that point. What relates to yourself is as follows:--‘Having thus offered you the sincere congratulations of Mrs. Collins and myself on this happy event, let me now add a short hint on the subject of another, of which we have been advertised by the same authority. Your daughter Elizabeth, it is presumed, will not long bear the name of Bennet, after her eldest sister has resigned it; and the chosen partner of her fate may be reasonably looked up to as one of the most illustrious personages in this land.’ Can you possibly guess, Lizzy, who is meant by this? ‘This young gentleman is blessed, in a peculiar way, with everything the heart of mortal can most desire,--splendid property, noble kindred, and extensive patronage. Yet, in spite of all these temptations, let me warn my cousin Elizabeth, and yourself, of what evils you may incur by a precipitate closure with this gentleman’s proposals, which, of course, you will be inclined to take immediate advantage of.’ Have you any idea, Lizzy, who this gentleman is? But now it comes out. ‘My motive for cautioning you is as follows:--We have reason to imagine that his aunt, Lady Catherine de Bourgh, does not look on the match with a friendly eye.’ _Mr. Darcy_, you see, is the man! Now, Lizzy, I think I _have_ surprised you. Could he, or the Lucases, have pitched on any man, within the circle of our acquaintance, whose name would have given the lie more effectually to what they related? Mr. Darcy, who never looks at any woman but to see a blemish, and who probably never looked at _you_ in his life! It is admirable!”"}
{"doc_id": "gutenberg_1342", "para_id": 1582, "text": "Elizabeth tried to join in her father’s pleasantry, but could only force one most reluctant smile. Never had his wit been directed in a manner so little agreeable to her."}
{"doc_id": "gutenberg_1342", "para_id": 1583, "text": "“‘After mentioning the likelihood of this marriage to her Ladyship last night, she immediately, with her usual condescension, expressed what she felt on the occasion; when it became apparent, that, on the score of some family objections on the part of my cousin, she would never give her consent to what she termed so disgraceful a match. I thought it my duty to give the speediest intelligence of this to my cousin, that she and her noble admirer may be aware of what they are about, and not run hastily into a marriage which has not been properly sanctioned.’ Mr. Collins, moreover, adds, ‘I am truly rejoiced that my cousin Lydia’s sad business has been so well hushed up, and am only concerned that their living together before the marriage took place should be so generally known. I must not, however, neglect the duties of my station, or refrain from declaring my amazement, at hearing that you received the young couple into your house as soon as they were married. It was an encouragement of vice; and had I been the rector of Longbourn, I should very strenuously have opposed it. You ought certainly to forgive them as a Christian, but never to admit them in your sight, or allow their names to be mentioned in your hearing.’ _That_ is his notion of Christian forgiveness! The rest of his letter is only about his dear Charlotte’s situation, and his expectation of a young olive-branch. But, Lizzy, you look as if you did not enjoy it. You are not going to be _missish_, I hope, and pretend to be affronted at an idle report. For what do we live, but to make sport for our neighbours, and laugh at them in our turn?”"}
{"doc_id": "gutenberg_1342", "para_id": 1584, "text": "“Yes, _that_ is what makes it amusing. Had they fixed on any other man it would have been nothing; but _his_ perfect indifference and _your_ pointed dislike make it so delightfully absurd! Much as I abominate writing, I would not give up Mr. Collins’s correspondence for any consideration. Nay, when I read a letter of his, I cannot help giving him the preference even over Wickham, much as I value the impudence and hypocrisy of my son-in-law. And pray, Lizzy, what said Lady Catherine about this report? Did she call to refuse her consent?”"}
{"doc_id": "gutenberg_1342", "para_id": 1585, "text": "To this question his daughter replied only with a laugh; and as it had been asked without the least suspicion, she was not distressed by his repeating it. Elizabeth had never been more at a loss to make her feelings appear what they were not. It was necessary to laugh when she would rather have cried. Her father had most cruelly mortified her by what he said of Mr. Darcy’s indifference; and she could do nothing but wonder at such a want of penetration, or fear that, perhaps, instead of his seeing too _little_, she might have fancied too _much_."}
{"doc_id": "gutenberg_1342", "para_id": 1586, "text": "Instead of receiving any such letter of excuse from his friend, as Elizabeth half expected Mr. Bingley to do, he was able to bring Darcy with him to Longbourn before many days had passed after Lady Catherine’s visit. The gentlemen arrived early; and, before Mrs. Bennet had time to tell him of their having seen his aunt, of which her daughter sat in momentary dread, Bingley, who wanted to be alone with Jane, proposed their all walking out. It was agreed to. Mrs. Bennet was not in the habit of walking, Mary could never spare time, but the remaining five set off together. Bingley and Jane, however, soon allowed the others to outstrip them. They lagged behind, while Elizabeth, Kitty, and Darcy were to entertain each other. Very little was said by either; Kitty was too much afraid of him to talk; Elizabeth was secretly forming a desperate resolution; and, perhaps, he might be doing the same."}
{"doc_id": "gutenberg_1342", "para_id": 1587, "text": "They walked towards the Lucases’, because Kitty wished to call upon Maria; and as Elizabeth saw no occasion for making it a general concern, when Kitty left them she went boldly on with him alone. Now was the moment for her resolution to be executed; and while her courage was high, she immediately said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1588, "text": "“Mr. Darcy, I am a very selfish creature, and for the sake of giving relief to my own feelings care not how much I may be wounding yours. I can no longer help thanking you for your unexampled kindness to my poor sister. Ever since I have known it I have been most anxious to acknowledge to you how gratefully I feel it. Were it known to the rest of my family I should not have merely my own gratitude to express.”"}
{"doc_id": "gutenberg_1342", "para_id": 1589, "text": "“I am sorry, exceedingly sorry,” replied Darcy, in a tone of surprise and emotion, “that you have ever been informed of what may, in a mistaken light, have given you uneasiness. I did not think Mrs. Gardiner was so little to be trusted.”"}
{"doc_id": "gutenberg_1342", "para_id": 1590, "text": "“You must not blame my aunt. Lydia’s thoughtlessness first betrayed to me that you had been concerned in the matter; and, of course, I could not rest till I knew the particulars. Let me thank you again and again, in the name of all my family, for that generous compassion which induced you to take so much trouble, and bear so many mortifications, for the sake of discovering them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1591, "text": "“If you _will_ thank me,” he replied, “let it be for yourself alone. That the wish of giving happiness to you might add force to the other inducements which led me on, I shall not attempt to deny. But your _family_ owe me nothing. Much as I respect them, I believe I thought only of _you_.”"}
{"doc_id": "gutenberg_1342", "para_id": 1592, "text": "Elizabeth was too much embarrassed to say a word. After a short pause, her companion added, “You are too generous to trifle with me. If your feelings are still what they were last April, tell me so at once. _My_ affections and wishes are unchanged; but one word from you will silence me on this subject for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 1593, "text": "Elizabeth, feeling all the more than common awkwardness and anxiety of his situation, now forced herself to speak; and immediately, though not very fluently, gave him to understand that her sentiments had undergone so material a change since the period to which he alluded, as to make her receive with gratitude and pleasure his present assurances. The happiness which this reply produced was such as he had probably never felt before; and he expressed himself on the occasion as sensibly and as warmly as a man violently in love can be supposed to do. Had Elizabeth been able to encounter his eyes, she might have seen how well the expression of heartfelt delight diffused over his face became him: but though she could not look she could listen; and he told her of feelings which, in proving of what importance she was to him, made his affection every moment more valuable."}
{"doc_id": "gutenberg_1342", "para_id": 1594, "text": "They walked on without knowing in what direction. There was too much to be thought, and felt, and said, for attention to any other objects. She soon learnt that they were indebted for their present good understanding to the efforts of his aunt, who _did_ call on him in her return through London, and there relate her journey to Longbourn, its motive, and the substance of her conversation with Elizabeth; dwelling emphatically on every expression of the latter, which, in her Ladyship’s apprehension, peculiarly denoted her perverseness and assurance, in the belief that such a relation must assist her endeavours to obtain that promise from her nephew which _she_ had refused to give. But, unluckily for her Ladyship, its effect had been exactly contrariwise."}
{"doc_id": "gutenberg_1342", "para_id": 1595, "text": "“It taught me to hope,” said he, “as I had scarcely ever allowed myself to hope before. I knew enough of your disposition to be certain, that had you been absolutely, irrevocably decided against me, you would have acknowledged it to Lady Catherine frankly and openly.”"}
{"doc_id": "gutenberg_1342", "para_id": 1596, "text": "Elizabeth coloured and laughed as she replied, “Yes, you know enough of my _frankness_ to believe me capable of _that_. After abusing you so abominably to your face, I could have no scruple in abusing you to all your relations.”"}
{"doc_id": "gutenberg_1342", "para_id": 1597, "text": "“What did you say of me that I did not deserve? For though your accusations were ill-founded, formed on mistaken premises, my behaviour to you at the time had merited the severest reproof. It was unpardonable. I cannot think of it without abhorrence.”"}
{"doc_id": "gutenberg_1342", "para_id": 1598, "text": "“We will not quarrel for the greater share of blame annexed to that evening,” said Elizabeth. “The conduct of neither, if strictly examined, will be irreproachable; but since then we have both, I hope, improved in civility.”"}
{"doc_id": "gutenberg_1342", "para_id": 1599, "text": "“I cannot be so easily reconciled to myself. The recollection of what I then said, of my conduct, my manners, my expressions during the whole of it, is now, and has been many months, inexpressibly painful to me. Your reproof, so well applied, I shall never forget: ‘Had you behaved in a more gentlemanlike manner.’ Those were your words. You know not, you can scarcely conceive, how they have tortured me; though it was some time, I confess, before I was reasonable enough to allow their justice.”"}
{"doc_id": "gutenberg_1342", "para_id": 1600, "text": "“I was certainly very far from expecting them to make so strong an impression. I had not the smallest idea of their being ever felt in such a way.”"}
{"doc_id": "gutenberg_1342", "para_id": 1601, "text": "“I can easily believe it. You thought me then devoid of every proper feeling, I am sure you did. The turn of your countenance I shall never forget, as you said that I could not have addressed you in any possible way that would induce you to accept me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1602, "text": "“Oh, do not repeat what I then said. These recollections will not do at all. I assure you that I have long been most heartily ashamed of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1603, "text": "Darcy mentioned his letter. “Did it,” said he,--“did it _soon_ make you think better of me? Did you, on reading it, give any credit to its contents?”"}
{"doc_id": "gutenberg_1342", "para_id": 1604, "text": "She explained what its effects on her had been, and how gradually all her former prejudices had been removed."}
{"doc_id": "gutenberg_1342", "para_id": 1605, "text": "“I knew,” said he, “that what I wrote must give you pain, but it was necessary. I hope you have destroyed the letter. There was one part, especially the opening of it, which I should dread your having the power of reading again. I can remember some expressions which might justly make you hate me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1606, "text": "“The letter shall certainly be burnt, if you believe it essential to the preservation of my regard; but, though we have both reason to think my opinions not entirely unalterable, they are not, I hope, quite so easily changed as that implies.”"}
{"doc_id": "gutenberg_1342", "para_id": 1607, "text": "“When I wrote that letter,” replied Darcy, “I believed myself perfectly calm and cool; but I am since convinced that it was written in a dreadful bitterness of spirit.”"}
{"doc_id": "gutenberg_1342", "para_id": 1608, "text": "“The letter, perhaps, began in bitterness, but it did not end so. The adieu is charity itself. But think no more of the letter. The feelings of the person who wrote and the person who received it are now so widely different from what they were then, that every unpleasant circumstance attending it ought to be forgotten. You must learn some of my philosophy. Think only of the past as its remembrance gives you pleasure.”"}
{"doc_id": "gutenberg_1342", "para_id": 1609, "text": "“I cannot give you credit for any philosophy of the kind. _Your_ retrospections must be so totally void of reproach, that the contentment arising from them is not of philosophy, but, what is much better, of ignorance. But with _me_, it is not so. Painful recollections will intrude, which cannot, which ought not to be repelled. I have been a selfish being all my life, in practice, though not in principle. As a child I was taught what was _right_, but I was not taught to correct my temper. I was given good principles, but left to follow them in pride and conceit. Unfortunately an only son (for many years an only _child_), I was spoiled by my parents, who, though good themselves, (my father particularly, all that was benevolent and amiable,) allowed, encouraged, almost taught me to be selfish and overbearing, to care for none beyond my own family circle, to think meanly of all the rest of the world, to _wish_ at least to think meanly of their sense and worth compared with my own. Such I was, from eight to eight-and-twenty; and such I might still have been but for you, dearest, loveliest Elizabeth! What do I not owe you! You taught me a lesson, hard indeed at first, but most advantageous. By you, I was properly humbled. I came to you without a doubt of my reception. You showed me how insufficient were all my pretensions to please a woman worthy of being pleased.”"}
{"doc_id": "gutenberg_1342", "para_id": 1610, "text": "“Indeed I had. What will you think of my vanity? I believed you to be wishing, expecting my addresses.”"}
{"doc_id": "gutenberg_1342", "para_id": 1611, "text": "“My manners must have been in fault, but not intentionally, I assure you. I never meant to deceive you, but my spirits might often lead me wrong. How you must have hated me after _that_ evening!”"}
{"doc_id": "gutenberg_1342", "para_id": 1612, "text": "“Hate you! I was angry, perhaps, at first, but my anger soon began to take a proper direction.”"}
{"doc_id": "gutenberg_1342", "para_id": 1613, "text": "“I am almost afraid of asking what you thought of me when we met at Pemberley. You blamed me for coming?”"}
{"doc_id": "gutenberg_1342", "para_id": 1614, "text": "“Your surprise could not be greater than _mine_ in being noticed by you. My conscience told me that I deserved no extraordinary politeness, and I confess that I did not expect to receive _more_ than my due.”"}
{"doc_id": "gutenberg_1342", "para_id": 1615, "text": "“My object _then_,” replied Darcy, “was to show you, by every civility in my power, that I was not so mean as to resent the past; and I hoped to obtain your forgiveness, to lessen your ill opinion, by letting you see that your reproofs had been attended to. How soon any other wishes introduced themselves, I can hardly tell, but I believe in about half an hour after I had seen you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1616, "text": "He then told her of Georgiana’s delight in her acquaintance, and of her disappointment at its sudden interruption; which naturally leading to the cause of that interruption, she soon learnt that his resolution of following her from Derbyshire in quest of her sister had been formed before he quitted the inn, and that his gravity and thoughtfulness there had arisen from no other struggles than what such a purpose must comprehend."}
{"doc_id": "gutenberg_1342", "para_id": 1617, "text": "She expressed her gratitude again, but it was too painful a subject to each to be dwelt on farther."}
{"doc_id": "gutenberg_1342", "para_id": 1618, "text": "After walking several miles in a leisurely manner, and too busy to know anything about it, they found at last, on examining their watches, that it was time to be at home."}
{"doc_id": "gutenberg_1342", "para_id": 1619, "text": "“What could have become of Mr. Bingley and Jane?” was a wonder which introduced the discussion of _their_ affairs. Darcy was delighted with their engagement; his friend had given him the earliest information of it."}
{"doc_id": "gutenberg_1342", "para_id": 1620, "text": "“That is to say, you had given your permission. I guessed as much.” And though he exclaimed at the term, she found that it had been pretty much the case."}
{"doc_id": "gutenberg_1342", "para_id": 1621, "text": "“On the evening before my going to London,” said he, “I made a confession to him, which I believe I ought to have made long ago. I told him of all that had occurred to make my former interference in his affairs absurd and impertinent. His surprise was great. He had never had the slightest suspicion. I told him, moreover, that I believed myself mistaken in supposing, as I had done, that your sister was indifferent to him; and as I could easily perceive that his attachment to her was unabated, I felt no doubt of their happiness together.”"}
{"doc_id": "gutenberg_1342", "para_id": 1622, "text": "“Did you speak from your own observation,” said she, “when you told him that my sister loved him, or merely from my information last spring?”"}
{"doc_id": "gutenberg_1342", "para_id": 1623, "text": "“From the former. I had narrowly observed her, during the two visits which I had lately made her here; and I was convinced of her affection.”"}
{"doc_id": "gutenberg_1342", "para_id": 1624, "text": "“It did. Bingley is most unaffectedly modest. His diffidence had prevented his depending on his own judgment in so anxious a case, but his reliance on mine made everything easy. I was obliged to confess one thing, which for a time, and not unjustly, offended him. I could not allow myself to conceal that your sister had been in town three months last winter, that I had known it, and purposely kept it from him. He was angry. But his anger, I am persuaded, lasted no longer than he remained in any doubt of your sister’s sentiments. He has heartily forgiven me now.”"}
{"doc_id": "gutenberg_1342", "para_id": 1625, "text": "Elizabeth longed to observe that Mr. Bingley had been a most delightful friend; so easily guided that his worth was invaluable; but she checked herself. She remembered that he had yet to learn to be laughed at, and it was rather too early to begin. In anticipating the happiness of Bingley, which of course was to be inferior only to his own, he continued the conversation till they reached the house. In the hall they parted."}
{"doc_id": "gutenberg_1342", "para_id": 1626, "text": "“My dear Lizzy, where can you have been walking to?” was a question which Elizabeth received from Jane as soon as she entered the room, and from all the others when they sat down to table. She had only to say in reply, that they had wandered about till she was beyond her own knowledge. She coloured as she spoke; but neither that, nor anything else, awakened a suspicion of the truth."}
{"doc_id": "gutenberg_1342", "para_id": 1627, "text": "The evening passed quietly, unmarked by anything extraordinary. The acknowledged lovers talked and laughed; the unacknowledged were silent. Darcy was not of a disposition in which happiness overflows in mirth; and Elizabeth, agitated and confused, rather _knew_ that she was happy than _felt_ herself to be so; for, besides the immediate embarrassment, there were other evils before her. She anticipated what would be felt in the family when her situation became known: she was aware that no one liked him but Jane; and even feared that with the others it was a _dislike_ which not all his fortune and consequence might do away."}
{"doc_id": "gutenberg_1342", "para_id": 1628, "text": "At night she opened her heart to Jane. Though suspicion was very far from Miss Bennet’s general habits, she was absolutely incredulous here."}
{"doc_id": "gutenberg_1342", "para_id": 1629, "text": "“You are joking, Lizzy. This cannot be! Engaged to Mr. Darcy! No, no, you shall not deceive me: I know it to be impossible.”"}
{"doc_id": "gutenberg_1342", "para_id": 1630, "text": "“This is a wretched beginning, indeed! My sole dependence was on you; and I am sure nobody else will believe me, if you do not. Yet, indeed, I am in earnest. I speak nothing but the truth. He still loves me, and we are engaged.”"}
{"doc_id": "gutenberg_1342", "para_id": 1631, "text": "Jane looked at her doubtingly. “Oh, Lizzy! it cannot be. I know how much you dislike him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1632, "text": "“You know nothing of the matter. _That_ is all to be forgot. Perhaps I did not always love him so well as I do now; but in such cases as these a good memory is unpardonable. This is the last time I shall ever remember it myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 1633, "text": "Miss Bennet still looked all amazement. Elizabeth again, and more seriously, assured her of its truth."}
{"doc_id": "gutenberg_1342", "para_id": 1634, "text": "“Good heaven! can it be really so? Yet now I must believe you,” cried Jane. “My dear, dear Lizzy, I would, I do congratulate you; but are you certain--forgive the question--are you quite certain that you can be happy with him?”"}
{"doc_id": "gutenberg_1342", "para_id": 1635, "text": "“There can be no doubt of that. It is settled between us already that we are to be the happiest couple in the world. But are you pleased, Jane? Shall you like to have such a brother?”"}
{"doc_id": "gutenberg_1342", "para_id": 1636, "text": "“Very, very much. Nothing could give either Bingley or myself more delight. But we considered it, we talked of it as impossible. And do you really love him quite well enough? Oh, Lizzy! do anything rather than marry without affection. Are you quite sure that you feel what you ought to do?”"}
{"doc_id": "gutenberg_1342", "para_id": 1637, "text": "“Oh, yes! You will only think I feel _more_ than I ought to do when I tell you all.”"}
{"doc_id": "gutenberg_1342", "para_id": 1638, "text": "“Why, I must confess that I love him better than I do Bingley. I am afraid you will be angry.”"}
{"doc_id": "gutenberg_1342", "para_id": 1639, "text": "“My dearest sister, now be, _be_ serious. I want to talk very seriously. Let me know everything that I am to know without delay. Will you tell me how long you have loved him?”"}
{"doc_id": "gutenberg_1342", "para_id": 1640, "text": "“It has been coming on so gradually, that I hardly know when it began; but I believe I must date it from my first seeing his beautiful grounds at Pemberley.”"}
{"doc_id": "gutenberg_1342", "para_id": 1641, "text": "Another entreaty that she would be serious, however, produced the desired effect; and she soon satisfied Jane by her solemn assurances of attachment. When convinced on that article, Miss Bennet had nothing further to wish."}
{"doc_id": "gutenberg_1342", "para_id": 1642, "text": "“Now I am quite happy,” said she, “for you will be as happy as myself. I always had a value for him. Were it for nothing but his love of you, I must always have esteemed him; but now, as Bingley’s friend and your husband, there can be only Bingley and yourself more dear to me. But, Lizzy, you have been very sly, very reserved with me. How little did you tell me of what passed at Pemberley and Lambton! I owe all that I know of it to another, not to you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1643, "text": "Elizabeth told her the motives of her secrecy. She had been unwilling to mention Bingley; and the unsettled state of her own feelings had made her equally avoid the name of his friend: but now she would no longer conceal from her his share in Lydia’s marriage. All was acknowledged, and half the night spent in conversation."}
{"doc_id": "gutenberg_1342", "para_id": 1644, "text": "“Good gracious!” cried Mrs. Bennet, as she stood at a window the next morning, “if that disagreeable Mr. Darcy is not coming here again with our dear Bingley! What can he mean by being so tiresome as to be always coming here? I had no notion but he would go a-shooting, or something or other, and not disturb us with his company. What shall we do with him? Lizzy, you must walk out with him again, that he may not be in Bingley’s way.”"}
{"doc_id": "gutenberg_1342", "para_id": 1645, "text": "Elizabeth could hardly help laughing at so convenient a proposal; yet was really vexed that her mother should be always giving him such an epithet."}
{"doc_id": "gutenberg_1342", "para_id": 1646, "text": "As soon as they entered, Bingley looked at her so expressively, and shook hands with such warmth, as left no doubt of his good information; and he soon afterwards said aloud, “Mrs. Bennet, have you no more lanes hereabouts in which Lizzy may lose her way again to-day?”"}
{"doc_id": "gutenberg_1342", "para_id": 1647, "text": "“I advise Mr. Darcy, and Lizzy, and Kitty,” said Mrs. Bennet, “to walk to Oakham Mount this morning. It is a nice long walk, and Mr. Darcy has never seen the view.”"}
{"doc_id": "gutenberg_1342", "para_id": 1648, "text": "“It may do very well for the others,” replied Mr. Bingley; “but I am sure it will be too much for Kitty. Won’t it, Kitty?”"}
{"doc_id": "gutenberg_1342", "para_id": 1649, "text": "Kitty owned that she had rather stay at home. Darcy professed a great curiosity to see the view from the Mount, and Elizabeth silently consented. As she went upstairs to get ready, Mrs. Bennet followed her, saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 1650, "text": "“I am quite sorry, Lizzy, that you should be forced to have that disagreeable man all to yourself; but I hope you will not mind it. It is all for Jane’s sake, you know; and there is no occasion for talking to him except just now and then; so do not put yourself to inconvenience.”"}
{"doc_id": "gutenberg_1342", "para_id": 1651, "text": "During their walk, it was resolved that Mr. Bennet’s consent should be asked in the course of the evening: Elizabeth reserved to herself the application for her mother’s. She could not determine how her mother would take it; sometimes doubting whether all his wealth and grandeur would be enough to overcome her abhorrence of the man; but whether she were violently set against the match, or violently delighted with it, it was certain that her manner would be equally ill adapted to do credit to her sense; and she could no more bear that Mr. Darcy should hear the first raptures of her joy, than the first vehemence of her disapprobation."}
{"doc_id": "gutenberg_1342", "para_id": 1652, "text": "In the evening, soon after Mr. Bennet withdrew to the library, she saw Mr. Darcy rise also and follow him, and her agitation on seeing it was extreme. She did not fear her father’s opposition, but he was going to be made unhappy, and that it should be through her means; that _she_, his favourite child, should be distressing him by her choice, should be filling him with fears and regrets in disposing of her, was a wretched reflection, and she sat in misery till Mr. Darcy appeared again, when, looking at him, she was a little relieved by his smile. In a few minutes he approached the table where she was sitting with Kitty; and, while pretending to admire her work, said in a whisper, “Go to your father; he wants you in the library.” She was gone directly."}
{"doc_id": "gutenberg_1342", "para_id": 1653, "text": "Her father was walking about the room, looking grave and anxious. “Lizzy,” said he, “what are you doing? Are you out of your senses to be accepting this man? Have not you always hated him?”"}
{"doc_id": "gutenberg_1342", "para_id": 1654, "text": "How earnestly did she then wish that her former opinions had been more reasonable, her expressions more moderate! It would have spared her from explanations and professions which it was exceedingly awkward to give; but they were now necessary, and she assured him, with some confusion, of her attachment to Mr. Darcy."}
{"doc_id": "gutenberg_1342", "para_id": 1655, "text": "“Or, in other words, you are determined to have him. He is rich, to be sure, and you may have more fine clothes and fine carriages than Jane. But will they make you happy?”"}
{"doc_id": "gutenberg_1342", "para_id": 1656, "text": "“Have you any other objection,” said Elizabeth, “than your belief of my indifference?”"}
{"doc_id": "gutenberg_1342", "para_id": 1657, "text": "“None at all. We all know him to be a proud, unpleasant sort of man; but this would be nothing if you really liked him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1658, "text": "“I do, I do like him,” she replied, with tears in her eyes; “I love him. Indeed he has no improper pride. He is perfectly amiable. You do not know what he really is; then pray do not pain me by speaking of him in such terms.”"}
{"doc_id": "gutenberg_1342", "para_id": 1659, "text": "“Lizzy,” said her father, “I have given him my consent. He is the kind of man, indeed, to whom I should never dare refuse anything, which he condescended to ask. I now give it to _you_, if you are resolved on having him. But let me advise you to think better of it. I know your disposition, Lizzy. I know that you could be neither happy nor respectable, unless you truly esteemed your husband, unless you looked up to him as a superior. Your lively talents would place you in the greatest danger in an unequal marriage. You could scarcely escape discredit and misery. My child, let me not have the grief of seeing _you_ unable to respect your partner in life. You know not what you are about.”"}
{"doc_id": "gutenberg_1342", "para_id": 1660, "text": "Elizabeth, still more affected, was earnest and solemn in her reply; and, at length, by repeated assurances that Mr. Darcy was really the object of her choice, by explaining the gradual change which her estimation of him had undergone, relating her absolute certainty that his affection was not the work of a day, but had stood the test of many months’ suspense, and enumerating with energy all his good qualities, she did conquer her father’s incredulity, and reconcile him to the match."}
{"doc_id": "gutenberg_1342", "para_id": 1661, "text": "“Well, my dear,” said he, when she ceased speaking, “I have no more to say. If this be the case, he deserves you. I could not have parted with you, my Lizzy, to anyone less worthy.”"}
{"doc_id": "gutenberg_1342", "para_id": 1662, "text": "To complete the favourable impression, she then told him what Mr. Darcy had voluntarily done for Lydia. He heard her with astonishment."}
{"doc_id": "gutenberg_1342", "para_id": 1663, "text": "“This is an evening of wonders, indeed! And so, Darcy did everything; made up the match, gave the money, paid the fellow’s debts, and got him his commission! So much the better. It will save me a world of trouble and economy. Had it been your uncle’s doing, I must and _would_ have paid him; but these violent young lovers carry everything their own way. I shall offer to pay him to-morrow, he will rant and storm about his love for you, and there will be an end of the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 1664, "text": "He then recollected her embarrassment a few days before on his reading Mr. Collins’s letter; and after laughing at her some time, allowed her at last to go, saying, as she quitted the room, “If any young men come for Mary or Kitty, send them in, for I am quite at leisure.”"}
{"doc_id": "gutenberg_1342", "para_id": 1665, "text": "Elizabeth’s mind was now relieved from a very heavy weight; and, after half an hour’s quiet reflection in her own room, she was able to join the others with tolerable composure. Everything was too recent for gaiety, but the evening passed tranquilly away; there was no longer anything material to be dreaded, and the comfort of ease and familiarity would come in time."}
{"doc_id": "gutenberg_1342", "para_id": 1666, "text": "When her mother went up to her dressing-room at night, she followed her, and made the important communication. Its effect was most extraordinary; for, on first hearing it, Mrs. Bennet sat quite still, and unable to utter a syllable. Nor was it under many, many minutes, that she could comprehend what she heard, though not in general backward to credit what was for the advantage of her family, or that came in the shape of a lover to any of them. She began at length to recover, to fidget about in her chair, get up, sit down again, wonder, and bless herself."}
{"doc_id": "gutenberg_1342", "para_id": 1667, "text": "“Good gracious! Lord bless me! only think! dear me! Mr. Darcy! Who would have thought it? And is it really true? Oh, my sweetest Lizzy! how rich and how great you will be! What pin-money, what jewels, what carriages you will have! Jane’s is nothing to it--nothing at all. I am so pleased--so happy. Such a charming man! so handsome! so tall! Oh, my dear Lizzy! pray apologize for my having disliked him so much before. I hope he will overlook it. Dear, dear Lizzy. A house in town! Everything that is charming! Three daughters married! Ten thousand a year! Oh, Lord! what will become of me? I shall go distracted.”"}
{"doc_id": "gutenberg_1342", "para_id": 1668, "text": "This was enough to prove that her approbation need not be doubted; and Elizabeth, rejoicing that such an effusion was heard only by herself, soon went away. But before she had been three minutes in her own room, her mother followed her."}
{"doc_id": "gutenberg_1342", "para_id": 1669, "text": "“My dearest child,” she cried, “I can think of nothing else. Ten thousand a year, and very likely more! ’Tis as good as a lord! And a special licence--you must and shall be married by a special licence. But, my dearest love, tell me what dish Mr. Darcy is particularly fond of, that I may have it to-morrow.”"}
{"doc_id": "gutenberg_1342", "para_id": 1670, "text": "This was a sad omen of what her mother’s behaviour to the gentleman himself might be; and Elizabeth found that, though in the certain possession of his warmest affection, and secure of her relations’ consent, there was still something to be wished for. But the morrow passed off much better than she expected; for Mrs. Bennet luckily stood in such awe of her intended son-in-law, that she ventured not to speak to him, unless it was in her power to offer him any attention, or mark her deference for his opinion."}
{"doc_id": "gutenberg_1342", "para_id": 1671, "text": "Elizabeth had the satisfaction of seeing her father taking pains to get acquainted with him; and Mr. Bennet soon assured her that he was rising every hour in his esteem."}
{"doc_id": "gutenberg_1342", "para_id": 1672, "text": "“I admire all my three sons-in-law highly,” said he. “Wickham, perhaps, is my favourite; but I think I shall like _your_ husband quite as well as Jane’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 1673, "text": "Elizabeth’s spirits soon rising to playfulness again, she wanted Mr. Darcy to account for his having ever fallen in love with her. “How could you begin?” said she. “I can comprehend your going on charmingly, when you had once made a beginning; but what could set you off in the first place?”"}
{"doc_id": "gutenberg_1342", "para_id": 1674, "text": "“I cannot fix on the hour, or the spot, or the look, or the words, which laid the foundation. It is too long ago. I was in the middle before I knew that I _had_ begun.”"}
{"doc_id": "gutenberg_1342", "para_id": 1675, "text": "“My beauty you had early withstood, and as for my manners--my behaviour to _you_ was at least always bordering on the uncivil, and I never spoke to you without rather wishing to give you pain than not. Now, be sincere; did you admire me for my impertinence?”"}
{"doc_id": "gutenberg_1342", "para_id": 1676, "text": "“You may as well call it impertinence at once. It was very little less. The fact is, that you were sick of civility, of deference, of officious attention. You were disgusted with the women who were always speaking, and looking, and thinking for _your_ approbation alone. I roused and interested you, because I was so unlike _them_. Had you not been really amiable you would have hated me for it: but in spite of the pains you took to disguise yourself, your feelings were always noble and just; and in your heart you thoroughly despised the persons who so assiduously courted you. There--I have saved you the trouble of accounting for it; and really, all things considered, I begin to think it perfectly reasonable. To be sure you know no actual good of me--but nobody thinks of _that_ when they fall in love.”"}
{"doc_id": "gutenberg_1342", "para_id": 1677, "text": "“Was there no good in your affectionate behaviour to Jane, while she was ill at Netherfield?”"}
{"doc_id": "gutenberg_1342", "para_id": 1678, "text": "“Dearest Jane! who could have done less for her? But make a virtue of it by all means. My good qualities are under your protection, and you are to exaggerate them as much as possible; and, in return, it belongs to me to find occasions for teasing and quarrelling with you as often as may be; and I shall begin directly, by asking you what made you so unwilling to come to the point at last? What made you so shy of me, when you first called, and afterwards dined here? Why, especially, when you called, did you look as if you did not care about me?”"}
{"doc_id": "gutenberg_1342", "para_id": 1679, "text": "“How unlucky that you should have a reasonable answer to give, and that I should be so reasonable as to admit it! But I wonder how long you _would_ have gone on, if you had been left to yourself. I wonder when you _would_ have spoken if I had not asked you! My resolution of thanking you for your kindness to Lydia had certainly great effect. _Too much_, I am afraid; for what becomes of the moral, if our comfort springs from a breach of promise, for I ought not to have mentioned the subject? This will never do.”"}
{"doc_id": "gutenberg_1342", "para_id": 1680, "text": "“You need not distress yourself. The moral will be perfectly fair. Lady Catherine’s unjustifiable endeavours to separate us were the means of removing all my doubts. I am not indebted for my present happiness to your eager desire of expressing your gratitude. I was not in a humour to wait for an opening of yours. My aunt’s intelligence had given me hope, and I was determined at once to know everything.”"}
{"doc_id": "gutenberg_1342", "para_id": 1681, "text": "“Lady Catherine has been of infinite use, which ought to make her happy, for she loves to be of use. But tell me, what did you come down to Netherfield for? Was it merely to ride to Longbourn and be embarrassed? or had you intended any more serious consequences?”"}
{"doc_id": "gutenberg_1342", "para_id": 1682, "text": "“My real purpose was to see _you_, and to judge, if I could, whether I might ever hope to make you love me. My avowed one, or what I avowed to myself, was to see whether your sister was still partial to Bingley, and if she were, to make the confession to him which I have since made.”"}
{"doc_id": "gutenberg_1342", "para_id": 1683, "text": "“Shall you ever have courage to announce to Lady Catherine what is to befall her?”"}
{"doc_id": "gutenberg_1342", "para_id": 1684, "text": "“I am more likely to want time than courage, Elizabeth. But it ought to be done; and if you will give me a sheet of paper it shall be done directly.”"}
{"doc_id": "gutenberg_1342", "para_id": 1685, "text": "“And if I had not a letter to write myself, I might sit by you, and admire the evenness of your writing, as another young lady once did. But I have an aunt, too, who must not be longer neglected.”"}
{"doc_id": "gutenberg_1342", "para_id": 1686, "text": "From an unwillingness to confess how much her intimacy with Mr. Darcy had been overrated, Elizabeth had never yet answered Mrs. Gardiner’s long letter; but now, having _that_ to communicate which she knew would be most welcome, she was almost ashamed to find that her uncle and aunt had already lost three days of happiness, and immediately wrote as follows:--"}
{"doc_id": "gutenberg_1342", "para_id": 1687, "text": "“I would have thanked you before, my dear aunt, as I ought to have done, for your long, kind, satisfactory detail of particulars; but, to say the truth, I was too cross to write. You supposed more than really existed. But _now_ suppose as much as you choose; give a loose to your fancy, indulge your imagination in every possible flight which the subject will afford, and unless you believe me actually married, you cannot greatly err. You must write again very soon, and praise him a great deal more than you did in your last. I thank you again and again, for not going to the Lakes. How could I be so silly as to wish it! Your idea of the ponies is delightful. We will go round the park every day. I am the happiest creature in the world. Perhaps other people have said so before, but no one with such justice. I am happier even than Jane; she only smiles, I laugh. Mr. Darcy sends you all the love in the world that can be spared from me. You are all to come to Pemberley at Christmas. Yours,” etc."}
{"doc_id": "gutenberg_1342", "para_id": 1688, "text": "Mr. Darcy’s letter to Lady Catherine was in a different style, and still different from either was what Mr. Bennet sent to Mr. Collins, in return for his last."}
{"doc_id": "gutenberg_1342", "para_id": 1689, "text": "“I must trouble you once more for congratulations. Elizabeth will soon be the wife of Mr. Darcy. Console Lady Catherine as well as you can. But, if I were you, I would stand by the nephew. He has more to give."}
{"doc_id": "gutenberg_1342", "para_id": 1690, "text": "Miss Bingley’s congratulations to her brother on his approaching marriage were all that was affectionate and insincere. She wrote even to Jane on the occasion, to express her delight, and repeat all her former professions of regard. Jane was not deceived, but she was affected; and though feeling no reliance on her, could not help writing her a much kinder answer than she knew was deserved."}
{"doc_id": "gutenberg_1342", "para_id": 1691, "text": "The joy which Miss Darcy expressed on receiving similar information was as sincere as her brother’s in sending it. Four sides of paper were insufficient to contain all her delight, and all her earnest desire of being loved by her sister."}
{"doc_id": "gutenberg_1342", "para_id": 1692, "text": "Before any answer could arrive from Mr. Collins, or any congratulations to Elizabeth from his wife, the Longbourn family heard that the Collinses were come themselves to Lucas Lodge. The reason of this sudden removal was soon evident. Lady Catherine had been rendered so exceedingly angry by the contents of her nephew’s letter, that Charlotte, really rejoicing in the match, was anxious to get away till the storm was blown over. At such a moment, the arrival of her friend was a sincere pleasure to Elizabeth, though in the course of their meetings she must sometimes think the pleasure dearly bought, when she saw Mr. Darcy exposed to all the parading and obsequious civility of her husband. He bore it, however, with admirable calmness. He could even listen to Sir William Lucas, when he complimented him on carrying away the brightest jewel of the country, and expressed his hopes of their all meeting frequently at St. James’s, with very decent composure. If he did shrug his shoulders, it was not till Sir William was out of sight."}
{"doc_id": "gutenberg_1342", "para_id": 1693, "text": "Mrs. Philips’s vulgarity was another, and, perhaps, a greater tax on his forbearance; and though Mrs. Philips, as well as her sister, stood in too much awe of him to speak with the familiarity which Bingley’s good-humour encouraged; yet, whenever she _did_ speak, she must be vulgar. Nor was her respect for him, though it made her more quiet, at all likely to make her more elegant. Elizabeth did all she could to shield him from the frequent notice of either, and was ever anxious to keep him to herself, and to those of her family with whom he might converse without mortification; and though the uncomfortable feelings arising from all this took from the season of courtship much of its pleasure, it added to the hope of the future; and she looked forward with delight to the time when they should be removed from society so little pleasing to either, to all the comfort and elegance of their family party at Pemberley."}
{"doc_id": "gutenberg_1342", "para_id": 1694, "text": "Happy for all her maternal feelings was the day on which Mrs. Bennet got rid of her two most deserving daughters. With what delighted pride she afterwards visited Mrs. Bingley, and talked of Mrs. Darcy, may be guessed. I wish I could say, for the sake of her family, that the accomplishment of her earnest desire in the establishment of so many of her children produced so happy an effect as to make her a sensible, amiable, well-informed woman for the rest of her life; though, perhaps, it was lucky for her husband, who might not have relished domestic felicity in so unusual a form, that she still was occasionally nervous and invariably silly."}
{"doc_id": "gutenberg_1342", "para_id": 1695, "text": "Mr. Bennet missed his second daughter exceedingly; his affection for her drew him oftener from home than anything else could do. He delighted in going to Pemberley, especially when he was least expected."}
{"doc_id": "gutenberg_1342", "para_id": 1696, "text": "Mr. Bingley and Jane remained at Netherfield only a twelvemonth. So near a vicinity to her mother and Meryton relations was not desirable even to _his_ easy temper, or _her_ affectionate heart. The darling wish of his sisters was then gratified: he bought an estate in a neighbouring county to Derbyshire; and Jane and Elizabeth, in addition to every other source of happiness, were within thirty miles of each other."}
{"doc_id": "gutenberg_1342", "para_id": 1697, "text": "Kitty, to her very material advantage, spent the chief of her time with her two elder sisters. In society so superior to what she had generally known, her improvement was great. She was not of so ungovernable a temper as Lydia; and, removed from the influence of Lydia’s example, she became, by proper attention and management, less irritable, less ignorant, and less insipid. From the further disadvantage of Lydia’s society she was of course carefully kept; and though Mrs. Wickham frequently invited her to come and stay with her, with the promise of balls and young men, her father would never consent to her going."}
{"doc_id": "gutenberg_1342", "para_id": 1698, "text": "Mary was the only daughter who remained at home; and she was necessarily drawn from the pursuit of accomplishments by Mrs. Bennet’s being quite unable to sit alone. Mary was obliged to mix more with the world, but she could still moralize over every morning visit; and as she was no longer mortified by comparisons between her sisters’ beauty and her own, it was suspected by her father that she submitted to the change without much reluctance."}
{"doc_id": "gutenberg_1342", "para_id": 1699, "text": "As for Wickham and Lydia, their characters suffered no revolution from the marriage of her sisters. He bore with philosophy the conviction that Elizabeth must now become acquainted with whatever of his ingratitude and falsehood had before been unknown to her; and, in spite of everything, was not wholly without hope that Darcy might yet be prevailed on to make his fortune. The congratulatory letter which Elizabeth received from Lydia on her marriage explained to her that, by his wife at least, if not by himself, such a hope was cherished. The letter was to this effect:--"}
{"doc_id": "gutenberg_1342", "para_id": 1700, "text": "“I wish you joy. If you love Mr. Darcy half so well as I do my dear Wickham, you must be very happy. It is a great comfort to have you so rich; and when you have nothing else to do, I hope you will think of us. I am sure Wickham would like a place at court very much; and I do not think we shall have quite money enough to live upon without some help. Any place would do of about three or four hundred a year; but, however, do not speak to Mr. Darcy about it, if you had rather not."}
{"doc_id": "gutenberg_1342", "para_id": 1701, "text": "As it happened that Elizabeth had much rather not, she endeavoured in her answer to put an end to every entreaty and expectation of the kind. Such relief, however, as it was in her power to afford, by the practice of what might be called economy in her own private expenses, she frequently sent them. It had always been evident to her that such an income as theirs, under the direction of two persons so extravagant in their wants, and heedless of the future, must be very insufficient to their support; and whenever they changed their quarters, either Jane or herself were sure of being applied to for some little assistance towards discharging their bills. Their manner of living, even when the restoration of peace dismissed them to a home, was unsettled in the extreme. They were always moving from place to place in quest of a cheap situation, and always spending more than they ought. His affection for her soon sunk into indifference: hers lasted a little longer; and, in spite of her youth and her manners, she retained all the claims to reputation which her marriage had given her. Though Darcy could never receive _him_ at Pemberley, yet, for Elizabeth’s sake, he assisted him further in his profession. Lydia was occasionally a visitor there, when her husband was gone to enjoy himself in London or Bath; and with the Bingleys they both of them frequently stayed so long, that even Bingley’s good-humour was overcome, and he proceeded so far as to _talk_ of giving them a hint to be gone."}
{"doc_id": "gutenberg_1342", "para_id": 1702, "text": "Miss Bingley was very deeply mortified by Darcy’s marriage; but as she thought it advisable to retain the right of visiting at Pemberley, she dropped all her resentment; was fonder than ever of Georgiana, almost as attentive to Darcy as heretofore, and paid off every arrear of civility to Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 1703, "text": "Pemberley was now Georgiana’s home; and the attachment of the sisters was exactly what Darcy had hoped to see. They were able to love each other, even as well as they intended. Georgiana had the highest opinion in the world of Elizabeth; though at first she often listened with an astonishment bordering on alarm at her lively, sportive manner of talking to her brother. He, who had always inspired in herself a respect which almost overcame her affection, she now saw the object of open pleasantry. Her mind received knowledge which had never before fallen in her way. By Elizabeth’s instructions she began to comprehend that a woman may take liberties with her husband, which a brother will not always allow in a sister more than ten years younger than himself."}
{"doc_id": "gutenberg_1342", "para_id": 1704, "text": "Lady Catherine was extremely indignant on the marriage of her nephew; and as she gave way to all the genuine frankness of her character, in her reply to the letter which announced its arrangement, she sent him language so very abusive, especially of Elizabeth, that for some time all intercourse was at an end. But at length, by Elizabeth’s persuasion, he was prevailed on to overlook the offence, and seek a reconciliation; and, after a little further resistance on the part of his aunt, her resentment gave way, either to her affection for him, or her curiosity to see how his wife conducted herself; and she condescended to wait on them at Pemberley, in spite of that pollution which its woods had received, not merely from the presence of such a mistress, but the visits of her uncle and aunt from the city."}
{"doc_id": "gutenberg_1342", "para_id": 1705, "text": "With the Gardiners they were always on the most intimate terms. Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them."}
{"doc_id": "gutenberg_1342", "para_id": 1706, "text": "CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO. TOOKS COURT, CHANCERY LANE, LONDON."}
