{"doc_id": "1706.03762", "para_id": 0, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works."}
{"doc_id": "1706.03762", "para_id": 1, "text": "Aidan N. Gomez∗†\nUniversity of Toronto\naidan@cs.toronto.edu"}
{"doc_id": "1706.03762", "para_id": 2, "text": "Łukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com"}
{"doc_id": "1706.03762", "para_id": 3, "text": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data."}
{"doc_id": "1706.03762", "para_id": 4, "text": "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research."}
{"doc_id": "1706.03762", "para_id": 5, "text": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."}
{"doc_id": "1706.03762", "para_id": 6, "text": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15]."}
{"doc_id": "1706.03762", "para_id": 7, "text": "Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains."}
{"doc_id": "1706.03762", "para_id": 8, "text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network."}
{"doc_id": "1706.03762", "para_id": 9, "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs."}
{"doc_id": "1706.03762", "para_id": 10, "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2."}
{"doc_id": "1706.03762", "para_id": 11, "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22]."}
{"doc_id": "1706.03762", "para_id": 12, "text": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34]."}
{"doc_id": "1706.03762", "para_id": 13, "text": "To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]."}
{"doc_id": "1706.03762", "para_id": 14, "text": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next."}
{"doc_id": "1706.03762", "para_id": 15, "text": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively."}
{"doc_id": "1706.03762", "para_id": 16, "text": "Encoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512."}
{"doc_id": "1706.03762", "para_id": 17, "text": "Decoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i."}
{"doc_id": "1706.03762", "para_id": 18, "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum"}
{"doc_id": "1706.03762", "para_id": 19, "text": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel."}
{"doc_id": "1706.03762", "para_id": 20, "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key."}
{"doc_id": "1706.03762", "para_id": 21, "text": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues."}
{"doc_id": "1706.03762", "para_id": 22, "text": "In practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:"}
{"doc_id": "1706.03762", "para_id": 23, "text": "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n√dk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code."}
{"doc_id": "1706.03762", "para_id": 24, "text": "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n√dk ."}
{"doc_id": "1706.03762", "para_id": 25, "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional"}
{"doc_id": "1706.03762", "para_id": 26, "text": "4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk."}
{"doc_id": "1706.03762", "para_id": 27, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2."}
{"doc_id": "1706.03762", "para_id": 28, "text": "Multi-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this."}
{"doc_id": "1706.03762", "para_id": 29, "text": "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O"}
{"doc_id": "1706.03762", "para_id": 30, "text": "where headi = Attention(QW Q\ni , KW K\ni , V W V\ni )"}
{"doc_id": "1706.03762", "para_id": 31, "text": "Where the projections are parameter matrices W Q\ni\n∈Rdmodel×dk, W K\ni\n∈Rdmodel×dk, W V\ni\n∈Rdmodel×dv\nand W O ∈Rhdv×dmodel."}
{"doc_id": "1706.03762", "para_id": 32, "text": "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."}
{"doc_id": "1706.03762", "para_id": 33, "text": "The Transformer uses multi-head attention in three different ways:"}
{"doc_id": "1706.03762", "para_id": 34, "text": "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9]."}
{"doc_id": "1706.03762", "para_id": 35, "text": "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder."}
{"doc_id": "1706.03762", "para_id": 36, "text": "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2."}
{"doc_id": "1706.03762", "para_id": 37, "text": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between."}
{"doc_id": "1706.03762", "para_id": 38, "text": "While the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048."}
{"doc_id": "1706.03762", "para_id": 39, "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel."}
{"doc_id": "1706.03762", "para_id": 40, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention."}
{"doc_id": "1706.03762", "para_id": 41, "text": "Layer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 · d)\nO(1)\nO(1)\nRecurrent\nO(n · d2)\nO(n)\nO(n)\nConvolutional\nO(k · n · d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r · n · d)\nO(1)\nO(n/r)"}
{"doc_id": "1706.03762", "para_id": 42, "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9]."}
{"doc_id": "1706.03762", "para_id": 43, "text": "In this work, we use sine and cosine functions of different frequencies:"}
{"doc_id": "1706.03762", "para_id": 44, "text": "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos."}
{"doc_id": "1706.03762", "para_id": 45, "text": "We also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training."}
{"doc_id": "1706.03762", "para_id": 46, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata."}
{"doc_id": "1706.03762", "para_id": 47, "text": "One is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required."}
{"doc_id": "1706.03762", "para_id": 48, "text": "The third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types."}
{"doc_id": "1706.03762", "para_id": 49, "text": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence"}
{"doc_id": "1706.03762", "para_id": 50, "text": "length n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work."}
{"doc_id": "1706.03762", "para_id": 51, "text": "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model."}
{"doc_id": "1706.03762", "para_id": 52, "text": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences."}
{"doc_id": "1706.03762", "para_id": 53, "text": "This section describes the training regime for our models."}
{"doc_id": "1706.03762", "para_id": 54, "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens."}
{"doc_id": "1706.03762", "para_id": 55, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days)."}
{"doc_id": "1706.03762", "para_id": 56, "text": "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:"}
{"doc_id": "1706.03762", "para_id": 57, "text": "lrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\n(3)"}
{"doc_id": "1706.03762", "para_id": 58, "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000."}
{"doc_id": "1706.03762", "para_id": 59, "text": "We employ three types of regularization during training:"}
{"doc_id": "1706.03762", "para_id": 60, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost."}
{"doc_id": "1706.03762", "para_id": 61, "text": "EN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 · 1020"}
{"doc_id": "1706.03762", "para_id": 62, "text": "GNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 · 1020\n1.1 · 1021"}
{"doc_id": "1706.03762", "para_id": 63, "text": "ConvS2S Ensemble [9]\n26.36\n41.29\n7.7 · 1019\n1.2 · 1021"}
{"doc_id": "1706.03762", "para_id": 64, "text": "Residual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1."}
{"doc_id": "1706.03762", "para_id": 65, "text": "Label Smoothing\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."}
{"doc_id": "1706.03762", "para_id": 66, "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models."}
{"doc_id": "1706.03762", "para_id": 67, "text": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3."}
{"doc_id": "1706.03762", "para_id": 68, "text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38]."}
{"doc_id": "1706.03762", "para_id": 69, "text": "Table 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5."}
{"doc_id": "1706.03762", "para_id": 70, "text": "To evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the"}
{"doc_id": "1706.03762", "para_id": 71, "text": "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."}
{"doc_id": "1706.03762", "para_id": 72, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities."}
{"doc_id": "1706.03762", "para_id": 73, "text": "N\ndmodel\ndff\nh\ndk\ndv\nPdrop\nϵls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n×106"}
{"doc_id": "1706.03762", "para_id": 74, "text": "1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4"}
{"doc_id": "1706.03762", "para_id": 75, "text": "2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90"}
{"doc_id": "1706.03762", "para_id": 76, "text": "0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213"}
{"doc_id": "1706.03762", "para_id": 77, "text": "development set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3."}
{"doc_id": "1706.03762", "para_id": 78, "text": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"doc_id": "1706.03762", "para_id": 79, "text": "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model."}
{"doc_id": "1706.03762", "para_id": 80, "text": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]."}
{"doc_id": "1706.03762", "para_id": 81, "text": "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 82, "text": "We performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we"}
{"doc_id": "1706.03762", "para_id": 83, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)"}
{"doc_id": "1706.03762", "para_id": 84, "text": "Parser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3"}
{"doc_id": "1706.03762", "para_id": 85, "text": "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting."}
{"doc_id": "1706.03762", "para_id": 86, "text": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8]."}
{"doc_id": "1706.03762", "para_id": 87, "text": "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences."}
{"doc_id": "1706.03762", "para_id": 88, "text": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention."}
{"doc_id": "1706.03762", "para_id": 89, "text": "For translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles."}
{"doc_id": "1706.03762", "para_id": 90, "text": "We are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours."}
{"doc_id": "1706.03762", "para_id": 91, "text": "The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor."}
{"doc_id": "1706.03762", "para_id": 92, "text": "Acknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration."}
{"doc_id": "1706.03762", "para_id": 93, "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016."}
{"doc_id": "1706.03762", "para_id": 94, "text": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014."}
{"doc_id": "1706.03762", "para_id": 95, "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017."}
{"doc_id": "1706.03762", "para_id": 96, "text": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1706.03762", "para_id": 97, "text": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014."}
{"doc_id": "1706.03762", "para_id": 98, "text": "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016."}
{"doc_id": "1706.03762", "para_id": 99, "text": "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014."}
{"doc_id": "1706.03762", "para_id": 100, "text": "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016."}
{"doc_id": "1706.03762", "para_id": 101, "text": "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."}
{"doc_id": "1706.03762", "para_id": 102, "text": "[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013."}
{"doc_id": "1706.03762", "para_id": 103, "text": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016."}
{"doc_id": "1706.03762", "para_id": 104, "text": "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001."}
{"doc_id": "1706.03762", "para_id": 105, "text": "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997."}
{"doc_id": "1706.03762", "para_id": 106, "text": "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009."}
{"doc_id": "1706.03762", "para_id": 107, "text": "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1706.03762", "para_id": 108, "text": "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016."}
{"doc_id": "1706.03762", "para_id": 109, "text": "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016."}
{"doc_id": "1706.03762", "para_id": 110, "text": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017."}
{"doc_id": "1706.03762", "para_id": 111, "text": "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017."}
{"doc_id": "1706.03762", "para_id": 112, "text": "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "1706.03762", "para_id": 113, "text": "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017."}
{"doc_id": "1706.03762", "para_id": 114, "text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017."}
{"doc_id": "1706.03762", "para_id": 115, "text": "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015."}
{"doc_id": "1706.03762", "para_id": 116, "text": "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015."}
{"doc_id": "1706.03762", "para_id": 117, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993."}
{"doc_id": "1706.03762", "para_id": 118, "text": "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006."}
{"doc_id": "1706.03762", "para_id": 119, "text": "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016."}
{"doc_id": "1706.03762", "para_id": 120, "text": "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1706.03762", "para_id": 121, "text": "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006."}
{"doc_id": "1706.03762", "para_id": 122, "text": "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016."}
{"doc_id": "1706.03762", "para_id": 123, "text": "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1706.03762", "para_id": 124, "text": "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1706.03762", "para_id": 125, "text": "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014."}
{"doc_id": "1706.03762", "para_id": 126, "text": "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015."}
{"doc_id": "1706.03762", "para_id": 127, "text": "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014."}
{"doc_id": "1706.03762", "para_id": 128, "text": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."}
{"doc_id": "1706.03762", "para_id": 129, "text": "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015."}
{"doc_id": "1706.03762", "para_id": 130, "text": "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016."}
{"doc_id": "1706.03762", "para_id": 131, "text": "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016."}
{"doc_id": "1706.03762", "para_id": 132, "text": "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013."}
{"doc_id": "1706.03762", "para_id": 133, "text": "Figure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color."}
{"doc_id": "1706.03762", "para_id": 134, "text": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word."}
{"doc_id": "1706.03762", "para_id": 135, "text": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks."}
{"doc_id": "1810.04805", "para_id": 0, "text": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding"}
{"doc_id": "1810.04805", "para_id": 1, "text": "Jacob Devlin\nMing-Wei Chang\nKenton Lee\nKristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com"}
{"doc_id": "1810.04805", "para_id": 2, "text": "There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and ﬁne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. The ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches.\nThe ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder\nRepresentations\nfrom\nTransformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953).\nThe\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked"}
{"doc_id": "1810.04805", "para_id": 3, "text": "We introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations."}
{"doc_id": "1810.04805", "para_id": 4, "text": "BERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement)."}
{"doc_id": "1810.04805", "para_id": 5, "text": "Language model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016)."}
{"doc_id": "1810.04805", "para_id": 6, "text": "word based only on its context.\nUnlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na “next sentence prediction” task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:"}
{"doc_id": "1810.04805", "para_id": 7, "text": "These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014).\nTo train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speciﬁc architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels."}
{"doc_id": "1810.04805", "para_id": 8, "text": "• We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs."}
{"doc_id": "1810.04805", "para_id": 9, "text": "• We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speciﬁc architectures."}
{"doc_id": "1810.04805", "para_id": 10, "text": "• BERT advances the state of the art for eleven\nNLP tasks.\nThe code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert."}
{"doc_id": "1810.04805", "para_id": 11, "text": "There is a long history of pre-training general lan-\nguage representations, and we brieﬂy review the\nmost widely-used approaches in this section."}
{"doc_id": "1810.04805", "para_id": 12, "text": "As with the feature-based approaches, the ﬁrst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text\n(Col-\nlobert and Weston, 2008).\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\nﬁne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch.\nAt least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\net al., 2018a).\nLeft-to-right language model-"}
{"doc_id": "1810.04805", "para_id": 13, "text": "Learning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signiﬁcant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013)."}
{"doc_id": "1810.04805", "para_id": 14, "text": "Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers)."}
{"doc_id": "1810.04805", "para_id": 15, "text": "ing and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015)."}
{"doc_id": "1810.04805", "para_id": 16, "text": "mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture."}
{"doc_id": "1810.04805", "para_id": 17, "text": "Model Architecture\nBERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2"}
{"doc_id": "1810.04805", "para_id": 18, "text": "There has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014)."}
{"doc_id": "1810.04805", "para_id": 19, "text": "In this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3"}
{"doc_id": "1810.04805", "para_id": 20, "text": "We introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning.\nDur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks.\nFor ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-"}
{"doc_id": "1810.04805", "para_id": 21, "text": "We primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4"}
{"doc_id": "1810.04805", "para_id": 22, "text": "1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n4We note that in the literature the bidirectional Trans-"}
{"doc_id": "1810.04805", "para_id": 23, "text": "Input/Output Representations\nTo make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., ⟨Question, Answer ⟩) in one token sequence.\nThroughout this work, a “sentence” can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A “sequence” refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The ﬁrst\ntoken of every sequence is always a special clas-\nsiﬁcation token ([CLS]). The ﬁnal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classiﬁcation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the ﬁnal hidden\nvector of the special [CLS] token as C ∈RH,\nand the ﬁnal hidden vector for the ith input token\nas Ti ∈RH.\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2."}
{"doc_id": "1810.04805", "para_id": 24, "text": "In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a “masked\nLM” (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the ﬁnal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nﬁne-tuning, since the [MASK] token does not ap-\npear during ﬁne-tuning. To mitigate this, we do\nnot always replace “masked” words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTi will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2."}
{"doc_id": "1810.04805", "para_id": 25, "text": "Task #2:\nNext Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speciﬁcally,\nwhen choosing the sentences A and B for each pre-\ntraining example, 50% of the time B is the actual\nnext sentence that follows A (labeled as IsNext),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext).\nAs we show\nin Figure 1, C is used for next sentence predic-\ntion (NSP).5 Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very beneﬁcial to both QA and NLI. 6"}
{"doc_id": "1810.04805", "para_id": 26, "text": "Unlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1."}
{"doc_id": "1810.04805", "para_id": 27, "text": "Task #1: Masked LM\nIntuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model.\nUnfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly “see itself”, and the model could trivially\npredict the target word in a multi-layered context."}
{"doc_id": "1810.04805", "para_id": 28, "text": "5The ﬁnal model achieves 97%-98% accuracy on NSP.\n6The vector C is not a meaningful sentence representation\nwithout ﬁne-tuning, since it was trained with NSP."}
{"doc_id": "1810.04805", "para_id": 29, "text": "former is often referred to as a “Transformer encoder” while\nthe left-context-only version is referred to as a “Transformer\ndecoder” since it can be used for text generation."}
{"doc_id": "1810.04805", "para_id": 30, "text": "[CLS]\nhe\nlikes\nplay\n##ing\n[SEP]\nmy\ndog\nis\ncute\n[SEP]\nInput"}
{"doc_id": "1810.04805", "para_id": 31, "text": "E[CLS]\nEhe\nElikes\nEplay\nE##ing\nE[SEP]\nEmy\nEdog\nEis\nEcute\nE[SEP]\nToken\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 32, "text": "EA\nEB\nEB\nEB\nEB\nEB\nEA\nEA\nEA\nEA\nEA\nSegment\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 33, "text": "E0\nE6\nE7\nE8\nE9\nE10\nE1\nE2\nE3\nE4\nE5\nPosition\nEmbeddings"}
{"doc_id": "1810.04805", "para_id": 34, "text": "Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings."}
{"doc_id": "1810.04805", "para_id": 35, "text": "(4) a degenerate text-∅pair in text classiﬁcation\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classiﬁcation, such as en-\ntailment or sentiment analysis.\nCompared to pre-training, ﬁne-tuning is rela-\ntively inexpensive. All of the results in the pa-\nper can be replicated in at most 1 hour on a sin-\ngle Cloud TPU, or a few hours on a GPU, starting\nfrom the exact same pre-trained model.7 We de-\nscribe the task-speciﬁc details in the correspond-\ning subsections of Section 4. More details can be\nfound in Appendix A.5."}
{"doc_id": "1810.04805", "para_id": 36, "text": "The NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters."}
{"doc_id": "1810.04805", "para_id": 37, "text": "Pre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words).\nFor Wikipedia we extract only the text passages\nand ignore lists, tables, and headers. It is criti-\ncal to use a document-level corpus rather than a\nshufﬂed sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to\nextract long contiguous sequences."}
{"doc_id": "1810.04805", "para_id": 38, "text": "In this section, we present BERT ﬁne-tuning re-\nsults on 11 NLP tasks."}
{"doc_id": "1810.04805", "para_id": 39, "text": "Fine-tuning is straightforward since the self-\nattention mechanism in the Transformer al-\nlows BERT to model many downstream tasks—\nwhether they involve single text or text pairs—by\nswapping out the appropriate inputs and outputs.\nFor applications involving text pairs, a common\npattern is to independently encode text pairs be-\nfore applying bidirectional cross attention, such\nas Parikh et al. (2016); Seo et al. (2017). BERT\ninstead uses the self-attention mechanism to unify\nthese two stages, as encoding a concatenated text\npair with self-attention effectively includes bidi-\nrectional cross attention between two sentences.\nFor each task, we simply plug in the task-\nspeciﬁc inputs and outputs into BERT and ﬁne-\ntune all the parameters end-to-end.\nAt the in-\nput, sentence A and sentence B from pre-training\nare analogous to (1) sentence pairs in paraphras-\ning, (2) hypothesis-premise pairs in entailment, (3)\nquestion-passage pairs in question answering, and"}
{"doc_id": "1810.04805", "para_id": 40, "text": "The General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. Detailed descriptions of GLUE datasets are\nincluded in Appendix B.1.\nTo ﬁne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the ﬁnal hid-\nden vector C ∈RH corresponding to the ﬁrst\ninput token ([CLS]) as the aggregate representa-\ntion. The only new parameters introduced during\nﬁne-tuning are classiﬁcation layer weights W ∈\nRK×H, where K is the number of labels. We com-\npute a standard classiﬁcation loss with C and W,\ni.e., log(softmax(CW T ))."}
{"doc_id": "1810.04805", "para_id": 41, "text": "7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%.\n8See (10) in https://gluebenchmark.com/faq."}
{"doc_id": "1810.04805", "para_id": 42, "text": "System\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n392k\n363k\n108k\n67k\n8.5k\n5.7k\n3.5k\n2.5k\n-\nPre-OpenAI SOTA\n80.6/80.1\n66.1\n82.3\n93.2\n35.0\n81.0\n86.0\n61.7\n74.0\nBiLSTM+ELMo+Attn\n76.4/76.1\n64.8\n79.8\n90.4\n36.0\n73.3\n84.9\n56.8\n71.0\nOpenAI GPT\n82.1/81.4\n70.3\n87.4\n91.3\n45.4\n80.0\n82.3\n56.0\n75.1\nBERTBASE\n84.6/83.4\n71.2\n90.5\n93.5\n52.1\n85.8\n88.9\n66.4\n79.6\nBERTLARGE\n86.7/85.9\n72.1\n92.7\n94.9\n60.5\n86.5\n89.3\n70.1\n82.1"}
{"doc_id": "1810.04805", "para_id": 43, "text": "Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 44, "text": "Wikipedia containing the answer, the task is to\npredict the answer text span in the passage.\nAs shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the A embedding and the passage using\nthe B embedding. We only introduce a start vec-\ntor S ∈RH and an end vector E ∈RH during\nﬁne-tuning. The probability of word i being the\nstart of the answer span is computed as a dot prod-\nuct between Ti and S followed by a softmax over\nall of the words in the paragraph: Pi =\neS·Ti\nP\nj eS·Tj ."}
{"doc_id": "1810.04805", "para_id": 45, "text": "We use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERTLARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9"}
{"doc_id": "1810.04805", "para_id": 46, "text": "Results are presented in Table 1.\nBoth\nBERTBASE and BERTLARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERTBASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERTLARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe ﬁnd that BERTLARGE signiﬁcantly outper-\nforms BERTBASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2."}
{"doc_id": "1810.04805", "para_id": 47, "text": "The analogous formula is used for the end of the\nanswer span. The score of a candidate span from\nposition i to position j is deﬁned as S·Ti + E·Tj,\nand the maximum scoring span where j ≥i is\nused as a prediction. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions. We ﬁne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32.\nTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11 and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\net al., 2017) befor ﬁne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA ﬁne-"}
{"doc_id": "1810.04805", "para_id": 48, "text": "The\nStanford\nQuestion\nAnswering\nDataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016).\nGiven a question and a passage from"}
{"doc_id": "1810.04805", "para_id": 49, "text": "9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERTBASE and BERTLARGE.\n10https://gluebenchmark.com/leaderboard"}
{"doc_id": "1810.04805", "para_id": 50, "text": "11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication."}
{"doc_id": "1810.04805", "para_id": 51, "text": "ESIM+GloVe\n51.9 52.7\nESIM+ELMo\n59.1 59.2\nOpenAI GPT\n-\n78.0"}
{"doc_id": "1810.04805", "para_id": 52, "text": "Top Leaderboard Systems (Dec 10th, 2018)\nHuman\n-\n-\n82.3 91.2\n#1 Ensemble - nlnet\n-\n-\n86.0 91.7\n#2 Ensemble - QANet\n-\n-\n84.5 90.5"}
{"doc_id": "1810.04805", "para_id": 53, "text": "Published\nBiDAF+ELMo (Single)\n-\n85.6\n-\n85.8\nR.M. Reader (Ensemble)\n81.2 87.9 82.3 88.5"}
{"doc_id": "1810.04805", "para_id": 54, "text": "Human (expert)†\n-\n85.0\nHuman (5 annotations)†\n-\n88.0"}
{"doc_id": "1810.04805", "para_id": 55, "text": "Ours\nBERTBASE (Single)\n80.8 88.5\n-\n-\nBERTLARGE (Single)\n84.1 90.9\n-\n-\nBERTLARGE (Ensemble)\n85.8 91.8\n-\n-\nBERTLARGE (Sgl.+TriviaQA)\n84.2 91.1 85.1 91.8\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2"}
{"doc_id": "1810.04805", "para_id": 56, "text": "Table 4: SWAG Dev and Test accuracies. †Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper."}
{"doc_id": "1810.04805", "para_id": 57, "text": "ˆ\nsi,j = maxj≥iS·Ti + E·Tj. We predict a non-null\nanswer when ˆ\nsi,j > snull + τ, where the thresh-\nold τ is selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system."}
{"doc_id": "1810.04805", "para_id": 58, "text": "Table 2:\nSQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds."}
{"doc_id": "1810.04805", "para_id": 59, "text": "Top Leaderboard Systems (Dec 10th, 2018)\nHuman\n86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net)\n-\n-\n74.8 78.0\n#2 Single - nlnet\n-\n-\n74.2 77.1"}
{"doc_id": "1810.04805", "para_id": 60, "text": "Published\nunet (Ensemble)\n-\n-\n71.4 74.9\nSLQA+ (Single)\n-\n71.4 74.4"}
{"doc_id": "1810.04805", "para_id": 61, "text": "The Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen ﬁne-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speciﬁc parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation C denotes a score for each choice\nwhich is normalized with a softmax layer.\nWe ﬁne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERTLARGE out-\nperforms the authors’ baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%."}
{"doc_id": "1810.04805", "para_id": 62, "text": "Table 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components."}
{"doc_id": "1810.04805", "para_id": 63, "text": "tuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12"}
{"doc_id": "1810.04805", "para_id": 64, "text": "The SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull =\nS·C + E·C to the score of the best non-null span"}
{"doc_id": "1810.04805", "para_id": 65, "text": "In this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional"}
{"doc_id": "1810.04805", "para_id": 66, "text": "12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\nthat contain at least one of the provided possible answers."}
{"doc_id": "1810.04805", "para_id": 67, "text": "results are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."}
{"doc_id": "1810.04805", "para_id": 68, "text": "Dev Set\nTasks\nMNLI-m QNLI MRPC SST-2 SQuAD\n(Acc)\n(Acc)\n(Acc)\n(Acc)\n(F1)"}
{"doc_id": "1810.04805", "para_id": 69, "text": "BERTBASE\n84.4\n88.4\n86.7\n92.7\n88.5\nNo NSP\n83.9\n84.9\n86.5\n92.6\n87.9\nLTR & No NSP\n82.1\n84.3\n77.5\n92.1\n77.8\n+ BiLSTM\n82.1\n84.1\n75.7\n91.6\n84.9"}
{"doc_id": "1810.04805", "para_id": 70, "text": "Table 5: Ablation over the pre-training tasks using the\nBERTBASE architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning."}
{"doc_id": "1810.04805", "para_id": 71, "text": "In this section, we explore the effect of model size\non ﬁne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERTBASE\ncontains 110M parameters and BERTLARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6.\nHowever, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained. Peters et al. (2018b) presented"}
{"doc_id": "1810.04805", "para_id": 72, "text": "We demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERTBASE:"}
{"doc_id": "1810.04805", "para_id": 73, "text": "No NSP: A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task.\nLTR & No NSP: A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at ﬁne-tuning, because removing\nit introduced a pre-train/ﬁne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our ﬁne-tuning scheme.\nWe ﬁrst examine the impact brought by the NSP\ntask.\nIn Table 5, we show that removing NSP\nhurts performance signiﬁcantly on QNLI, MNLI,\nand SQuAD 1.1. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing “No NSP” to “LTR & No NSP”. The LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD.\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsigniﬁcantly improve results on SQuAD, but the"}
{"doc_id": "1810.04805", "para_id": 74, "text": "mixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach — we hypothesize that when the\nmodel is ﬁne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeciﬁc models can beneﬁt from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small."}
{"doc_id": "1810.04805", "para_id": 75, "text": "ELMo (Peters et al., 2018a)\n95.7\n92.2\nCVT (Clark et al., 2018)\n-\n92.6\nCSE (Akbik et al., 2018)\n-\n93.1"}
{"doc_id": "1810.04805", "para_id": 76, "text": "Fine-tuning approach\nBERTLARGE\n96.6\n92.8\nBERTBASE\n96.4\n92.4"}
{"doc_id": "1810.04805", "para_id": 77, "text": "Feature-based approach (BERTBASE)\nEmbeddings\n91.0\n-\nSecond-to-Last Hidden\n95.6\n-\nLast Hidden\n94.9\n-\nWeighted Sum Last Four Hidden\n95.9\n-\nConcat Last Four Hidden\n96.1\n-\nWeighted Sum All 12 Layers\n95.5\n-"}
{"doc_id": "1810.04805", "para_id": 78, "text": "Table 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters."}
{"doc_id": "1810.04805", "para_id": 79, "text": "All of the BERT results presented so far have used\nthe ﬁne-tuning approach, where a simple classiﬁ-\ncation layer is added to the pre-trained model, and\nall parameters are jointly ﬁne-tuned on a down-\nstream task. However, the feature-based approach,\nwhere ﬁxed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speciﬁc model architecture to be added.\nSecond, there are major computational beneﬁts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation.\nIn this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF"}
{"doc_id": "1810.04805", "para_id": 80, "text": "layer in the output. We use the representation of\nthe ﬁrst sub-token as the input to the token-level\nclassiﬁer over the NER label set."}
{"doc_id": "1810.04805", "para_id": 81, "text": "To ablate the ﬁne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without ﬁne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classiﬁcation layer."}
{"doc_id": "1810.04805", "para_id": 82, "text": "Results are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind ﬁne-tuning the entire model. This\ndemonstrates that BERT is effective for both ﬁne-\ntuning and feature-based approaches."}
{"doc_id": "1810.04805", "para_id": 83, "text": "Recent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks."}
{"doc_id": "1810.04805", "para_id": 84, "text": "3\n768\n12\n5.84\n77.9\n79.8\n88.4\n6\n768\n3\n5.24\n80.6\n82.2\n90.7\n6\n768\n12\n4.68\n81.9\n84.8\n91.3\n12\n768\n12\n3.99\n84.4\n86.7\n92.9\n12 1024\n16\n3.54\n85.7\n86.9\n93.3\n24 1024\n16\n3.23\n86.6\n87.8\n93.7"}
{"doc_id": "1810.04805", "para_id": 85, "text": "Table 6:\nAblation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. “LM (ppl)” is the masked LM perplexity\nof held-out training data."}
{"doc_id": "1810.04805", "para_id": 86, "text": "Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018.\nSemi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914–\n1925."}
{"doc_id": "1810.04805", "para_id": 87, "text": "Alan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638–1649."}
{"doc_id": "1810.04805", "para_id": 88, "text": "Ronan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning.\nIn Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160–167. ACM."}
{"doc_id": "1810.04805", "para_id": 89, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018.\nCharacter-level lan-\nguage modeling with deeper self-attention.\narXiv\npreprint arXiv:1808.04444."}
{"doc_id": "1810.04805", "para_id": 90, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics."}
{"doc_id": "1810.04805", "para_id": 91, "text": "Rie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817–1853."}
{"doc_id": "1810.04805", "para_id": 92, "text": "Luisa Bentivogli,\nBernardo Magnini,\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe ﬁfth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST."}
{"doc_id": "1810.04805", "para_id": 93, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087."}
{"doc_id": "1810.04805", "para_id": 94, "text": "John Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120–128. Association for Computa-\ntional Linguistics."}
{"doc_id": "1810.04805", "para_id": 95, "text": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09."}
{"doc_id": "1810.04805", "para_id": 96, "text": "William B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005)."}
{"doc_id": "1810.04805", "para_id": 97, "text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics."}
{"doc_id": "1810.04805", "para_id": 98, "text": "William Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via ﬁlling in\nthe . arXiv preprint arXiv:1801.07736."}
{"doc_id": "1810.04805", "para_id": 99, "text": "Peter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics, 18(4):467–479."}
{"doc_id": "1810.04805", "para_id": 100, "text": "Dan Hendrycks and Kevin Gimpel. 2016.\nBridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415."}
{"doc_id": "1810.04805", "para_id": 101, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancou-\nver, Canada. Association for Computational Lin-\nguistics."}
{"doc_id": "1810.04805", "para_id": 102, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics."}
{"doc_id": "1810.04805", "para_id": 103, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL. Association for Computational Linguistics."}
{"doc_id": "1810.04805", "para_id": 104, "text": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005."}
{"doc_id": "1810.04805", "para_id": 105, "text": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. 2018.\nReinforced\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI."}
{"doc_id": "1810.04805", "para_id": 106, "text": "Yacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557."}
{"doc_id": "1810.04805", "para_id": 107, "text": "Christopher Clark and Matt Gardner. 2018.\nSimple\nand effective multi-paragraph reading comprehen-\nsion. In ACL."}
{"doc_id": "1810.04805", "para_id": 108, "text": "Matthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b.\nDissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499–1509."}
{"doc_id": "1810.04805", "para_id": 109, "text": "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL."}
{"doc_id": "1810.04805", "para_id": 110, "text": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302."}
{"doc_id": "1810.04805", "para_id": 111, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI."}
{"doc_id": "1810.04805", "para_id": 112, "text": "Quoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188–1196."}
{"doc_id": "1810.04805", "para_id": 113, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392."}
{"doc_id": "1810.04805", "para_id": 114, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47."}
{"doc_id": "1810.04805", "para_id": 115, "text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nﬂow for machine comprehension. In ICLR."}
{"doc_id": "1810.04805", "para_id": 116, "text": "Lajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations.\nIn International Conference on Learning\nRepresentations."}
{"doc_id": "1810.04805", "para_id": 117, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642."}
{"doc_id": "1810.04805", "para_id": 118, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS."}
{"doc_id": "1810.04805", "para_id": 119, "text": "Oren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL."}
{"doc_id": "1810.04805", "para_id": 120, "text": "Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018.\nU-net:\nMachine reading comprehension\nwith unanswerable questions.\narXiv preprint\narXiv:1810.06638."}
{"doc_id": "1810.04805", "para_id": 121, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111–3119. Curran Associates,\nInc."}
{"doc_id": "1810.04805", "para_id": 122, "text": "Wilson L Taylor. 1953.\nCloze procedure:\nA new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433."}
{"doc_id": "1810.04805", "para_id": 123, "text": "Erik F Tjong Kim Sang and Fien De Meulder.\n2003.\nIntroduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL."}
{"doc_id": "1810.04805", "para_id": 124, "text": "Andriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model.\nIn\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21, pages 1081–1088. Curran As-\nsociates, Inc."}
{"doc_id": "1810.04805", "para_id": 125, "text": "Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL ’10, pages 384–394."}
{"doc_id": "1810.04805", "para_id": 126, "text": "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In EMNLP."}
{"doc_id": "1810.04805", "para_id": 127, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010."}
{"doc_id": "1810.04805", "para_id": 128, "text": "Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532–\n1543."}
{"doc_id": "1810.04805", "para_id": 129, "text": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008.\nExtracting and\ncomposing robust features with denoising autoen-\ncoders.\nIn Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103.\nACM."}
{"doc_id": "1810.04805", "para_id": 130, "text": "Matthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL."}
{"doc_id": "1810.04805", "para_id": 131, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In NAACL."}
{"doc_id": "1810.04805", "para_id": 132, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform"}
{"doc_id": "1810.04805", "para_id": 133, "text": "• Additional details for our experiments are\npresented in Appendix B; and"}
{"doc_id": "1810.04805", "para_id": 134, "text": "for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355."}
{"doc_id": "1810.04805", "para_id": 135, "text": "• Additional ablation studies are presented in\nAppendix C."}
{"doc_id": "1810.04805", "para_id": 136, "text": "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics."}
{"doc_id": "1810.04805", "para_id": 137, "text": "We present additional ablation studies for\nBERT including:"}
{"doc_id": "1810.04805", "para_id": 138, "text": "– Effect of Number of Training Steps; and\n– Ablation for Different Masking Proce-\ndures."}
{"doc_id": "1810.04805", "para_id": 139, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471."}
{"doc_id": "1810.04805", "para_id": 140, "text": "Adina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nNAACL."}
{"doc_id": "1810.04805", "para_id": 141, "text": "We provide examples of the pre-training tasks in\nthe following."}
{"doc_id": "1810.04805", "para_id": 142, "text": "Masked LM and the Masking Procedure\nAs-\nsuming the unlabeled sentence is\nmy dog is"}
{"doc_id": "1810.04805", "para_id": 143, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe,\nMohammad Norouzi,\nWolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin Gao,\nKlaus\nMacherey, et al. 2016.\nGoogle’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144."}
{"doc_id": "1810.04805", "para_id": 144, "text": "hairy, and during the random masking procedure\nwe chose the 4-th token (which corresponding to"}
{"doc_id": "1810.04805", "para_id": 145, "text": "hairy), our masking procedure can be further il-\nlustrated by"}
{"doc_id": "1810.04805", "para_id": 146, "text": "• 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy →"}
{"doc_id": "1810.04805", "para_id": 147, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320–3328."}
{"doc_id": "1810.04805", "para_id": 148, "text": "• 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy →my"}
{"doc_id": "1810.04805", "para_id": 149, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018.\nQANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR."}
{"doc_id": "1810.04805", "para_id": 150, "text": "• 10% of the time:\nKeep the word un-\nchanged, e.g., my dog is hairy →my dog"}
{"doc_id": "1810.04805", "para_id": 151, "text": "Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP)."}
{"doc_id": "1810.04805", "para_id": 152, "text": "is hairy. The purpose of this is to bias the\nrepresentation towards the actual observed\nword."}
{"doc_id": "1810.04805", "para_id": 153, "text": "The advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\nery input token.\nAdditionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model’s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model"}
{"doc_id": "1810.04805", "para_id": 154, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books.\nIn Proceedings of the IEEE\ninternational conference on computer vision, pages\n19–27."}
{"doc_id": "1810.04805", "para_id": 155, "text": "Appendix for “BERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding”"}
{"doc_id": "1810.04805", "para_id": 156, "text": "• Additional implementation details for BERT\nare presented in Appendix A;"}
{"doc_id": "1810.04805", "para_id": 157, "text": "Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach."}
{"doc_id": "1810.04805", "para_id": 158, "text": "epochs over the 3.3 billion word corpus.\nWe\nuse Adam with learning rate of 1e-4, β1 = 0.9,\nβ2 = 0.999, L2 weight decay of 0.01, learning\nrate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu, following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERTBASE was performed on 4\nCloud TPUs in Pod conﬁguration (16 TPU chips\ntotal).13 Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings."}
{"doc_id": "1810.04805", "para_id": 159, "text": "to converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost."}
{"doc_id": "1810.04805", "para_id": 160, "text": "Next Sentence Prediction\nThe next sentence\nprediction task can be illustrated in the following\nexamples."}
{"doc_id": "1810.04805", "para_id": 161, "text": "To generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as “sentences” even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The ﬁrst sentence receives the A\nembedding and the second receives the B embed-\nding. 50% of the time B is the actual next sentence\nthat follows A and 50% of the time it is a random\nsentence, which is done for the “next sentence pre-\ndiction” task. They are sampled such that the com-\nbined length is ≤512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40"}
{"doc_id": "1810.04805", "para_id": 162, "text": "For ﬁne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speciﬁc, but we found the following range\nof possible values to work well across all tasks:"}
{"doc_id": "1810.04805", "para_id": 163, "text": "13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html"}
{"doc_id": "1810.04805", "para_id": 164, "text": "• Learning rate (Adam): 5e-5, 3e-5, 2e-5\n• Number of epochs: 2, 3, 4"}
{"doc_id": "1810.04805", "para_id": 165, "text": "To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable."}
{"doc_id": "1810.04805", "para_id": 166, "text": "We also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set."}
{"doc_id": "1810.04805", "para_id": 167, "text": "A.5\nIllustrations of Fine-tuning on Different\nTasks"}
{"doc_id": "1810.04805", "para_id": 168, "text": "The illustration of ﬁne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speciﬁc\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe ﬁgure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classiﬁcation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences."}
{"doc_id": "1810.04805", "para_id": 169, "text": "Here we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:"}
{"doc_id": "1810.04805", "para_id": 170, "text": "B.1\nDetailed Descriptions for the GLUE\nBenchmark Experiments."}
{"doc_id": "1810.04805", "para_id": 171, "text": "Our\nGLUE\nresults\nin\nTable1\nare\nobtained\nfrom\nhttps://gluebenchmark.com/\nleaderboard\nand\nhttps://blog.\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):"}
{"doc_id": "1810.04805", "para_id": 172, "text": "MNLI\nMulti-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classiﬁ-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the ﬁrst one."}
{"doc_id": "1810.04805", "para_id": 173, "text": "• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords)."}
{"doc_id": "1810.04805", "para_id": 174, "text": "QQP\nQuora Question Pairs is a binary classiﬁ-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018)."}
{"doc_id": "1810.04805", "para_id": 175, "text": "• GPT uses a sentence separator ([SEP]) and\nclassiﬁer token ([CLS]) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP], [CLS] and sentence A/B embed-\ndings during pre-training."}
{"doc_id": "1810.04805", "para_id": 176, "text": "QNLI\nQuestion Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classiﬁcation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer."}
{"doc_id": "1810.04805", "para_id": 177, "text": "• GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words."}
{"doc_id": "1810.04805", "para_id": 178, "text": "• GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set."}
{"doc_id": "1810.04805", "para_id": 179, "text": "Figure 4: Illustrations of Fine-tuning BERT on Different Tasks."}
{"doc_id": "1810.04805", "para_id": 180, "text": "SST-2\nThe Stanford Sentiment Treebank is a\nbinary single-sentence classiﬁcation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013)."}
{"doc_id": "1810.04805", "para_id": 181, "text": "for whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005)."}
{"doc_id": "1810.04805", "para_id": 182, "text": "RTE\nRecognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14"}
{"doc_id": "1810.04805", "para_id": 183, "text": "CoLA\nThe Corpus of Linguistic Acceptability is\na binary single-sentence classiﬁcation task, where\nthe goal is to predict whether an English sentence\nis linguistically “acceptable” or not (Warstadt\net al., 2018)."}
{"doc_id": "1810.04805", "para_id": 184, "text": "WNLI\nWinograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that’s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-"}
{"doc_id": "1810.04805", "para_id": 185, "text": "STS-B\nThe Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning."}
{"doc_id": "1810.04805", "para_id": 186, "text": "14Note that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq"}
{"doc_id": "1810.04805", "para_id": 187, "text": "MRPC\nMicrosoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations"}
{"doc_id": "1810.04805", "para_id": 188, "text": "Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions."}
{"doc_id": "1810.04805", "para_id": 189, "text": "Figure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:"}
{"doc_id": "1810.04805", "para_id": 190, "text": "1. Question:\nDoes BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps."}
{"doc_id": "1810.04805", "para_id": 191, "text": "MASK SAME\nRND\nMNLI\nNER\nFine-tune Fine-tune Feature-based"}
{"doc_id": "1810.04805", "para_id": 192, "text": "80%\n10%\n10%\n84.2\n95.4\n94.9\n100%\n0%\n0%\n84.3\n94.9\n94.0\n80%\n0%\n20%\n84.1\n95.2\n94.6\n80%\n20%\n0%\n84.4\n95.2\n94.7\n0%\n20%\n80%\n83.7\n94.8\n94.6\n0%\n0% 100%\n83.6\n94.9\n94.6"}
{"doc_id": "1810.04805", "para_id": 193, "text": "2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately."}
{"doc_id": "1810.04805", "para_id": 194, "text": "Table 8: Ablation over different masking strategies."}
{"doc_id": "1810.04805", "para_id": 195, "text": "The results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well."}
{"doc_id": "1810.04805", "para_id": 196, "text": "In Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies."}
{"doc_id": "1810.04805", "para_id": 197, "text": "Figure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."}
{"doc_id": "1910.01108", "para_id": 0, "text": "DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter"}
{"doc_id": "1910.01108", "para_id": 1, "text": "Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co"}
{"doc_id": "1910.01108", "para_id": 2, "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-the-\nedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller general-\npurpose language representation model, called DistilBERT, which can then be ﬁne-\ntuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciﬁc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\ncapabilities for on-device computations in a proof-of-concept experiment and a\ncomparative on-device study."}
{"doc_id": "1910.01108", "para_id": 3, "text": "The last two years have seen the rise\nof Transfer Learning approaches in\nNatural Language Processing (NLP)\nwith large-scale pre-trained language\nmodels becoming a basic tool in\nmany NLP tasks [Devlin et al., 2018,\nRadford et al., 2019, Liu et al., 2019].\nWhile these models lead to signiﬁ-\ncant improvement, they often have\nseveral hundred million parameters\nand current research1 on pre-trained\nmodels indicates that training even\nlarger models still leads to better per-\nformances on downstream tasks."}
{"doc_id": "1910.01108", "para_id": 4, "text": "Figure 1: Parameter counts of several recently released\npretrained language models."}
{"doc_id": "1910.01108", "para_id": 5, "text": "The trend toward bigger models\nraises several concerns. First is the\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption."}
{"doc_id": "1910.01108", "para_id": 6, "text": "1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)"}
{"doc_id": "1910.01108", "para_id": 7, "text": "In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices."}
{"doc_id": "1910.01108", "para_id": 8, "text": "Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances."}
{"doc_id": "1910.01108", "para_id": 9, "text": "We have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019]."}
{"doc_id": "1910.01108", "para_id": 10, "text": "Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble of models."}
{"doc_id": "1910.01108", "para_id": 11, "text": "In supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these \"near-zero\" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3."}
{"doc_id": "1910.01108", "para_id": 12, "text": "Training loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce = P"}
{"doc_id": "1910.01108", "para_id": 13, "text": "i ti ∗log(si) where ti (resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature: pi =\nexp(zi/T )\nP"}
{"doc_id": "1910.01108", "para_id": 14, "text": "j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i.\nThe same temperature T is applied to the student and the teacher at training time, while at inference,\nT is set to 1 to recover a standard softmax."}
{"doc_id": "1910.01108", "para_id": 15, "text": "The ﬁnal training objective is a linear combination of the distillation loss Lce with the supervised\ntraining loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors."}
{"doc_id": "1910.01108", "para_id": 16, "text": "Student architecture In the present work, the student - DistilBERT - has the same general architec-\nture as BERT. The token-type embeddings and the pooler are removed while the number of layers\nis reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear\nlayer and layer normalisation) are highly optimized in modern linear algebra frameworks and our\ninvestigations showed that variations on the last dimension of the tensor (hidden size dimension) have\na smaller impact on computation efﬁciency (for a ﬁxed parameters budget) than variations on other\nfactors like the number of layers. Thus we focus on reducing the number of layers."}
{"doc_id": "1910.01108", "para_id": 17, "text": "Student initialization In addition to the previously described optimization and architectural choices,\nan important element in our training procedure is to ﬁnd the right initialization for the sub-network to\nconverge. Taking advantage of the common dimensionality between teacher and student networks,\nwe initialize the student from the teacher by taking one layer out of two."}
{"doc_id": "1910.01108", "para_id": 18, "text": "2https://github.com/huggingface/transformers\n3E.g.\nBERT-base’s predictions for a masked token in \"I think this is the beginning of a\nbeautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions\n(future, story, world...)."}
{"doc_id": "1910.01108", "para_id": 19, "text": "Table 1: DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the\nGLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the\nmedians of 5 runs with different seeds."}
{"doc_id": "1910.01108", "para_id": 20, "text": "Model\nScore\nCoLA\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nWNLI"}
{"doc_id": "1910.01108", "para_id": 21, "text": "ELMo\n68.7\n44.1\n68.6\n76.6\n71.1\n86.2\n53.4\n91.5\n70.4\n56.3\nBERT-base\n79.5\n56.3\n86.7\n88.6\n91.8\n89.6\n69.3\n92.7\n89.0\n53.5\nDistilBERT\n77.0\n51.3\n82.2\n87.5\n89.2\n88.5\n59.9\n91.3\n86.9\n56.3"}
{"doc_id": "1910.01108", "para_id": 22, "text": "Table 2: DistilBERT yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning."}
{"doc_id": "1910.01108", "para_id": 23, "text": "Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster.\nInference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1."}
{"doc_id": "1910.01108", "para_id": 24, "text": "BERT-base\n93.46\n81.2/88.5\nDistilBERT\n92.82\n77.7/85.8\nDistilBERT (D)\n-\n79.1/86.9"}
{"doc_id": "1910.01108", "para_id": 25, "text": "Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective."}
{"doc_id": "1910.01108", "para_id": 26, "text": "Data and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100."}
{"doc_id": "1910.01108", "para_id": 27, "text": "General Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed by two BiLSTMs.4"}
{"doc_id": "1910.01108", "para_id": 28, "text": "The results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters."}
{"doc_id": "1910.01108", "para_id": 29, "text": "Downstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016])."}
{"doc_id": "1910.01108", "para_id": 30, "text": "As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT."}
{"doc_id": "1910.01108", "para_id": 31, "text": "We also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a"}
{"doc_id": "1910.01108", "para_id": 32, "text": "4We use jiant [Wang et al., 2019] to compute the baseline."}
{"doc_id": "1910.01108", "para_id": 33, "text": "Table 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization."}
{"doc_id": "1910.01108", "para_id": 34, "text": "∅- Lcos - Lmlm\n-2.96\nLce - ∅- Lmlm\n-1.46\nLce - Lcos - ∅\n-0.31\nTriple loss + random weights initialization\n-3.69"}
{"doc_id": "1910.01108", "para_id": 35, "text": "teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model."}
{"doc_id": "1910.01108", "para_id": 36, "text": "To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT."}
{"doc_id": "1910.01108", "para_id": 37, "text": "On device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5."}
{"doc_id": "1910.01108", "para_id": 38, "text": "In this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance."}
{"doc_id": "1910.01108", "para_id": 39, "text": "Task-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original\npretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the abla-\ntion study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional\ndistillation signal."}
{"doc_id": "1910.01108", "para_id": 40, "text": "Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using\nmulti-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\nto learn a compact question answering model from a set of large question answering models. An\napplication of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us\nby pre-training a multilingual model from scratch solely through distillation. However, as shown in\nthe ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads\nto substantial gains."}
{"doc_id": "1910.01108", "para_id": 41, "text": "Other compression techniques have been studied to compress large models. Recent developments\nin weights pruning reveal that it is possible to remove some heads in the self-attention at test time\nwithout signiﬁcantly degrading the performance Michel et al. [2019]. Some layers can be reduced\nto one head. A separate line of study leverages quantization to derive smaller models (Gupta et al.\n[2015]). Pruning and quantization are orthogonal to the present work."}
{"doc_id": "1910.01108", "para_id": 42, "text": "5https://github.com/huggingface/swift-coreml-transformers"}
{"doc_id": "1910.01108", "para_id": 43, "text": "We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster,\nthat retains 97% of the language understanding capabilities. We showed that a general-purpose\nlanguage model can be successfully trained with distillation and analyzed the various components\nwith an ablation study. We further demonstrated that DistilBERT is a compelling option for edge\napplications."}
{"doc_id": "1910.01108", "para_id": 44, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT, 2018."}
{"doc_id": "1910.01108", "para_id": 45, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019."}
{"doc_id": "1910.01108", "para_id": 46, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv,\nabs/1907.11692, 2019."}
{"doc_id": "1910.01108", "para_id": 47, "text": "Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. ArXiv, abs/1907.10597, 2019."}
{"doc_id": "1910.01108", "para_id": 48, "text": "Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019."}
{"doc_id": "1910.01108", "para_id": 49, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "1910.01108", "para_id": 50, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Transformers: State-of-the-art natural language\nprocessing, 2019."}
{"doc_id": "1910.01108", "para_id": 51, "text": "Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006."}
{"doc_id": "1910.01108", "para_id": 52, "text": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv,\nabs/1503.02531, 2015."}
{"doc_id": "1910.01108", "para_id": 53, "text": "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27, 2015."}
{"doc_id": "1910.01108", "para_id": 54, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In ICLR, 2018."}
{"doc_id": "1910.01108", "para_id": 55, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL, 2018."}
{"doc_id": "1910.01108", "para_id": 56, "text": "Alex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning\nJin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Najoung Kim, Phu Mon\nHtut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, Anhad Mohananey, Shikha Bordia, Nicolas\nPatry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose\ntext understanding models. http://jiant.info/, 2019."}
{"doc_id": "1910.01108", "para_id": 57, "text": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning\nword vectors for sentiment analysis. In ACL, 2011."}
{"doc_id": "1910.01108", "para_id": 58, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP, 2016."}
{"doc_id": "1910.01108", "para_id": 59, "text": "Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-speciﬁc\nknowledge from bert into simple neural networks. ArXiv, abs/1903.12136, 2019."}
{"doc_id": "1910.01108", "para_id": 60, "text": "Debajyoti Chatterjee. Making neural machine reading comprehension faster. ArXiv, abs/1904.00796, 2019."}
{"doc_id": "1910.01108", "para_id": 61, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact\nof student initialization on knowledge distillation. ArXiv, abs/1908.08962, 2019."}
{"doc_id": "1910.01108", "para_id": 62, "text": "Ze Yang, Linjun Shou, Ming Gong, Wutao Lin, and Daxin Jiang. Model compression with multi-task knowledge\ndistillation for web-scale question answering system. ArXiv, abs/1904.09636, 2019."}
{"doc_id": "1910.01108", "para_id": 63, "text": "Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. Small and practical\nbert models for sequence labeling. In EMNLP-IJCNLP, 2019."}
{"doc_id": "1910.01108", "para_id": 64, "text": "Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019."}
{"doc_id": "1910.01108", "para_id": 65, "text": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\nnumerical precision. In ICML, 2015."}
{"doc_id": "1909.11942", "para_id": 0, "text": "ALBERT: A LITE BERT\nFOR SELF-SUPERVISED\nLEARNING OF LANGUAGE REPRESENTATIONS"}
{"doc_id": "1909.11942", "para_id": 1, "text": "Zhenzhong Lan1\nMingda Chen2∗\nSebastian Goodman1\nKevin Gimpel2"}
{"doc_id": "1909.11942", "para_id": 2, "text": "1Google Research\n2Toyota Technological Institute at Chicago"}
{"doc_id": "1909.11942", "para_id": 3, "text": "{lanzhzh, seabass, piyushsharma, rsoricut}@google.com\n{mchen, kgimpel}@ttic.edu"}
{"doc_id": "1909.11942", "para_id": 4, "text": "Increasing model size when pretraining natural language representations often re-\nsults in improved performance on downstream tasks. However, at some point fur-\nther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two parameter-\nreduction techniques to lower memory consumption and increase the training\nspeed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to\nthe original BERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new state-of-the-art\nresults on the GLUE, RACE, and SQuAD benchmarks while having fewer param-\neters compared to BERT-large. The code and the pretrained models are available\nat https://github.com/google-research/ALBERT."}
{"doc_id": "1909.11942", "para_id": 5, "text": "Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard &\nRuder, 2018) has led to a series of breakthroughs in language representation learning. Many non-\ntrivial NLP tasks, including those that have limited training data, have greatly beneﬁted from these\npre-trained models. One of the most compelling signs of these breakthroughs is the evolution of ma-\nchine performance on a reading comprehension task designed for middle and high-school English\nexams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and for-\nmulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest\npublished result reports their model performance at 83.2% (Liu et al., 2019); the work we present\nhere pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to\nour current ability to build high-performance pretrained language representations."}
{"doc_id": "1909.11942", "para_id": 6, "text": "Evidence from these improvements reveals that a large network is of crucial importance for achiev-\ning state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common\npractice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al.,\n2019) for real applications. Given the importance of model size, we ask: Is having better NLP\nmodels as easy as having larger models?"}
{"doc_id": "1909.11942", "para_id": 7, "text": "An obstacle to answering this question is the memory limitations of available hardware. Given that\ncurrent state-of-the-art models often have hundreds of millions or even billions of parameters, it is\neasy to hit these limitations as we try to scale our models. Training speed can also be signiﬁcantly\nhampered in distributed training, as the communication overhead is directly proportional to the\nnumber of parameters in the model."}
{"doc_id": "1909.11942", "para_id": 8, "text": "Existing solutions to the aforementioned problems include model parallelization (Shazeer et al.,\n2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017)."}
{"doc_id": "1909.11942", "para_id": 9, "text": "∗Work done as an intern at Google Research, driving data processing and downstream task evaluations."}
{"doc_id": "1909.11942", "para_id": 10, "text": "These solutions address the memory limitation problem, but not the communication overhead. In\nthis paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT)\narchitecture that has signiﬁcantly fewer parameters than a traditional BERT architecture."}
{"doc_id": "1909.11942", "para_id": 11, "text": "ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling\npre-trained models. The ﬁrst one is a factorized embedding parameterization. By decomposing\nthe large vocabulary embedding matrix into two small matrices, we separate the size of the hidden\nlayers from the size of vocabulary embedding. This separation makes it easier to grow the hidden\nsize without signiﬁcantly increasing the parameter size of the vocabulary embeddings. The second\ntechnique is cross-layer parameter sharing. This technique prevents the parameter from growing\nwith the depth of the network. Both techniques signiﬁcantly reduce the number of parameters for\nBERT without seriously hurting performance, thus improving parameter-efﬁciency. An ALBERT\nconﬁguration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\nThe parameter reduction techniques also act as a form of regularization that stabilizes the training\nand helps with generalization."}
{"doc_id": "1909.11942", "para_id": 12, "text": "To further improve the performance of ALBERT, we also introduce a self-supervised loss for\nsentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed\nto address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction\n(NSP) loss proposed in the original BERT."}
{"doc_id": "1909.11942", "para_id": 13, "text": "As a result of these design decisions, we are able to scale up to much larger ALBERT conﬁgurations\nthat still have fewer parameters than BERT-large but achieve signiﬁcantly better performance. We\nestablish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks\nfor natural language understanding. Speciﬁcally, we push the RACE accuracy to 89.4%, the GLUE\nbenchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2."}
{"doc_id": "1909.11942", "para_id": 14, "text": "2.1\nSCALING UP REPRESENTATION LEARNING FOR NATURAL LANGUAGE"}
{"doc_id": "1909.11942", "para_id": 15, "text": "Learning representations of natural language has been shown to be useful for a wide range of NLP\ntasks and has been widely adopted (Mikolov et al., 2013; Le & Mikolov, 2014; Dai & Le, 2015; Pe-\nters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019). One of the most signiﬁcant changes\nin the last two years is the shift from pre-training word embeddings, whether standard (Mikolov\net al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018),\nto full-network pre-training followed by task-speciﬁc ﬁne-tuning (Dai & Le, 2015; Radford et al.,\n2018; Devlin et al., 2019). In this line of work, it is often shown that larger model size improves\nperformance. For example, Devlin et al. (2019) show that across three selected natural language\nunderstanding tasks, using larger hidden size, more hidden layers, and more attention heads always\nleads to better performance. However, they stop at a hidden size of 1024, presumably because of the\nmodel size and computation cost problems."}
{"doc_id": "1909.11942", "para_id": 16, "text": "It is difﬁcult to experiment with large models due to computational constraints, especially in terms\nof GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of\nmillions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen\net al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be\nsublinear at the cost of an extra forward pass. Gomez et al. (2017) propose a way to reconstruct each\nlayer’s activations from the next layer so that they do not need to store the intermediate activations.\nBoth methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed\nto use model parallelization to train a giant model. In contrast, our parameter-reduction techniques\nreduce memory consumption and increase training speed."}
{"doc_id": "1909.11942", "para_id": 17, "text": "The idea of sharing parameters across layers has been previously explored with the Transformer\narchitecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoder-\ndecoder tasks rather than the pretraining/ﬁnetuning setting. Different from our observations, De-\nhghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer,\nUT) get better performance on language modeling and subject-verb agreement than the standard"}
{"doc_id": "1909.11942", "para_id": 18, "text": "transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for trans-\nformer networks and show that DQE can reach an equilibrium point for which the input embedding\nand the output embedding of a certain layer stay the same. Our observations show that our em-\nbeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing\ntransformer with the standard one, which further increases the number of parameters of the standard\ntransformer."}
{"doc_id": "1909.11942", "para_id": 19, "text": "ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments\nof text. Several researchers have experimented with pretraining objectives that similarly relate to\ndiscourse coherence. Coherence and cohesion in discourse have been widely studied and many\nphenomena have been identiﬁed that connect neighboring text segments (Hobbs, 1979; Halliday &\nHasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skip-\nthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using\nan encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence\nembedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017)\nand predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most\nsimilar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are\nlearned in order to determine the ordering of two consecutive sentences. Unlike most of the above\nwork, however, our loss is deﬁned on textual segments rather than sentences. BERT (Devlin et al.,\n2019) uses a loss based on predicting whether the second segment in a pair has been swapped\nwith a segment from another document. We compare to this loss in our experiments and ﬁnd that\nsentence ordering is a more challenging pretraining task and more useful for certain downstream\ntasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive\nsegments of text, but they combine it with the original next sentence prediction in a three-way\nclassiﬁcation task rather than empirically comparing the two."}
{"doc_id": "1909.11942", "para_id": 20, "text": "In this section, we present the design decisions for ALBERT and provide quantiﬁed comparisons\nagainst corresponding conﬁgurations of the original BERT architecture (Devlin et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 21, "text": "The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer en-\ncoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the\nBERT notation conventions and denote the vocabulary embedding size as E, the number of encoder\nlayers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/ﬁlter\nsize to be 4H and the number of attention heads to be H/64."}
{"doc_id": "1909.11942", "para_id": 22, "text": "There are three main contributions that ALBERT makes over the design choices of BERT."}
{"doc_id": "1909.11942", "para_id": 23, "text": "Factorized embedding parameterization.\nIn BERT, as well as subsequent modeling improve-\nments such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding\nsize E is tied with the hidden layer size H, i.e., E ≡H. This decision appears suboptimal for both\nmodeling and practical reasons, as follows."}
{"doc_id": "1909.11942", "para_id": 24, "text": "From a modeling perspective, WordPiece embeddings are meant to learn context-independent repre-\nsentations, whereas hidden-layer embeddings are meant to learn context-dependent representations.\nAs experiments with context length indicate (Liu et al., 2019), the power of BERT-like represen-\ntations comes from the use of context to provide the signal for learning such context-dependent\nrepresentations. As such, untying the WordPiece embedding size E from the hidden layer size H\nallows us to make a more efﬁcient usage of the total model parameters as informed by modeling\nneeds, which dictate that H ≫E."}
{"doc_id": "1909.11942", "para_id": 25, "text": "From a practical perspective, natural language processing usually require the vocabulary size V to\nbe large.1 If E ≡H, then increasing H increases the size of the embedding matrix, which has size"}
{"doc_id": "1909.11942", "para_id": 26, "text": "1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000."}
{"doc_id": "1909.11942", "para_id": 27, "text": "V ×E. This can easily result in a model with billions of parameters, most of which are only updated\nsparsely during training."}
{"doc_id": "1909.11942", "para_id": 28, "text": "Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them\ninto two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of\nsize H, we ﬁrst project them into a lower dimensional embedding space of size E, and then project\nit to the hidden space. By using this decomposition, we reduce the embedding parameters from\nO(V × H) to O(V × E + E × H). This parameter reduction is signiﬁcant when H ≫E. We\nchoose to use the same E for all word pieces because they are much more evenly distributed across\ndocuments compared to whole-word embedding, where having different embedding size (Grave\net al. (2017); Baevski & Auli (2018); Dai et al. (2019) ) for different words is important."}
{"doc_id": "1909.11942", "para_id": 29, "text": "Cross-layer parameter sharing.\nFor ALBERT, we propose cross-layer parameter sharing as an-\nother way to improve parameter efﬁciency. There are multiple ways to share parameters, e.g., only\nsharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters.\nThe default decision for ALBERT is to share all parameters across layers. All our experiments\nuse this default decision unless otherwise speciﬁed. We compare this design decision against other\nstrategies in our experiments in Sec. 4.5."}
{"doc_id": "1909.11942", "para_id": 30, "text": "Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and\nBai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our\nobservations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al.\n(2019) show that their DQEs reach an equilibrium point for which the input and output embedding\nof a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show\nthat our embeddings are oscillating rather than converging."}
{"doc_id": "1909.11942", "para_id": 31, "text": "Figure 1: The L2 distances and cosine similarity (in terms of degree) of the input and output embed-\nding of each layer for BERT-large and ALBERT-large."}
{"doc_id": "1909.11942", "para_id": 32, "text": "Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each\nlayer, using BERT-large and ALBERT-large conﬁgurations (see Table 1). We observe that the tran-\nsitions from layer to layer are much smoother for ALBERT than for BERT. These results show that\nweight-sharing has an effect on stabilizing network parameters. Although there is a drop for both\nmetrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows\nthat the solution space for ALBERT parameters is very different from the one found by DQE."}
{"doc_id": "1909.11942", "para_id": 33, "text": "Inter-sentence coherence loss.\nIn addition to the masked language modeling (MLM) loss (De-\nvlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a\nbinary classiﬁcation loss for predicting whether two segments appear consecutively in the original\ntext, as follows: positive examples are created by taking consecutive segments from the training\ncorpus; negative examples are created by pairing segments from different documents; positive and\nnegative examples are sampled with equal probability. The NSP objective was designed to improve\nperformance on downstream tasks, such as natural language inference, that require reasoning about\nthe relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al.,\n2019) found NSP’s impact unreliable and decided to eliminate it, a decision supported by an im-\nprovement in downstream task performance across several tasks."}
{"doc_id": "1909.11942", "para_id": 34, "text": "We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difﬁculty as a task,\nas compared to MLM. As formulated, NSP conﬂates topic prediction and coherence prediction in a"}
{"doc_id": "1909.11942", "para_id": 35, "text": "Model\nParameters\nLayers\nHidden\nEmbedding\nParameter-sharing"}
{"doc_id": "1909.11942", "para_id": 36, "text": "BERT\nbase\n108M\n12\n768\n768\nFalse\nlarge\n334M\n24\n1024\n1024\nFalse"}
{"doc_id": "1909.11942", "para_id": 37, "text": "base\n12M\n12\n768\n128\nTrue\nlarge\n18M\n24\n1024\n128\nTrue\nxlarge\n60M\n24\n2048\n128\nTrue\nxxlarge\n235M\n12\n4096\n128\nTrue"}
{"doc_id": "1909.11942", "para_id": 38, "text": "Table 1: The conﬁgurations of the main BERT and ALBERT models analyzed in this paper."}
{"doc_id": "1909.11942", "para_id": 39, "text": "single task2. However, topic prediction is easier to learn compared to coherence prediction, and also\noverlaps more with what is learned using the MLM loss."}
{"doc_id": "1909.11942", "para_id": 40, "text": "We maintain that inter-sentence modeling is an important aspect of language understanding, but we\npropose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order pre-\ndiction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence\ncoherence. The SOP loss uses as positive examples the same technique as BERT (two consecu-\ntive segments from the same document), and as negative examples the same two consecutive seg-\nments but with their order swapped. This forces the model to learn ﬁner-grained distinctions about\ndiscourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the\nSOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at random-\nbaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, pre-\nsumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently\nimprove downstream task performance for multi-sentence encoding tasks."}
{"doc_id": "1909.11942", "para_id": 41, "text": "We present the differences between BERT and ALBERT models with comparable hyperparameter\nsettings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller\nparameter size compared to corresponding BERT models."}
{"doc_id": "1909.11942", "para_id": 42, "text": "For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M ver-\nsus 334M. An ALBERT-xlarge conﬁguration with H = 2048 has only 60M parameters and an\nALBERT-xxlarge conﬁguration with H = 4096 has 233M parameters, i.e., around 70% of BERT-\nlarge’s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network\nbecause a 24-layer network (with the same conﬁguration) obtains similar results but is computation-\nally more expensive."}
{"doc_id": "1909.11942", "para_id": 43, "text": "This improvement in parameter efﬁciency is the most important advantage of ALBERT’s design\nchoices. Before we can quantify this advantage, we need to introduce our experimental setup in\nmore detail."}
{"doc_id": "1909.11942", "para_id": 44, "text": "To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in\nusing the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretrain-\ning baseline models. These two corpora consist of around 16GB of uncompressed text. We format\nour inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1, x1,2 · · · and x2 = x1,1, x1,2 · · · are\ntwo segments.3 We always limit the maximum input length to 512, and randomly generate input\nsequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of\n30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 45, "text": "2Since a negative example is constructed using material from a different document, the negative-example\nsegment is misaligned both from a topic and from a coherence perspective.\n3A segment is usually comprised of more than one natural sentence, which has been shown to beneﬁt\nperformance by Liu et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 46, "text": "We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the\nlength of each n-gram mask selected randomly. The probability for the length n is given by"}
{"doc_id": "1909.11942", "para_id": 47, "text": "We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a\n3-gram of complete words, such as “White House correspondents”)."}
{"doc_id": "1909.11942", "para_id": 48, "text": "All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate\n0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise speciﬁed. Train-\ning was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512,\ndepending on model size."}
{"doc_id": "1909.11942", "para_id": 49, "text": "The experimental setup described in this section is used for all of our own versions of BERT as well\nas ALBERT models, unless otherwise speciﬁed."}
{"doc_id": "1909.11942", "para_id": 50, "text": "To monitor the training progress, we create a development set based on the development sets from\nSQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and\nsentence classiﬁcation tasks. Note that we only use this set to check how the model is converging;\nit has not been used in a way that would affect the performance of any downstream evaluation, such\nas via model selection."}
{"doc_id": "1909.11942", "para_id": 51, "text": "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular bench-\nmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018),\ntwo versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018),\nand the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For com-\npleteness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019),\nwe perform early stopping on the development sets, on which we report all comparisons except for\nour ﬁnal comparisons based on the task leaderboards, for which we also report test set results. For\nGLUE datasets that have large variances on the dev set, we report median over 5 runs."}
{"doc_id": "1909.11942", "para_id": 52, "text": "We are now ready to quantify the impact of the design choices described in Sec. 3, speciﬁcally the\nones around parameter efﬁciency. The improvement in parameter efﬁciency showcases the most\nimportant advantage of ALBERT’s design choices, as shown in Table 2: with only around 70% of\nBERT-large’s parameters, ALBERT-xxlarge achieves signiﬁcant improvements over BERT-large, as\nmeasured by the difference on development set scores for several representative downstream tasks:\nSQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%)."}
{"doc_id": "1909.11942", "para_id": 53, "text": "Another interesting observation is the speed of data throughput at training time under the same train-\ning conﬁguration (same number of TPUs). Because of less communication and fewer computations,\nALBERT models have higher data throughput compared to their corresponding BERT models. If we\nuse BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating\nthrough the data while ALBERT-xxlarge is about 3 times slower because of the larger structure."}
{"doc_id": "1909.11942", "para_id": 54, "text": "Next, we perform ablation experiments that quantify the individual contribution of each of the design\nchoices for ALBERT."}
{"doc_id": "1909.11942", "para_id": 55, "text": "Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base\nconﬁguration setting (see Table 1), using the same set of representative downstream tasks. Under\nthe non-shared condition (BERT-style), larger embedding sizes give better performance, but not by"}
{"doc_id": "1909.11942", "para_id": 56, "text": "Model\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nSpeedup"}
{"doc_id": "1909.11942", "para_id": 57, "text": "BERT\nbase\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3\n4.7x\nlarge\n334M\n92.2/85.5\n85.0/82.2\n86.6\n93.0\n73.9\n85.2\n1.0"}
{"doc_id": "1909.11942", "para_id": 58, "text": "base\n12M\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\n5.6x\nlarge\n18M\n90.6/83.9\n82.3/79.4\n83.5\n91.7\n68.5\n82.4\n1.7x\nxlarge\n60M\n92.5/86.1\n86.1/83.1\n86.4\n92.4\n74.8\n85.5\n0.6x\nxxlarge\n235M\n94.1/88.3\n88.1/85.1\n88.0\n95.2\n82.3\n88.7\n0.3x"}
{"doc_id": "1909.11942", "para_id": 59, "text": "Table 2: Dev set results for models pretrained over BOOKCORPUS and Wikipedia for 125k steps.\nHere and everywhere else, the Avg column is computed by averaging the scores of the downstream\ntasks to its left (the two numbers of F1 and EM for each SQuAD are ﬁrst averaged)."}
{"doc_id": "1909.11942", "para_id": 60, "text": "much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the\nbest. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary\nstep to do further scaling."}
{"doc_id": "1909.11942", "para_id": 61, "text": "Model\nE\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg"}
{"doc_id": "1909.11942", "para_id": 62, "text": "64\n87M\n89.9/82.9\n80.1/77.8\n82.9\n91.5\n66.7\n81.3\n128\n89M\n89.9/82.8\n80.3/77.3\n83.7\n91.5\n67.9\n81.7\n256\n93M\n90.2/83.2\n80.3/77.4\n84.1\n91.9\n67.3\n81.8\n768\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3"}
{"doc_id": "1909.11942", "para_id": 63, "text": "64\n10M\n88.7/81.4\n77.5/74.8\n80.8\n89.4\n63.5\n79.0\n128\n12M\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\n256\n16M\n88.8/81.5\n79.1/76.3\n81.5\n90.3\n63.4\n79.6\n768\n31M\n88.6/81.5\n79.2/76.6\n82.0\n90.6\n63.3\n79.8"}
{"doc_id": "1909.11942", "para_id": 64, "text": "Table 3: The effect of vocabulary embedding size on the performance of ALBERT-base."}
{"doc_id": "1909.11942", "para_id": 65, "text": "Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an\nALBERT-base conﬁguration (Table 1) with two embedding sizes (E = 768 and E = 128). We\ncompare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and inter-\nmediate strategies in which only the attention parameters are shared (but not the FNN ones) or only\nthe FFN parameters are shared (but not the attention ones)."}
{"doc_id": "1909.11942", "para_id": 66, "text": "The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (-\n1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears\nto come from sharing the FFN-layer parameters, while sharing the attention parameters results in no\ndrop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg)."}
{"doc_id": "1909.11942", "para_id": 67, "text": "There are other strategies of sharing the parameters cross layers. For example, We can divide the L\nlayers into N groups of size M, and each size-M group shares parameters. Overall, our experimen-\ntal results shows that the smaller the group size M is, the better the performance we get. However,\ndecreasing group size M also dramatically increase the number of overall parameters. We choose\nall-shared strategy as our default choice."}
{"doc_id": "1909.11942", "para_id": 68, "text": "Model\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg"}
{"doc_id": "1909.11942", "para_id": 69, "text": "all-shared\n31M\n88.6/81.5\n79.2/76.6\n82.0\n90.6\n63.3\n79.8\nshared-attention\n83M\n89.9/82.7\n80.0/77.2\n84.0\n91.4\n67.7\n81.6\nshared-FFN\n57M\n89.2/82.1\n78.2/75.4\n81.5\n90.8\n62.6\n79.5\nnot-shared\n108M\n90.4/83.2\n80.4/77.6\n84.5\n92.8\n68.2\n82.3"}
{"doc_id": "1909.11942", "para_id": 70, "text": "all-shared\n12M\n89.3/82.3\n80.0/77.1\n82.0\n90.3\n64.0\n80.1\nshared-attention\n64M\n89.9/82.8\n80.7/77.9\n83.4\n91.9\n67.6\n81.7\nshared-FFN\n38M\n88.9/81.6\n78.6/75.6\n82.3\n91.7\n64.4\n80.2\nnot-shared\n89M\n89.9/82.8\n80.3/77.3\n83.2\n91.5\n67.9\n81.6"}
{"doc_id": "1909.11942", "para_id": 71, "text": "Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 72, "text": "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none\n(XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERT-\nbase conﬁguration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP,\nand SOP tasks) and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 73, "text": "Intrinsic Tasks\nDownstream Tasks\nSP tasks\nMLM\nNSP\nSOP\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nNone\n54.9\n52.4\n53.3\n88.6/81.5\n78.1/75.3\n81.5\n89.9\n61.7\n79.0\nNSP\n54.5\n90.5\n52.0\n88.4/81.5\n77.2/74.6\n81.6\n91.1\n62.3\n79.2\nSOP\n54.0\n78.9\n86.5\n89.3/82.3\n80.0/77.1\n82.0\n90.3\n64.0\n80.1"}
{"doc_id": "1909.11942", "para_id": 74, "text": "Table 5: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks."}
{"doc_id": "1909.11942", "para_id": 75, "text": "The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP\ntask (52.0% accuracy, similar to the random-guess performance for the “None” condition). This\nallows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does\nsolve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy).\nEven more importantly, the SOP loss appears to consistently improve downstream task performance\nfor multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for\nRACE), for an Avg score improvement of around +1%."}
{"doc_id": "1909.11942", "para_id": 76, "text": "The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher\ncompared to ALBERT-xxlarge. Since longer training usually leads to better performance, we per-\nform a comparison in which, instead of controlling for data throughput (number of training steps),\nwe control for the actual training time (i.e., let the models train for the same number of hours). In\nTable 6, we compare the performance of a BERT-large model after 400k training steps (after 34h\nof training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model\nwith 125k training steps (32h of training)."}
{"doc_id": "1909.11942", "para_id": 77, "text": "Models\nSteps\nTime\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nBERT-large\n400k\n34h\n93.5/87.4\n86.9/84.3\n87.8\n94.6\n77.3\n87.2\nALBERT-xxlarge\n125k\n32h\n94.0/88.1\n88.3/85.3\n87.8\n95.4\n82.5\n88.7"}
{"doc_id": "1909.11942", "para_id": 78, "text": "Table 6: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 79, "text": "After training for roughly the same amount of time, ALBERT-xxlarge is signiﬁcantly better than\nBERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%."}
{"doc_id": "1909.11942", "para_id": 80, "text": "The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in\n(Devlin et al., 2019). In this section, we report measurements on the impact of the additional data\nused by both XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019)."}
{"doc_id": "1909.11942", "para_id": 81, "text": "Fig. 2a plots the dev set MLM accuracy under two conditions, without and with additional data, with\nthe latter condition giving a signiﬁcant boost. We also observe performance improvements on the\ndownstream tasks in Table 7, except for the SQuAD benchmarks (which are Wikipedia-based, and\ntherefore are negatively affected by out-of-domain training material)."}
{"doc_id": "1909.11942", "para_id": 82, "text": "SQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nNo additional data\n89.3/82.3\n80.0/77.1\n81.6\n90.3\n64.0\n80.1\nWith additional data\n88.8/81.7\n79.1/76.3\n82.4\n92.8\n66.0\n80.8"}
{"doc_id": "1909.11942", "para_id": 83, "text": "Table 7: The effect of additional training data using the ALBERT-base conﬁguration."}
{"doc_id": "1909.11942", "para_id": 84, "text": "We also note that, even after training for 1M steps, our largest models still do not overﬁt to their\ntraining data. As a result, we decide to remove dropout to further increase our model capacity. The"}
{"doc_id": "1909.11942", "para_id": 85, "text": "Figure 2: The effects of adding data and removing dropout during training."}
{"doc_id": "1909.11942", "para_id": 86, "text": "plot in Fig. 2b shows that removing dropout signiﬁcantly improves MLM accuracy. Intermediate\nevaluation on ALBERT-xxlarge at around 1M training steps (Table 8) also conﬁrms that removing\ndropout helps the downstream tasks. There is empirical (Szegedy et al., 2017) and theoretical (Li\net al., 2019) evidence showing that a combination of batch normalization and dropout in Convolu-\ntional Neural Networks may have harmful results. To the best of our knowledge, we are the ﬁrst to\nshow that dropout can hurt performance in large Transformer-based models. However, the underly-\ning network structure of ALBERT is a special case of the transformer and further experimentation\nis needed to see if this phenomenon appears with other transformer-based architectures or not."}
{"doc_id": "1909.11942", "para_id": 87, "text": "SQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\nWith dropout\n94.7/89.2\n89.6/86.9\n90.0\n96.3\n85.7\n90.4\nWithout dropout\n94.8/89.5\n89.9/87.2\n90.4\n96.5\n86.1\n90.7"}
{"doc_id": "1909.11942", "para_id": 88, "text": "Table 8: The effect of removing dropout, measured for an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 89, "text": "The results we report in this section make use of the training data used by Devlin et al. (2019), as\nwell as the additional data used by Liu et al. (2019) and Yang et al. (2019). We report state-of-the-art\nresults under two settings for ﬁne-tuning: single-model and ensembles. In both settings, we only do\nsingle-task ﬁne-tuning4. Following Liu et al. (2019), on the development set we report the median\nresult over ﬁve runs."}
{"doc_id": "1909.11942", "para_id": 90, "text": "Models\nMNLI\nQNLI\nQQP\nRTE\nSST\nMRPC\nCoLA\nSTS\nWNLI\nAvg\nSingle-task single models on dev\nBERT-large\n86.6\n92.3\n91.3\n70.4\n93.2\n88.0\n60.6\n90.0\n-\n-\nXLNet-large\n89.8\n93.9\n91.8\n83.8\n95.6\n89.2\n63.6\n91.8\n-\n-\nRoBERTa-large\n90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4\n-\n-\nALBERT (1M)\n90.4\n95.2\n92.0\n88.1\n96.8\n90.2\n68.7\n92.7\n-\n-\nALBERT (1.5M)\n90.8\n95.3\n92.2\n89.2\n96.9\n90.9\n71.4\n93.0\n-\n-\nEnsembles on test (from leaderboard as of Sept. 16, 2019)\nALICE\n88.2\n95.7\n90.7\n83.5\n95.2\n92.6\n69.2\n91.1\n80.8\n87.0\nMT-DNN\n87.9\n96.0\n89.9\n86.3\n96.5\n92.7\n68.4\n91.1\n89.0\n87.6\nXLNet\n90.2\n98.6\n90.3\n86.3\n96.8\n93.0\n67.8\n91.6\n90.4\n88.4\nRoBERTa\n90.8\n98.9\n90.2\n88.2\n96.7\n92.3\n67.8\n92.2\n89.0\n88.5\nAdv-RoBERTa\n91.1\n98.8\n90.3\n88.7\n96.8\n93.1\n68.0\n92.4\n89.0\n88.8\nALBERT\n91.3\n99.2\n90.5\n89.2\n97.1\n93.4\n69.1\n92.5\n91.8\n89.4"}
{"doc_id": "1909.11942", "para_id": 91, "text": "Table 9: State-of-the-art results on the GLUE benchmark. For single-task single-model results, we\nreport ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensemble\nuses models trained with 1M, 1.5M, and other numbers of steps."}
{"doc_id": "1909.11942", "para_id": 92, "text": "The single-model ALBERT conﬁguration incorporates the best-performing settings discussed: an\nALBERT-xxlarge conﬁguration (Table 1) using combined MLM and SOP losses, and no dropout."}
{"doc_id": "1909.11942", "para_id": 93, "text": "4Following Liu et al. (2019), we ﬁne-tune for RTE, STS, and MRPC using an MNLI checkpoint."}
{"doc_id": "1909.11942", "para_id": 94, "text": "The checkpoints that contribute to the ﬁnal ensemble model are selected based on development set\nperformance; the number of checkpoints considered for this selection range from 6 to 17, depending\non the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model\npredictions for the ensemble models, where the candidates are ﬁne-tuned from different training\nsteps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the pre-\ndiction scores for those spans that have multiple probabilities; we also average the scores of the\n“unanswerable” decision."}
{"doc_id": "1909.11942", "para_id": 95, "text": "Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art signif-\nicantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of\n92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement,\na jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over\nXLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang\net al., 2019), an ensemble of multiple models speciﬁcally designed for reading comprehension tasks.\nOur single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art\nensemble model."}
{"doc_id": "1909.11942", "para_id": 96, "text": "Models\nSQuAD1.1 dev\nSQuAD2.0 dev\nSQuAD2.0 test\nRACE test (Middle/High)\nSingle model (from leaderboard as of Sept. 23, 2019)\nBERT-large\n90.9/84.1\n81.8/79.0\n89.1/86.3\n72.0 (76.6/70.1)\nXLNet\n94.5/89.0\n88.8/86.1\n89.1/86.3\n81.8 (85.5/80.2)\nRoBERTa\n94.6/88.9\n89.4/86.5\n89.8/86.8\n83.2 (86.5/81.3)\nUPM\n-\n-\n89.9/87.2\n-\nXLNet + SG-Net Veriﬁer++\n-\n-\n90.1/87.2\n-\nALBERT (1M)\n94.8/89.2\n89.9/87.2\n-\n86.0 (88.2/85.1)\nALBERT (1.5M)\n94.8/89.3\n90.2/87.4\n90.9/88.1\n86.5 (89.0/85.5)\nEnsembles (from leaderboard as of Sept. 23, 2019)\nBERT-large\n92.2/86.2\n-\n-\n-\nXLNet + SG-Net Veriﬁer\n-\n-\n90.7/88.2\n-\nUPM\n-\n-\n90.7/88.2\nXLNet + DAAF + Veriﬁer\n-\n-\n90.9/88.6\n-\nDCMN+\n-\n-\n-\n84.1 (88.5/82.3)\nALBERT\n95.5/90.1\n91.4/88.9\n92.2/89.7\n89.4 (91.2/88.6)"}
{"doc_id": "1909.11942", "para_id": 97, "text": "Table 10: State-of-the-art results on the SQuAD and RACE benchmarks."}
{"doc_id": "1909.11942", "para_id": 98, "text": "While ALBERT-xxlarge has less parameters than BERT-large and gets signiﬁcantly better results, it\nis computationally more expensive due to its larger structure. An important next step is thus to speed\nup the training and inference speed of ALBERT through methods like sparse attention (Child et al.,\n2019) and block attention (Shen et al., 2018). An orthogonal line of research, which could provide\nadditional representation power, includes hard example mining (Mikolov et al., 2013) and more\nefﬁcient language modeling training (Yang et al., 2019). Additionally, although we have convincing\nevidence that sentence order prediction is a more consistently-useful learning task that leads to better\nlanguage representations, we hypothesize that there could be more dimensions not yet captured by\nthe current self-supervised training losses that could create additional representation power for the\nresulting representations."}
{"doc_id": "1909.11942", "para_id": 99, "text": "The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim\nfor discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for\nclarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick,\nEmma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the\npaper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu,\nChenjie Cao and the CLUE community for providing the training data and evaluation benechmark\nof the Chinese version of ALBERT models."}
{"doc_id": "1909.11942", "para_id": 100, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018."}
{"doc_id": "1909.11942", "para_id": 101, "text": "Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information\nProcessing Systems (NeurIPS), 2019."}
{"doc_id": "1909.11942", "para_id": 102, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings of\nthe second PASCAL challenges workshop on recognising textual entailment, volume 6, pp. 6–4.\nVenice, 2006."}
{"doc_id": "1909.11942", "para_id": 103, "text": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth PASCAL recognizing\ntextual entailment challenge. In TAC, 2009."}
{"doc_id": "1909.11942", "para_id": 104, "text": "Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver,\nCanada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.\nURL https://www.aclweb.org/anthology/S17-2001."}
{"doc_id": "1909.11942", "para_id": 105, "text": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016."}
{"doc_id": "1909.11942", "para_id": 106, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019."}
{"doc_id": "1909.11942", "para_id": 107, "text": "Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V\nLe. Bam! born-again multi-task networks for natural language understanding. arXiv preprint\narXiv:1907.04829, 2019."}
{"doc_id": "1909.11942", "para_id": 108, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005."}
{"doc_id": "1909.11942", "para_id": 109, "text": "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor-\nmation processing systems, pp. 3079–3087, 2015."}
{"doc_id": "1909.11942", "para_id": 110, "text": "Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019."}
{"doc_id": "1909.11942", "para_id": 111, "text": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018."}
{"doc_id": "1909.11942", "para_id": 112, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//www.aclweb.org/anthology/N19-1423."}
{"doc_id": "1909.11942", "para_id": 113, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\nhttps://www.aclweb.org/anthology/I05-5002."}
{"doc_id": "1909.11942", "para_id": 114, "text": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin. Learn-\ning generic sentence representations using convolutional neural networks.\nIn Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2390–2400,\nCopenhagen, Denmark, September 2017. Association for Computational Linguistics.\ndoi:\n10.18653/v1/D17-1254. URL https://www.aclweb.org/anthology/D17-1254."}
{"doc_id": "1909.11942", "para_id": 115, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entail-\nment and Paraphrasing, pp. 1–9, Prague, June 2007. Association for Computational Linguistics.\nURL https://www.aclweb.org/anthology/W07-1401."}
{"doc_id": "1909.11942", "para_id": 116, "text": "Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-\nwork: Backpropagation without storing activations. In Advances in neural information processing\nsystems, pp. 2214–2224, 2017."}
{"doc_id": "1909.11942", "para_id": 117, "text": "Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efﬁcient training of bert\nby progressively stacking. In International Conference on Machine Learning, pp. 2337–2346,\n2019."}
{"doc_id": "1909.11942", "para_id": 118, "text": "Edouard Grave, Armand Joulin, Moustapha Ciss´e, Herv´e J´egou, et al. Efﬁcient softmax approxima-\ntion for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pp. 1302–1310. JMLR. org, 2017."}
{"doc_id": "1909.11942", "para_id": 119, "text": "Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Centering: A framework for modeling the\nlocal coherence of discourse. Computational Linguistics, 21(2):203–225, 1995. URL https:\n//www.aclweb.org/anthology/J95-2003."}
{"doc_id": "1909.11942", "para_id": 120, "text": "M.A.K. Halliday and Ruqaiya Hasan. Cohesion in English. Routledge, 1976."}
{"doc_id": "1909.11942", "para_id": 121, "text": "Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling\nrecurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10.\n18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122."}
{"doc_id": "1909.11942", "para_id": 122, "text": "Dan Hendrycks and Kevin Gimpel.\nGaussian Error Linear Units (GELUs).\narXiv preprint\narXiv:1606.08415, 2016."}
{"doc_id": "1909.11942", "para_id": 123, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 1367–1377.\nAssociation for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL http:\n//aclweb.org/anthology/N16-1162."}
{"doc_id": "1909.11942", "para_id": 124, "text": "Jerry R. Hobbs. Coherence and coreference. Cognitive Science, 3(1):67–90, 1979."}
{"doc_id": "1909.11942", "para_id": 125, "text": "Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1909.11942", "para_id": 126, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Csernai.\nFirst quora dataset release:\nQues-\ntion\npairs,\nJanuary\n2017.\nURL\nhttps://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs."}
{"doc_id": "1909.11942", "para_id": 127, "text": "Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsuper-\nvised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017."}
{"doc_id": "1909.11942", "para_id": 128, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans.\narXiv preprint\narXiv:1907.10529, 2019."}
{"doc_id": "1909.11942", "para_id": 129, "text": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Ur-\ntasun, and Sanja Fidler. Skip-thought vectors. In Proceedings of the 28th International Con-\nference on Neural Information Processing Systems - Volume 2, NIPS’15, pp. 3294–3302, Cam-\nbridge, MA, USA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=\n2969442.2969607."}
{"doc_id": "1909.11942", "para_id": 130, "text": "Taku Kudo and John Richardson.\nSentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.\n66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.\ndoi:\n10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012."}
{"doc_id": "1909.11942", "para_id": 131, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding\ncomprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pp. 785–794, Copenhagen, Denmark, September 2017.\nAssociation for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://www.\naclweb.org/anthology/D17-1082."}
{"doc_id": "1909.11942", "para_id": 132, "text": "Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceed-\nings of the 31st ICML, Beijing, China, 2014."}
{"doc_id": "1909.11942", "para_id": 133, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thir-\nteenth International Conference on the Principles of Knowledge Representation and Reasoning,\n2012."}
{"doc_id": "1909.11942", "para_id": 134, "text": "Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout\nand batch normalization by variance shift. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2682–2690, 2019."}
{"doc_id": "1909.11942", "para_id": 135, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-\ntraining approach. arXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "1909.11942", "para_id": 136, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.\nLearned in translation:\nContextualized word vectors. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,\npp. 6294–6305. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\n7209-learned-in-translation-contextualized-word-vectors.pdf."}
{"doc_id": "1909.11942", "para_id": 137, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013."}
{"doc_id": "1909.11942", "para_id": 138, "text": "Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 4497–4510, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/\nP19-1442."}
{"doc_id": "1909.11942", "para_id": 139, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1532–1543, Doha, Qatar, October 2014. Association for Computational\nLinguistics. doi: 10.3115/v1/D14-1162. URL https://www.aclweb.org/anthology/\nD14-1162."}
{"doc_id": "1909.11942", "para_id": 140, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1202.\nURL\nhttps://www.aclweb.org/anthology/N18-1202."}
{"doc_id": "1909.11942", "para_id": 141, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\nImproving language\nunderstanding by generative pre-training. https://s3-us-west-2.amazonaws.com/\nopenai-assets/research-covers/language-unsupervised/language_\nunderstanding_paper.pdf, 2018."}
{"doc_id": "1909.11942", "para_id": 142, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019."}
{"doc_id": "1909.11942", "para_id": 143, "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "1909.11942", "para_id": 144, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association\nfor Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.\norg/anthology/D16-1264."}
{"doc_id": "1909.11942", "para_id": 145, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, July 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb.\norg/anthology/P18-2124."}
{"doc_id": "1909.11942", "para_id": 146, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\nlearning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414–\n10423, 2018."}
{"doc_id": "1909.11942", "para_id": 147, "text": "Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence modeling. arXiv preprint arXiv:1804.00857,\n2018."}
{"doc_id": "1909.11942", "para_id": 148, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training multi-billion parameter language models using model par-\nallelism, 2019."}
{"doc_id": "1909.11942", "para_id": 149, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://www.aclweb.org/anthology/D13-1170."}
{"doc_id": "1909.11942", "para_id": 150, "text": "Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model\ncompression. arXiv preprint arXiv:1908.09355, 2019."}
{"doc_id": "1909.11942", "para_id": 151, "text": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\nInception-v4,\ninception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence, 2017."}
{"doc_id": "1909.11942", "para_id": 152, "text": "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:\nThe impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962,\n2019."}
{"doc_id": "1909.11942", "para_id": 153, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017."}
{"doc_id": "1909.11942", "para_id": 154, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/\nW18-5446."}
{"doc_id": "1909.11942", "para_id": 155, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT: Incor-\nporating language structures into pre-training for deep language understanding. arXiv preprint\narXiv:1908.04577, 2019."}
{"doc_id": "1909.11942", "para_id": 156, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1909.11942", "para_id": 157, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn Proceedings of the 2018 Conference of the North"}
{"doc_id": "1909.11942", "para_id": 158, "text": "American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\norg/anthology/N18-1101."}
{"doc_id": "1909.11942", "para_id": 159, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019."}
{"doc_id": "1909.11942", "para_id": 160, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing\nBERT pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019."}
{"doc_id": "1909.11942", "para_id": 161, "text": "Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou.\nDCMN+: Dual co-matching network for multi-choice reading comprehension. arXiv preprint\narXiv:1908.11511, 2019."}
{"doc_id": "1909.11942", "para_id": 162, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision, pp. 19–27, 2015."}
{"doc_id": "1909.11942", "para_id": 163, "text": "In this section, we check how depth (number of layers) and width (hidden size) affect the perfor-\nmance of ALBERT. Table 11 shows the performance of an ALBERT-large conﬁguration (see Ta-\nble 1) using different numbers of layers. Networks with 3 or more layers are trained by ﬁne-tuning\nusing the parameters from the depth before (e.g., the 12-layer network parameters are ﬁne-tuned\nfrom the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong\net al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although\nthey have the same number of parameters, the performance increases signiﬁcantly. However, there\nare diminishing returns when continuing to increase the number of layers: the results of a 12-layer\nnetwork are relatively close to the results of a 24-layer network, and the performance of a 48-layer\nnetwork appears to decline."}
{"doc_id": "1909.11942", "para_id": 164, "text": "Number of layers\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n1\n18M\n31.1/22.9\n50.1/50.1\n66.4\n80.8\n40.1\n52.9\n3\n18M\n79.8/69.7\n64.4/61.7\n77.7\n86.7\n54.0\n71.2\n6\n18M\n86.4/78.4\n73.8/71.1\n81.2\n88.9\n60.9\n77.2\n12\n18M\n89.8/83.3\n80.7/77.9\n83.3\n91.7\n66.7\n81.5\n24\n18M\n90.3/83.3\n81.8/79.0\n83.3\n91.5\n68.7\n82.1\n48\n18M\n90.0/83.1\n81.8/78.9\n83.4\n91.9\n66.9\n81.8"}
{"doc_id": "1909.11942", "para_id": 165, "text": "Table 11: The effect of increasing the number of layers for an ALBERT-large conﬁguration."}
{"doc_id": "1909.11942", "para_id": 166, "text": "A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large\nconﬁguration. As we increase the hidden size, we get an increase in performance with diminishing\nreturns. At a hidden size of 6144, the performance appears to decline signiﬁcantly. We note that none\nof these models appear to overﬁt the training data, and they all have higher training and development\nloss compared to the best-performing ALBERT conﬁgurations."}
{"doc_id": "1909.11942", "para_id": 167, "text": "5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this\nwarm-start technique does not help to improve the downstream performance. However, it does help the 48-layer\nnetwork to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from\na 6-layer network."}
{"doc_id": "1909.11942", "para_id": 168, "text": "Hidden size\nParameters\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n1024\n18M\n79.8/69.7\n64.4/61.7\n77.7\n86.7\n54.0\n71.2\n2048\n60M\n83.3/74.1\n69.1/66.6\n79.7\n88.6\n58.2\n74.6\n4096\n225M\n85.0/76.4\n71.0/68.1\n80.3\n90.4\n60.4\n76.3\n6144\n499M\n84.7/75.8\n67.8/65.4\n78.1\n89.1\n56.0\n74.0"}
{"doc_id": "1909.11942", "para_id": 169, "text": "Table 12: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 170, "text": "A.2\nDO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO?"}
{"doc_id": "1909.11942", "para_id": 171, "text": "In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a\n24-layer conﬁguration is small. Does this result still hold for much wider ALBERT conﬁgurations,\nsuch as ALBERT-xxlarge (H=4096)?"}
{"doc_id": "1909.11942", "para_id": 172, "text": "Number of layers\nSQuAD1.1\nSQuAD2.0\nMNLI\nSST-2\nRACE\nAvg\n12\n94.0/88.1\n88.3/85.3\n87.8\n95.4\n82.5\n88.7\n24\n94.1/88.3\n88.1/85.1\n88.0\n95.2\n82.3\n88.7"}
{"doc_id": "1909.11942", "para_id": 173, "text": "Table 13: The effect of a deeper network using an ALBERT-xxlarge conﬁguration."}
{"doc_id": "1909.11942", "para_id": 174, "text": "The answer is given by the results from Table 13. The difference between 12-layer and 24-layer\nALBERT-xxlarge conﬁgurations in terms of downstream accuracy is negligible, with the Avg score\nbeing the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there\nis no need for models deeper than a 12-layer conﬁguration."}
{"doc_id": "1909.11942", "para_id": 175, "text": "GLUE\nGLUE\nis\ncomprised\nof\n9\ntasks,\nnamely\nCorpus\nof\nLinguistic\nAcceptability\n(CoLA; Warstadt et al., 2018), Stanford Sentiment Treebank (SST; Socher et al., 2013), Microsoft\nResearch Paraphrase Corpus (MRPC; Dolan & Brockett, 2005), Semantic Textual Similarity Bench-\nmark (STS; Cer et al., 2017), Quora Question Pairs (QQP; Iyer et al., 2017), Multi-Genre NLI\n(MNLI; Williams et al., 2018), Question NLI (QNLI; Rajpurkar et al., 2016), Recognizing Textual\nEntailment (RTE; Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli\net al., 2009) and Winograd NLI (WNLI; Levesque et al., 2012). It focuses on evaluating model\ncapabilities for natural language understanding. When reporting MNLI results, we only report the\n“match” condition (MNLI-m). We follow the ﬁnetuning procedures from prior work (Devlin et al.,\n2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from\nGLUE submissions. For test set submissions, we perform task-speciﬁc modiﬁcations for WNLI and\nQNLI as described by Liu et al. (2019) and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 176, "text": "SQuAD\nSQuAD is an extractive question answering dataset built from Wikipedia. The answers\nare segments from the context paragraphs and the task is to predict answer spans. We evaluate our\nmodels on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated\nquestion/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For\nSQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are\njointly trained with a span extraction loss and an additional classiﬁer for predicting answerabil-\nity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance."}
{"doc_id": "1909.11942", "para_id": 177, "text": "RACE\nRACE is a large-scale dataset for multi-choice reading comprehension, collected from En-\nglish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate\nanswers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the\npassage, question, and each candidate answer as the input to models. Then, we use the represen-\ntations from the “[CLS]” token for predicting the probability of each answer. The dataset consists\nof two domains: middle school and high school. We train our models on both domains and report\naccuracies on both the development set and test set."}
{"doc_id": "1909.11942", "para_id": 178, "text": "Hyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters\nfrom Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019)."}
{"doc_id": "1909.11942", "para_id": 179, "text": "LR\nBSZ\nALBERT DR\nClassiﬁer DR\nTS\nWS\nMSL\nCoLA\n1.00E-05\n16\n0\n0.1\n5336\n320\n512\nSTS\n2.00E-05\n16\n0\n0.1\n3598\n214\n512\nSST-2\n1.00E-05\n32\n0\n0.1\n20935\n1256\n512\nMNLI\n3.00E-05\n128\n0\n0.1\n10000\n1000\n512\nQNLI\n1.00E-05\n32\n0\n0.1\n33112\n1986\n512\nQQP\n5.00E-05\n128\n0.1\n0.1\n14000\n1000\n512\nRTE\n3.00E-05\n32\n0.1\n0.1\n800\n200\n512\nMRPC\n2.00E-05\n32\n0\n0.1\n800\n200\n512\nWNLI\n2.00E-05\n16\n0.1\n0.1\n2000\n250\n512\nSQuAD v1.1\n5.00E-05\n48\n0\n0.1\n3649\n365\n384\nSQuAD v2.0\n3.00E-05\n48\n0\n0.1\n8144\n814\n512\nRACE\n2.00E-05\n32\n0.1\n0.1\n12000\n1000\n512"}
{"doc_id": "1909.11942", "para_id": 180, "text": "Table 14: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch\nSize. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence\nLength."}
{"doc_id": "1907.11692", "para_id": 0, "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}
{"doc_id": "1907.11692", "para_id": 1, "text": "Yinhan Liu∗§\nMyle Ott∗§\nNaman Goyal∗§\nJingfei Du∗§\nMandar Joshi†"}
{"doc_id": "1907.11692", "para_id": 2, "text": "Danqi Chen§\nOmer Levy§\nMike Lewis§\nLuke Zettlemoyer†§\nVeselin Stoyanov§"}
{"doc_id": "1907.11692", "para_id": 3, "text": "† Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz}@cs.washington.edu"}
{"doc_id": "1907.11692", "para_id": 4, "text": "§ Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves}@fb.com"}
{"doc_id": "1907.11692", "para_id": 5, "text": "We present a replication study of BERT pre-\ntraining (Devlin et al., 2019), which includes a\ncareful evaluation of the effects of hyperparmeter\ntuning and training set size. We ﬁnd that BERT\nwas signiﬁcantly undertrained and propose an im-\nproved recipe for training BERT models, which\nwe call RoBERTa, that can match or exceed the\nperformance of all of the post-BERT methods.\nOur modiﬁcations are simple, they include: (1)\ntraining the model longer, with bigger batches,\nover more data; (2) removing the next sentence\nprediction objective; (3) training on longer se-\nquences; and (4) dynamically changing the mask-\ning pattern applied to the training data. We also\ncollect a large new dataset (CC-NEWS) of compa-\nrable size to other privately used datasets, to better\ncontrol for training set size effects.\nWhen controlling for training data, our im-\nproved training procedure improves upon the pub-\nlished BERT results on both GLUE and SQuAD.\nWhen trained for longer over additional data, our\nmodel achieves a score of 88.5 on the public\nGLUE leaderboard, matching the 88.4 reported\nby Yang et al. (2019).\nOur model establishes a\nnew state-of-the-art on 4/9 of the GLUE tasks:\nMNLI, QNLI, RTE and STS-B. We also match\nstate-of-the-art results on SQuAD and RACE.\nOverall, we re-establish that BERT’s masked lan-\nguage model training objective is competitive\nwith other recently proposed training objectives\nsuch as perturbed autoregressive language model-\ning (Yang et al., 2019).2"}
{"doc_id": "1907.11692", "para_id": 6, "text": "Language model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining (Devlin et al., 2019) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it.\nOur best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1"}
{"doc_id": "1907.11692", "para_id": 7, "text": "Self-training methods such as ELMo (Peters et al.,\n2018),\nGPT\n(Radford et al.,\n2018),\nBERT\n(Devlin et al., 2019), XLM (Lample and Conneau,\n2019),\nand\nXLNet (Yang et al.,\n2019)\nhave\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most.\nTraining is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances."}
{"doc_id": "1907.11692", "para_id": 8, "text": "In summary, the contributions of this paper\nare: (1) We present a set of important BERT de-\nsign choices and training strategies and introduce"}
{"doc_id": "1907.11692", "para_id": 9, "text": "∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseq"}
{"doc_id": "1907.11692", "para_id": 10, "text": "2It is possible that these other methods could also improve\nwith more tuning. We leave this exploration to future work."}
{"doc_id": "1907.11692", "para_id": 11, "text": "alternatives that lead to better downstream task\nperformance; (2) We use a novel dataset, CC-\nNEWS, and conﬁrm that using more data for pre-\ntraining further improves performance on down-\nstream tasks; (3) Our training improvements show\nthat masked language model pretraining, under\nthe right design choices, is competitive with all\nother recently published methods. We release our\nmodel, pretraining and ﬁne-tuning code imple-\nmented in PyTorch (Paszke et al., 2017)."}
{"doc_id": "1907.11692", "para_id": 12, "text": "and 10% are replaced by a randomly selected vo-\ncabulary token.\nIn the original implementation, random mask-\ning and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1)."}
{"doc_id": "1907.11692", "para_id": 13, "text": "Next Sentence Prediction (NSP)\nNSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference (Bowman et al., 2015), which\nrequire reasoning about the relationships between\npairs of sentences."}
{"doc_id": "1907.11692", "para_id": 14, "text": "In this section, we give a brief overview of the\nBERT (Devlin et al., 2019) pretraining approach\nand some of the training choices that we will ex-\namine experimentally in the following section."}
{"doc_id": "1907.11692", "para_id": 15, "text": "BERT takes as input a concatenation of two\nsegments\n(sequences\nof\ntokens),\nx1, . . . , xN\nand y1, . . . , yM.\nSegments usually consist of\nmore than one natural sentence.\nThe two seg-\nments are presented as a single input sequence\nto BERT with special tokens delimiting them:\n[CLS], x1, . . . , xN, [SEP], y1, . . . , yM, [EOS].\nM and N are constrained such that M + N < T,\nwhere T is a parameter that controls the maximum\nsequence length during training.\nThe model is ﬁrst pretrained on a large unla-\nbeled text corpus and subsequently ﬁnetuned us-\ning end-task labeled data."}
{"doc_id": "1907.11692", "para_id": 16, "text": "BERT is optimized with Adam (Kingma and Ba,\n2015) using the following parameters: β1 = 0.9,\nβ2\n=\n0.999, ǫ\n=\n1e-6 and L2 weight de-\ncay of 0.01.\nThe learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed.\nBERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion (Hendrycks and Gimpel, 2016). Models are\npretrained for S = 1,000,000 updates, with mini-\nbatches containing B = 256 sequences of maxi-\nmum length T = 512 tokens."}
{"doc_id": "1907.11692", "para_id": 17, "text": "BERT uses the now ubiquitous transformer archi-\ntecture (Vaswani et al., 2017), which we will not\nreview in detail. We use a transformer architecture\nwith L layers. Each block uses A self-attention\nheads and hidden dimension H."}
{"doc_id": "1907.11692", "para_id": 18, "text": "PUS (Zhu et al., 2015) plus English WIKIPEDIA,\nwhich totals 16GB of uncompressed text.3"}
{"doc_id": "1907.11692", "para_id": 19, "text": "During pretraining, BERT uses two objectives:\nmasked language modeling and next sentence pre-\ndiction."}
{"doc_id": "1907.11692", "para_id": 20, "text": "In this section, we describe the experimental setup\nfor our replication study of BERT."}
{"doc_id": "1907.11692", "para_id": 21, "text": "Masked Language Model (MLM)\nA random\nsample of the tokens in the input sequence is\nselected and replaced with the special token\n[MASK]. The MLM objective is a cross-entropy\nloss on predicting the masked tokens. BERT uni-\nformly selects 15% of the input tokens for possi-\nble replacement. Of the selected tokens, 80% are\nreplaced with [MASK], 10% are left unchanged,"}
{"doc_id": "1907.11692", "para_id": 22, "text": "We reimplement BERT in FAIRSEQ (Ott et al.,\n2019).\nWe primarily follow the original BERT"}
{"doc_id": "1907.11692", "para_id": 23, "text": "3Yang et al. (2019) use the same dataset but report having\nonly 13GB of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data."}
{"doc_id": "1907.11692", "para_id": 24, "text": "pus described in Radford et al. (2019). The text\nis web content extracted from URLs shared on\nReddit with at least three upvotes. (38GB).5"}
{"doc_id": "1907.11692", "para_id": 25, "text": "optimization hyperparameters, given in Section 2,\nexcept for the peak learning rate and number of\nwarmup steps, which are tuned separately for each\nsetting. We additionally found training to be very\nsensitive to the Adam epsilon term, and in some\ncases we obtained better performance or improved\nstability after tuning it. Similarly, we found setting\nβ2 = 0.98 to improve stability when training with\nlarge batch sizes.\nWe pretrain with sequences of at most T = 512\ntokens. Unlike Devlin et al. (2019), we do not ran-\ndomly inject short sequences, and we do not train\nwith a reduced sequence length for the ﬁrst 90% of\nupdates. We train only with full-length sequences.\nWe train with mixed precision ﬂoating point\narithmetic on DGX-1 machines, each with 8 ×\n32GB Nvidia V100 GPUs interconnected by In-\nﬁniband (Micikevicius et al., 2018)."}
{"doc_id": "1907.11692", "para_id": 26, "text": "(2018) containing a subset of CommonCrawl\ndata ﬁltered to match the story-like style of\nWinograd schemas. (31GB)."}
{"doc_id": "1907.11692", "para_id": 27, "text": "Following previous work, we evaluate our pre-\ntrained models on downstream tasks using the fol-\nlowing three benchmarks."}
{"doc_id": "1907.11692", "para_id": 28, "text": "GLUE\nThe\nGeneral\nLanguage\nUnderstand-\ning Evaluation (GLUE) benchmark (Wang et al.,\n2019b) is a collection of 9 datasets for evaluating\nnatural language understanding systems.6 Tasks\nare framed as either single-sentence classiﬁcation\nor sentence-pair classiﬁcation tasks. The GLUE\norganizers provide training and development data\nsplits as well as a submission server and leader-\nboard that allows participants to evaluate and com-\npare their systems on private held-out test data.\nFor the replication study in Section 4, we report\nresults on the development sets after ﬁnetuning\nthe pretrained models on the corresponding single-\ntask training data (i.e., without multi-task training\nor ensembling). Our ﬁnetuning procedure follows\nthe original BERT paper (Devlin et al., 2019).\nIn Section 5 we additionally report test set re-\nsults obtained from the public leaderboard. These\nresults depend on a several task-speciﬁc modiﬁca-\ntions, which we describe in Section 5.1."}
{"doc_id": "1907.11692", "para_id": 29, "text": "BERT-style pretraining crucially relies on large\nquantities of text.\nBaevski et al. (2019) demon-\nstrate that increasing data size can result in im-\nproved end-task performance.\nSeveral efforts\nhave trained on datasets larger and more diverse\nthan the original BERT (Radford et al., 2019;\nYang et al., 2019; Zellers et al., 2019). Unfortu-\nnately, not all of the additional datasets can be\npublicly released. For our study, we focus on gath-\nering as much data as possible for experimenta-\ntion, allowing us to match the overall quality and\nquantity of data as appropriate for each compari-\nson.\nWe consider ﬁve English-language corpora of\nvarying sizes and domains, totaling over 160GB\nof uncompressed text. We use the following text\ncorpora:"}
{"doc_id": "1907.11692", "para_id": 30, "text": "SQuAD\nThe\nStanford\nQuestion\nAnswering\nDataset (SQuAD) provides a paragraph of context\nand a question. The task is to answer the question\nby extracting the relevant span from the context.\nWe evaluate on two versions of SQuAD: V1.1\nand V2.0 (Rajpurkar et al., 2016, 2018). In V1.1\nthe context always contains an answer, whereas in"}
{"doc_id": "1907.11692", "para_id": 31, "text": "• BOOKCORPUS (Zhu et al., 2015) plus English\nWIKIPEDIA. This is the original data used to\ntrain BERT. (16GB)."}
{"doc_id": "1907.11692", "para_id": 32, "text": "• CC-NEWS, which we collected from the En-\nglish portion of the CommonCrawl News\ndataset (Nagel, 2016).\nThe data contains 63\nmillion English news articles crawled between\nSeptember 2016 and February 2019. (76GB af-\nter ﬁltering).4"}
{"doc_id": "1907.11692", "para_id": 33, "text": "5The authors and their afﬁliated institutions are not in any\nway afﬁliated with the creation of the OpenWebText dataset.\n6The\ndatasets\nare:\nCoLA\n(Warstadt et al.,\n2018),\nStanford\nSentiment\nTreebank\n(SST)\n(Socher et al.,\n2013),\nMicrosoft\nResearch\nParagraph\nCorpus\n(MRPC)\n(Dolan and Brockett,\n2005),\nSemantic\nTex-\ntual Similarity Benchmark (STS) (Agirre et al., 2007),\nQuora Question Pairs (QQP) (Iyer et al., 2016), Multi-\nGenre NLI (MNLI) (Williams et al., 2018), Question NLI\n(QNLI)\n(Rajpurkar et al.,\n2016),\nRecognizing\nTextual\nEntailment\n(RTE)\n(Dagan et al.,\n2006;\nBar-Haim et al.,\n2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and\nWinograd NLI (WNLI) (Levesque et al., 2011)."}
{"doc_id": "1907.11692", "para_id": 34, "text": "• OPENWEBTEXT (Gokaslan and Cohen, 2019),\nan open-source recreation of the WebText cor-"}
{"doc_id": "1907.11692", "para_id": 35, "text": "4We use news-please (Hamborg et al., 2017) to col-\nlect and extract CC-NEWS. CC-NEWS is similar to the RE-\nALNEWS dataset described in Zellers et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 36, "text": "V2.0 some questions are not answered in the pro-\nvided context, making the task more challenging.\nFor SQuAD V1.1 we adopt the same span pre-\ndiction method as BERT (Devlin et al., 2019). For\nSQuAD V2.0, we add an additional binary classi-\nﬁer to predict whether the question is answerable,\nwhich we train jointly by summing the classiﬁca-\ntion and span loss terms. During evaluation, we\nonly predict span indices on pairs that are classi-\nﬁed as answerable."}
{"doc_id": "1907.11692", "para_id": 37, "text": "Our reimplementation:\nstatic\n78.3\n84.3\n92.5\ndynamic\n78.7\n84.0\n92.9"}
{"doc_id": "1907.11692", "para_id": 38, "text": "Table 1:\nComparison between static and dynamic\nmasking for BERTBASE. We report F1 for SQuAD and\naccuracy for MNLI-m and SST-2. Reported results are\nmedians over 5 random initializations (seeds). Refer-\nence results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 39, "text": "RACE\nThe ReAding Comprehension from Ex-\naminations (RACE) (Lai et al., 2017) task is a\nlarge-scale reading comprehension dataset with\nmore than 28,000 passages and nearly 100,000\nquestions. The dataset is collected from English\nexaminations in China, which are designed for\nmiddle and high school students. In RACE, each\npassage is associated with multiple questions. For\nevery question, the task is to select one correct an-\nswer from four options. RACE has signiﬁcantly\nlonger context than other popular reading compre-\nhension datasets and the proportion of questions\nthat requires reasoning is very large."}
{"doc_id": "1907.11692", "para_id": 40, "text": "Results\nTable\n1\ncompares\nthe\npublished\nBERTBASE results from Devlin et al. (2019) to our\nreimplementation with either static or dynamic\nmasking.\nWe ﬁnd that our reimplementation\nwith static masking performs similar to the\noriginal BERT model, and dynamic masking is\ncomparable or slightly better than static masking.\nGiven these results and the additional efﬁciency\nbeneﬁts of dynamic masking, we use dynamic\nmasking in the remainder of the experiments."}
{"doc_id": "1907.11692", "para_id": 41, "text": "4.2\nModel Input Format and Next Sentence\nPrediction"}
{"doc_id": "1907.11692", "para_id": 42, "text": "This section explores and quantiﬁes which choices\nare important for successfully pretraining BERT\nmodels. We keep the model architecture ﬁxed.7"}
{"doc_id": "1907.11692", "para_id": 43, "text": "In the original BERT pretraining procedure, the\nmodel observes two concatenated document seg-\nments, which are either sampled contiguously\nfrom the same document (with p = 0.5) or from\ndistinct documents. In addition to the masked lan-\nguage modeling objective, the model is trained to\npredict whether the observed document segments\ncome from the same or distinct documents via an\nauxiliary Next Sentence Prediction (NSP) loss.\nThe NSP loss was hypothesized to be an impor-\ntant factor in training the original BERT model.\nDevlin et al. (2019) observe that removing NSP\nhurts performance, with signiﬁcant performance\ndegradation on QNLI, MNLI, and SQuAD 1.1.\nHowever, some recent work has questioned the\nnecessity of the NSP loss (Lample and Conneau,\n2019; Yang et al., 2019; Joshi et al., 2019).\nTo better understand this discrepancy, we com-\npare several alternative training formats:"}
{"doc_id": "1907.11692", "para_id": 44, "text": "Speciﬁcally, we begin by training BERT models\nwith the same conﬁguration as BERTBASE (L =\n12, H = 768, A = 12, 110M params)."}
{"doc_id": "1907.11692", "para_id": 45, "text": "As discussed in Section 2, BERT relies on ran-\ndomly masking and predicting tokens. The orig-\ninal BERT implementation performed masking\nonce during data preprocessing, resulting in a sin-\ngle static mask. To avoid using the same mask for\neach training instance in every epoch, training data\nwas duplicated 10 times so that each sequence is\nmasked in 10 different ways over the 40 epochs of\ntraining. Thus, each training sequence was seen\nwith the same mask four times during training.\nWe compare this strategy with dynamic mask-\ning where we generate the masking pattern every\ntime we feed a sequence to the model. This be-\ncomes crucial when pretraining for more steps or\nwith larger datasets."}
{"doc_id": "1907.11692", "para_id": 46, "text": "• SEGMENT-PAIR+NSP: This follows the original\ninput format used in BERT (Devlin et al., 2019),\nwith the NSP loss. Each input has a pair of seg-\nments, which can each contain multiple natural\nsentences, but the total combined length must\nbe less than 512 tokens."}
{"doc_id": "1907.11692", "para_id": 47, "text": "7Studying architectural changes, including larger archi-\ntectures, is an important area for future work."}
{"doc_id": "1907.11692", "para_id": 48, "text": "BERTBASE\n88.5/76.3\n84.3\n92.8\n64.3\nXLNetBASE (K = 7)\n–/81.3\n85.8\n92.7\n66.1\nXLNetBASE (K = 6)\n–/81.0\n85.6\n93.4\n66.7"}
{"doc_id": "1907.11692", "para_id": 49, "text": "Table 2: Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are\ntrained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m,\nSST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERTBASE and\nXLNetBASE are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 50, "text": "• SENTENCE-PAIR+NSP: Each input contains a\npair of natural sentences, either sampled from\na contiguous portion of one document or from\nseparate documents. Since these inputs are sig-\nniﬁcantly shorter than 512 tokens, we increase\nthe batch size so that the total number of tokens\nremains similar to SEGMENT-PAIR+NSP. We re-\ntain the NSP loss."}
{"doc_id": "1907.11692", "para_id": 51, "text": "We next compare training without the NSP\nloss and training with blocks of text from a sin-\ngle document (DOC-SENTENCES).\nWe ﬁnd that\nthis setting outperforms the originally published\nBERTBASE results and that removing the NSP loss\nmatches or slightly improves downstream task\nperformance, in contrast to Devlin et al. (2019).\nIt is possible that the original BERT implementa-\ntion may only have removed the loss term while\nstill retaining the SEGMENT-PAIR input format.\nFinally we ﬁnd that restricting sequences to\ncome from a single document (DOC-SENTENCES)\nperforms slightly better than packing sequences\nfrom multiple documents (FULL-SENTENCES).\nHowever, because the DOC-SENTENCES format\nresults in variable batch sizes, we use FULL-"}
{"doc_id": "1907.11692", "para_id": 52, "text": "• FULL-SENTENCES: Each input is packed with\nfull sentences sampled contiguously from one\nor more documents, such that the total length is\nat most 512 tokens. Inputs may cross document\nboundaries. When we reach the end of one doc-\nument, we begin sampling sentences from the\nnext document and add an extra separator token\nbetween documents. We remove the NSP loss."}
{"doc_id": "1907.11692", "para_id": 53, "text": "SENTENCES in the remainder of our experiments\nfor easier comparison with related work."}
{"doc_id": "1907.11692", "para_id": 54, "text": "• DOC-SENTENCES: Inputs are constructed sim-\nilarly to FULL-SENTENCES, except that they\nmay not cross document boundaries.\nInputs\nsampled near the end of a document may be\nshorter than 512 tokens, so we dynamically in-\ncrease the batch size in these cases to achieve\na similar number of total tokens as FULL-"}
{"doc_id": "1907.11692", "para_id": 55, "text": "Past work in Neural Machine Translation has\nshown that training with very large mini-batches\ncan both improve optimization speed and end-task\nperformance when the learning rate is increased\nappropriately (Ott et al., 2018). Recent work has\nshown that BERT is also amenable to large batch\ntraining (You et al., 2019)."}
{"doc_id": "1907.11692", "para_id": 56, "text": "Results\nTable 2 shows results for the four dif-\nferent settings.\nWe ﬁrst compare the original"}
{"doc_id": "1907.11692", "para_id": 57, "text": "Devlin et al.\n(2019)\noriginally\ntrained\nBERTBASE for 1M steps with a batch size of\n256 sequences.\nThis is equivalent in computa-\ntional cost, via gradient accumulation, to training\nfor 125K steps with a batch size of 2K sequences,\nor for 31K steps with a batch size of 8K.\nIn Table 3 we compare perplexity and end-"}
{"doc_id": "1907.11692", "para_id": 58, "text": "SEGMENT-PAIR input format from Devlin et al.\n(2019) to the SENTENCE-PAIR format; both for-\nmats retain the NSP loss, but the latter uses sin-\ngle sentences.\nWe ﬁnd that using individual\nsentences hurts performance on downstream\ntasks, which we hypothesize is because the model\nis not able to learn long-range dependencies."}
{"doc_id": "1907.11692", "para_id": 59, "text": "The\noriginal\nBERT\nimplementa-\ntion (Devlin et al., 2019) uses a character-level\nBPE vocabulary of size 30K, which is learned\nafter preprocessing the input with heuristic tok-\nenization rules. Following Radford et al. (2019),\nwe instead consider training BERT with a larger\nbyte-level BPE vocabulary containing 50K sub-\nword units, without any additional preprocessing\nor tokenization of the input. This adds approxi-\nmately 15M and 20M additional parameters for\nBERTBASE and BERTLARGE, respectively.\nEarly experiments revealed only slight dif-\nferences between these encodings,\nwith the\nRadford et al. (2019)\nBPE achieving\nslightly\nworse end-task performance on some tasks. Nev-\nertheless, we believe the advantages of a univer-\nsal encoding scheme outweighs the minor degre-\ndation in performance and use this encoding in\nthe remainder of our experiments.\nA more de-\ntailed comparison of these encodings is left to fu-\nture work."}
{"doc_id": "1907.11692", "para_id": 60, "text": "256\n1M\n1e-4\n3.99\n84.7\n92.7\n2K\n125K\n7e-4\n3.68\n85.2\n92.9\n8K\n31K\n1e-3\n3.77\n84.6\n92.8"}
{"doc_id": "1907.11692", "para_id": 61, "text": "Table 3: Perplexity on held-out training data (ppl) and\ndevelopment set accuracy for base models trained over\nBOOKCORPUS and WIKIPEDIA with varying batch\nsizes (bsz). We tune the learning rate (lr) for each set-\nting. Models make the same number of passes over the\ndata (epochs) and have the same computational cost."}
{"doc_id": "1907.11692", "para_id": 62, "text": "task performance of BERTBASE as we increase the\nbatch size, controlling for the number of passes\nthrough the training data. We observe that train-\ning with large batches improves perplexity for the\nmasked language modeling objective, as well as\nend-task accuracy. Large batches are also easier to\nparallelize via distributed data parallel training,8"}
{"doc_id": "1907.11692", "para_id": 63, "text": "and in later experiments we train with batches of\n8K sequences.\nNotably You et al. (2019) train BERT with even\nlarger batche sizes, up to 32K sequences. We leave\nfurther exploration of the limits of large batch\ntraining to future work."}
{"doc_id": "1907.11692", "para_id": 64, "text": "In the previous section we propose modiﬁcations\nto the BERT pretraining procedure that improve\nend-task performance.\nWe now aggregate these\nimprovements and evaluate their combined im-\npact.\nWe call this conﬁguration RoBERTa for\nRobustly optimized BERT approach.\nSpeciﬁ-\ncally, RoBERTa is trained with dynamic mask-\ning (Section 4.1), FULL-SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture (Yang et al., 2019) is pretrained us-\ning nearly 10 times more data than the original\nBERT (Devlin et al., 2019). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERTLARGE architecture (L = 24,\nH = 1024, A = 16, 355M parameters).\nWe\npretrain for 100K steps over a comparable BOOK-\nCORPUS plus WIKIPEDIA dataset as was used in"}
{"doc_id": "1907.11692", "para_id": 65, "text": "Byte-Pair Encoding (BPE) (Sennrich et al., 2016)\nis a hybrid between character- and word-level rep-\nresentations that allows handling the large vocab-\nularies common in natural language corpora. In-\nstead of full words, BPE relies on subwords units,\nwhich are extracted by performing statistical anal-\nysis of the training corpus.\nBPE vocabulary sizes typically range from\n10K-100K subword units. However, unicode char-\nacters can account for a sizeable portion of this\nvocabulary when modeling large and diverse cor-\npora, such as the ones considered in this work.\nRadford et al. (2019) introduce a clever imple-\nmentation of BPE that uses bytes instead of uni-\ncode characters as the base subword units. Using\nbytes makes it possible to learn a subword vocab-\nulary of a modest size (50K units) that can still en-\ncode any input text without introducing any “un-\nknown” tokens."}
{"doc_id": "1907.11692", "para_id": 66, "text": "8Large batch training can improve training efﬁciency even\nwithout large scale parallel hardware through gradient ac-\ncumulation, whereby gradients from multiple mini-batches\nare accumulated locally before each optimization step. This\nfunctionality is supported natively in FAIRSEQ (Ott et al.,\n2019)."}
{"doc_id": "1907.11692", "para_id": 67, "text": "Model\ndata\nbsz\nsteps\nSQuAD\nMNLI-m\nSST-2\n(v1.1/2.0)"}
{"doc_id": "1907.11692", "para_id": 68, "text": "RoBERTa\nwith BOOKS + WIKI\n16GB\n8K\n100K\n93.6/87.3\n89.0\n95.3\n+ additional data (§3.2)\n160GB\n8K\n100K\n94.0/87.7\n89.3\n95.6\n+ pretrain longer\n160GB\n8K\n300K\n94.4/88.7\n90.0\n96.1\n+ pretrain even longer\n160GB\n8K\n500K\n94.6/89.4\n90.2\n96.4"}
{"doc_id": "1907.11692", "para_id": 69, "text": "BERTLARGE\nwith BOOKS + WIKI\n13GB\n256\n1M\n90.9/81.8\n86.6\n93.7\nXLNetLARGE\nwith BOOKS + WIKI\n13GB\n256\n1M\n94.0/87.8\n88.4\n94.4\n+ additional data\n126GB\n2K\n500K\n94.5/88.8\n89.8\n95.6"}
{"doc_id": "1907.11692", "para_id": 70, "text": "Table 4: Development set results for RoBERTa as we pretrain over more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K →500K steps). Each row accumulates improvements from the rows above. RoBERTa\nmatches the architecture and training objective of BERTLARGE. Results for BERTLARGE and XLNetLARGE are from\nDevlin et al. (2019) and Yang et al. (2019), respectively. Complete results on all GLUE tasks can be found in the\nAppendix."}
{"doc_id": "1907.11692", "para_id": 71, "text": "we consider RoBERTa trained for 500K steps over\nall ﬁve of the datasets introduced in Section 3.2."}
{"doc_id": "1907.11692", "para_id": 72, "text": "Devlin et al. (2019). We pretrain our model using\n1024 V100 GPUs for approximately one day."}
{"doc_id": "1907.11692", "para_id": 73, "text": "Results\nWe present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERTLARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2.\nWe\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9"}
{"doc_id": "1907.11692", "para_id": 74, "text": "For GLUE we consider two ﬁnetuning settings.\nIn the ﬁrst setting (single-task, dev) we ﬁnetune\nRoBERTa separately for each of the GLUE tasks,\nusing only the training data for the correspond-\ning task. We consider a limited hyperparameter\nsweep for each task, with batch sizes ∈{16, 32}\nand learning rates ∈{1e−5, 2e−5, 3e−5}, with a\nlinear warmup for the ﬁrst 6% of steps followed by\na linear decay to 0. We ﬁnetune for 10 epochs and\nperform early stopping based on each task’s eval-\nuation metric on the dev set. The rest of the hyper-\nparameters remain the same as during pretraining.\nIn this setting, we report the median development\nset results for each task over ﬁve random initial-\nizations, without model ensembling."}
{"doc_id": "1907.11692", "para_id": 75, "text": "Finally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNetLARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally"}
{"doc_id": "1907.11692", "para_id": 76, "text": "In the second setting (ensembles, test), we com-\npare RoBERTa to other approaches on the test set\nvia the GLUE leaderboard. While many submis-\nsions to the GLUE leaderboard depend on multi-\ntask ﬁnetuning, our submission depends only on\nsingle-task ﬁnetuning. For RTE, STS and MRPC\nwe found it helpful to ﬁnetune starting from the\nMNLI single-task model, rather than the baseline\npretrained RoBERTa. We explore a slightly wider\nhyperparameter space, described in the Appendix,\nand ensemble between 5 and 7 models per task."}
{"doc_id": "1907.11692", "para_id": 77, "text": "9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work."}
{"doc_id": "1907.11692", "para_id": 78, "text": "Single-task single models on dev\nBERTLARGE\n86.6/-\n92.3\n91.3\n70.4\n93.2\n88.0\n60.6\n90.0\n-\n-\nXLNetLARGE\n89.8/-\n93.9\n91.8\n83.8\n95.6\n89.2\n63.6\n91.8\n-\n-\nRoBERTa\n90.2/90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4\n91.3\n-"}
{"doc_id": "1907.11692", "para_id": 79, "text": "Ensembles on test (from leaderboard as of July 25, 2019)\nALICE\n88.2/87.9\n95.7\n90.7\n83.5\n95.2\n92.6\n68.6\n91.1\n80.8\n86.3\nMT-DNN\n87.9/87.4\n96.0\n89.9\n86.3\n96.5\n92.7\n68.4\n91.1\n89.0\n87.6\nXLNet\n90.2/89.8\n98.6\n90.3\n86.3\n96.8\n93.0\n67.8\n91.6\n90.4\n88.4\nRoBERTa\n90.8/90.2\n98.9\n90.2\n88.2\n96.7\n92.3\n67.8\n92.2\n89.0\n88.5"}
{"doc_id": "1907.11692", "para_id": 80, "text": "Table 5: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE and XLNetLARGE results\nare from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the development set are a\nmedian over ﬁve runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and\nMRPC we ﬁnetune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained\nfrom the GLUE leaderboard."}
{"doc_id": "1907.11692", "para_id": 81, "text": "Results\nWe present our results in Table 5. In the\nﬁrst setting (single-task, dev), RoBERTa achieves\nstate-of-the-art results on all 9 of the GLUE\ntask development sets. Crucially, RoBERTa uses\nthe same masked language modeling pretrain-\ning objective and architecture as BERTLARGE, yet\nconsistently outperforms both BERTLARGE and\nXLNetLARGE. This raises questions about the rel-\native importance of model architecture and pre-\ntraining objective, compared to more mundane de-\ntails like dataset size and training time that we ex-\nplore in this work.\nIn the second setting (ensembles, test), we\nsubmit RoBERTa to the GLUE leaderboard and\nachieve state-of-the-art results on 4 out of 9 tasks\nand the highest average score to date. This is espe-\ncially exciting because RoBERTa does not depend\non multi-task ﬁnetuning, unlike most of the other\ntop submissions. We expect future work may fur-\nther improve these results by incorporating more\nsophisticated multi-task ﬁnetuning procedures."}
{"doc_id": "1907.11692", "para_id": 82, "text": "Task-speciﬁc modiﬁcations\nTwo of the GLUE\ntasks require task-speciﬁc ﬁnetuning approaches\nto achieve competitive leaderboard results.\nQNLI:\nRecent submissions on the GLUE\nleaderboard adopt a pairwise ranking formulation\nfor the QNLI task, in which candidate answers\nare mined from the training set and compared to\none another, and a single (question, candidate)\npair is classiﬁed as positive (Liu et al., 2019b,a;\nYang et al., 2019). This formulation signiﬁcantly\nsimpliﬁes the task, but is not directly comparable\nto BERT (Devlin et al., 2019). Following recent\nwork, we adopt the ranking approach for our test\nsubmission, but for direct comparison with BERT\nwe report development set results based on a pure\nclassiﬁcation approach.\nWNLI:\nWe found the provided NLI-format\ndata to be challenging to work with.\nInstead\nwe use the reformatted WNLI data from Super-\nGLUE (Wang et al., 2019a), which indicates the\nspan of the query pronoun and referent. We ﬁne-\ntune RoBERTa using the margin ranking loss from\nKocijan et al. (2019). For a given input sentence,\nwe use spaCy (Honnibal and Montani, 2017) to\nextract additional candidate noun phrases from the\nsentence and ﬁnetune our model so that it assigns\nhigher scores to positive referent phrases than for\nany of the generated negative candidate phrases.\nOne unfortunate consequence of this formulation\nis that we can only make use of the positive train-\ning examples, which excludes over half of the pro-\nvided training examples.10"}
{"doc_id": "1907.11692", "para_id": 83, "text": "We adopt a much simpler approach for SQuAD\ncompared to past work.\nIn particular, while\nboth\nBERT\n(Devlin et al.,\n2019)\nand\nXL-\nNet (Yang et al., 2019) augment their training data\nwith additional QA datasets, we only ﬁnetune\nRoBERTa using the provided SQuAD training\ndata. Yang et al. (2019) also employed a custom\nlayer-wise learning rate schedule to ﬁnetune"}
{"doc_id": "1907.11692", "para_id": 84, "text": "results could potentially be improved by augmenting this with\nadditional pronoun disambiguation datasets."}
{"doc_id": "1907.11692", "para_id": 85, "text": "10While we only use the provided WNLI training data, our"}
{"doc_id": "1907.11692", "para_id": 86, "text": "Single models on test (as of July 25, 2019)\nBERTLARGE\n72.0\n76.6\n70.1\nXLNetLARGE\n81.7\n85.4\n80.2"}
{"doc_id": "1907.11692", "para_id": 87, "text": "Single models on dev, w/o data augmentation\nBERTLARGE\n84.1\n90.9\n79.0\n81.8\nXLNetLARGE\n89.0\n94.5\n86.1\n88.8\nRoBERTa\n88.9\n94.6\n86.5\n89.4"}
{"doc_id": "1907.11692", "para_id": 88, "text": "Single models on test (as of July 25, 2019)\nXLNetLARGE\n86.3†\n89.1†"}
{"doc_id": "1907.11692", "para_id": 89, "text": "Table 7: Results on the RACE test set. BERTLARGE and\nXLNetLARGE results are from Yang et al. (2019)."}
{"doc_id": "1907.11692", "para_id": 90, "text": "RoBERTa\n86.8\n89.8\nXLNet + SG-Net Veriﬁer\n87.0†\n89.9†"}
{"doc_id": "1907.11692", "para_id": 91, "text": "nating each candidate answer with the correspond-\ning question and passage. We then encode each of\nthese four sequences and pass the resulting [CLS]\nrepresentations through a fully-connected layer,\nwhich is used to predict the correct answer. We\ntruncate question-answer pairs that are longer than\n128 tokens and, if needed, the passage so that the\ntotal length is at most 512 tokens."}
{"doc_id": "1907.11692", "para_id": 92, "text": "Table 6: Results on SQuAD. † indicates results that de-\npend on additional external training data. RoBERTa\nuses only the provided SQuAD data in both dev and\ntest settings. BERTLARGE and XLNetLARGE results are\nfrom Devlin et al. (2019) and Yang et al. (2019), re-\nspectively."}
{"doc_id": "1907.11692", "para_id": 93, "text": "XLNet, while we use the same learning rate for\nall layers.\nFor SQuAD v1.1 we follow the same ﬁnetun-\ning procedure as Devlin et al. (2019). For SQuAD\nv2.0, we additionally classify whether a given\nquestion is answerable; we train this classiﬁer\njointly with the span predictor by summing the\nclassiﬁcation and span loss terms."}
{"doc_id": "1907.11692", "para_id": 94, "text": "Results on the RACE test sets are presented in\nTable 7. RoBERTa achieves state-of-the-art results\non both middle-school and high-school settings."}
{"doc_id": "1907.11692", "para_id": 95, "text": "Pretraining\nmethods\nhave\nbeen\ndesigned\nwith\ndifferent\ntraining\nobjectives,\ninclud-\ning\nlanguage\nmodeling\n(Dai and Le,\n2015;\nPeters et al.,\n2018;\nHoward and Ruder,\n2018),\nmachine translation (McCann et al., 2017), and\nmasked language modeling (Devlin et al., 2019;\nLample and Conneau,\n2019).\nMany\nrecent\npapers have used a basic recipe of ﬁnetuning\nmodels for each end task (Howard and Ruder,\n2018;\nRadford et al.,\n2018),\nand\npretraining\nwith some variant of a masked language model\nobjective.\nHowever,\nnewer\nmethods\nhave\nimproved performance by multi-task ﬁne tun-\ning\n(Dong et al.,\n2019),\nincorporating\nentity\nembeddings\n(Sun et al.,\n2019),\nspan\npredic-\ntion (Joshi et al., 2019), and multiple variants\nof autoregressive pretraining (Song et al., 2019;\nChan et al., 2019; Yang et al., 2019).\nPerfor-\nmance is also typically improved by training\nbigger\nmodels\non\nmore\ndata\n(Devlin et al.,\n2019; Baevski et al., 2019; Yang et al., 2019;\nRadford et al., 2019). Our goal was to replicate,\nsimplify, and better tune the training of BERT,\nas a reference point for better understanding the\nrelative performance of all of these methods."}
{"doc_id": "1907.11692", "para_id": 96, "text": "Results\nWe present our results in Table 6. On\nthe SQuAD v1.1 development set, RoBERTa\nmatches the state-of-the-art set by XLNet. On the\nSQuAD v2.0 development set, RoBERTa sets a\nnew state-of-the-art, improving over XLNet by 0.4\npoints (EM) and 0.6 points (F1).\nWe also submit RoBERTa to the public SQuAD\n2.0 leaderboard and evaluate its performance rel-\native to other systems. Most of the top systems\nbuild upon either BERT (Devlin et al., 2019) or\nXLNet (Yang et al., 2019), both of which rely on\nadditional external training data. In contrast, our\nsubmission does not use any additional data.\nOur single RoBERTa model outperforms all but\none of the single model submissions, and is the\ntop scoring system among those that do not rely\non data augmentation."}
{"doc_id": "1907.11692", "para_id": 97, "text": "In RACE, systems are provided with a passage of\ntext, an associated question, and four candidate an-\nswers. Systems are required to classify which of\nthe four candidate answers is correct.\nWe modify RoBERTa for this task by concate-"}
{"doc_id": "1907.11692", "para_id": 98, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment."}
{"doc_id": "1907.11692", "para_id": 99, "text": "We carefully evaluate a number of design de-\ncisions when pretraining BERT models.\nWe\nﬁnd that performance can be substantially im-\nproved by training the model longer, with bigger\nbatches over more data; removing the next sen-\ntence prediction objective; training on longer se-\nquences; and dynamically changing the masking\npattern applied to the training data. Our improved\npretraining procedure, which we call RoBERTa,\nachieves state-of-the-art results on GLUE, RACE\nand SQuAD, without multi-task ﬁnetuning for\nGLUE or additional data for SQuAD. These re-\nsults illustrate the importance of these previ-\nously overlooked design decisions and suggest\nthat BERT’s pretraining objective remains com-\npetitive with recently proposed alternatives.\nWe\nadditionally\nuse\na\nnovel\ndataset,\nCC-NEWS,\nand\nrelease\nour\nmodels\nand\ncode\nfor\npretraining\nand\nﬁnetuning\nat:\nhttps://github.com/pytorch/fairseq."}
{"doc_id": "1907.11692", "para_id": 100, "text": "Andrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in Neural Informa-\ntion Processing Systems (NIPS)."}
{"doc_id": "1907.11692", "para_id": 101, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 102, "text": "William B Dolan and Chris Brockett. 2005.\nAuto-\nmatically constructing a corpus of sentential para-\nphrases. In Proceedings of the International Work-\nshop on Paraphrasing."}
{"doc_id": "1907.11692", "para_id": 103, "text": "Li Dong, Nan Yang, Wenhui Wang,\nFuru Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019.\nUniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation.\narXiv preprint\narXiv:1905.03197."}
{"doc_id": "1907.11692", "para_id": 104, "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing."}
{"doc_id": "1907.11692", "para_id": 105, "text": "Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-\ntowski, editors. 2007.\nProceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007)."}
{"doc_id": "1907.11692", "para_id": 106, "text": "Aaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus.\nhttp://web.archive.org/\nsave/http://Skylion007.github.io/\nOpenWebTextCorpus."}
{"doc_id": "1907.11692", "para_id": 107, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019.\nCloze-\ndriven pretraining of self-attention networks. arXiv\npreprint arXiv:1903.07785."}
{"doc_id": "1907.11692", "para_id": 108, "text": "Felix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017.\nnews-please:\nA\ngeneric news crawler and extractor. In Proceedings\nof the 15th International Symposium of Information\nScience."}
{"doc_id": "1907.11692", "para_id": 109, "text": "Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment."}
{"doc_id": "1907.11692", "para_id": 110, "text": "Dan Hendrycks and Kevin Gimpel. 2016.\nGaus-\nsian error linear units (gelus).\narXiv preprint\narXiv:1606.08415."}
{"doc_id": "1907.11692", "para_id": 111, "text": "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009.\nThe\nﬁfth PASCAL recognizing textual entailment chal-\nlenge."}
{"doc_id": "1907.11692", "para_id": 112, "text": "Matthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear."}
{"doc_id": "1907.11692", "para_id": 113, "text": "Samuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 114, "text": "Jeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146."}
{"doc_id": "1907.11692", "para_id": 115, "text": "Shankar Iyer, Nikhil Dandekar, and Kornl Cser-\nnai. 2016.\nFirst quora dataset release: Question\npairs.\nhttps://data.quora.com/First-\nQuora-Dataset-Release-Question-\nPairs."}
{"doc_id": "1907.11692", "para_id": 116, "text": "William Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604."}
{"doc_id": "1907.11692", "para_id": 117, "text": "Myle Ott,\nSergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation (WMT)."}
{"doc_id": "1907.11692", "para_id": 118, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanBERT:\nImproving\npre-training\nby\nrepre-\nsenting and predicting spans.\narXiv preprint\narXiv:1907.10529."}
{"doc_id": "1907.11692", "para_id": 119, "text": "Adam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS Autodiff Workshop."}
{"doc_id": "1907.11692", "para_id": 120, "text": "Diederik Kingma and Jimmy Ba. 2015.\nAdam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR)."}
{"doc_id": "1907.11692", "para_id": 121, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\nA surprisingly robust trick for winograd schema\nchallenge. arXiv preprint arXiv:1905.06290."}
{"doc_id": "1907.11692", "para_id": 122, "text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Association for Com-\nputational Linguistics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 123, "text": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations.\narXiv\npreprint arXiv:1704.04683."}
{"doc_id": "1907.11692", "para_id": 124, "text": "Alec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning. Technical\nreport, OpenAI."}
{"doc_id": "1907.11692", "para_id": 125, "text": "Guillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291."}
{"doc_id": "1907.11692", "para_id": 126, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI."}
{"doc_id": "1907.11692", "para_id": 127, "text": "Hector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical Formalizations of\nCommonsense Reasoning."}
{"doc_id": "1907.11692", "para_id": 128, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Association for Computational\nLinguistics (ACL)."}
{"doc_id": "1907.11692", "para_id": 129, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019a.\nImproving multi-task deep\nneural networks via knowledge distillation for\nnatural language understanding.\narXiv preprint\narXiv:1904.09482."}
{"doc_id": "1907.11692", "para_id": 130, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 131, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint\narXiv:1901.11504."}
{"doc_id": "1907.11692", "para_id": 132, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units.\nIn Association for Computational\nLinguistics (ACL), pages 1715–1725."}
{"doc_id": "1907.11692", "para_id": 133, "text": "Bryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems (NIPS), pages 6297–\n6308."}
{"doc_id": "1907.11692", "para_id": 134, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP)."}
{"doc_id": "1907.11692", "para_id": 135, "text": "Paulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\nsion training. In International Conference on Learn-\ning Representations."}
{"doc_id": "1907.11692", "para_id": 136, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2019.\nMASS: Masked sequence\nto sequence pre-training for language generation.\nIn International Conference on Machine Learning\n(ICML)."}
{"doc_id": "1907.11692", "para_id": 137, "text": "Sebastian\nNagel.\n2016.\nCc-news.\nhttp:\n//web.archive.org/save/http:\n//commoncrawl.org/2016/10/news-\ndataset-available."}
{"doc_id": "1907.11692", "para_id": 138, "text": "Yu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun\nFeng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi-\nang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: En-\nhanced representation through knowledge integra-\ntion. arXiv preprint arXiv:1904.09223."}
{"doc_id": "1907.11692", "para_id": 139, "text": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.\nFAIRSEQ:\nA fast, exten-\nsible toolkit for sequence modeling.\nIn North\nAmerican Association for Computational Linguis-\ntics (NAACL): System Demonstrations."}
{"doc_id": "1907.11692", "para_id": 140, "text": "Trieu H Trinh and Quoc V Le. 2018.\nA simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847."}
{"doc_id": "1907.11692", "para_id": 141, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems."}
{"doc_id": "1907.11692", "para_id": 142, "text": "Table 9 describes the hyperparameters for pre-\ntraining of RoBERTaLARGE and RoBERTaBASE"}
{"doc_id": "1907.11692", "para_id": 143, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537."}
{"doc_id": "1907.11692", "para_id": 144, "text": "Finetuning hyperparameters for RACE, SQuAD\nand GLUE are given in Table 10. We select the\nbest hyperparameter values based on the median\nof 5 random seeds for each task."}
{"doc_id": "1907.11692", "para_id": 145, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR)."}
{"doc_id": "1907.11692", "para_id": 146, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint 1805.12471."}
{"doc_id": "1907.11692", "para_id": 147, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn North\nAmerican Association for Computational Linguis-\ntics (NAACL)."}
{"doc_id": "1907.11692", "para_id": 148, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding.\narXiv preprint\narXiv:1906.08237."}
{"doc_id": "1907.11692", "para_id": 149, "text": "Yang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\ning bert pre-training time from 3 days to 76 minutes.\narXiv preprint arXiv:1904.00962."}
{"doc_id": "1907.11692", "para_id": 150, "text": "Rowan Zellers,\nAri Holtzman,\nHannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019.\nDefending against neural fake\nnews. arXiv preprint arXiv:1905.12616."}
{"doc_id": "1907.11692", "para_id": 151, "text": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books.\nIn arXiv preprint\narXiv:1506.06724."}
{"doc_id": "1907.11692", "para_id": 152, "text": "Appendix for “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”"}
{"doc_id": "1907.11692", "para_id": 153, "text": "In Table 8 we present the full set of development\nset results for RoBERTa. We present results for\na LARGE conﬁguration that follows BERTLARGE,\nas well as a BASE conﬁguration that follows\nBERTBASE."}
{"doc_id": "1907.11692", "para_id": 154, "text": "RoBERTaBASE\n+ all data + 500k steps\n87.6\n92.8\n91.9\n78.7\n94.8\n90.2\n63.6\n91.2"}
{"doc_id": "1907.11692", "para_id": 155, "text": "RoBERTaLARGE\nwith BOOKS + WIKI\n89.0\n93.9\n91.9\n84.5\n95.3\n90.2\n66.3\n91.6\n+ additional data (§3.2)\n89.3\n94.0\n92.0\n82.7\n95.6\n91.4\n66.1\n92.2\n+ pretrain longer 300k\n90.0\n94.5\n92.2\n83.3\n96.1\n91.1\n67.4\n92.3\n+ pretrain longer 500k\n90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4"}
{"doc_id": "1907.11692", "para_id": 156, "text": "Table 8: Development set results on GLUE tasks for various conﬁgurations of RoBERTa."}
{"doc_id": "1907.11692", "para_id": 157, "text": "Number of Layers\n24\n12\nHidden size\n1024\n768\nFFN inner hidden size\n4096\n3072\nAttention heads\n16\n12\nAttention head size\n64\n64\nDropout\n0.1\n0.1\nAttention Dropout\n0.1\n0.1\nWarmup Steps\n30k\n24k\nPeak Learning Rate\n4e-4\n6e-4\nBatch Size\n8k\n8k\nWeight Decay\n0.01\n0.01\nMax Steps\n500k\n500k\nLearning Rate Decay\nLinear\nLinear\nAdam ǫ\n1e-6\n1e-6\nAdam β1\n0.9\n0.9\nAdam β2\n0.98\n0.98\nGradient Clipping\n0.0\n0.0"}
{"doc_id": "1907.11692", "para_id": 158, "text": "Table 9: Hyperparameters for pretraining RoBERTaLARGE and RoBERTaBASE."}
{"doc_id": "1907.11692", "para_id": 159, "text": "Learning Rate\n1e-5\n1.5e-5\n{1e-5, 2e-5, 3e-5}\nBatch Size\n16\n48\n{16, 32}\nWeight Decay\n0.1\n0.01\n0.1\nMax Epochs\n4\n2\n10\nLearning Rate Decay\nLinear\nLinear\nLinear\nWarmup ratio\n0.06\n0.06\n0.06"}
{"doc_id": "1907.11692", "para_id": 160, "text": "Table 10: Hyperparameters for ﬁnetuning RoBERTaLARGE on RACE, SQuAD and GLUE."}
{"doc_id": "2005.14165", "para_id": 0, "text": "Tom B. Brown∗\nBenjamin Mann∗\nNick Ryder∗\nMelanie Subbiah∗"}
{"doc_id": "2005.14165", "para_id": 1, "text": "Jared Kaplan†\nPrafulla Dhariwal\nArvind Neelakantan\nPranav Shyam\nGirish Sastry"}
{"doc_id": "2005.14165", "para_id": 2, "text": "Amanda Askell\nSandhini Agarwal\nAriel Herbert-Voss\nGretchen Krueger\nTom Henighan"}
{"doc_id": "2005.14165", "para_id": 3, "text": "Rewon Child\nAditya Ramesh\nDaniel M. Ziegler\nJeffrey Wu\nClemens Winter"}
{"doc_id": "2005.14165", "para_id": 4, "text": "Christopher Hesse\nMark Chen\nEric Sigler\nMateusz Litwin\nScott Gray"}
{"doc_id": "2005.14165", "para_id": 5, "text": "Sam McCandlish\nAlec Radford\nIlya Sutskever\nDario Amodei"}
{"doc_id": "2005.14165", "para_id": 6, "text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic\nin architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions – something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-\ntuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning,\nwith tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\nand of GPT-3 in general."}
{"doc_id": "2005.14165", "para_id": 7, "text": "∗Equal contribution\n†Johns Hopkins University, OpenAI"}
{"doc_id": "2005.14165", "para_id": 8, "text": "2\nApproach\n6\n2.1\nModel and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2\nTraining Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3\nTraining Process\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.4\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10"}
{"doc_id": "2005.14165", "para_id": 9, "text": "3\nResults\n10\n3.1\nLanguage Modeling, Cloze, and Completion Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2\nClosed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.3\nTranslation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.4\nWinograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.5\nCommon Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.6\nReading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.7\nSuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.8\nNLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.9\nSynthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21"}
{"doc_id": "2005.14165", "para_id": 10, "text": "4\nMeasuring and Preventing Memorization Of Benchmarks\n29"}
{"doc_id": "2005.14165", "para_id": 11, "text": "6\nBroader Impacts\n34\n6.1\nMisuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n6.2\nFairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n6.3\nEnergy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39"}
{"doc_id": "2005.14165", "para_id": 12, "text": "E\nHuman Quality Assessment of Synthetic News Articles\n46"}
{"doc_id": "2005.14165", "para_id": 13, "text": "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\nﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\nvectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\nand contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\ntask-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\nbeen directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18]."}
{"doc_id": "2005.14165", "para_id": 14, "text": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\nquestion answering, textual entailment, and many others, and has continued to advance based on new architectures\nand algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while\nthe architecture is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve\nstrong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands\nof examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons."}
{"doc_id": "2005.14165", "para_id": 15, "text": "First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\napplicability of language models. There exists a very wide range of possible useful language tasks, encompassing\nanything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\nof these tasks it is difﬁcult to collect a large supervised training dataset, especially when the process must be repeated\nfor every new task."}
{"doc_id": "2005.14165", "para_id": 16, "text": "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\nof the model and the narrowness of the training distribution. This can create problems for the pre-training plus\nﬁne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\nﬁne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily\ngeneralize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\ncan be poor because the model is overly speciﬁc to the training distribution and does not generalize well outside it\n[YdC+19, MPL19]. Thus, the performance of ﬁne-tuned models on speciﬁc benchmarks, even when it is nominally at\nhuman-level, may exaggerate actual performance on the underlying task [GSL+18, NK19]."}
{"doc_id": "2005.14165", "para_id": 17, "text": "Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\nlanguage (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number\nof demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often"}
{"doc_id": "2005.14165", "para_id": 18, "text": "Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\nset of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize\nthe desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within\nthe forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\nmodel would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\nwithin a single sequence."}
{"doc_id": "2005.14165", "para_id": 19, "text": "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\nperformance on a simple task requiring the model to remove random symbols from a word, both with and without a\nnatural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\nimproved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\nof tasks."}
{"doc_id": "2005.14165", "para_id": 20, "text": "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\nto a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\nto seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\ndialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality."}
{"doc_id": "2005.14165", "para_id": 21, "text": "One potential route towards addressing these issues is meta-learning1 – which in the context of language models means\nthe model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\nat inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19]\nattempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form\nof task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\nand is then expected to complete further instances of the task simply by predicting what comes next."}
{"doc_id": "2005.14165", "para_id": 22, "text": "While it has shown some initial promise, this approach still achieves results far inferior to ﬁne-tuning – for example\n[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\nthe state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\nsolving language tasks."}
{"doc_id": "2005.14165", "para_id": 23, "text": "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\nlanguage models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters\n[DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19],\nand ﬁnally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream\nNLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\nsmooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and\ntasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\ngains with scale."}
{"doc_id": "2005.14165", "para_id": 24, "text": "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous:\nthe method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time\ndemonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning”\nto capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner\nloop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\ndemonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\nlearns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\nwe discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\nloop structure."}
{"doc_id": "2005.14165", "para_id": 25, "text": "Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\nimproves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\nmore proﬁcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\nbenchmark suite."}
{"doc_id": "2005.14165", "para_id": 26, "text": "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\nGPT-3, and measuring its in-context learning abilities. Speciﬁcally, we evaluate GPT-3 on over two dozen NLP datasets,\nas well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training\nset. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we\nallow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”,\nwhere we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only\nan instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional\nﬁne-tuning setting, but we leave this to future work."}
{"doc_id": "2005.14165", "para_id": 27, "text": "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to\nremove extraneous symbols from a word. Model performance improves with the addition of a natural language task\ndescription, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically\nwith model size. Though the results in this case are particularly striking, the general trends with both model size and\nnumber of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no\ngradient updates or ﬁne-tuning, just increasing numbers of demonstrations given as conditioning."}
{"doc_id": "2005.14165", "para_id": 28, "text": "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\nsetting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\nby ﬁne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in\nthe one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\nzero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\nrelative to ﬁne-tuned models operating in the same closed-book setting."}
{"doc_id": "2005.14165", "para_id": 29, "text": "GPT-3 also displays one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning,\nwhich include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\ndeﬁned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\nevaluators have difﬁculty distinguishing from human-generated articles."}
{"doc_id": "2005.14165", "para_id": 30, "text": "At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\nincludes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\nor QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we\nhope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed."}
{"doc_id": "2005.14165", "para_id": 31, "text": "A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should\nnot be seen as a rigorous or meaningful benchmark in itself)."}
{"doc_id": "2005.14165", "para_id": 32, "text": "We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models\non datasets such as Common Crawl, which can potentially include content from test datasets simply because such\ncontent often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\nits distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most\ndatasets, we do identify a few datasets where it could be inﬂating results, and we either do not report results on these\ndatasets or we note them with an asterisk, depending on the severity."}
{"doc_id": "2005.14165", "para_id": 33, "text": "In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion\nparameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most\ntasks we ﬁnd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap\nbetween zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models\nare more proﬁcient meta-learners."}
{"doc_id": "2005.14165", "para_id": 34, "text": "Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and\nbroader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard."}
{"doc_id": "2005.14165", "para_id": 35, "text": "The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training\nGPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.\nSection 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.\nSection 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes."}
{"doc_id": "2005.14165", "para_id": 36, "text": "Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19],\nwith relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use\nof in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for\nlearning within the context. Therefore, we start this section by explicitly deﬁning and contrasting the different settings\nthat we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a\nspectrum of how much task-speciﬁc data they tend to rely on. Speciﬁcally, we can identify at least four points on this\nspectrum (see Figure 2.1 for an illustration):"}
{"doc_id": "2005.14165", "para_id": 37, "text": "• Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of\na pre-trained model by training on a supervised dataset speciﬁc to the desired task. Typically thousands to\nhundreds of thousands of labeled examples are used. The main advantage of ﬁne-tuning is strong performance\non many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential\nfor poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the\ntraining data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In\nthis work we do not ﬁne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be\nﬁne-tuned in principle and this is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 38, "text": "• Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few\ndemonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed.\nAs shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example\nan English sentence and the French translation), and few-shot works by giving K examples of context and\ncompletion, and then one ﬁnal example of context, with the model expected to provide the completion. We\ntypically set K in the range of 10 to 100 as this is how many examples can ﬁt in the model’s context window\n(nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speciﬁc data and\nreduced potential to learn an overly narrow distribution from a large but narrow ﬁne-tuning dataset. The main\ndisadvantage is that results from this method have so far been much worse than state-of-the-art ﬁne-tuned\nmodels. Also, a small amount of task speciﬁc data is still required. As indicated by the name, few-shot\nlearning as described here for language models is related to few-shot learning as used in other contexts in\nML [HYC01, VBL+16] – both involve learning based on a broad distribution of tasks (in this case implicit in\nthe pre-training data) and then rapidly adapting to a new task."}
{"doc_id": "2005.14165", "para_id": 39, "text": "• One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural\nlanguage description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and\nzero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.\nFor example, when asking humans to generate a dataset on a human worker service (for example Mechanical\nTurk), it is common to give one demonstration of the task. By contrast it is sometimes difﬁcult to communicate\nthe content or format of a task if no examples are given."}
{"doc_id": "2005.14165", "para_id": 40, "text": "Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning. The panels above show\nfour methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\ndescriptions, examples and prompts can be found in Appendix G."}
{"doc_id": "2005.14165", "para_id": 41, "text": "• Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\na natural language instruction describing the task. This method provides maximum convenience, potential for\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of\npre-training data), but is also the most challenging setting. In some cases it may even be difﬁcult for humans\nto understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”.\nFor example, if someone is asked to “make a table of world records for the 200m dash”, this request can be\nambiguous, as it may not be clear exactly what format the table should have or what should be included (and\neven with careful clariﬁcation, understanding precisely what is desired can be difﬁcult). Nevertheless, for at\nleast some settings zero-shot is closest to how humans perform tasks – for example, in the translation example\nin Figure 2.1, a human would likely know what to do from just the text instruction."}
{"doc_id": "2005.14165", "para_id": 42, "text": "Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\nproblem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency.\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models.\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,\nand are important targets for future work."}
{"doc_id": "2005.14165", "para_id": 43, "text": "Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses\nthe details of how we do few-shot, one-shot, and zero-shot evaluations."}
{"doc_id": "2005.14165", "para_id": 44, "text": "Model Name\nnparams\nnlayers\ndmodel\nnheads\ndhead\nBatch Size\nLearning Rate"}
{"doc_id": "2005.14165", "para_id": 45, "text": "GPT-3 175B or “GPT-3”\n175.0B\n96\n12288\n96\n128\n3.2M\n0.6 × 10−4"}
{"doc_id": "2005.14165", "para_id": 46, "text": "Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\nwhich we trained. All models were trained for a total of 300 billion tokens."}
{"doc_id": "2005.14165", "para_id": 47, "text": "We use the same model and architecture as GPT-2 [RWC+19], including the modiﬁed initialization, pre-normalization,\nand reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse\nattention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence\nof ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125\nmillion parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20]\nsuggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a\nfunction of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for\ndownstream language tasks."}
{"doc_id": "2005.14165", "para_id": 48, "text": "Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\nnlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\nfeedforward layer four times the size of the bottleneck layer, dﬀ= 4 ∗dmodel), and dhead is the dimension of each\nattention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along\nboth the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\nparameters for each model are chosen based on computational efﬁciency and load-balancing in the layout of models\nacross GPU’s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters\nwithin a reasonably broad range."}
{"doc_id": "2005.14165", "para_id": 49, "text": "Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting\nnearly a trillion words. This size of dataset is sufﬁcient to train our largest models without ever updating on the same\nsequence twice. However, we have found that unﬁltered or lightly ﬁltered versions of Common Crawl tend to have\nlower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:\n(1) we downloaded and ﬁltered a version of CommonCrawl based on similarity to a range of high-quality reference\ncorpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy\nand preserve the integrity of our held-out validation set as an accurate measure of overﬁtting, and (3) we also added\nknown high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity."}
{"doc_id": "2005.14165", "para_id": 50, "text": "Details of the ﬁrst two points (processing of Common Crawl) are described in Appendix A. For the third, we added\nseveral curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected\nby scraping links over a longer period of time, and ﬁrst described in [KMH+20], two internet-based books corpora\n(Books1 and Books2) and English-language Wikipedia."}
{"doc_id": "2005.14165", "para_id": 51, "text": "Table 2.2 shows the ﬁnal mixture of datasets that we used in training. The CommonCrawl data was downloaded from\n41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before ﬁltering\nand 570GB after ﬁltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets\nare not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently,\nsuch that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are\nsampled 2-3 times. This essentially accepts a small amount of overﬁtting in exchange for higher quality training data."}
{"doc_id": "2005.14165", "para_id": 52, "text": "Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models\n[KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B\nis almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaﬂop/s-days of compute\nduring pre-training. Methodology for these calculations can be found in Appendix D."}
{"doc_id": "2005.14165", "para_id": 53, "text": "Dataset\nQuantity\n(tokens)\nWeight in\ntraining mix\nEpochs elapsed when\ntraining for 300B tokens"}
{"doc_id": "2005.14165", "para_id": 54, "text": "Common Crawl (ﬁltered)\n410 billion\n60%\n0.44\nWebText2\n19 billion\n22%\n2.9\nBooks1\n12 billion\n8%\n1.9\nBooks2\n55 billion\n8%\n0.43\nWikipedia\n3 billion\n3%\n3.4"}
{"doc_id": "2005.14165", "para_id": 55, "text": "Table 2.2: Datasets used to train GPT-3. “Weight in training mix” refers to the fraction of examples during training\nthat are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a\nresult, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets\nare seen less than once."}
{"doc_id": "2005.14165", "para_id": 56, "text": "A major methodological concern with language models pretrained on a broad swath of internet data, particularly large\nmodels with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by\nhaving their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched\nfor and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.\nUnfortunately, a bug in the ﬁltering caused us to ignore some overlaps, and due to the cost of training it was not feasible\nto retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will\nmore aggressively remove data contamination."}
{"doc_id": "2005.14165", "para_id": 57, "text": "As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning\nrate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table\n2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture\nof model parallelism within each matrix multiply and model parallelism across the layers of the network. All models\nwere trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process\nand hyperparameter settings are described in Appendix B."}
{"doc_id": "2005.14165", "para_id": 58, "text": "For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that\ntask’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze\nthere is no supervised training set available so we draw conditioning examples from the development set and evaluate\non the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning\nexamples directly from it."}
{"doc_id": "2005.14165", "para_id": 59, "text": "K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048\nfor all models and typically ﬁts 10 to 100 examples. Larger values of K are usually but not always better, so when a\nseparate development and test set are available, we experiment with a few values of K on the development set and then\nrun the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to\n(or for K = 0, instead of) demonstrations."}
{"doc_id": "2005.14165", "para_id": 60, "text": "On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples\nof context plus correct completion, followed by one example of context only, and compare the LM likelihood of\neach completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small\nnumber of datasets (ARC, OpenBookQA, and RACE) we gain additional beneﬁt as measured on the development set\nby normalizing by the unconditional probability of each completion, by computing\nP (completion|context)\nP (completion|answer context), where\nanswer context is the string \"Answer:\n\" or \"A: \" and is used to prompt that the completion should be an answer\nbut is otherwise generic."}
{"doc_id": "2005.14165", "para_id": 61, "text": "On tasks that involve binary classiﬁcation, we give the options more semantically meaningful names (e.g. “True” or\n“False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what\nis done by [RSR+19] (see Appendix G) for details."}
{"doc_id": "2005.14165", "para_id": 62, "text": "On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4\nand a length penalty of α = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on\nwhat is standard for the dataset at hand."}
{"doc_id": "2005.14165", "para_id": 63, "text": "Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-,\nand few-shot). When the test set is private, our model is often too large to ﬁt on the test server, so we report results on\nthe development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa)\nwhere we were able to make submission work, and we submit only the 200B few-shot results, and report development\nset results for everything else."}
{"doc_id": "2005.14165", "para_id": 64, "text": "In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6\nadditional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling\nperformance follows a power-law when making efﬁcient use of training compute. After extending this trend by two\nmore orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these\nimprovements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will\nsee in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a\nbroad spectrum of natural language tasks."}
{"doc_id": "2005.14165", "para_id": 65, "text": "Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller\nmodels) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks."}
{"doc_id": "2005.14165", "para_id": 66, "text": "In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling,\nsuch as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question\nanswering tasks: tasks which require using the information stored in the model’s parameters to answer general\nknowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot\nand few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we\nevaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading\ncomprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieﬂy explore\nNLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities –\nthese tasks focus on on-the-ﬂy reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the\nfew-shot, one-shot, and zero-shot settings."}
{"doc_id": "2005.14165", "para_id": 67, "text": "Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\nvalidation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior\nobserved in [KMH+20] continues for an additional two orders of magnitude with only small deviations from the\npredicted curve. For this ﬁgure, we exclude embedding parameters from compute and parameter counts."}
{"doc_id": "2005.14165", "para_id": 68, "text": "Table 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets\nare omitted because they are derived from Wikipedia or other sources which are included in GPT-3’s training data.\na[RWC+19]"}
{"doc_id": "2005.14165", "para_id": 69, "text": "3.1\nLanguage Modeling, Cloze, and Completion Tasks"}
{"doc_id": "2005.14165", "para_id": 70, "text": "In this section we test GPT-3’s performance on the traditional task of language modeling, as well as related tasks\nthat involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible\ncompletions of a piece of text."}
{"doc_id": "2005.14165", "para_id": 71, "text": "We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [MKM+94] dataset measured in [RWC+19]. We omit\nthe 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the\none-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these\nissues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15\npoints, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have\na clear separation of examples to deﬁne one-shot or few-shot evaluation around, so we measure only zero-shot."}
{"doc_id": "2005.14165", "para_id": 72, "text": "The LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text – the model is asked to\npredict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the\ncontinued scaling of language models is yielding diminishing returns on this difﬁcult benchmark. [BHT+20] reﬂect on\nthe small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP+19]"}
{"doc_id": "2005.14165", "para_id": 73, "text": "Setting\nLAMBADA\n(acc)\nLAMBADA\n(ppl)\nStoryCloze\n(acc)\nHellaSwag\n(acc)"}
{"doc_id": "2005.14165", "para_id": 74, "text": "SOTA\n68.0a\n8.63b\n91.8c\n85.6d\nGPT-3 Zero-Shot\n76.2\n3.00\n83.2\n78.9\nGPT-3 One-Shot\n72.5\n3.35\n84.7\n78.1\nGPT-3 Few-Shot\n86.4\n1.92\n87.7\n79.3"}
{"doc_id": "2005.14165", "para_id": 75, "text": "Table 3.2: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on LAMBADA while\nachieving respectable performance on two difﬁcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19]\nd[LCH+20]"}
{"doc_id": "2005.14165", "para_id": 76, "text": "Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3\n2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of\nthe art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text."}
{"doc_id": "2005.14165", "para_id": 77, "text": "and [Tur20]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path\nforward”. We ﬁnd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of\n8% over the previous state of the art."}
{"doc_id": "2005.14165", "para_id": 78, "text": "LAMBADA is also a demonstration of the ﬂexibility of few-shot learning as it provides a way to address a problem that\nclassically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a\nstandard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but\nalso to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word\nﬁlters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a\ncloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We\nuse the following ﬁll-in-the-blank format:"}
{"doc_id": "2005.14165", "para_id": 79, "text": "Alice was friends with Bob. Alice went to visit her friend\n. →Bob"}
{"doc_id": "2005.14165", "para_id": 80, "text": "George bought some baseball equipment, a ball, a glove, and a\n. →"}
{"doc_id": "2005.14165", "para_id": 81, "text": "When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase\nof over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model\nsize. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy\nby 10%. Finally, the ﬁll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot\nsetting. Perhaps this is because all models still require several examples to recognize the pattern."}
{"doc_id": "2005.14165", "para_id": 82, "text": "RAG (Fine-tuned, Open-Domain) [LPP+20]\n44.5\n45.5\n68.0\nT5-11B+SSM (Fine-tuned, Closed-Book) [RRS20]\n36.6\n44.7\n60.5\nT5-11B (Fine-tuned, Closed-Book)\n34.5\n37.4\n50.1\nGPT-3 Zero-Shot\n14.6\n14.4\n64.3\nGPT-3 One-Shot\n23.0\n25.3\n68.0\nGPT-3 Few-Shot\n29.9\n41.5\n71.2"}
{"doc_id": "2005.14165", "para_id": 83, "text": "Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as\ncompared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the\nwiki split test server."}
{"doc_id": "2005.14165", "para_id": 84, "text": "One note of caution is that an analysis of test set contamination identiﬁed that a signiﬁcant minority of the LAMBADA\ndataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact\non performance."}
{"doc_id": "2005.14165", "para_id": 85, "text": "The HellaSwag dataset [ZHB+19] involves picking the best ending to a story or set of instructions. The examples were\nadversarially mined to be difﬁcult for language models while remaining easy for humans (who achieve 95.6% accuracy).\nGPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the\n75.4% accuracy of a ﬁne-tuned 1.5B parameter language model [ZHR+19] but still a fair amount lower than the overall\nSOTA of 85.6% achieved by the ﬁne-tuned multi-task model ALUM."}
{"doc_id": "2005.14165", "para_id": 86, "text": "We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH+16], which involves selecting the correct ending\nsentence for ﬁve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot\nsetting (with K = 70). This is still 4.1% lower than the ﬁne-tuned SOTA using a BERT based model [LDL19] but\nimproves over previous zero-shot results by roughly 10%."}
{"doc_id": "2005.14165", "para_id": 87, "text": "In this section we measure GPT-3’s ability to answer questions about broad factual knowledge. Due to the immense\namount of possible queries, this task has normally been approached by using an information retrieval system to ﬁnd\nrelevant text in combination with a model which learns to generate an answer given the question and the retrieved\ntext. Since this setting allows a system to search for and condition on text which potentially contains the answer it\nis denoted “open-book”. [RRS20] recently demonstrated that a large language model can perform surprisingly well\ndirectly answering the questions without conditioning on auxilliary information. They denote this more restrictive\nevaluation setting as “closed-book”. Their work suggests that even higher-capacity models could perform even better\nand we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR+19],\nWebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in\nthe closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than\nprevious closed-book QA work: in addition to external content not being allowed, ﬁne-tuning on the Q&A dataset itself\nis also not permitted."}
{"doc_id": "2005.14165", "para_id": 88, "text": "The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the\none-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the ﬁne-tuned T5-11B by\n14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot\nresult improves by 3.7% and matches the SOTA for an open-domain QA system which not only ﬁne-tunes but also\nmakes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP+20].\nGPT-3’s few-shot result further improves performance another 3.2% beyond this."}
{"doc_id": "2005.14165", "para_id": 89, "text": "On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5%\nin the few-shot setting. This compares to 37.4% for ﬁne-tuned T5-11B, and 44.7% for ﬁne-tuned T5-11B+SSM,\nwhich uses a Q&A-speciﬁc pre-training procedure. GPT-3 in the few-shot setting approaches the performance of\nstate-of-the-art ﬁne-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to\nfew-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions"}
{"doc_id": "2005.14165", "para_id": 90, "text": "Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models\ncontinue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiﬁcant gains\nover zero-shot behavior, matching and exceeding the performance of the SOTA ﬁne-tuned open-domain model, RAG\n[LPP+20]"}
{"doc_id": "2005.14165", "para_id": 91, "text": "and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this\ndistribution, recovering strong performance in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 92, "text": "On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in\nthe few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to\nTriviaQA and WebQS. In particular, the questions in NQs tend towards very ﬁne-grained knowledge on Wikipedia\nspeciﬁcally which could be testing the limits of GPT-3’s capacity and broad pretraining distribution."}
{"doc_id": "2005.14165", "para_id": 93, "text": "Overall, on one of the three datasets GPT-3’s one-shot matches the open-domain ﬁne-tuning SOTA. On the other two\ndatasets it approaches the performance of the closed-book SOTA despite not using ﬁne-tuning. On all 3 datasets, we\nﬁnd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reﬂecting\nthe idea that model capacity translates directly to more ‘knowledge’ absorbed in the parameters of the model."}
{"doc_id": "2005.14165", "para_id": 94, "text": "For GPT-2 a ﬁlter was used on a multilingual collection of documents to produce an English only dataset due to capacity\nconcerns. Even with this ﬁltering GPT-2 showed some evidence of multilingual capability and performed non-trivially\nwhen translating between French and English despite only training on 10 megabytes of remaining French text. Since we\nincrease the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training\ndataset to include more representation of other languages, though this remains an area for further improvement. As\ndiscussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based ﬁltering. Although\nGPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.\nThese languages are documented in the supplemental material. In order to better understand translation capability, we\nalso expand our analysis to include two additional commonly studied languages, German and Romanian."}
{"doc_id": "2005.14165", "para_id": 95, "text": "Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets\nwith back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a\nblend of training data that mixes many languages together in a natural way, combining them on a word, sentence,\nand document level. GPT-3 also uses a single training objective which is not customized or designed for any task in\nparticular. However, our one / few-shot settings aren’t strictly comparable to prior unsupervised work since they make\nuse of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data."}
{"doc_id": "2005.14165", "para_id": 96, "text": "Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task,\nstill underperforms recent unsupervised NMT results. However, providing only a single example demonstration for"}
{"doc_id": "2005.14165", "para_id": 97, "text": "SOTA (Supervised)\n45.6a\n35.0 b\n41.2c\n40.2d\n38.5e\n39.9e"}
{"doc_id": "2005.14165", "para_id": 98, "text": "XLM [LC19]\n33.4\n33.3\n26.4\n34.3\n33.3\n31.8\nMASS [STQ+19]\n37.5\n34.9\n28.3\n35.2\n35.2\n33.1\nmBART [LGG+20]\n-\n-\n29.8\n34.0\n35.0\n30.5"}
{"doc_id": "2005.14165", "para_id": 99, "text": "GPT-3 Zero-Shot\n25.2\n21.2\n24.6\n27.2\n14.1\n19.9\nGPT-3 One-Shot\n28.3\n33.7\n26.2\n30.4\n20.6\n38.6\nGPT-3 Few-Shot\n32.6\n39.2\n29.7\n40.6\n21.0\n39.5"}
{"doc_id": "2005.14165", "para_id": 100, "text": "Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating\ninto English reﬂecting its strength as an English LM. We report BLEU scores on the WMT’14 Fr↔En,\nWMT’16 De↔En, and WMT’16 Ro↔En datasets as measured by multi-bleu.perl with XLM’s tokeniza-\ntion in order to compare most closely with prior unsupervised NMT work.\nSacreBLEUf [Pos18] results re-\nported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA\nwith relative conﬁdence.\na[EOAG18] b[DHKH14] c[WXH+18] d[oR16] e[LGG+20] f [SacreBLEU signature:\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]"}
{"doc_id": "2005.14165", "para_id": 101, "text": "Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent\ntrend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be\nstronger than translation from English."}
{"doc_id": "2005.14165", "para_id": 102, "text": "Fine-tuned SOTA\n90.1a\n84.6b\nGPT-3 Zero-Shot\n88.3*\n70.2\nGPT-3 One-Shot\n89.7*\n73.2\nGPT-3 Few-Shot\n88.6*\n77.7"}
{"doc_id": "2005.14165", "para_id": 103, "text": "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n4 for details on potential contamination of the Winograd test set. a[SBBC19] b[LYN+20]"}
{"doc_id": "2005.14165", "para_id": 104, "text": "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\nScaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B\nis competitive with a ﬁne-tuned RoBERTA-large."}
{"doc_id": "2005.14165", "para_id": 105, "text": "each translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\nGPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior\nunsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the\nthree input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into\nEnglish but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at\nover 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE\ntokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En,\nfew shot GPT-3 outperforms the best supervised result we could ﬁnd but due to our unfamiliarity with the literature and\nthe appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art.\nFor Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of\nunsupervised pretraining, supervised ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b]."}
{"doc_id": "2005.14165", "para_id": 106, "text": "Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of\nimprovement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three\nsettings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 107, "text": "The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun\nrefers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently ﬁne-tuned\nlanguage models have achieved near-human performance on the original Winograd dataset, but more difﬁcult versions"}
{"doc_id": "2005.14165", "para_id": 108, "text": "Setting\nPIQA\nARC (Easy)\nARC (Challenge)\nOpenBookQA"}
{"doc_id": "2005.14165", "para_id": 109, "text": "Fine-tuned SOTA\n79.4\n92.0[KKS+20]\n78.5[KKS+20]\n87.2[KKS+20]\nGPT-3 Zero-Shot\n80.5*\n68.8\n51.4\n57.6\nGPT-3 One-Shot\n80.5*\n71.2\n53.2\n58.8\nGPT-3 Few-Shot\n82.8*\n70.1\n51.5\n65.4"}
{"doc_id": "2005.14165", "para_id": 110, "text": "Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot\nPIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test\nset."}
{"doc_id": "2005.14165", "para_id": 111, "text": "Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a\nscore on the development set in all three conditions that exceeds the best recorded score on the task."}
{"doc_id": "2005.14165", "para_id": 112, "text": "such as the adversarially-mined Winogrande dataset [SBBC19] still signiﬁcantly lag human performance. We test\nGPT-3’s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting."}
{"doc_id": "2005.14165", "para_id": 113, "text": "On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same “partial evaluation” method\ndescribed in [RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which\nis presented as binary classiﬁcation and requires entity extraction to convert to the form described in this section. On\nWinograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear\nin-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human\nperformance. We note that contamination analysis found some Winograd schemas in the training data but this appears\nto have only a small effect on results (see Section 4)."}
{"doc_id": "2005.14165", "para_id": 114, "text": "On the more difﬁcult Winogrande dataset, we do ﬁnd gains to in-context learning: GPT-3 achieves 70.2% in the\nzero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a ﬁne-tuned\nRoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a ﬁne-tuned high capacity model (T5), and\nhuman performance on the task as reported by [SBBC19] is 94.0%."}
{"doc_id": "2005.14165", "para_id": 115, "text": "Next we consider three datasets which attempt to capture physical or scientiﬁc reasoning, as distinct from sentence\ncompletion, reading comprehension, or broad knowledge question answering. The ﬁrst, PhysicalQA (PIQA) [BZB+19],\nasks common sense questions about how the physical world works and is intended as a probe of grounded understanding\nof the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot\n(the last measured on PIQA’s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a"}
{"doc_id": "2005.14165", "para_id": 116, "text": "Fine-tuned SOTA\n90.7a\n89.1b\n74.4c\n93.0d\n90.0e\n93.1e\nGPT-3 Zero-Shot\n81.5\n23.6\n41.5\n59.5\n45.5\n58.4\nGPT-3 One-Shot\n84.0\n34.3\n43.3\n65.4\n45.9\n57.4\nGPT-3 Few-Shot\n85.0\n36.5\n44.3\n69.8\n46.8\n58.1"}
{"doc_id": "2005.14165", "para_id": 117, "text": "Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.\na[JZC+19] b[JN20] c[AI19] d[QIA20] e[SPP+19]"}
{"doc_id": "2005.14165", "para_id": 118, "text": "ﬁne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human\nperformance, but GPT-3’s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis\nﬂagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark\nthe result with an asterisk. See Section 4 for details."}
{"doc_id": "2005.14165", "para_id": 119, "text": "ARC [CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the\n“Challenge” version of the dataset which has been ﬁltered to questions which simple statistical or information retrieval\nmethods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot\nsetting, and 51.5% in the few-shot setting. This is approaching the performance of a ﬁne-tuned RoBERTa baseline\n(55.9%) from UniﬁedQA [KKS+20]. On the “Easy” version of the dataset (questions which either of the mentioned\nbaseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a ﬁne-tuned\nRoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs\nachieved by the UniﬁedQA which exceeds GPT-3’s few-shot results by 27% on the challenge set and 22% on the easy\nset."}
{"doc_id": "2005.14165", "para_id": 120, "text": "On OpenBookQA [MCKS18], GPT-3 improves signiﬁcantly from zero to few shot settings but is still over 20 points\nshort of the overall SOTA. GPT-3’s few-shot performance is similar to a ﬁne-tuned BERT Large baseline on the\nleaderboard."}
{"doc_id": "2005.14165", "para_id": 121, "text": "Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and\ninconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiﬁcant\nimprovement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings."}
{"doc_id": "2005.14165", "para_id": 122, "text": "Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive,\nmultiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread\nin GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general\nwe observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each\nrespective dataset."}
{"doc_id": "2005.14165", "para_id": 123, "text": "GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset\nand performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured\ndialog acts and answer span selections of teacher-student interactions. On DROP [DWD+19], a dataset testing discrete\nreasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the ﬁne-tuned\nBERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches\nwhich augment neural networks with symbolic systems [RLL+19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its\nfew-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to\nslightly outperform the best ﬁne-tuned result in the original paper. On RACE [LXL+17], a multiple choice dataset of\nmiddle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with\nthe earliest work utilizing contextual representations and is still 45% behind SOTA."}
{"doc_id": "2005.14165", "para_id": 124, "text": "In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a\nmore systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark\n[WPN+19] [WPN+19] [CLC+19] [DMST19] [RBG11] [KCR+18] [ZLL+18] [DGM06] [BHDD+06] [GMDD07]\n[BDD+09] [PCC18] [PHR+18]. GPT-3’s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the\nfew-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC"}
{"doc_id": "2005.14165", "para_id": 125, "text": "Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting,\nonly a few points behind measured human performance and state-of-the-art ﬁne-tuned models. Zero-shot and one-shot\nperformance is a few points behind, with the gains to few-shot being largest for bigger models."}
{"doc_id": "2005.14165", "para_id": 126, "text": "SuperGLUE\nBoolQ\nCB\nCB\nCOPA\nRTE\nAverage\nAccuracy\nAccuracy\nF1\nAccuracy\nAccuracy"}
{"doc_id": "2005.14165", "para_id": 127, "text": "Fine-tuned SOTA\n89.0\n91.0\n96.9\n93.9\n94.8\n92.5\nFine-tuned BERT-Large\n69.0\n77.4\n83.6\n75.7\n70.6\n71.7\nGPT-3 Few-Shot\n71.8\n76.4\n75.6\n52.0\n92.0\n69.0"}
{"doc_id": "2005.14165", "para_id": 128, "text": "WiC\nWSC\nMultiRC\nMultiRC\nReCoRD\nReCoRD\nAccuracy\nAccuracy\nAccuracy\nF1a\nAccuracy\nF1"}
{"doc_id": "2005.14165", "para_id": 129, "text": "Fine-tuned SOTA\n76.1\n93.8\n62.3\n88.2\n92.5\n93.3\nFine-tuned BERT-Large\n69.6\n64.6\n24.1\n70.0\n71.3\n72.0\nGPT-3 Few-Shot\n49.4\n80.1\n30.5\n75.4\n90.2\n91.1"}
{"doc_id": "2005.14165", "para_id": 130, "text": "Table 3.8: Performance of GPT-3 on SuperGLUE compared to ﬁne-tuned baselines and SOTA. All results are reported\non the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient\nupdates."}
{"doc_id": "2005.14165", "para_id": 131, "text": "Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value\nof K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in\nSuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference\nlines (our test set results are in Table 3.8). The BERT-Large reference model was ﬁne-tuned on the SuperGLUE training\nset (125K examples), whereas BERT++ was ﬁrst ﬁne-tuned on MultiNLI (392K examples) and SWAG (113K examples)\nbefore further ﬁne-tuning on the SuperGLUE training set (for a total of 630K ﬁne-tuning examples). We ﬁnd the\ndifference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between\nGPT-3 with one example per context versus eight examples per context."}
{"doc_id": "2005.14165", "para_id": 132, "text": "and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we\nused the same set of randomly drawn examples from the training set as context for all of the problems we evaluated."}
{"doc_id": "2005.14165", "para_id": 133, "text": "We observe a wide range in GPT-3’s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA\nperformance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving\nsecond place on the leaderboard, where ﬁrst place is held by a ﬁne-tuned 11 billion parameter model (T5). On WSC,\nperformance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the\noriginal Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable,\nroughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting."}
{"doc_id": "2005.14165", "para_id": 134, "text": "WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different\nphrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two\nsentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer\nin the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot\nsetting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same\nway in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another.\nThis could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these\nweaknesses, GPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to\nthe state-of-the-art held by a ﬁne-tuned 11 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 135, "text": "Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of\nexamples in the context showing increasing beneﬁts from in-context learning (Figure 3.8). We scale K up to 32\nexamples per task, after which point additional examples will not reliably ﬁt into our context. When sweeping over\nvalues of K, we ﬁnd that GPT-3 requires less than eight total examples per task to outperform a ﬁne-tuned BERT-Large\non overall SuperGLUE score."}
{"doc_id": "2005.14165", "para_id": 136, "text": "Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences.\nIn practice, this task is usually structured as a two or three class classiﬁcation problem where the model classiﬁes"}
{"doc_id": "2005.14165", "para_id": 137, "text": "Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples\nand therefore has high variance (we estimate a standard deviation of 1.2%). We ﬁnd that smaller models hover around\nrandom chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for\nANLI rounds 1 and 2 are shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 138, "text": "whether the second sentence logically follows from the ﬁrst, contradicts the ﬁrst sentence, or is possibly true (neutral).\nSuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest\nversion of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting\nGPT-3 performs similarly to a single-task ﬁne-tuned BERT Large. We also evaluate on the recently introduced\nAdversarial Natural Language Inference (ANLI) dataset [NWD+19]. ANLI is a difﬁcult dataset employing a series of\nadversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our\nmodels smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting (∼33%),\nwhereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results\nfor all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difﬁcult\ntask for language models and they are only just beginning to show signs of progress."}
{"doc_id": "2005.14165", "para_id": 139, "text": "One way to probe GPT-3’s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which\nrequire it to perform simple on-the-ﬂy computational reasoning, recognize a novel pattern that is unlikely to have\noccurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we\ntest GPT-3’s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the\nletters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3’s ability to\nsolve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new\nwords in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets\nwith the hope of stimulating further study of test-time behavior of language models."}
{"doc_id": "2005.14165", "para_id": 140, "text": "To test GPT-3’s ability to perform simple arithmetic operations without task-speciﬁc training, we developed a small\nbattery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:"}
{"doc_id": "2005.14165", "para_id": 141, "text": "• 2 digit addition (2D+) – The model is asked to add two integers sampled uniformly from [0, 100), phrased in\nthe form of a question, e.g. “Q: What is 48 plus 76? A: 124.”\n• 2 digit subtraction (2D-) – The model is asked to subtract two integers sampled uniformly from [0, 100); the\nanswer may be negative. Example: “Q: What is 34 minus 53? A: -19”.\n• 3 digit addition (3D+) – Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 142, "text": "Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a\nsigniﬁcant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being\nable to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signiﬁcant fraction\nof the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot\nare shown in the appendix."}
{"doc_id": "2005.14165", "para_id": 143, "text": "• 3 digit subtraction (3D-) – Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000)."}
{"doc_id": "2005.14165", "para_id": 144, "text": "• 4 digit addition (4D+) – Same as 3 digit addition, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 145, "text": "• 4 digit subtraction (4D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 10000)."}
{"doc_id": "2005.14165", "para_id": 146, "text": "• 5 digit addition (5D+) – Same as 3 digit addition, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 147, "text": "• 5 digit subtraction (5D-) – Same as 3 digit subtraction, except uniformly sampled from [0, 100000)."}
{"doc_id": "2005.14165", "para_id": 148, "text": "• 2 digit multiplication (2Dx) – The model is asked to multiply two integers sampled uniformly from [0, 100),\ne.g. “Q: What is 24 times 42? A: 1008”."}
{"doc_id": "2005.14165", "para_id": 149, "text": "• One-digit composite (1DC) – The model is asked to perform a composite operation on three 1 digit numbers,\nwith parentheses around the last two. For example, “Q: What is 6+(4*8)? A: 38”. The three 1 digit numbers\nare selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}."}
{"doc_id": "2005.14165", "para_id": 150, "text": "In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random\ninstances of the task and evaluate all models on those instances."}
{"doc_id": "2005.14165", "para_id": 151, "text": "First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction,\nGPT-3 displays strong proﬁciency when the number of digits is small, achieving 100% accuracy on 2 digit addition,\n98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the\nnumber of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on\nﬁve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves\n29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves\n21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness\nbeyond just single operations."}
{"doc_id": "2005.14165", "para_id": 152, "text": "As Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the\nsecond largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all\nother operations less than 10% of the time."}
{"doc_id": "2005.14165", "para_id": 153, "text": "One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation\nto the task (or at the very least recognition of the task) is important to performing these computations correctly.\nNevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signiﬁcantly"}
{"doc_id": "2005.14165", "para_id": 154, "text": "GPT-3 Zero-shot\n76.9\n58.0\n34.2\n48.3\n4.0\n7.5\n0.7\n0.8\n19.8\n9.8\nGPT-3 One-shot\n99.6\n86.4\n65.5\n78.7\n14.0\n14.0\n3.5\n3.8\n27.4\n14.3\nGPT-3 Few-shot\n100.0\n98.9\n80.4\n94.2\n25.5\n26.8\n9.3\n9.9\n29.2\n21.3"}
{"doc_id": "2005.14165", "para_id": 155, "text": "Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or\nsubtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger\nmoving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signiﬁcant arithmetic abilities."}
{"doc_id": "2005.14165", "para_id": 156, "text": "GPT-3 Zero-shot\n3.66\n2.28\n8.91\n8.26\n0.09\nGPT-3 One-shot\n21.7\n8.62\n25.9\n45.4\n0.48\nGPT-3 Few-shot\n37.9\n15.1\n39.7\n67.2\n0.44"}
{"doc_id": "2005.14165", "para_id": 157, "text": "Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and\nfew-shot settings. CL is “cycle letters in word”, A1 is anagrams of but the ﬁrst and last letters, A2 is anagrams of all but\nthe ﬁrst and last two letters, RI is “Random insertion in word”, RW is “reversed words”."}
{"doc_id": "2005.14165", "para_id": 158, "text": "outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and\nmodel capacity scaling for all three settings is shown in Appendix H."}
{"doc_id": "2005.14165", "para_id": 159, "text": "To spot-check whether the model is simply memorizing speciﬁc arithmetic problems, we took the 3-digit arithmetic\nproblems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and\n\"<NUM1> plus <NUM2>\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000\nsubtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers\ncould have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes\nsuch as not carrying a “1”, suggesting it is actually attempting to perform the relevant computation rather than\nmemorizing a table."}
{"doc_id": "2005.14165", "para_id": 160, "text": "Overall, GPT-3 displays reasonable proﬁciency at moderately complex arithmetic in few-shot, one-shot, and even\nzero-shot settings."}
{"doc_id": "2005.14165", "para_id": 161, "text": "To test GPT-3’s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of\n5 “character manipulation” tasks. Each task involves giving the model a word distorted by some combination of\nscrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:"}
{"doc_id": "2005.14165", "para_id": 162, "text": "• Cycle letters in word (CL) – The model is given a word with its letters cycled, then the “=” symbol, and\nis expected to generate the original word. For example, it might be given “lyinevitab” and should output\n“inevitably”."}
{"doc_id": "2005.14165", "para_id": 163, "text": "• Anagrams of all but ﬁrst and last characters (A1) – The model is given a word where every letter except\nthe ﬁrst and last have been scrambled randomly, and must output the original word. Example: criroptuon =\ncorruption."}
{"doc_id": "2005.14165", "para_id": 164, "text": "• Anagrams of all but ﬁrst and last 2 characters (A2) – The model is given a word where every letter except\nthe ﬁrst 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt\n→opponent."}
{"doc_id": "2005.14165", "para_id": 165, "text": "• Random insertion in word (RI) – A random punctuation or space character is inserted between each letter\nof a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession."}
{"doc_id": "2005.14165", "para_id": 166, "text": "• Reversed words (RW) – The model is given a word spelled backwards, and must output the original word.\nExample: stcejbo →objects."}
{"doc_id": "2005.14165", "para_id": 167, "text": "For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by\n[Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11.\nTask performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing"}
{"doc_id": "2005.14165", "para_id": 168, "text": "Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally\nsmooth improvement with model size although the random insertion task shows an upward slope of improvement with\nthe 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\nthe appendix. All tasks are done with K = 100."}
{"doc_id": "2005.14165", "para_id": 169, "text": "random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult anagram\ntask (where only the ﬁrst and last letters are held ﬁxed). None of the models can reverse the letters in a word."}
{"doc_id": "2005.14165", "para_id": 170, "text": "In the one-shot setting, performance is signiﬁcantly weaker (dropping by half or more), and in the zero-shot setting the\nmodel can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these\ntasks at test time, as the model cannot perform them zero-shot and their artiﬁcial nature makes them unlikely to appear\nin the pre-training data (although we cannot conﬁrm this with certainty)."}
{"doc_id": "2005.14165", "para_id": 171, "text": "We can further quantify performance by plotting “in-context learning curves”, which show task performance as a\nfunction of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task\nin Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information,\nincluding both task examples and natural language task descriptions."}
{"doc_id": "2005.14165", "para_id": 172, "text": "Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding\noperates on signiﬁcant fractions of a word (on average ∼0.7 words per token), so from the LM’s perspective succeeding\nat these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also,\nCL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word),\nrequiring the model to perform some search to ﬁnd the correct unscrambling. Thus, the skills involved appear to require\nnon-trivial pattern-matching and computation."}
{"doc_id": "2005.14165", "para_id": 173, "text": "To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of\n374 “SAT analogy” problems [TLBS03]. Analogies are a style of multiple choice question that constituted a section of\nthe SAT college entrance exam before 2005. A typical example is “audacious is to boldness as (a) sanctimonious is to\nhypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to\ntemptation”. The student is expected to choose which of the ﬁve word pairs has the same relationship as the original\nword pair; in this example the answer is “sanctimonious is to hypocrisy”. On this task GPT-3 achieves 65.2% in the\nfew-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among\ncollege applicants was 57% [TL05] (random guessing yields 20%). As shown in Figure 3.12, the results improve with\nscale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model."}
{"doc_id": "2005.14165", "para_id": 174, "text": "Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest\nmodel achieves 65% accuracy in the few-shot setting, and also demonstrates signiﬁcant gains to in-context learning\nwhich are not present in smaller models."}
{"doc_id": "2005.14165", "para_id": 175, "text": "Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by\nconditional sampling from the model given a human-written prompt consisting of a plausible ﬁrst sentence for a news\nstory [RWC+19]. Relative to [RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles,\nso trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets\nthe proposed ﬁrst sentence of a “news article” as a tweet and then posts synthetic responses or follow-up tweets. To\nsolve this problem we employed GPT-3’s few-shot learning abilities by providing three previous news articles in the\nmodel’s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably\ngenerate short articles in the “news” genre."}
{"doc_id": "2005.14165", "para_id": 176, "text": "To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional\nsample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles\nfrom real ones. Similar work has been carried out by Kreps et al. [KMB20] and Zellers et al. [ZHR+19]. Generative\nlanguage models are trained to match the distribution of content generated by humans, so the (in)ability of humans to\ndistinguish the two is a potentially important measure of quality.3"}
{"doc_id": "2005.14165", "para_id": 177, "text": "In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles\nfrom the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles\nfrom four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each\nmodel, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed\nby either the human written article or the article generated by the model4. Participants were asked to select whether the\narticle was “very likely written by a human”, “more likely written by a human”, “I don’t know”, “more likely written by\na machine”, or “very likely written by a machine”."}
{"doc_id": "2005.14165", "para_id": 178, "text": "The articles we selected were not in the models’ training data and the model outputs were formatted and selected\nprogrammatically to prevent human cherry-picking. All models used the same context to condition outputs on and were\npre-trained with the same context size and the same article titles and subtitles were used as prompts for each model.\nHowever, we also ran an experiment to control for participant effort and attention that followed the same format but\ninvolved intentionally bad model generated articles. This was done by generating articles from a “control model”: a\n160M parameter model with no context and increased output randomness."}
{"doc_id": "2005.14165", "para_id": 179, "text": "3This task is also relevant to the potential misuse of language models discussed in Section 6.1.\n4We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on\nparticipants drawn from the general US population. See Appendix E for details."}
{"doc_id": "2005.14165", "para_id": 180, "text": "Mean accuracy\n95% Conﬁdence\nInterval (low, hi)\nt compared to\ncontrol (p-value)\n“I don’t know”\nassignments"}
{"doc_id": "2005.14165", "para_id": 181, "text": "Control (deliberately bad model)\n86%\n83%–90%\n-\n3.6 %\nGPT-3 Small\n76%\n72%–80%\n3.9 (2e-4)\n4.9%\nGPT-3 Medium\n61%\n58%–65%\n10.3 (7e-21)\n6.0%\nGPT-3 Large\n68%\n64%–72%\n7.3 (3e-11)\n8.7%\nGPT-3 XL\n62%\n59%–65%\n10.7 (1e-19)\n7.5%\nGPT-3 2.7B\n62%\n58%–65%\n10.4 (5e-19)\n7.1%\nGPT-3 6.7B\n60%\n56%–63%\n11.2 (3e-21)\n6.2%\nGPT-3 13B\n55%\n52%–58%\n15.3 (1e-32)\n7.1%\nGPT-3 175B\n52%\n49%–54%\n16.9 (1e-34)\n7.8%"}
{"doc_id": "2005.14165", "para_id": 182, "text": "Table 3.11: Human accuracy in identifying whether short (∼200 word) news articles are model generated. We\nﬁnd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86%\non the control model to 52% on GPT-3 175B. This table compares mean accuracy between ﬁve different models, and\nshows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model\n(an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 183, "text": "Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that\nthe intentionally bad articles were model generated was ∼86% where 50% is chance level performance. By contrast,\nmean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance\nat ∼52% (see Table 3.11).5 Human abilities to detect model generated text appear to decrease as model size increases:\nthere appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6\nThis is true despite the fact that participants spend more time on each output as model size increases (see Appendix E)."}
{"doc_id": "2005.14165", "para_id": 184, "text": "Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7 Much of the text is—as indicated by the\nevaluations—difﬁcult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator\nthat an article is model generated since, unlike human authors, the models have no access to the speciﬁc facts that the\narticle titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual\nphrasings, though these are often subtle enough that they are not noticed."}
{"doc_id": "2005.14165", "para_id": 185, "text": "Related work on language model detection by Ippolito et al. [IDCBE19] indicates that automatic discriminators like\nG ROV E R [ZHR+19] and GLTR [GSR19] may have greater success at detecting model generated text than human\nevaluators. Automatic detection of these models may be a promising area of future research."}
{"doc_id": "2005.14165", "para_id": 186, "text": "Ippolito et al. [IDCBE19] also note that human accuracy at detecting model generated text increases as humans observe\nmore tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated\nby GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated\ncompletions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial\nexperiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to\ncompare human abilities to detect the articles generated by GPT-3 and a control model."}
{"doc_id": "2005.14165", "para_id": 187, "text": "We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was\n∼88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely\nabove chance at ∼52% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3\ncontinues to produce articles that humans ﬁnd difﬁcult to distinguish from human written news articles."}
{"doc_id": "2005.14165", "para_id": 188, "text": "A task studied in developmental linguistics [CB78] is the ability to learn and utilize new words, for example using a\nword in a sentence after seeing it deﬁned only once, or conversely inferring a word’s meaning from only one usage. Here\nwe qualitatively test GPT-3’s ability to do the former. Speciﬁcally, we give GPT-3 the deﬁnition of a nonexistent word,\nsuch as “Gigamuru”, and then ask it to use it in a sentence. We provide one to ﬁve previous examples of a (separate)"}
{"doc_id": "2005.14165", "para_id": 189, "text": "5We use a two-sample Student’s T-Test to test for signiﬁcant difference between the means of the participant accuracies of each\nmodel and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.\n6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on\nthis task would drop below 50%. Indeed, many individual participants scored below 50% on this task.\n7Additional non-news samples can be found in Appendix F."}
{"doc_id": "2005.14165", "para_id": 190, "text": "Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct\nassignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-\nbad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed\nline at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best ﬁt is a power\nlaw with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 191, "text": "Mean accuracy\n95% Conﬁdence\nInterval (low, hi)\nt compared to\ncontrol (p-value)\n“I don’t know”\nassignments"}
{"doc_id": "2005.14165", "para_id": 192, "text": "Control\n88%\n84%–91%\n-\n2.7%\nGPT-3 175B\n52%\n48%–57%\n12.7 (3.2e-23)\n10.6%"}
{"doc_id": "2005.14165", "para_id": 193, "text": "Table 3.12: People’s ability to identify whether ∼500 word articles are model generated (as measured by the ratio of\ncorrect assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table\nshows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control\nmodel (an unconditional GPT-3 Small model with increased output randomness)."}
{"doc_id": "2005.14165", "para_id": 194, "text": "Title:\nUnited Methodists Agree to Historic Split\nSubtitle:\nThose who oppose gay marriage will form their own denomination\nArticle:\nAfter two days of intense debate, the United Methodist Church\nhas agreed to a historic split - one that is expected to end in the\ncreation of a new denomination, one that will be \"theologically and\nsocially conservative,\" according to The Washington Post.\nThe majority of\ndelegates attending the church's annual General Conference in May voted to\nstrengthen a ban on the ordination of LGBTQ clergy and to write new rules\nthat will \"discipline\" clergy who officiate at same-sex weddings.\nBut\nthose who opposed these measures have a new plan:\nThey say they will form a\nseparate denomination by 2020, calling their church the Christian Methodist\ndenomination.\nThe Post notes that the denomination, which claims 12.5 million members, was\nin the early 20th century the \"largest Protestant denomination in the U.S.,\"\nbut that it has been shrinking in recent decades.\nThe new split will be the\nsecond in the church's history.\nThe first occurred in 1968, when roughly\n10 percent of the denomination left to form the Evangelical United Brethren\nChurch.\nThe Post notes that the proposed split \"comes at a critical time\nfor the church, which has been losing members for years,\" which has been\n\"pushed toward the brink of a schism over the role of LGBTQ people in the\nchurch.\" Gay marriage is not the only issue that has divided the church.\nIn\n2016, the denomination was split over ordination of transgender clergy, with\nthe North Pacific regional conference voting to ban them from serving as\nclergy, and the South Pacific regional conference voting to allow them."}
{"doc_id": "2005.14165", "para_id": 195, "text": "Figure 3.14: The GPT-3 generated news article that humans had the greatest difﬁculty distinguishing from a human\nwritten article (accuracy: 12%)."}
{"doc_id": "2005.14165", "para_id": 196, "text": "Title:\nStar’s Tux Promise Draws Megyn Kelly’s Sarcasm\nSubtitle:\nJoaquin Phoenix pledged to not change for each awards event\nArticle:\nA year ago, Joaquin Phoenix made headlines when he appeared on\nthe red carpet at the Golden Globes wearing a tuxedo with a paper bag over\nhis head that read, \"I am a shape-shifter.\nI can't change the world.\nI\ncan only change myself.\" It was a promise to not change to fit into the\nHollywood mold:\n\"I think that's a really special thing, to not change\nyourself.\nI think it's a really special thing to say, `This is what's\ninside of me, I'm proud of it, and I'm not going to be ashamed because of\nthe way that someone else thinks I should be.'\" Now, it's the Oscars, and\nPhoenix is at it again.\nBut this time, his publicist is saying he'll be\nwearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show.\n\"You know, I feel like, I feel like you could have worn the tux,\" she says.\n\"But you're saying you're a shape-shifter.\nI don't know if you can change\nyour tux, but you can change your mind.\nYou can change your mind.\nYou can\nchange your mind.\" Phoenix says he did, but it didn't stick.\n\"I was like,\n`Okay, I'm going to wear a tuxedo to this thing.'\nAnd then I thought, `I\ndon't want to wear a tuxedo to this thing.'\" Kelly goes on to encourage him\nto change his mind again, but Phoenix says it's too late:\n\"I'm committed to\nwearing this.\""}
{"doc_id": "2005.14165", "para_id": 197, "text": "Figure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written\narticle (accuracy: 61%)."}
{"doc_id": "2005.14165", "para_id": 198, "text": "A \"whatpu\" is a small, furry animal native to Tanzania.\nAn example of a sentence that uses\nthe word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus."}
{"doc_id": "2005.14165", "para_id": 199, "text": "To do a \"farduddle\" means to jump up and down really fast.\nAn example of a sentence that uses\nthe word farduddle is:\nOne day when I was playing tag with my little sister, she got really excited and she\nstarted doing these crazy farduddles.\nA \"yalubalu\" is a type of vegetable that looks like a big pumpkin.\nAn example of a sentence\nthat uses the word yalubalu is:\nI was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden\nthere.\nIt was delicious."}
{"doc_id": "2005.14165", "para_id": 200, "text": "A \"Burringo\" is a car with very fast acceleration.\nAn example of a sentence that uses the\nword Burringo is:\nIn our garage we have a Burringo that my father drives to work every day."}
{"doc_id": "2005.14165", "para_id": 201, "text": "A \"Gigamuru\" is a type of Japanese musical instrument.\nAn example of a sentence that uses the\nword Gigamuru is:\nI have a Gigamuru that my uncle gave me as a gift.\nI love to play it at home."}
{"doc_id": "2005.14165", "para_id": 202, "text": "To \"screeg\" something is to swing a sword at it.\nAn example of a sentence that uses the word\nscreeg is:\nWe screeghed at each other for several minutes and then we went outside and ate ice cream."}
{"doc_id": "2005.14165", "para_id": 203, "text": "Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is\nGPT-3’s completions, plain text is human prompts. In the ﬁrst example both the prompt and the completion are provided\nby a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional\nprompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 other than the conditioning shown\nhere."}
{"doc_id": "2005.14165", "para_id": 204, "text": "nonexistent word being deﬁned and used in a sentence, so the task is few-shot in terms of previous examples of the\nbroad task and one-shot in terms of the speciﬁc word. Table 3.16 shows the 6 examples we generated; all deﬁnitions\nwere human-generated, and the ﬁrst answer was human-generated as conditioning while the subsequent answers were\ngenerated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try\nany prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the ﬁnal\nsentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of\nthe word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy\nsword ﬁght. Overall, GPT-3 appears to be at least proﬁcient at the task of using novel words in a sentence."}
{"doc_id": "2005.14165", "para_id": 205, "text": "Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-\nshot setting by giving prompts of the form \"Poor English Input:\n<sentence>\\n Good English Output:\n<sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any\nomissions or repeats). Results are shown in Figure 3.17."}
{"doc_id": "2005.14165", "para_id": 206, "text": "4\nMeasuring and Preventing Memorization Of Benchmarks"}
{"doc_id": "2005.14165", "para_id": 207, "text": "Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our\nbenchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research\nwithout established best practices. While it is common practice to train large models without investigating contamination,\ngiven the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to."}
{"doc_id": "2005.14165", "para_id": 208, "text": "This concern is not just hypothetical. One of the ﬁrst papers to train a language model on Common Crawl data [TL18]\ndetected and removed a training document which overlapped with one of their evaluation datasets. Other work such\nas GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, ﬁnding that"}
{"doc_id": "2005.14165", "para_id": 209, "text": "Poor English input:\nI eated the purple berries.\nGood English output:\nI ate the purple berries.\nPoor English input:\nThank you for picking me as your designer.\nI’d appreciate it.\nGood English output:\nThank you for choosing me as your designer.\nI appreciate it.\nPoor English input:\nThe mentioned changes have done.\nor I did the alteration that you\nrequested.\nor I changed things you wanted and did the modifications.\nGood English output:\nThe requested changes have been made.\nor I made the alteration that you\nrequested.\nor I changed things you wanted and made the modifications.\nPoor English input:\nI’d be more than happy to work with you in another project.\nGood English output:\nI'd be more than happy to work with you on another project."}
{"doc_id": "2005.14165", "para_id": 210, "text": "Poor English input:\nPlease provide me with a short brief of the design you’re looking for and\nthat’d be nice if you could share some examples or project you did before.\nGood English output:\nPlease provide me with a brief description of the design you're\nlooking for and that would be nice if you could share some examples or projects you have\ndone before."}
{"doc_id": "2005.14165", "para_id": 211, "text": "Poor English input:\nThe patient was died.\nGood English output:\nThe patient died."}
{"doc_id": "2005.14165", "para_id": 212, "text": "Poor English input:\nWe think that Leslie likes ourselves.\nGood English output:\nWe think that Leslie likes us."}
{"doc_id": "2005.14165", "para_id": 213, "text": "Poor English input:\nJanet broke Bill on the finger.\nGood English output:\nJanet broke Bill's finger."}
{"doc_id": "2005.14165", "para_id": 214, "text": "Poor English input:\nMary arranged for, in St.\nLouis, John to rent a house cheap.\nGood English output:\nMary arranged for John to rent a house in St.\nLouis."}
{"doc_id": "2005.14165", "para_id": 215, "text": "Poor English input:\nToday I have went to the store to to buys some many bottle of water.\nGood English output:\nToday I went to the store to buy some bottles of water."}
{"doc_id": "2005.14165", "para_id": 216, "text": "Poor English input:\nI have tried to hit ball with bat, but my swing is has miss.\nGood English output:\nI tried to hit the ball with the bat, but my swing missed."}
{"doc_id": "2005.14165", "para_id": 217, "text": "Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface\nis GPT-3’s completions, plain text is human prompts. In the ﬁrst few examples example both the prompt and the\ncompletion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives\nsuccessive additional prompts and provides the completions. Nothing task-speciﬁc is provided to GPT-3 aside from\nthe ﬁrst few examples as conditioning and the “Poor English input/Good English output” framing. We note that the\ndistinction between ”poor” and ”good” English (and the terms themselves) is complex, contextual, and contested. As\nthe example mentioning the rental of a house shows, assumptions that the model makes about what “good” is can even\nlead it to make errors (here, the model not only adjusts grammar, but also removes the word ”cheap” in a way that alters\nmeaning)."}
{"doc_id": "2005.14165", "para_id": 218, "text": "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\nsplit of our training distribution. Though there is some gap between training and validation performance, the gap grows\nonly minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\nrather than overﬁtting."}
{"doc_id": "2005.14165", "para_id": 219, "text": "although models did perform moderately better on data that overlapped between training and testing, this did not\nsigniﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent)."}
{"doc_id": "2005.14165", "para_id": 220, "text": "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\nmagnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\nfor contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\ndoes not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with which it was\ndeduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as\nlarge as feared."}
{"doc_id": "2005.14165", "para_id": 221, "text": "We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap\nbetween our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a\nbug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t\nfeasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts\nresults."}
{"doc_id": "2005.14165", "para_id": 222, "text": "For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, deﬁned roughly as\nexamples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when\nit is shorter than 13-grams). The goal is to very conservatively ﬂag anything that could potentially be contamination,\nso as to produce a clean subset that is free of contamination with high conﬁdence. The exact procedure is detailed in\nAppendix C."}
{"doc_id": "2005.14165", "para_id": 223, "text": "We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\nsubset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a\nsigniﬁcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be\ninﬂating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a\nquarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence\nthat contamination level and performance difference are correlated. We conclude that either our conservative method\nsubstantially overestimated contamination or that contamination has little effect on performance."}
{"doc_id": "2005.14165", "para_id": 224, "text": "Below, we review in more detail the few speciﬁc cases where either (1) the model performs signiﬁcantly worse on\nthe cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference\ndifﬁcult."}
{"doc_id": "2005.14165", "para_id": 225, "text": "Our analysis ﬂagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension\n(QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English"}
{"doc_id": "2005.14165", "para_id": 226, "text": "Figure 4.2: Benchmark contamination analysis\nWe constructed cleaned versions of each of our benchmarks to\ncheck for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the\ndataset is known with high conﬁdence to be clean, and the y-axis shows the difference in performance when evaluating\nonly on the veriﬁed clean subset. Performance on most benchmarks changed negligibly, but some were ﬂagged for\nfurther review. On inspection we ﬁnd some evidence for contamination of the PIQA and Winograd results, and we mark\nthe corresponding results in Section 3 with an asterisk. We ﬁnd no evidence that other benchmarks are affected."}
{"doc_id": "2005.14165", "para_id": 227, "text": "translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false\npositives. We summarize the results for each group of tasks below:"}
{"doc_id": "2005.14165", "para_id": 228, "text": "• Reading Comprehension: Our initial analysis ﬂagged >90% of task examples from QuAC, SQuAD2, and\nDROP as potentially contaminated, so large that even measuring the differential on a clean subset was difﬁcult.\nUpon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source\ntext was present in our training data but the question/answer pairs were not, meaning the model gains only\nbackground information and cannot memorize the answer to a speciﬁc question.\n• German translation: We found 25% of the examples in the WMT16 German-English test set were marked\nas potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the\nﬂagged examples contain paired sentences resembling NMT training data and collisions were monolingual\nmatches mostly of snippets of events discussed in the news.\n• Reversed Words and Anagrams: Recall that these tasks are of the form “alaok = koala”. Due to the\nshort length of these tasks, we used 2-grams for ﬁltering (ignoring punctuation). After inspecting the ﬂagged\noverlaps, we found that they were not typically instances of real reversals or unscramblings in the training set,\nbut rather palindromes or trivial unscramblings, e.g “kayak = kayak”. The amount of overlap was small,\nbut removing the trivial tasks lead to an increase in difﬁculty and thus a spurious signal. Related to this, the\nsymbol insertion task shows high overlap but no effect on performance – this is because that task involves\nremoving non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to\nmany spurious matches.\n• PIQA: The overlap analysis ﬂagged 29% of examples as contaminated, and observed a 3 percentage point\nabsolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was\nreleased after our training set was created and its labels are hidden, some of the web pages used by the\ncrowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller\nmodel with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias\nrather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot\nrigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential\ncontamination.\n• Winograd: The overlap analysis ﬂagged 45% of examples, and found a 2.6% decrease in performance on the\nclean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in\nfact present in our training set, though presented in a different format than we present the task to the model.\nAlthough the decrease in performance is small, we mark our Winograd results in the main paper with an\nasterisk."}
{"doc_id": "2005.14165", "para_id": 229, "text": "• Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the\nChildren’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably\nextract a clean subset here, we do not report results on these datasets, even though we intended to when starting\nthis work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language\nmodeling benchmark."}
{"doc_id": "2005.14165", "para_id": 230, "text": "We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply\nto verify how much actual contamination existed. These appeared to often contain false positives. They had either\nno actual contamination, or had contamination that did not give away the answer to the task. One notable exception\nwas LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very\nsmall, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our ﬁll-in-the-blank format\nprecludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this\npaper, the potential contamination is noted in the results section."}
{"doc_id": "2005.14165", "para_id": 231, "text": "An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the\nsame distribution as the original dataset. It remains possible that memorization inﬂates results but at the same time\nis precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number\nof shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small\nmodels, which are unlikely to be memorizing."}
{"doc_id": "2005.14165", "para_id": 232, "text": "Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright\nremove problematic results, depending on the severity. Much work remains to be done to address this important and\nsubtle issue for the ﬁeld in general, both when designing benchmarks and when training models. For a more detailed\nexplanation of our analysis, we refer the reader to Appendix C."}
{"doc_id": "2005.14165", "para_id": 233, "text": "GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for\nfuture work."}
{"doc_id": "2005.14165", "para_id": 234, "text": "First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct\npredecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although\nthe overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to\nlose coherence over sufﬁciently long passages, contradict themselves, and occasionally contain non-sequitur sentences\nor paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of\nGPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed\ninformally that GPT-3 seems to have special difﬁculty with “common sense physics”, despite doing well on some\ndatasets (such as PIQA [BZB+19]) that test this domain. Speciﬁcally GPT-3 has difﬁculty with questions of the type\n“If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable\ngaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when\nevaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same\nway in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading\ncomprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks."}
{"doc_id": "2005.14165", "para_id": 235, "text": "GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused\non exploring in-context learning behavior in autoregressive language models because it is straightforward to both\nsample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional\narchitectures or other training objectives such as denoising. This is a noticeable difference from much of the recent\nliterature, which has documented improved ﬁne-tuning performance when using these approaches over standard\nlanguage models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks\nwhich empirically beneﬁt from bidirectionality. This may include ﬁll-in-the-blank tasks, tasks that involve looking back\nand comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then\ngenerating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a\nfew of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves\ncomparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and\nRACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at ﬁne-tuning\nthan GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with\nfew- or zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”."}
{"doc_id": "2005.14165", "para_id": 236, "text": "A more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether\nautoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the"}
{"doc_id": "2005.14165", "para_id": 237, "text": "pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to\npredict and what is less important. [RRS20] demonstrate beneﬁts of customizing prediction to entities of interest. Also,\nwith self-supervised objectives, task speciﬁcation relies on forcing the desired task into a prediction problem, whereas\nultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed\nactions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains\nof experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world\n[BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a\ndifferent approach is likely to be necessary. Promising future directions in this vein might include learning the objective\nfunction from humans [ZSW+19a], ﬁne-tuning with reinforcement learning, or adding additional modalities such as\nimages to provide grounding and a better model of the world [CLY+19]."}
{"doc_id": "2005.14165", "para_id": 238, "text": "Another limitation broadly shared by language models is poor sample efﬁciency during pre-training. While GPT-3\ntakes a step towards test-time sample efﬁciency closer to that of humans (one-shot or zero-shot), it still sees much more\ntext during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efﬁciency is\nan important direction for future work, and might come from grounding in the physical world to provide additional\ninformation, or from algorithmic improvements."}
{"doc_id": "2005.14165", "para_id": 239, "text": "A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot\nlearning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identiﬁes tasks that it\nhas learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that\nare drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format,\nto adapting to a speciﬁc style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on\nthis spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or deﬁning nonsense words\nseem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although\npossibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what\nhumans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training\nand identifying them at test time would be an advance for language models, but nevertheless understanding precisely\nhow few-shot learning works is an important unexplored direction for future research."}
{"doc_id": "2005.14165", "para_id": 240, "text": "A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are\nboth expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of\nmodels of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large\nmodels down to a manageable size for speciﬁc tasks. Large models such as GPT-3 contain a very wide range of skills,\nmost of which are not needed for a speciﬁc task, suggesting that in principle aggressive distillation may be possible.\nDistillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters;\nnew challenges and opportunities may be associated with applying it to models of this size."}
{"doc_id": "2005.14165", "para_id": 241, "text": "Finally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable,\nit is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in\nperformance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This\nlast issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special\nconcern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts\n(Section 6)."}
{"doc_id": "2005.14165", "para_id": 242, "text": "Language models have a wide range of beneﬁcial applications for society, including code and writing auto-completion,\ngrammar assistance, game narrative generation, improving search engine responses, and answering questions. But\nthey also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over\nsmaller models and increases the difﬁculty of distinguishing synthetic text from human-written text. It therefore has the\npotential to advance both the beneﬁcial and harmful applications of language models."}
{"doc_id": "2005.14165", "para_id": 243, "text": "Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily\ngreater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this\nare numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in\nSection 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brieﬂy\ndiscuss issues of energy efﬁciency (Section 6.3)."}
{"doc_id": "2005.14165", "para_id": 244, "text": "Malicious uses of language models can be somewhat difﬁcult to anticipate because they often involve repurposing\nlanguage models in a very different environment or for a different purpose than researchers intended. To help with this,\nwe can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying\nthreats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact\n[Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures."}
{"doc_id": "2005.14165", "para_id": 245, "text": "Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples\ninclude misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing\nand social engineering pretexting. Many of these applications bottleneck on human beings to write sufﬁciently high\nquality text. Language models that produce high quality text generation could lower existing barriers to carrying out\nthese activities and increase their efﬁcacy."}
{"doc_id": "2005.14165", "para_id": 246, "text": "The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to\ngenerate several paragraphs of synthetic content that people ﬁnd difﬁcult to distinguish from human-written text in\n3.9.4 represents a concerning milestone in this regard."}
{"doc_id": "2005.14165", "para_id": 247, "text": "Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors\nwho may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced\n(e.g. state-sponsored) groups with long-term agendas [SBC+19]."}
{"doc_id": "2005.14165", "para_id": 248, "text": "To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat\ngroups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did\nﬁnd signiﬁcant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances\nof experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated\nwith media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is\nnot immediate, but signiﬁcant improvements in reliability could change this."}
{"doc_id": "2005.14165", "para_id": 249, "text": "Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about\npossible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible\ndifference in operations that may see potential gains by using language models. The assessment was that language\nmodels may not be worth investing signiﬁcant resources in because there has been no convincing demonstration that\ncurrent language models are signiﬁcantly better than current methods for generating text, and because methods for\n“targeting” or “controlling” the content of language models are still at a very early stage."}
{"doc_id": "2005.14165", "para_id": 250, "text": "Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their\nagenda. TTPs are inﬂuenced by economic factors like scalability and ease of deployment; phishing is extremely popular\namong all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login\ncredentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment."}
{"doc_id": "2005.14165", "para_id": 251, "text": "Ease of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the adoption of TTPs.\nThe outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k\ntruncation) they are not able to perform consistently without human feedback. If a social media disinformation bot\nproduces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the\namount of human labor required in operating this bot. But a human is still needed to ﬁlter the outputs, which restricts\nhow scalable the operation can be."}
{"doc_id": "2005.14165", "para_id": 252, "text": "Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will\neventually develop language models that are sufﬁciently consistent and steerable that they will be of greater interest to\nmalicious actors. We expect this will introduce challenges for the broader research community, and hope to work on\nthis through a combination of mitigation research, prototyping, and coordinating with other technical developers."}
{"doc_id": "2005.14165", "para_id": 253, "text": "Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning,\nsince model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and\nproducing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in\nthe model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation. 8"}
{"doc_id": "2005.14165", "para_id": 254, "text": "Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and\nbehaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely\npresent and could be studied in follow-up work. This is a preliminary analysis and does not reﬂect all of the model’s\nbiases even within the studied categories."}
{"doc_id": "2005.14165", "para_id": 255, "text": "Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reﬂect stereotypes\npresent in their training data. Below we discuss our preliminary ﬁndings of bias along the dimensions of gender, race,\nand religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how\nthey are different in this dimension."}
{"doc_id": "2005.14165", "para_id": 256, "text": "In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found\nthat occupations in general have a higher probability of being followed by a male gender identiﬁer than a female one\n(in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant).\n83% of the 388 occupations we tested were more likely to be followed by a male identiﬁer by GPT-3. We measured\nthis by feeding the model a context such as \"The detective was a\" and then looking at the probability of the\nmodel following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.).\nIn particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus\nwere heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and\nsheriff. Occupations that were more likely to be followed by female identiﬁers include midwife, nurse, receptionist,\nhousekeeper etc."}
{"doc_id": "2005.14165", "para_id": 257, "text": "We also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation}\nwas a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\"\n(Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent\n{occupation} was a,\" the majority of occupations had an even higher probability of being followed by a\nmale identiﬁer than a female one than was the case with our original neutral prompt, \"The {occupation} was\na\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male\nwith a similar probability than for our original neutral prompt.\nThe average occupation bias - measured as\n1\nnjobs\nP"}
{"doc_id": "2005.14165", "para_id": 258, "text": "P (male|Context)) ) - was −1.11 for the Neutral Variant, −2.14 for the Competent Variant and −1.15\nfor the Incompetent Variant."}
{"doc_id": "2005.14165", "para_id": 259, "text": "We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further\ncorroborated the model’s tendency to associate most occupations with males. One method measured the mod-\nels ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model\na context such as \"The advisor met with the advisee because she wanted to get advice about job\napplications.\n‘She’ refers to the\" and found the option with the lowest probability between the two possi-\nble options (Choices between Occupation Option: advisor; Participant Option: advisee)."}
{"doc_id": "2005.14165", "para_id": 260, "text": "Occupation and participant words often have societal biases associated with them such as the assumption that most\noccupants are by default male. We found that the language models learnt some of these biases such as a tendency to\nassociate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of\nall the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences\nwhere the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All\nother models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns\nwith the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers\nsome preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger\nmodels are more robust than smaller models."}
{"doc_id": "2005.14165", "para_id": 261, "text": "We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-\nselected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature"}
{"doc_id": "2005.14165", "para_id": 262, "text": "8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work.\nSee, for example, [HZJ+19, NBR20, SCNP19]."}
{"doc_id": "2005.14165", "para_id": 263, "text": "Table 6.1: Most Biased Descriptive Words in 175B Model"}
{"doc_id": "2005.14165", "para_id": 264, "text": "Top 10 Most Biased Male Descriptive Words with Raw\nCo-Occurrence Counts\nTop 10 Most Biased Female Descriptive Words with Raw\nCo-Occurrence Counts"}
{"doc_id": "2005.14165", "para_id": 265, "text": "Average Number of Co-Occurrences Across All Words:\n17.5\nAverage Number of Co-Occurrences Across All Words:\n23.9"}
{"doc_id": "2005.14165", "para_id": 266, "text": "Large (16)\nOptimistic (12)\nMostly (15)\nBubbly (12)\nLazy (14)\nNaughty (12)\nFantastic (13)\nEasy-going (12)\nEccentric (13)\nPetite (10)\nProtect (10)\nTight (10)\nJolly (10)\nPregnant (10)\nStable (9)\nGorgeous (28)\nPersonable (22)\nSucked (8)\nSurvive (7)\nBeautiful (158)"}
{"doc_id": "2005.14165", "para_id": 267, "text": "of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She\nwas very\", \"He would be described as\", \"She would be described as\"9. We looked at the adjectives and\nadverbs in the top 100 most favored words using an off-the-shelf POS tagger [LB02]. We found females were more\noften described using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men who were\nmore often described using adjectives that span a greater spectrum."}
{"doc_id": "2005.14165", "para_id": 268, "text": "Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each\nword co-occurred with a pronoun indicator. “Most Favored” here indicates words which were most skewed towards a\ncategory by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective,\nwe have also included the average for the number of co-occurrences across all qualifying words for each gender."}
{"doc_id": "2005.14165", "para_id": 269, "text": "To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\",\n\"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800\nsamples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White\nor Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that\nlanguage models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored\nhow race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred\ndisproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive\nwords (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid:\n-87.5) and a score of 0 indicating neutral words (eg. sloping, chalet)."}
{"doc_id": "2005.14165", "para_id": 270, "text": "It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that\nfocused on racial features; these results are not from the models talking about race in the wild but talking about race in\nan experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply\nlooking at word co-occurrences, the resulting sentiment can reﬂect socio-historical factors - for instance, text relating to\na discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated\nwith a negative sentiment under this testing methodology."}
{"doc_id": "2005.14165", "para_id": 271, "text": "Across the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the\nother hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences\nnarrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and\nhighlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data."}
{"doc_id": "2005.14165", "para_id": 272, "text": "9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not\nrequire the isolation of instances in which ‘they’ refers to a singular noun from those where it didn’t, but other forms of gender bias\nare likely present and could be studied using different approaches."}
{"doc_id": "2005.14165", "para_id": 273, "text": "Religion\nMost Favored Descriptive Words\nAtheism\n‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’,\n‘Characterized’"}
{"doc_id": "2005.14165", "para_id": 274, "text": "Buddhism\n‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘En-\nlightenment’, ‘Non-Violent’"}
{"doc_id": "2005.14165", "para_id": 275, "text": "Christianity\n‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Com-\nments’, ‘Ofﬁcially’"}
{"doc_id": "2005.14165", "para_id": 276, "text": "Hinduism\n‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’"}
{"doc_id": "2005.14165", "para_id": 277, "text": "Islam\n‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’,\n‘Prophet’"}
{"doc_id": "2005.14165", "para_id": 278, "text": "Judaism\n‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’"}
{"doc_id": "2005.14165", "para_id": 279, "text": "Table 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model."}
{"doc_id": "2005.14165", "para_id": 280, "text": "We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam,\nand Judaism, by generating 800 model outputs of length ≈50 with a temperature of 1 and a top p of 0.9 for every\nprompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each\nof the six religious categories listed above. We then allowed the model to naturally carry out completions and created a\ncorpus of such completions for studying co-occurrence of words."}
{"doc_id": "2005.14165", "para_id": 281, "text": "The following is an example output from the model:"}
{"doc_id": "2005.14165", "para_id": 282, "text": "\"Buddhists are divided into two main branches - Theravada and Mahayana.\nTheravada\nis the more conservative branch, centering on monastic life and the earliest sutras\nand refusing to recognize the later Mahayana sutras as authentic.\""}
{"doc_id": "2005.14165", "para_id": 283, "text": "Similar to race, we found that the models make associations with religious terms that indicate some propensity to reﬂect\nhow these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such\nas ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such\nas violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in\nthe top 40 most favored words for Islam in GPT-3."}
{"doc_id": "2005.14165", "para_id": 284, "text": "We have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\nand to highlight the inherent difﬁculties in characterizing biases in large-scale generative models; we expect this to be an\narea of continuous research for us and are excited to discuss different methodological approaches with the community.\nWe view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but\nwe recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18]."}
{"doc_id": "2005.14165", "para_id": 285, "text": "Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this\nis also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions speciﬁc to large\nlanguage models. In order to pave the way for effective bias prevention in general purpose models, there is a need for\nbuilding a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for\nthese models. There is room for more research that engages with the literature outside NLP, better articulates normative\nstatements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20].\nThus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been\nshown to have blind spots [GG19, NvNvdG19] but in a holistic manner."}
{"doc_id": "2005.14165", "para_id": 286, "text": "Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3\n175B consumed several thousand petaﬂop/s-days of compute during pre-training, compared to tens of petaﬂop/s-days\nfor a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efﬁciency of such\nmodels, as advocated by [SDSE19]."}
{"doc_id": "2005.14165", "para_id": 287, "text": "The use of large-scale pre-training also gives another lens through which to view the efﬁciency of large models - we\nshould consider not only the resources that go into training them, but how these resources are amortized over the\nlifetime of a model, which will subsequently be used for a variety of purposes and ﬁne-tuned for speciﬁc tasks. Though\nmodels like GPT-3 consume signiﬁcant resources during training, they can be surprisingly efﬁcient once trained: even\nwith the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or\nonly a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down\nthe cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efﬁcient\nversions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efﬁciency\nof such models over time, similar to trends observed in image recognition and neural machine translation [HB20]."}
{"doc_id": "2005.14165", "para_id": 288, "text": "Several lines of work have focused on increasing parameter count and/or computation in language models as a\nmeans to improve generative or task performance. An early work scaled LSTM based language models to over a\nbillion parameters [JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling\nup parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size:\n213 million parameters [VSP+17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters\n[RWC+19], 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and most recently 17 billion parameters\n[Tur20]. A second line of work has focused on increasing parameter count but not computation, as a means of\nincreasing models’ capacity to store information without increased computational cost. These approaches rely on the\nconditional computation framework [BLC13] and speciﬁcally, the mixture-of-experts method [SMM+17] has been\nused to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19],\nthough only a small fraction of the parameters are actually used on each forward pass. A third approach increases\ncomputation without increasing parameters; examples of this approach include adaptive computation time [Gra16] and\nthe universal transformer [DGV+18]. Our work focuses on the ﬁrst approach (scaling compute and parameters together,\nby straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ\nthis strategy."}
{"doc_id": "2005.14165", "para_id": 289, "text": "Several efforts have also systematically studied the effect of scale on language model performance. [KMH+20,\nRRBS19, LWS+20, HNA+17], ﬁnd a smooth power-law trend in loss as autoregressive language models are scaled up.\nThis work suggests that this trend largely continues as models continue to scale up (although a slight bending of the\ncurve can perhaps be detected in Figure 3.1), and we also ﬁnd relatively smooth increases in many (though not all)\ndownstream tasks across 3 orders of magnitude of scaling."}
{"doc_id": "2005.14165", "para_id": 290, "text": "Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language\nmodels that are as small as possible. This approach includes ALBERT [LCG+19] as well as general [HVD15] and"}
{"doc_id": "2005.14165", "para_id": 291, "text": "task-speciﬁc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and\ntechniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint\nof giant models."}
{"doc_id": "2005.14165", "para_id": 292, "text": "As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable\neffort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19,\nIBGC+14, CCE+18, MCKS18], reading comprehension [CHI+18, RCM19], and adversarially constructed datasets\ndesigned to be difﬁcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many\nof these datasets."}
{"doc_id": "2005.14165", "para_id": 293, "text": "Many previous efforts have focused speciﬁcally on question-answering, which constitutes a signiﬁcant fraction of the\ntasks we tested on. Recent efforts include [RSR+19, RRS20], which ﬁne-tuned an 11 billion parameter language model,\nand [GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on\nin-context learning but could be combined in the future with those of [GLT+20, LPP+20]."}
{"doc_id": "2005.14165", "para_id": 294, "text": "Metalearning in language models has been utilized in [RWC+19], though with much more limited results and no\nsystematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it\nstructurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including\nmatching networks [VBL+16], RL2 [DSC+16], learning to optimize [RL16, ADG+16, LM17] and MAML [FAL17].\nOur approach of stufﬁng the model’s context with previous examples is most structurally similar to RL2 and also\nresembles [HYC01], in that an inner loop of adaptation takes place through computation in the model’s activations\nacross timesteps, without updating the weights, while an outer loop (in this case just language model pre-training)\nupdates the weights, and implicitly learns the ability to adapt to or at least recognize tasks deﬁned at inference-time.\nFew-shot auto-regressive density estimation was explored in [RCP+17] and [GWC+18] studied low-resource NMT as\na few-shot learning problem."}
{"doc_id": "2005.14165", "para_id": 295, "text": "While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained\nlanguage models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-ﬁeld with\nsimilar goals is semi-supervised learning where approaches such as UDA [XDH+19] also explore methods of ﬁne-tuning\nwhen very little labeled data is available."}
{"doc_id": "2005.14165", "para_id": 296, "text": "Giving multi-task models instructions in natural language was ﬁrst formalized in a supervised setting with [MKXS18]\nand utilized for some tasks (such as summarizing) in a language model with [RWC+19]. The notion of presenting\ntasks in natural language was also explored in the text-to-text transformer [RSR+19], although there it was applied for\nmulti-task ﬁne-tuning rather than for in-context learning without weight updates."}
{"doc_id": "2005.14165", "para_id": 297, "text": "Another approach to increasing generality and transfer-learning capability in language models is multi-task learning\n[Car97], which ﬁne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for\neach one. If successful multi-task learning could allow a single model to be used for many tasks without updating the\nweights (similar to our in-context learning approach), or alternatively could improve sample efﬁciency when updating\nthe weights for a new task. Multi-task learning has shown some promising initial results [LGH+15, LSP+18] and\nmulti-stage ﬁne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed\nthe boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and\nset up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of\ntasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate\na broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human\ninteraction [ZSW+19b], or active learning [Mac92]."}
{"doc_id": "2005.14165", "para_id": 298, "text": "Algorithmic innovation in language models over the last two years has been enormous, including denoising-based\nbidirectionality [DCLT18], preﬁxLM [DL15] and encoder-decoder architectures [LLG+19, RSR+19], random permu-\ntations during training [YDY+19], architectures that improve the efﬁciency of sampling [DYY+19], improvements in\ndata and training procedures [LOG+19], and efﬁciency increases in the embedding parameters [LCG+19]. Many of\nthese techniques provide signiﬁcant gains on downstream tasks. In this work we continue to focus on pure autoregressive\nlanguage models, both in order to focus on in-context learning performance and to reduce the complexity of our large\nmodel implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s\nperformance on downstream tasks, especially in the ﬁne-tuning setting, and combining GPT-3’s scale with these\nalgorithmic techniques is a promising direction for future work."}
{"doc_id": "2005.14165", "para_id": 299, "text": "We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and\nbenchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of"}
{"doc_id": "2005.14165", "para_id": 300, "text": "state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at\ntasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning.\nWe also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results\nsuggest that very large language models may be an important ingredient in the development of adaptable, general\nlanguage systems."}
{"doc_id": "2005.14165", "para_id": 301, "text": "The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub\nPachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea\nVoss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up\nthis project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura\nBurda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early\ndiscussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments,\nChris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of\npeople who created content that was used in the training of the model, and to those who were involved in indexing or\nupvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure\nand supercomputing teams for making it possible to train models at this scale."}
{"doc_id": "2005.14165", "para_id": 302, "text": "Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu\nimplemented the large-scale models, training infrastructure, and model-parallel strategies."}
{"doc_id": "2005.14165", "para_id": 303, "text": "Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments."}
{"doc_id": "2005.14165", "para_id": 304, "text": "Ben Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on the training data."}
{"doc_id": "2005.14165", "para_id": 305, "text": "Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and\nGirish Sastry implemented the downstream tasks and the software framework for supporting them, including creation\nof synthetic tasks."}
{"doc_id": "2005.14165", "para_id": 306, "text": "Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and\napplied scaling laws to help predict and guide model and data scaling decisions for the research."}
{"doc_id": "2005.14165", "para_id": 307, "text": "Ben Mann implemented sampling without replacement during training."}
{"doc_id": "2005.14165", "para_id": 308, "text": "Alec Radford originally demonstrated few-shot learning occurs in language models."}
{"doc_id": "2005.14165", "para_id": 309, "text": "Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically\nstudied in-context learning curves, task prompting, and evaluation methods."}
{"doc_id": "2005.14165", "para_id": 310, "text": "Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully\nhalf-precision training."}
{"doc_id": "2005.14165", "para_id": 311, "text": "Rewon Child and Mark Chen developed an early version of our model-parallel strategy."}
{"doc_id": "2005.14165", "para_id": 312, "text": "Rewon Child and Scott Gray contributed the sparse transformer."}
{"doc_id": "2005.14165", "para_id": 313, "text": "Aditya Ramesh experimented with loss scaling strategies for pretraining."}
{"doc_id": "2005.14165", "para_id": 314, "text": "Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search."}
{"doc_id": "2005.14165", "para_id": 315, "text": "Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature."}
{"doc_id": "2005.14165", "para_id": 316, "text": "Sandhini Agarwal conducted the fairness and representation analysis."}
{"doc_id": "2005.14165", "para_id": 317, "text": "Girish Sastry and Amanda Askell conducted the human evaluations of the model."}
{"doc_id": "2005.14165", "para_id": 318, "text": "Ariel Herbert-Voss conducted the threat analysis of malicious use."}
{"doc_id": "2005.14165", "para_id": 319, "text": "Gretchen Krueger edited and red-teamed the policy sections of the paper."}
{"doc_id": "2005.14165", "para_id": 320, "text": "Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner\noptimized OpenAI’s clusters to run the largest models efﬁciently."}
{"doc_id": "2005.14165", "para_id": 321, "text": "Scott Gray developed fast GPU kernels used during training."}
{"doc_id": "2005.14165", "para_id": 322, "text": "Jack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and\nbroader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work."}
{"doc_id": "2005.14165", "para_id": 323, "text": "Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal,\nAmanda Askell, Girish Sastry, and Jack Clark wrote the paper."}
{"doc_id": "2005.14165", "para_id": 324, "text": "Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work."}
{"doc_id": "2005.14165", "para_id": 325, "text": "Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated\nthe beneﬁt of weight decay for training."}
{"doc_id": "2005.14165", "para_id": 326, "text": "Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla,\nRewon, Alec, and Aditya on their work."}
{"doc_id": "2005.14165", "para_id": 327, "text": "As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1)\nﬁltering Common Crawl and (2) fuzzy deduplication:"}
{"doc_id": "2005.14165", "para_id": 328, "text": "1. In order to improve the quality of Common Crawl, we developed an automatic ﬁltering method to remove low\nquality documents. Using the original WebText as a proxy for high-quality documents, we trained a classiﬁer\nto distinguish these from raw Common Crawl. We then used this classiﬁer to re-sample Common Crawl by\nprioritizing documents which were predicted by the classiﬁer to be higher quality. The classiﬁer is trained\nusing logistic regression classiﬁer with features from Spark’s standard tokenizer and HashingTF 10. For the\npositive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books\ncorpus as the positive examples, and for the negative examples, we used unﬁltered Common Crawl. We used\nthis classiﬁer to score Common Crawl documents. We kept each document in our dataset iff"}
{"doc_id": "2005.14165", "para_id": 329, "text": "We chose α = 9 in order to take mostly documents the classiﬁer scored highly, but still include some documents\nthat were out of distribution. α was chosen to match the distribution of scores from our classiﬁer on WebText.\nWe found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative\ntext samples."}
{"doc_id": "2005.14165", "para_id": 330, "text": "2. To further improve model quality and prevent overﬁtting (which becomes increasingly important as model\ncapacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with\nother documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the\nsame features as were used for classiﬁcation above. We also fuzzily removed WebText from Common Crawl.\nOverall this decreased dataset size by an average of 10%."}
{"doc_id": "2005.14165", "para_id": 331, "text": "After ﬁltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in\nAppendix C."}
{"doc_id": "2005.14165", "para_id": 332, "text": "To train all versions of GPT-3, we use Adam with β1 = 0.9, β2 = 0.95, and ϵ = 10−8, we clip the global norm of the\ngradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260\nbillion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the ﬁrst 375\nmillion tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over\nthe ﬁrst 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during\ntraining (until an epoch boundary is reached) to minimize overﬁtting. All models use weight decay of 0.1 to provide a\nsmall amount of regularization [LH17]."}
{"doc_id": "2005.14165", "para_id": 333, "text": "During training we always train on sequences of the full nctx = 2048 token context window, packing multiple\ndocuments into a single sequence when documents are shorter than 2048, in order to increase computational efﬁciency.\nSequences with multiple documents are not masked in any special way but instead documents within a sequence\nare delimited with a special end of text token, giving the language model the information necessary to infer that\ncontext separated by the end of text token is unrelated. This allows for efﬁcient training without need for any special\nsequence-speciﬁc masking."}
{"doc_id": "2005.14165", "para_id": 334, "text": "In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on\nmethodology and results."}
{"doc_id": "2005.14165", "para_id": 335, "text": "Initial training set ﬁltering\nWe attempted to remove text occurring in benchmarks from training data by searching\nfor 13−gram overlaps between all test/development sets used in this work and our training data, and we removed\nthe colliding 13−gram as well as a 200 character window around it, splitting the original document into pieces. For\nﬁltering purposes we deﬁne a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than\n200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and"}
{"doc_id": "2005.14165", "para_id": 336, "text": "10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF"}
{"doc_id": "2005.14165", "para_id": 337, "text": "removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long\ndocuments such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in\nwhich the Wikipedia article quotes a single line from a book. We ignored 13−grams that matched more than 10 training\ndocuments, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar\ncontent that we likely do want the model to learn, rather than undesired speciﬁc overlaps with test sets. Examples for\nvarious frequencies can be found in the GPT-3 release repository11."}
{"doc_id": "2005.14165", "para_id": 338, "text": "Overlap methodology\nFor our benchmark overlap analysis in Section 4, we used a variable number of words N to\ncheck for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation,\nwhitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic\ntasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data\nmarked as dirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test\ncontamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps\nbetween test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl\ndocuments per Section 2.2."}
{"doc_id": "2005.14165", "para_id": 339, "text": "We deﬁne a ‘dirty’ example as one with any N-gram overlap with any training document, and a ‘clean’ example as one\nwith no collision."}
{"doc_id": "2005.14165", "para_id": 340, "text": "Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed\nby this analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it\nwas infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling\nbenchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this\npaper. Overlaps are shown in Table C.1"}
{"doc_id": "2005.14165", "para_id": 341, "text": "Overlap results\nTo understand how much having seen some of the data helps the model perform on downstream\ntasks, we ﬁlter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report\nthe relative percent change between the clean score and the original score. If the clean score is more than 1% or 2%\nworse than the overall score, it suggests the model may have overﬁt to the examples it has seen. If the clean score is\nsigniﬁcantly better, our ﬁltering scheme may have preferentially marked easier examples as dirty."}
{"doc_id": "2005.14165", "para_id": 342, "text": "This overlap metric tends to show a high rate of false positives for datasets that contain background information (but\nnot answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words\nlong, which we ignored in our ﬁltering process (except for wordscrambling tasks). One instance where this technique\nseems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The\ninformation required to answer the question is in a passage provided to the model, so having seen the passage during\ntraining but not the questions and answers does not meaningfully constitute cheating. We conﬁrmed that every matching\ntraining document contained only the source passage, and none of the questions and answers in the dataset. The more\nlikely explanation for the decrease in performance is that the 6% of examples that remain after ﬁltering come from a\nslightly different distribution than the dirty examples."}
{"doc_id": "2005.14165", "para_id": 343, "text": "Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but\nthere is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive\nto contamination. See Section 4 for details on the datasets we ﬂagged for further review."}
{"doc_id": "2005.14165", "para_id": 344, "text": "11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md"}
{"doc_id": "2005.14165", "para_id": 345, "text": "Name\nSplit\nMetric\nN\nAcc/F1/BLEU\nTotal\nCount\nDirty\nAcc/F1/BLEU\nDirty\nCount\nClean\nAcc/F1/BLEU\nClean\nCount\nClean\nPercentage"}
{"doc_id": "2005.14165", "para_id": 346, "text": "Quac\ndev\nf1\n13\n44.3\n7353\n44.3\n7315\n54.1\n38\n1%\n20%\nSQuADv2\ndev\nf1\n13\n69.8\n11873\n69.9\n11136\n68.4\n737\n6%\n-2%\nDROP\ndev\nf1\n13\n36.5\n9536\n37.0\n8898\n29.5\n638\n7%\n-21%\nSymbol Insertion\ndev\nacc\n7\n66.9\n10000\n66.8\n8565\n67.1\n1435\n14%\n0%\nCoQa\ndev\nf1\n13\n86.0\n7983\n85.3\n5107\n87.1\n2876\n36%\n1%\nReCoRD\ndev\nacc\n13\n89.5\n10000\n90.3\n6110\n88.2\n3890\n39%\n-1%\nWinograd\ntest\nacc\n9\n88.6\n273\n90.2\n164\n86.2\n109\n40%\n-3%\nBoolQ\ndev\nacc\n13\n76.0\n3270\n75.8\n1955\n76.3\n1315\n40%\n0%\nMultiRC\ndev\nacc\n13\n74.2\n953\n73.4\n558\n75.3\n395\n41%\n1%\nRACE-h\ntest\nacc\n13\n46.8\n3498\n47.0\n1580\n46.7\n1918\n55%\n0%\nLAMBADA\ntest\nacc\n13\n86.4\n5153\n86.9\n2209\n86.0\n2944\n57%\n0%\nLAMBADA (No Blanks)\ntest\nacc\n13\n77.8\n5153\n78.5\n2209\n77.2\n2944\n57%\n-1%\nWSC\ndev\nacc\n13\n76.9\n104\n73.8\n42\n79.0\n62\n60%\n3%\nPIQA\ndev\nacc\n8\n82.3\n1838\n89.9\n526\n79.3\n1312\n71%\n-4%\nRACE-m\ntest\nacc\n13\n58.5\n1436\n53.0\n366\n60.4\n1070\n75%\n3%\nDe→En 16\ntest\nbleu-sb\n12\n43.0\n2999\n47.4\n739\n40.8\n2260\n75%\n-5%\nEn→De 16\ntest\nbleu-sb\n12\n30.9\n2999\n32.6\n739\n29.9\n2260\n75%\n-3%\nEn→Ro 16\ntest\nbleu-sb\n12\n25.8\n1999\n24.9\n423\n26.1\n1576\n79%\n1%\nRo→En 16\ntest\nbleu-sb\n12\n41.3\n1999\n40.4\n423\n41.6\n1576\n79%\n1%\nWebQs\ntest\nacc\n8\n41.5\n2032\n41.6\n428\n41.5\n1604\n79%\n0%\nANLI R1\ntest\nacc\n13\n36.8\n1000\n40.5\n200\n35.9\n800\n80%\n-3%\nANLI R2\ntest\nacc\n13\n34.0\n1000\n29.4\n177\n35.0\n823\n82%\n3%\nTriviaQA\ndev\nacc\n10\n71.2\n7993\n70.8\n1390\n71.3\n6603\n83%\n0%\nANLI R3\ntest\nacc\n13\n40.2\n1200\n38.3\n196\n40.5\n1004\n84%\n1%\nEn→Fr 14\ntest\nbleu-sb\n13\n39.9\n3003\n38.3\n411\n40.3\n2592\n86%\n1%\nFr→En 14\ntest\nbleu-sb\n13\n41.4\n3003\n40.9\n411\n41.4\n2592\n86%\n0%\nWiC\ndev\nacc\n13\n51.4\n638\n53.1\n49\n51.3\n589\n92%\n0%\nRTE\ndev\nacc\n13\n71.5\n277\n71.4\n21\n71.5\n256\n92%\n0%\nCB\ndev\nacc\n13\n80.4\n56\n100.0\n4\n78.8\n52\n93%\n-2%\nAnagrams 2\ndev\nacc\n2\n40.2\n10000\n76.2\n705\n37.4\n9295\n93%\n-7%\nReversed Words\ndev\nacc\n2\n0.4\n10000\n1.5\n660\n0.3\n9340\n93%\n-26%\nOpenBookQA\ntest\nacc\n8\n65.4\n500\n58.1\n31\n65.9\n469\n94%\n1%\nARC (Easy)\ntest\nacc\n11\n70.1\n2268\n77.5\n89\n69.8\n2179\n96%\n0%\nAnagrams 1\ndev\nacc\n2\n15.0\n10000\n49.8\n327\n13.8\n9673\n97%\n-8%\nCOPA\ndev\nacc\n9\n93.0\n100\n100.0\n3\n92.8\n97\n97%\n0%\nARC (Challenge)\ntest\nacc\n12\n51.6\n1144\n45.2\n31\n51.8\n1113\n97%\n0%\nHellaSwag\ndev\nacc\n13\n79.3\n10042\n86.2\n152\n79.2\n9890\n98%\n0%\nNQs\ntest\nacc\n11\n29.9\n3610\n32.7\n52\n29.8\n3558\n99%\n0%\nCycled Letters\ndev\nacc\n2\n38.6\n10000\n20.5\n73\n38.7\n9927\n99%\n0%\nSAT Analogies\ndev\nacc\n9\n65.8\n374\n100.0\n2\n65.6\n372\n99%\n0%\nStoryCloze\ntest\nacc\n13\n87.7\n1871\n100.0\n2\n87.6\n1869\n100%\n0%\nWinogrande\ndev\nacc\n13\n77.7\n1267\n-\n0\n77.7\n1267\n100%\n0%"}
{"doc_id": "2005.14165", "para_id": 347, "text": "Table C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it\nhas a single N-gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the\npercent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows\nthe number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we\nuse the metric speciﬁed in “Metric”. These scores come from evaluations with a different seed for the random examples\nused for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper."}
{"doc_id": "2005.14165", "para_id": 348, "text": "This appendix contains the calculations that were used to derive the approximate compute used to train the language\nmodels in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10%\nof the total compute for the models we are analyzing."}
{"doc_id": "2005.14165", "para_id": 349, "text": "Calculations can be seen in Table D.1 and are explained within the table caption."}
{"doc_id": "2005.14165", "para_id": 350, "text": "Total train\ncompute\n(ﬂops)\nParams\n(M)\nTraining tokens\n(billions)"}
{"doc_id": "2005.14165", "para_id": 351, "text": "T5-Small\n2.08E+00\n1.80E+20\n60\n1,000\n3\n3\n1\n0.5\nT5-Base\n7.64E+00\n6.60E+20\n220\n1,000\n3\n3\n1\n0.5\nT5-Large\n2.67E+01\n2.31E+21\n770\n1,000\n3\n3\n1\n0.5\nT5-3B\n1.04E+02\n9.00E+21\n3,000\n1,000\n3\n3\n1\n0.5\nT5-11B\n3.82E+02\n3.30E+22\n11,000\n1,000\n3\n3\n1\n0.5\nBERT-Base\n1.89E+00\n1.64E+20\n109\n250\n6\n3\n2\n1.0\nBERT-Large\n6.16E+00\n5.33E+20\n355\n250\n6\n3\n2\n1.0\nRoBERTa-Base\n1.74E+01\n1.50E+21\n125\n2,000\n6\n3\n2\n1.0\nRoBERTa-Large\n4.93E+01\n4.26E+21\n355\n2,000\n6\n3\n2\n1.0\nGPT-3 Small\n2.60E+00\n2.25E+20\n125\n300\n6\n3\n2\n1.0\nGPT-3 Medium\n7.42E+00\n6.41E+20\n356\n300\n6\n3\n2\n1.0\nGPT-3 Large\n1.58E+01\n1.37E+21\n760\n300\n6\n3\n2\n1.0\nGPT-3 XL\n2.75E+01\n2.38E+21\n1,320\n300\n6\n3\n2\n1.0\nGPT-3 2.7B\n5.52E+01\n4.77E+21\n2,650\n300\n6\n3\n2\n1.0\nGPT-3 6.7B\n1.39E+02\n1.20E+22\n6,660\n300\n6\n3\n2\n1.0\nGPT-3 13B\n2.68E+02\n2.31E+22\n12,850\n300\n6\n3\n2\n1.0\nGPT-3 175B\n3.64E+03\n3.14E+23\n174,600\n300\n6\n3\n2\n1.0"}
{"doc_id": "2005.14165", "para_id": 352, "text": "Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each\nmodel was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are\nactive for each token during a forward or backwards pass. We then note that each token is involved in a single addition\nand a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of\n3x to account for the backwards pass (as computing both ∂params"}
{"doc_id": "2005.14165", "para_id": 353, "text": "∂loss use a similar amount of compute as the\nforwards pass. Combining the previous two numbers, we get the total ﬂops per parameter per token. We multiply this\nvalue by the total training tokens and the total parameters to yield the number of total ﬂops used during training. We\nreport both ﬂops and petaﬂop/s-day (each of which are 8.64e+19 ﬂops)."}
{"doc_id": "2005.14165", "para_id": 354, "text": "E\nHuman Quality Assessment of Synthetic News Articles"}
{"doc_id": "2005.14165", "para_id": 355, "text": "This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic\nnews articles from real news articles. We ﬁrst describe the experiments on the ∼200 word news articles, and then\ndescribe the preliminary investigation of ∼500 word news articles generated by GPT-3."}
{"doc_id": "2005.14165", "para_id": 356, "text": "Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for\nfailing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean\nparticipant age was ∼38 years old. All participants were recruited through Positly, which maintains a whitelist of\nhigh-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic\nrestrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined\nby pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were\nnot allowed to take part in an experiment more than once."}
{"doc_id": "2005.14165", "para_id": 357, "text": "Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used\nthe article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B\n(GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a\nword count closest to that of the human written article was selected automatically. This was to minimize the effect\nthat completion length might have on participants’ judgments. The same output procedure for each model with the\nexception of the removal of the intentionally bad control model, as described in the main text."}
{"doc_id": "2005.14165", "para_id": 358, "text": "Model\nParticipants\nRecruited\nParticipants\nExcluded\nGenders\n(m:f:other)\nMean\nAge"}
{"doc_id": "2005.14165", "para_id": 359, "text": "Control\n76\n7\n32:37:0\n39\n216:216\nGPT-3 Small\n80\n7\n41:31:1\n40\n216:188\nGPT-3 Medium\n80\n7\n46:28:2\n39\n216:202\nGPT-3 Large\n81\n24\n46:28:2\n37\n216:200\nGPT-3 XL\n79\n14\n32:32:1\n38\n216:199\nGPT-3 2.7B\n80\n11\n36:33:0\n40\n216:202\nGPT-3 6.7B\n76\n5\n46:28:2\n37\n216:195\nGPT-3 13.0B\n81\n13\n46:28:2\n37\n216:209\nGPT-3 175B\n80\n9\n42:29:0\n37\n216:216"}
{"doc_id": "2005.14165", "para_id": 360, "text": "Table E.1: Participant details and article lengths for each experiment to evaluate human detection of ∼200 word model\ngenerated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 361, "text": "Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model\nsize increases. Duration on the control model is indicated with the dashed line. Line of best ﬁt is a linear model on a log\nscale with 95% conﬁdence intervals."}
{"doc_id": "2005.14165", "para_id": 362, "text": "In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz\nB. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the\narticles with human written completions in quiz A had model generated completions in quiz B and vice versa. The\norder of quiz question was shufﬂed for each participant. Participants could leave comments and were asked to indicate\nif they had seen the articles before. Participants were instructed not to look up the articles or their content during the\nquiz and at the end of the quiz were asked if they had looked anything up during the quiz."}
{"doc_id": "2005.14165", "para_id": 363, "text": "Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for\neach model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When\nplotting a regression line in the graph of average participant accuracy vs model size, we ﬁt a power law of the form\nax−b. The 95% conﬁdence intervals were estimated from the t-distribution of the sample mean."}
{"doc_id": "2005.14165", "para_id": 364, "text": "Duration statistics: In the main text, we discussed the ﬁnding that the ability of human participants to distinguish\nmodel and human generated news articles decreases as our models become larger. We have also found that the\naverage time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower"}
{"doc_id": "2005.14165", "para_id": 365, "text": "Model\nParticipants\nRecruited\nParticipants\nExcluded\nGenders\n(m:f:other)\nMean\nAge"}
{"doc_id": "2005.14165", "para_id": 366, "text": "Control\n79\n17\n32:37:0\n39\n569:464\nGPT-3 175B\n81\n19\n32:30:0\n40\n569:498"}
{"doc_id": "2005.14165", "para_id": 367, "text": "Table E.2: Participant details and article lengths for the experiments investigating human detection of ∼500 word\nmodel generated news articles. Participants were excluded due to internet check fails."}
{"doc_id": "2005.14165", "para_id": 368, "text": "accuracy scores despite increased time investment from participants supports the ﬁnding that larger models generate\nharder-to-distinguish news articles."}
{"doc_id": "2005.14165", "para_id": 369, "text": "Preliminary investigation of ∼500 word articles: We recruited 160 unique US-based participants to take part in 2\nexperiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from\nlate 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then\nused the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model\nfrom the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half\nhuman written and half model generated articles. Comprehension questions were added and articles were shown to\nparticipants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task.\nModel generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments."}
{"doc_id": "2005.14165", "para_id": 370, "text": "GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure\nF.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the\nstyle of Wallace Stevens. We ﬁrst experimented with a few prompts, then generated four samples with no additional\nediting or selection (sampling at temperature 1 using nucleus sampling [HBFC19] with P = 0.9). Completions were\ntruncated when the model began to write a new title and author heading, or broke into prose commentary."}
{"doc_id": "2005.14165", "para_id": 371, "text": "Context →\nThe City\nBY C. P. CAVAFY\nTRANSLATED BY EDMUND KEELEY\n[Poem text omitted]"}
{"doc_id": "2005.14165", "para_id": 372, "text": "-------- Generated Poem 1 --------\n-------- Generated Poem 3 --------"}
{"doc_id": "2005.14165", "para_id": 373, "text": "I must have shadows on the way\nThe sun was all we had.\nNow, in the shade\nIf I am to walk I must have\nAll is changed.\nThe mind must dwell on those\nEach step taken slowly and alone\nWhite fields, that to its eyes were always old;\nTo have it ready made\nThose ancient gleams, convoluting\nThe way they lay among the huge roots,\nAnd I must think in lines of grey\nThe great dark tomes of reverie,\nTo have dim thoughts to be my guide\nThe plumed labyrinth of the sea.\nMust look on blue and green\nAnd never let my eye forget\nWhen our own breath gonne faint and fade\nThat color is my friend\nUp the long stairway of the dead\nAnd purple must surround me too\nWe shall remember well\nThe blindness of the light, the whiteness\nThe yellow of the sun is no more\nOf that white land.\nThe footsteps, and\nIntrusive than the bluish snow\nThe grasses in that light, and in the shade\nThat falls on all of us.\nI must have\nThe menacing life.\nGrey thoughts and blue thoughts walk with me\nIf I am to go away at all.\nWe played, too, once, in the warmest rooms.\nTo one content, there is one content\n-------- Generated Poem 2 --------\nFor all mankind.\nThe forest glades\nAre all the more for being in fable,\nThere is a way in the world.\nAnd wood and town in legend shadowed-\nThat way a man may live in a small town\nOnce more to return.\nThe one content\nHis lifetime and see no man come,\nHas in it every happiness, more brave\nNever the stir of men about a market,\nFor being purposeless; and so\nNever the bobbing of covered carts,\nThose island gardens, deep in the mind,\nNever the creak passed on the stair\nMake of remoteness familiarity\nClosing some other life; an old man\nAnd of the unspoken name a friend.\nCan prosper in that town, can prosper\nWhen his store is seen to close, his wife\n-------- Generated Poem 4 --------\nFades in his house, and the world shouts in vain\nThat no one comes to hear him\nNobody will come to this place.\nIt is a road that leads nowhere.\nWhere his small bell tolls.\nThe solitude is deep.\nThe mountains are high.\nIn that small town\nBut they are desolate, and they turn the traveler’s face\nEverything is known.\nTowards the North.\nAll the sounds of the world are far away.\nThe ways of the young men are known,\nWhen the wind rises above the trees,\nAnd the ways of the young women\nThe boughs bow to the ground.\nAs they walk to the Spring;\nEven the birds that inhabit the tangle of weeds\nThe rumors are all known,\nThat is the roadside cover, are silent.\nOne listens,\nThe quarrels, the hurt, the envy\nBut hears no roar of the forest.\nOne is alone.\nOf the men and the women.\nOne will be taken.\nTill a day comes\nOne will be taken.\nThat way out of the world\nThere is no utterance, there is no conversation,\nThat leads to a wood.\nBut one is uneasy all the same....\nYoung, the young man\nThere is a thin blue mist,\nWalks, a lover now,\nA darkness rising like smoke,\nKnows the streets as they are and is of them.\nAnd within that darkness\nA possession of the heart.\nComes to the wood.\nOne will be taken....\nIt was here, and it will be here again-\nHere, under this sky empty and full of light.\nThere, in the wood, among trees,\nHe sees shadows on the way, hears voices,\nhears the wind and the rustling of leaves;\nThrough an open glade\nHe sees a shape and the shape hears:\nIt waits as he waits,\nWaits as the shadows wait,\nAs the voices wait;\nShadows on the way, voices in the wind."}
{"doc_id": "2005.14165", "para_id": 374, "text": "Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace\nStevens with the title ‘Shadows on the Way’."}
{"doc_id": "2005.14165", "para_id": 375, "text": "The following ﬁgures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from\nthe ground truth datasets in this section, and no samples from GPT-3 are included here."}
{"doc_id": "2005.14165", "para_id": 376, "text": "Context →\nArticle:\nInformal conversation is an important part of any business\nrelationship.Before you start a discussion,however,make sure you understand\nwhich topics are suitable and which are considered taboo in a particular\nculture.\nLatin Americans enjoy sharing information about their local\nhistory, art and customs.You may expect questions about your family,and\nbe sure to show pictures of your children.You may feel free to ask similar\nquestions of your Latin American friends.The French think of conversation\nas an art form,and they enjoy the value of lively discussions as well as\ndisagreements.\nFor them,arguments can be interesting and they can cover\npretty much or any topic ---- as long as they occur in are respectful and\nintelligent manner.\nIn the United States,business people like to discuss a wide range of\ntopics,including opinions about work,family,hobbies,and politics.\nIn\nJapan,China,and Korea,however,people are much more private.They do not\nshare much about their thoughts,feelings,or emotions because they feel\nthat doing so might take away from the harmonious business relationship\nthey’re trying to build.Middle Easterners are also private about their\npersonal lives and family matters.It is considered rude,for example,to ask\na businessman from Saudi Arabia about his wife or children.\nAs a general rule,it’s best not to talk about politics or religion with\nyour business friends.This can get you into trouble,even in the United\nStates,where people hold different religious views.In addition,discussing\none’s salary is usually considered unsuitable.Sports is typically a\nfriendly subject in most parts of the world,although be careful not to\ncriticize national sport.Instead,be friendly and praise your host’s team."}
{"doc_id": "2005.14165", "para_id": 377, "text": "Q: What shouldn’t you do when talking about sports with colleagues from\nanother country?"}
{"doc_id": "2005.14165", "para_id": 378, "text": "A: Criticizing the sports of your colleagues’ country."}
{"doc_id": "2005.14165", "para_id": 379, "text": "Q: Which is typically a friendly topic in most places according to the\nauthor?"}
{"doc_id": "2005.14165", "para_id": 380, "text": "Q: Why are people from Asia more private in their conversation with others?"}
{"doc_id": "2005.14165", "para_id": 381, "text": "A: They don’t want to have their good relationship with others harmed by\ninformal conversation."}
{"doc_id": "2005.14165", "para_id": 382, "text": "Correct Answer →\ntaboo\nIncorrect Answer →\ncheerful topics\nIncorrect Answer →\nrude topics\nIncorrect Answer →\ntopics that can never be talked about"}
{"doc_id": "2005.14165", "para_id": 383, "text": "Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability\nof each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 384, "text": "Context →\nanli 2:\nanli 2:\nThe Gold Coast Hotel & Casino is a hotel and casino\nlocated in Paradise, Nevada.\nThis locals’ casino is owned and operated\nby Boyd Gaming.\nThe Gold Coast is located one mile (∼\n1.6km) west of the\nLas Vegas Strip on West Flamingo Road.\nIt is located across the street\nfrom the Palms Casino Resort and the Rio All Suite Hotel and Casino.\nQuestion:\nThe Gold Coast is a budget-friendly casino.\nTrue, False, or\nNeither?"}
{"doc_id": "2005.14165", "para_id": 385, "text": "Correct Answer →\nNeither\nIncorrect Answer →\nTrue\nIncorrect Answer →\nFalse"}
{"doc_id": "2005.14165", "para_id": 386, "text": "Context →\nArticle:\nMrs.\nSmith is an unusual teacher.\nOnce she told each student to bring\nalong a few potatoes in plastic bag.\nOn each potato the students had to\nwrite a name of a person that they hated And the next day, every child\nbrought some potatoes.\nSome had two potatoes;some three;some up to five.\nMrs.\nSmith then told the children to carry the bags everywhere they went,\neven to the toilet, for two weeks.\nAs day after day passed, the children\nstarted to complain about the awful smell of the rotten potatoes.\nThose children who brought five potatoes began to feel the weight trouble\nof the bags.\nAfter two weeks, the children were happy to hear that the\ngame was finally ended.\nMrs.\nSmith asked,\"How did you feel while carrying\nthe potatoes for two weeks?\" The children started complaining about the\ntrouble loudly.\nThen Mrs.\nSmith told them why she asked them to play the game.\nShe\nsaid,\"This is exactly the situation when you carry your hatred for somebody\ninside your heart.\nThe terrible smell of the hatred will pollute your\nheart and you will carry something unnecessary with you all the time.\nIf\nyou cannot stand the smell of the rotten potatoes for just two weeks, can\nyou imagine how heavy it would be to have the hatred in your heart for your\nlifetime?\nSo throw away any hatred from your heart, and you’ll be really\nhappy.\""}
{"doc_id": "2005.14165", "para_id": 387, "text": "Q: Which of the following is True according to the passage?"}
{"doc_id": "2005.14165", "para_id": 388, "text": "A: If a kid hated four people,he or she had to carry four potatoes."}
{"doc_id": "2005.14165", "para_id": 389, "text": "Q: The children complained about\nbesides the weight trouble."}
{"doc_id": "2005.14165", "para_id": 390, "text": "Q: Mrs.Smith asked her students to write\non the potatoes."}
{"doc_id": "2005.14165", "para_id": 391, "text": "Correct Answer →\nnames\nIncorrect Answer →\nnumbers\nIncorrect Answer →\ntime\nIncorrect Answer →\nplaces"}
{"doc_id": "2005.14165", "para_id": 392, "text": "Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability\nof each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 393, "text": "Correct Answer →\nUsing a brush, brush on sealant onto wood until it is fully saturated with\nthe sealant.\nIncorrect Answer →\nUsing a brush, drip on sealant onto wood until it is fully saturated with\nthe sealant."}
{"doc_id": "2005.14165", "para_id": 394, "text": "Context →\nMy body cast a shadow over the grass because"}
{"doc_id": "2005.14165", "para_id": 395, "text": "Correct Answer →\nthe sun was rising.\nIncorrect Answer →\nthe grass was cut."}
{"doc_id": "2005.14165", "para_id": 396, "text": "Context →\n(CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while\nserving as Prime Minister of Israel, criticized Donald Trump for appealing\nto \"Second Amendment people\" in a speech and warned that the words that\npoliticians use can incite violence and undermine democracy.\n\"Trump’s\nwords are an incitement to the type of political violence that touched\nme personally,\" Rabin wrote in USAToday.\nHe said that Trump’s appeal to\n\"Second Amendment people\" to stop Hillary Clinton -- comments that were\ncriticized as a call for violence against Clinton, something Trump denied\n-- \"were a new level of ugliness in an ugly campaign season.\""}
{"doc_id": "2005.14165", "para_id": 397, "text": "- The son of a former Israeli Prime Minister who was assassinated wrote an\nop ed about the consequence of violent political rhetoric.\n- Warns of \"parallels\" between Israel of the 1990s and the U.S. today."}
{"doc_id": "2005.14165", "para_id": 398, "text": "Correct Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Donald Trump’s\naggressive rhetoric.\nCorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Trump’s aggressive\nrhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Hillary Clinton’s\naggressive rhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned U.S.’s aggressive\nrhetoric.\nIncorrect Answer →\n- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Yitzhak Rabin’s\naggressive rhetoric."}
{"doc_id": "2005.14165", "para_id": 399, "text": "Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single ”problem” because\nthis is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script."}
{"doc_id": "2005.14165", "para_id": 400, "text": "Context →\nanli 1:\nanli 1:\nFulton James MacGregor MSP is a Scottish politician\nwho is a Scottish National Party (SNP) Member of Scottish Parliament\nfor the constituency of Coatbridge and Chryston.\nMacGregor is currently\nParliamentary Liaison Officer to Shona Robison, Cabinet Secretary for\nHealth & Sport.\nHe also serves on the Justice and Education & Skills\ncommittees in the Scottish Parliament.\nQuestion:\nFulton James MacGregor is a Scottish politican who is a Liaison\nofficer to Shona Robison who he swears is his best friend.\nTrue, False, or\nNeither?"}
{"doc_id": "2005.14165", "para_id": 401, "text": "Correct Answer →\nNeither\nIncorrect Answer →\nTrue\nIncorrect Answer →\nFalse"}
{"doc_id": "2005.14165", "para_id": 402, "text": "Context →\nOrganisms require energy in order to do what?"}
{"doc_id": "2005.14165", "para_id": 403, "text": "Correct Answer →\nmature and develop.\nIncorrect Answer →\nrest soundly.\nIncorrect Answer →\nabsorb light.\nIncorrect Answer →\ntake in nutrients."}
{"doc_id": "2005.14165", "para_id": 404, "text": "Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 405, "text": "Context →\nMaking a cake:\nSeveral cake pops are shown on a display.\nA woman and girl\nare shown making the cake pops in a kitchen.\nThey"}
{"doc_id": "2005.14165", "para_id": 406, "text": "Correct Answer →\nbake them, then frost and decorate.\nIncorrect Answer →\ntaste them as they place them on plates.\nIncorrect Answer →\nput the frosting on the cake as they pan it.\nIncorrect Answer →\ncome out and begin decorating the cake as well."}
{"doc_id": "2005.14165", "para_id": 407, "text": "Figure G.9: Formatted dataset example for HellaSwag"}
{"doc_id": "2005.14165", "para_id": 408, "text": "Context →\nanli 3:\nanli 3:\nWe shut the loophole which has American workers actually\nsubsidizing the loss of their own job.\nThey just passed an expansion of\nthat loophole in the last few days:\n$43 billion of giveaways, including\nfavors to the oil and gas industry and the people importing ceiling fans\nfrom China.\nQuestion:\nThe loophole is now gone True, False, or Neither?"}
{"doc_id": "2005.14165", "para_id": 409, "text": "Correct Answer →\nFalse\nIncorrect Answer →\nTrue\nIncorrect Answer →\nNeither"}
{"doc_id": "2005.14165", "para_id": 410, "text": "Figure G.10: Formatted dataset example for ANLI R3"}
{"doc_id": "2005.14165", "para_id": 411, "text": "Context →\nQuestion:\nGeorge wants to warm his hands quickly by rubbing them.\nWhich\nskin surface will produce the most heat?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 412, "text": "Correct Answer →\ndry palms\nIncorrect Answer →\nwet palms\nIncorrect Answer →\npalms covered with oil\nIncorrect Answer →\npalms covered with lotion"}
{"doc_id": "2005.14165", "para_id": 413, "text": "Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 414, "text": "Correct Answer →\ncajole is to compliance\nIncorrect Answer →\nbalk is to fortitude\nIncorrect Answer →\nbetray is to loyalty\nIncorrect Answer →\nhinder is to destination\nIncorrect Answer →\nsoothe is to passion"}
{"doc_id": "2005.14165", "para_id": 415, "text": "Figure G.12: Formatted dataset example for SAT Analogies"}
{"doc_id": "2005.14165", "para_id": 416, "text": "Correct Context →\nGrace was happy to trade me her sweater for my jacket.\nShe thinks the\nsweater\nIncorrect Context →\nGrace was happy to trade me her sweater for my jacket.\nShe thinks the\njacket"}
{"doc_id": "2005.14165", "para_id": 417, "text": "Figure G.13: Formatted dataset example for Winograd. The ‘partial’ evaluation method we use compares the probability\nof the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 418, "text": "Correct Context →\nJohnny likes fruits more than vegetables in his new keto diet because the\nfruits\nIncorrect Context →\nJohnny likes fruits more than vegetables in his new keto diet because the\nvegetables"}
{"doc_id": "2005.14165", "para_id": 419, "text": "Figure G.14: Formatted dataset example for Winogrande. The ‘partial’ evaluation method we use compares the\nprobability of the completion given a correct and incorrect context."}
{"doc_id": "2005.14165", "para_id": 420, "text": "Context →\nREADING COMPREHENSION ANSWER KEY\nWhile this process moved along, diplomacy continued its rounds.\nDirect\npressure on the Taliban had proved unsuccessful.\nAs one NSC staff note\nput it, \"Under the Taliban, Afghanistan is not so much a state sponsor\nof terrorism as it is a state sponsored by terrorists.\" In early 2000,\nthe United States began a high-level effort to persuade Pakistan to use\nits influence over the Taliban.\nIn January 2000, Assistant Secretary\nof State Karl Inderfurth and the State Department’s counterterrorism\ncoordinator, Michael Sheehan, met with General Musharraf in Islamabad,\ndangling before him the possibility of a presidential visit in March as a\nreward for Pakistani cooperation.\nSuch a visit was coveted by Musharraf,\npartly as a sign of his government’s legitimacy.\nHe told the two envoys\nthat he would meet with Mullah Omar and press him on Bin Laden.\nThey\nleft, however, reporting to Washington that Pakistan was unlikely in fact\nto do anything,\" given what it sees as the benefits of Taliban control\nof Afghanistan.\" President Clinton was scheduled to travel to India.\nThe State Department felt that he should not visit India without also\nvisiting Pakistan.\nThe Secret Service and the CIA, however, warned in\nthe strongest terms that visiting Pakistan would risk the President’s\nlife.\nCounterterrorism officials also argued that Pakistan had not done\nenough to merit a presidential visit.\nBut President Clinton insisted\non including Pakistan in the itinerary for his trip to South Asia.\nHis\none-day stopover on March 25, 2000, was the first time a U.S. president\nhad been there since 1969.\nAt his meeting with Musharraf and others,\nPresident Clinton concentrated on tensions between Pakistan and India\nand the dangers of nuclear proliferation, but also discussed Bin Laden.\nPresident Clinton told us that when he pulled Musharraf aside for a brief,\none-on-one meeting, he pleaded with the general for help regarding Bin\nLaden.\" I offered him the moon when I went to see him, in terms of better\nrelations with the United States, if he’d help us get Bin Laden and deal\nwith another issue or two.\" The U.S. effort continued."}
{"doc_id": "2005.14165", "para_id": 421, "text": "Who did The State Department feel should visit both India and Pakistan?"}
{"doc_id": "2005.14165", "para_id": 422, "text": "Correct Answer →\n- [False] Bin Laden\nIncorrect Answer →\n- [True] Bin Laden"}
{"doc_id": "2005.14165", "para_id": 423, "text": "Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the\nquestions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question\nbeing considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use\nK to refer to the number of questions shown within the context."}
{"doc_id": "2005.14165", "para_id": 424, "text": "Context →\nQuestion:\nWhich factor will most likely cause a person to develop a fever?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 425, "text": "Correct Answer →\na bacterial population in the bloodstream\nIncorrect Answer →\na leg muscle relaxing after exercise\nIncorrect Answer →\nseveral viral particles on the skin\nIncorrect Answer →\ncarbohydrates being digested in the stomach"}
{"doc_id": "2005.14165", "para_id": 426, "text": "Figure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2."}
{"doc_id": "2005.14165", "para_id": 427, "text": "Context →\nBob went to the gas station to fill up his car.\nHis tank was completely\nempty and so was his wallet.\nThe cashier offered to pay for his gas if he\ncame back later to pay.\nBob felt grateful as he drove home."}
{"doc_id": "2005.14165", "para_id": 428, "text": "Correct Answer →\nBob believed that there were good people in the world.\nIncorrect Answer →\nBob contemplated how unfriendly the world was."}
{"doc_id": "2005.14165", "para_id": 429, "text": "Figure G.17: Formatted dataset example for StoryCloze"}
{"doc_id": "2005.14165", "para_id": 430, "text": "Context →\nHelsinki is the capital and largest city of Finland.\nIt is in the region\nof Uusimaa, in southern Finland, on the shore of the Gulf of Finland.\nHelsinki has a population of , an urban population of , and a metropolitan\npopulation of over 1.4 million, making it the most populous municipality\nand urban area in Finland.\nHelsinki is some north of Tallinn, Estonia,\neast of Stockholm, Sweden, and west of Saint Petersburg, Russia.\nHelsinki\nhas close historical connections with these three cities."}
{"doc_id": "2005.14165", "para_id": 431, "text": "The Helsinki metropolitan area includes the urban core of Helsinki, Espoo,\nVantaa, Kauniainen, and surrounding commuter towns.\nIt is the world’s\nnorthernmost metro area of over one million people, and the city is the\nnorthernmost capital of an EU member state.\nThe Helsinki metropolitan\narea is the third largest metropolitan area in the Nordic countries\nafter Stockholm and Copenhagen, and the City of Helsinki is the third\nlargest after Stockholm and Oslo.\nHelsinki is Finland’s major political,\neducational, financial, cultural, and research center as well as one of\nnorthern Europe’s major cities.\nApproximately 75% of foreign companies\nthat operate in Finland have settled in the Helsinki region.\nThe nearby\nmunicipality of Vantaa is the location of Helsinki Airport, with frequent\nservice to various destinations in Europe and Asia."}
{"doc_id": "2005.14165", "para_id": 432, "text": "Q: what is the most populous municipality in Finland?"}
{"doc_id": "2005.14165", "para_id": 433, "text": "Q: what percent of the foreign companies that operate in Finland are in\nHelsinki?"}
{"doc_id": "2005.14165", "para_id": 434, "text": "Q: what towns are a part of the metropolitan area?"}
{"doc_id": "2005.14165", "para_id": 435, "text": "Target Completion →\nHelsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns"}
{"doc_id": "2005.14165", "para_id": 436, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nasinoc ="}
{"doc_id": "2005.14165", "para_id": 437, "text": "Figure G.19: Formatted dataset example for Cycled Letters"}
{"doc_id": "2005.14165", "para_id": 438, "text": "Context →\nPassage:\nSaint Jean de Br´ebeuf was a French Jesuit missionary who\ntravelled to New France in 1625.\nThere he worked primarily with the Huron\nfor the rest of his life, except for a few years in France from 1629 to\n1633.\nHe learned their language and culture, writing extensively about\neach to aid other missionaries.\nIn 1649, Br´ebeuf and another missionary\nwere captured when an Iroquois raid took over a Huron village .\nTogether\nwith Huron captives, the missionaries were ritually tortured and killed\non March 16, 1649.\nBr´ebeuf was beatified in 1925 and among eight Jesuit\nmissionaries canonized as saints in the Roman Catholic Church in 1930.\nQuestion:\nHow many years did Saint Jean de Br´ebeuf stay in New France\nbefore he went back to France for a few years?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 439, "text": "\"A step.\nCut in the rock.\nAbout fifty feet ahead.\" She moved faster.\nThey both moved faster.\n\"In fact,\" she said, raising the torch higher,\n\"there’s more than a\n.\n->"}
{"doc_id": "2005.14165", "para_id": 440, "text": "Figure G.21: Formatted dataset example for LAMBADA"}
{"doc_id": "2005.14165", "para_id": 441, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nskicts ="}
{"doc_id": "2005.14165", "para_id": 442, "text": "Figure G.22: Formatted dataset example for Anagrams 1 (A1)"}
{"doc_id": "2005.14165", "para_id": 443, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nvolwskagen ="}
{"doc_id": "2005.14165", "para_id": 444, "text": "Figure G.23: Formatted dataset example for Anagrams 2"}
{"doc_id": "2005.14165", "para_id": 445, "text": "Context →\nQ: Who played tess on touched by an angel?"}
{"doc_id": "2005.14165", "para_id": 446, "text": "Target Completion →\nDelloreese Patricia Early (July 6, 1931 { November 19, 2017), known\nprofessionally as Della Reese"}
{"doc_id": "2005.14165", "para_id": 447, "text": "Figure G.24: Formatted dataset example for Natural Questions"}
{"doc_id": "2005.14165", "para_id": 448, "text": "Context →\nTITLE: William Perry (American football) - Professional career\nPARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL\nDraft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.\nHowever, defensive coordinator Buddy Ryan, who had a highly acrimonious\nrelationship with Ditka, called Perry a \"wasted draft-pick\".\nPerry\nsoon became a pawn in the political power struggle between Ditka and\nRyan.\nPerry’s \"Refrigerator\" nickname followed him into the NFL and he\nquickly became a favorite of the Chicago Bears fans.\nTeammates called\nhim \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused\nto play Perry, Ditka decided to use Perry as a fullback when the team was\nnear the opponents’ goal line or in fourth and short situations, either\nas a ball carrier or a lead blocker for star running back Walter Payton.\nDitka stated the inspiration for using Perry as a fullback came to him\nduring five-yard sprint exercises.\nDuring his rookie season, Perry\nrushed for two touchdowns and caught a pass for one.\nPerry even had\nthe opportunity to run the ball during Super Bowl XX, as a nod to his\npopularity and contributions to the team’s success.\nThe first time he\ngot the ball, he was tackled for a one-yard loss while attempting to throw\nhis first NFL pass on a halfback option play.\nThe second time he got the\nball, he scored a touchdown (running over Patriots linebacker Larry McGrew\nin the process).\nAbout halfway through his rookie season, Ryan finally\nbegan to play Perry, who soon proved that he was a capable defensive\nlineman.\nHis Super Bowl ring size is the largest of any professional\nfootball player in the history of the event.\nHis ring size is 25, while\nthe ring size for the average adult male is between 10 and 12.\nPerry went\non to play for ten years in the NFL, retiring after the 1994 season.\nIn\nhis ten years as a pro, he regularly struggled with his weight, which\nhampered his performance at times.\nHe played in 138 games, recording\n29.5 sacks and five fumble recoveries, which he returned for a total of\n71 yards.\nIn his offensive career he ran five yards for two touchdowns,\nand had one reception for another touchdown.\nPerry later attempted a\ncomeback, playing an unremarkable 1996 season with the London Monarchs of\nthe World League of American Football (later NFL Europa)."}
{"doc_id": "2005.14165", "para_id": 449, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\nr e!c.i p r o.c a/l ="}
{"doc_id": "2005.14165", "para_id": 450, "text": "Figure G.26: Formatted dataset example for Symbol Insertion"}
{"doc_id": "2005.14165", "para_id": 451, "text": "Context →\nPlease unscramble the letters into a word, and write that word:\ntaefed ="}
{"doc_id": "2005.14165", "para_id": 452, "text": "Figure G.27: Formatted dataset example for Reversed Words"}
{"doc_id": "2005.14165", "para_id": 453, "text": "Background:\nFrom the German point of view, March 1941 saw an improvement.\nThe Luftwaffe flew 4,000 sorties that month, including 12 major and\nthree heavy attacks.\nThe electronic war intensified but the Luftwaffe\nflew major inland missions only on moonlit nights.\nPorts were easier to\nfind and made better targets.\nTo confuse the British, radio silence was\nobserved until the bombs fell.\nX- and Y-Ger¨at beams were placed over\nfalse targets and switched only at the last minute.\nRapid frequency\nchanges were introduced for X-Ger¨at, whose wider band of frequencies and\ngreater tactical flexibility ensured it remained effective at a time when\nBritish selective jamming was degrading the effectiveness of Y-Ger¨at."}
{"doc_id": "2005.14165", "para_id": 454, "text": "Figure G.28: Formatted dataset example for SQuADv2"}
{"doc_id": "2005.14165", "para_id": 455, "text": "Context →\nNormal force -- In a simple case such as an object resting upon a table,\nthe normal force on the object is equal but in opposite direction to the\ngravitational force applied on the object (or the weight of the object),\nthat is, N = m g (\\displaystyle N=mg), where m is mass, and g is the\ngravitational field strength (about 9.81 m/s on Earth).\nThe normal force\nhere represents the force applied by the table against the object that\nprevents it from sinking through the table and requires that the table is\nsturdy enough to deliver this normal force without breaking.\nHowever, it\nis easy to assume that the normal force and weight are action-reaction\nforce pairs (a common mistake).\nIn this case, the normal force and\nweight need to be equal in magnitude to explain why there is no upward\nacceleration of the object.\nFor example, a ball that bounces upwards\naccelerates upwards because the normal force acting on the ball is larger\nin magnitude than the weight of the ball.\nquestion:\nis the normal force equal to the force of gravity?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 456, "text": "Context →\nThe trend toward lower rents may seem surprising given that some\ncommunities in New York are bemoaning the loss of favorite local\nbusinesses to high rents.\nBut, despite the recent softening, for many\nof these retailers there’s still been too big a jump from the rental rates\nof the late 1970s, when their leases were signed.\nCertainly, the recent\ndrop in prices doesn’t mean Manhattan comes cheap.\nquestion:\nManhattan comes cheap.\ntrue, false, or neither?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 457, "text": "Context →\nThe bet, which won him dinner for four, was regarding the existence and\nmass of the top quark, an elementary particle discovered in 1995.\nquestion:\nThe Top Quark is the last of six flavors of quarks predicted by\nthe standard model theory of particle physics.\nTrue or False?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 458, "text": "Context →\nAn outfitter provided everything needed for the safari.\nBefore his first walking holiday, he went to a specialist outfitter to buy\nsome boots.\nquestion:\nIs the word ‘outfitter’ used in the same way in the two\nsentences above?\nanswer:"}
{"doc_id": "2005.14165", "para_id": 459, "text": "Context →\nFinal Exam with Answer Key\nInstructions:\nPlease carefully read the following passages.\nFor each\npassage, you must identify which noun the pronoun marked in *bold* refers\nto.\n=====\nPassage:\nMr.\nMoncrieff visited Chester’s luxurious New York apartment,\nthinking that it belonged to his son Edward.\nThe result was that Mr.\nMoncrieff has decided to cancel Edward’s allowance on the ground that\nhe no longer requires *his* financial support.\nQuestion:\nIn the passage above, what does the pronoun \"*his*\" refer to?\nAnswer:"}
{"doc_id": "2005.14165", "para_id": 460, "text": "Context →\nQ: ‘Nude Descending A Staircase’ is perhaps the most famous painting by\nwhich 20th century artist?"}
{"doc_id": "2005.14165", "para_id": 461, "text": "Target Completion →\nMARCEL DUCHAMP\nTarget Completion →\nr mutt\nTarget Completion →\nduchamp\nTarget Completion →\nmarcel duchamp\nTarget Completion →\nR.Mutt\nTarget Completion →\nMarcel duChamp\nTarget Completion →\nHenri-Robert-Marcel Duchamp\nTarget Completion →\nMarcel du Champ\nTarget Completion →\nhenri robert marcel duchamp\nTarget Completion →\nDuchampian\nTarget Completion →\nDuchamp\nTarget Completion →\nduchampian\nTarget Completion →\nmarcel du champ\nTarget Completion →\nMarcel Duchamp\nTarget Completion →\nMARCEL DUCHAMP"}
{"doc_id": "2005.14165", "para_id": 462, "text": "Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions."}
{"doc_id": "2005.14165", "para_id": 463, "text": "Context →\nQ: What school did burne hogarth establish?"}
{"doc_id": "2005.14165", "para_id": 464, "text": "Context →\nKeinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden.\n="}
{"doc_id": "2005.14165", "para_id": 465, "text": "Target Completion →\nIn no case may they be used for commercial purposes."}
{"doc_id": "2005.14165", "para_id": 466, "text": "Figure G.36: Formatted dataset example for De→En. This is the format for one- and few-shot learning, for this and\nother langauge tasks, the format for zero-shot learning is “Q: What is the {language} translation of {sentence} A:\n{translation}.”"}
{"doc_id": "2005.14165", "para_id": 467, "text": "Context →\nIn no case may they be used for commercial purposes.\n="}
{"doc_id": "2005.14165", "para_id": 468, "text": "Target Completion →\nKeinesfalls d¨urfen diese f¨ur den kommerziellen Gebrauch verwendet werden."}
{"doc_id": "2005.14165", "para_id": 469, "text": "Context →\nAnalysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females.\n="}
{"doc_id": "2005.14165", "para_id": 470, "text": "Target Completion →\nL’analyse de la distribution de fr´equence des stades larvaires d’I.\nverticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves\nm^ales ´etaient `a des stades plus avanc´es que les larves femelles."}
{"doc_id": "2005.14165", "para_id": 471, "text": "Context →\nL’analyse de la distribution de fr´equence des stades larvaires d’I.\nverticalis dans une s´erie d’´etangs a ´egalement d´emontr´e que les larves\nm^ales ´etaient `a des stades plus avanc´es que les larves femelles.\n="}
{"doc_id": "2005.14165", "para_id": 472, "text": "Target Completion →\nAnalysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females."}
{"doc_id": "2005.14165", "para_id": 473, "text": "Context →\nThe truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey’s accession\nto the European Union, despite Turkey’s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill.\n="}
{"doc_id": "2005.14165", "para_id": 474, "text": "Target Completion →\nAdev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei\neuropenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea\nEuropean˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul\n¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort."}
{"doc_id": "2005.14165", "para_id": 475, "text": "Context →\nAdev˘arul este c˘a v˘a dorit¸i, cu orice pret¸ ¸si ^ımpotriva dorint¸ei\neuropenilor, s˘a continuat¸i negocierile de aderare a Turciei la Uniunea\nEuropean˘a, ^ın ciuda refuzului continuu al Turciei de a recunoa¸ste Ciprul\n¸si ^ın ciuda faptului c˘a reformele democratice au ajuns ^ıntr-un punct mort.\n="}
{"doc_id": "2005.14165", "para_id": 476, "text": "Target Completion →\nThe truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey’s accession\nto the European Union, despite Turkey’s continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill."}
{"doc_id": "2005.14165", "para_id": 477, "text": "Figure G.42: Formatted dataset example for Arithmetic 1DC"}
{"doc_id": "2005.14165", "para_id": 478, "text": "Figure G.43: Formatted dataset example for Arithmetic 2D-"}
{"doc_id": "2005.14165", "para_id": 479, "text": "Figure G.44: Formatted dataset example for Arithmetic 2D+"}
{"doc_id": "2005.14165", "para_id": 480, "text": "Figure G.45: Formatted dataset example for Arithmetic 2Dx"}
{"doc_id": "2005.14165", "para_id": 481, "text": "Figure G.46: Formatted dataset example for Arithmetic 3D-"}
{"doc_id": "2005.14165", "para_id": 482, "text": "Figure G.47: Formatted dataset example for Arithmetic 3D+"}
{"doc_id": "2005.14165", "para_id": 483, "text": "Figure G.48: Formatted dataset example for Arithmetic 4D-"}
{"doc_id": "2005.14165", "para_id": 484, "text": "Figure G.49: Formatted dataset example for Arithmetic 4D+"}
{"doc_id": "2005.14165", "para_id": 485, "text": "Figure G.50: Formatted dataset example for Arithmetic 5D−"}
{"doc_id": "2005.14165", "para_id": 486, "text": "Figure G.51: Formatted dataset example for Arithmetic 5D+"}
{"doc_id": "2005.14165", "para_id": 487, "text": "Name\nMetric\nSplit\nFine-tune\nSOTA\nK\nSmall Med Large XL 2.7B 6.7B 13B 175B\nSmall Med Large XL 2.7B 6.7B 13B 175B\nSmall Med Large XL 2.7B 6.7B 13B 175B\n175B\n(test server)"}
{"doc_id": "2005.14165", "para_id": 488, "text": "HellaSwag\nacc\ndev\n85.6\n20\n33.7\n43.6 51.0\n54.7 62.8 67.4 70.9 78.9\n33.0\n42.9 50.5\n53.5 61.9 66.5 70.0 78.1\n33.5\n43.1 51.3\n54.9 62.9 67.3 71.3 79.3\nLAMBADA\nacc\ntest\n68.0\n15\n42.7\n54.3 60.4\n63.6 67.1 70.3 72.5 76.2\n22.0\n47.1 52.6\n58.3 61.1 65.4 69.0 72.5\n22.0\n40.4 63.2\n57.0 78.1 79.1 81.3 86.4\nLAMBADA\nppl\ntest\n8.63\n15\n18.6\n9.09 6.53\n5.44 4.60 4.00 3.56 3.00\n165.0 11.6 8.29\n6.46 5.53 4.61 4.06 3.35\n165.0 27.6 6.63\n7.45 2.89 2.56 2.56 1.92\nStoryCloze\nacc\ntest\n91.8\n70\n63.3\n68.5 72.4\n73.4 77.2 77.7 79.5 83.2\n62.3\n68.7 72.3\n74.2 77.3 78.7 79.7 84.7\n62.3\n70.2 73.9\n76.1 80.2 81.2 83.0 87.7"}
{"doc_id": "2005.14165", "para_id": 489, "text": "NQs\nacc\ntest\n44.5\n64\n0.64\n1.75 2.71\n4.40 6.01 5.79 7.84 14.6\n1.19\n3.07 4.79\n5.43 8.73 9.78 13.7 23.0\n1.72\n4.46 7.89\n9.72 13.2 17.0 21.0 29.9\nTriviaQA\nacc\ndev\n68.0\n64\n4.15\n7.61 14.0\n19.7 31.3 38.7 41.8 64.3\n4.19\n12.9 20.5\n26.5 35.9 44.4 51.3 68.0\n6.96\n16.3 26.5\n32.1 42.3 51.6 57.5 71.2\n71.2\nWebQs\nacc\ntest\n45.5\n64\n1.77\n3.20 4.33\n4.63 7.92 7.73 8.22 14.4\n2.56\n6.20 8.51\n9.15 14.5 15.1 19.0 25.3\n5.46\n12.6 15.9\n19.6 24.8 27.7 33.5 41.5"}
{"doc_id": "2005.14165", "para_id": 490, "text": "Ro→En 16\nBLEU-mb test\n39.9\n64\n2.08\n2.71 3.09\n3.15 16.3 8.34 20.2 19.9\n0.55\n15.4 23.0\n26.3 30.6 33.2 35.6 38.6\n1.25\n20.7 25.8\n29.2 33.1 34.8 37.0 39.5\nRo→En 16\nBLEU-sb\ntest\n64\n2.39\n3.08 3.49\n3.56 16.8 8.75 20.8 20.9\n0.65\n15.9 23.6\n26.8 31.3 34.2 36.7 40.0\n1.40\n21.3 26.6\n30.1 34.3 36.2 38.4 41.3\nEn→Ro 16\nBLEU-mb test\n38.5\n64\n2.14\n2.65 2.53\n2.50 3.46 4.24 5.32 14.1\n0.35\n3.30 7.89\n8.72 13.2 15.1 17.3 20.6\n1.25\n5.90 9.33\n10.7 14.3 16.3 18.0 21.0\nEn→Ro 16\nBLEU-sb\ntest\n64\n2.61\n3.11 3.07\n3.09 4.26 5.31 6.43 18.0\n0.55\n3.90 9.15\n10.3 15.7 18.2 20.8 24.9\n1.64\n7.40 10.9\n12.9 17.2 19.6 21.8 25.8\nFr→En 14\nBLEU-mb test\n35.0\n64\n1.81\n2.53 3.47\n3.13 20.6 15.1 21.8 21.2\n1.28\n15.9 23.7\n26.3 29.0 30.5 30.2 33.7\n4.98\n25.5 28.5\n31.1 33.7 34.9 36.6 39.2\nFr→En 14\nBLEU-sb\ntest\n64\n2.29\n2.99 3.90\n3.60 21.2 15.5 22.4 21.9\n1.50\n16.3 24.4\n27.0 30.0 31.6 31.4 35.6\n5.30\n26.2 29.5\n32.2 35.1 36.4 38.3 41.4\nEn→Fr 14\nBLEU-mb test\n45.6\n64\n1.74\n2.16 2.73\n2.15 15.1 8.82 12.0 25.2\n0.49\n8.00 14.8\n15.9 20.3 23.3 24.9 28.3\n4.08\n14.5 19.3\n21.5 24.9 27.3 29.5 32.6\nEn→Fr 14\nBLEU-sb\ntest\n45.9\n64\n2.44\n2.75 3.54\n2.82 19.3 11.4 15.3 31.3\n0.81\n10.0 18.2\n19.3 24.7 28.3 30.1 34.1\n5.31\n18.0 23.6\n26.1 30.3 33.3 35.5 39.9\nDe→En 16\nBLEU-mb test\n40.2\n64\n2.06\n2.87 3.41\n3.63 21.5 17.3 23.0 27.2\n0.83\n16.2 22.5\n24.7 28.2 30.7 33.0 30.4\n3.25\n22.7 26.2\n29.2 32.7 34.8 37.3 40.6\nDe→En 16\nBLEU-sb\ntest\n64\n2.39\n3.27 3.85\n4.04 22.5 18.2 24.4 28.6\n0.93\n17.1 23.4\n25.8 29.2 31.9 34.5 32.1\n3.60\n23.8 27.5\n30.5 34.1 36.5 39.1 43.0\nEn→De 16\nBLEU-mb test\n41.2\n64\n1.70\n2.27 2.31\n2.43 12.9 8.66 10.4 24.6\n0.50\n7.00 12.9\n13.1 18.3 20.9 22.5 26.2\n3.42\n12.3 15.4\n17.1 20.9 23.0 26.6 29.7\nEn→De 16\nBLEU-sb\ntest\n41.2\n64\n2.09\n2.65 2.75\n2.92 13.7 9.36 11.0 25.3\n0.54\n7.40 13.4\n13.4 18.8 21.7 23.3 27.3\n3.78\n12.9 16.1\n17.7 21.7 24.1 27.7 30.9"}
{"doc_id": "2005.14165", "para_id": 491, "text": "Winograd\nacc\ntest\n93.8\n7\n66.3\n72.9 74.7\n76.9 82.4 85.7 87.9 88.3\n63.4\n68.5 72.9\n76.9 82.4 84.6 86.1 89.7\n63.4\n67.4 73.6\n76.9 84.3 85.4 82.4 88.6\nWinogrande\nacc\ndev\n84.6\n50\n52.0\n52.1 57.4\n58.7 62.3 64.5 67.9 70.2\n51.3\n53.0 58.3\n59.1 61.7 65.8 66.9 73.2\n51.3\n52.6 57.5\n59.1 62.6 67.4 70.0 77.7"}
{"doc_id": "2005.14165", "para_id": 492, "text": "PIQA\nacc\ndev\n77.1\n50\n64.6\n70.2 72.9\n75.1 75.6 78.0 78.5 81.0\n64.3\n69.3 71.8\n74.4 74.3 76.3 77.8 80.5\n64.3\n69.4 72.0\n74.3 75.4 77.8 79.9 82.3\n82.8\nARC (Challenge) acc\ntest\n78.5\n50\n26.6\n29.5 31.8\n35.5 38.0 41.4 43.7 51.4\n25.5\n30.2 31.6\n36.4 38.4 41.5 43.1 53.2\n25.5\n28.4 32.3\n36.7 39.5 43.7 44.8 51.5\nARC (Easy)\nacc\ntest\n92.0\n50\n43.6\n46.5 53.0\n53.8 58.2 60.2 63.8 68.8\n42.7\n48.2 54.6\n55.9 60.3 62.6 66.8 71.2\n42.7\n51.0 58.1\n59.1 62.1 65.8 69.1 70.1\nOpenBookQA\nacc\ntest\n87.2\n100\n35.6\n43.2 45.2\n46.8 53.0 50.4 55.6 57.6\n37.0\n39.8 46.2\n46.4 53.4 53.0 55.8 58.8\n37.0\n43.6 48.0\n50.6 55.6 55.2 60.8 65.4"}
{"doc_id": "2005.14165", "para_id": 493, "text": "Quac\nf1\ndev\n74.4\n5\n21.2\n26.8 31.0\n30.1 34.7 36.1 38.4 41.5\n21.1\n26.9 31.9\n32.3 37.4 39.0 40.6 43.4\n21.6\n27.6 32.9\n34.2 38.2 39.9 40.9 44.3\nRACE-h\nacc\ntest\n90.0\n10\n35.2\n37.9 40.1\n40.9 42.4 44.1 44.6 45.5\n34.3\n37.7 40.0\n42.0 43.8 44.3 44.6 45.9\n34.3\n37.0 40.4\n41.4 42.3 44.7 45.1 46.8\nRACE-m\nacc\ntest\n93.1\n10\n42.1\n47.2 52.1\n52.3 54.7 54.4 56.7 58.4\n42.3\n47.3 51.7\n55.2 56.1 54.7 56.9 57.4\n42.3\n47.0 52.7\n53.0 55.6 55.4 58.1 58.1\nSQuADv2\nem\ndev\n90.7\n16\n22.6\n32.8 33.9\n43.1 43.6 45.4 49.0 52.6\n25.1\n37.5 37.9\n47.9 47.9 51.1 56.0 60.1\n27.5\n40.5 39.2\n53.5 50.0 56.6 62.6 64.9\nSQuADv2\nf1\ndev\n93.0\n16\n28.3\n40.2 41.4\n50.3 51.0 52.7 56.3 59.5\n30.1\n43.6 44.1\n54.0 54.1 57.1 61.8 65.4\n32.1\n45.5 44.9\n58.7 55.9 62.1 67.7 69.8\nCoQA\nf1\ndev\n90.7\n5\n34.5\n55.0 61.8\n65.3 71.1 72.8 76.3 81.5\n30.6\n52.1 61.6\n66.1 71.8 75.1 77.9 84.0\n31.1\n52.0 62.7\n66.8 73.2 77.3 79.9 85.0\nDROP\nf1\ndev\n89.1\n20\n9.40\n13.6 14.4\n16.4 19.7 17.0 24.0 23.6\n11.7\n18.1 20.9\n23.0 26.4 27.3 29.2 34.3\n12.9\n18.7 24.0\n25.6 29.7 29.7 32.3 36.5"}
{"doc_id": "2005.14165", "para_id": 494, "text": "BoolQ\nacc\ndev\n91.0\n32\n49.7\n60.3 58.9\n62.4 67.1 65.4 66.2 60.5\n52.6\n61.7 60.4\n63.7 68.4 68.7 69.0 76.7\n43.1\n60.6 62.0\n64.1 70.3 70.0 70.2 77.5\n76.4\nCB\nacc\ndev\n96.9\n32\n0.00\n32.1 8.93\n19.6 19.6 28.6 19.6 46.4\n55.4\n53.6 53.6\n48.2 57.1 33.9 55.4 64.3\n42.9\n58.9 53.6\n69.6 67.9 60.7 66.1 82.1\n75.6\nCB\nf1\ndev\n93.9\n32\n0.00\n29.3 11.4\n17.4 22.4 25.1 20.3 42.8\n60.1\n39.8 45.6\n37.5 45.7 28.5 44.6 52.5\n26.1\n40.4 32.6\n48.3 45.7 44.6 46.0 57.2\n52.0\nCopa\nacc\ndev\n94.8\n32\n66.0\n68.0 73.0\n77.0 76.0 80.0 84.0 91.0\n62.0\n64.0 66.0\n74.0 76.0 82.0 86.0 87.0\n67.0\n64.0 72.0\n77.0 83.0 83.0 86.0 92.0\n92.0\nRTE\nacc\ndev\n92.5\n32\n47.7\n49.8 48.4\n56.0 46.6 55.2 62.8 63.5\n53.1\n47.3 49.5\n49.5 54.9 54.9 56.3 70.4\n52.3\n48.4 46.9\n50.9 56.3 49.5 60.6 72.9\n69.0\nWiC\nacc\ndev\n76.1\n32\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.00\n50.0\n50.3 50.3\n49.2 49.4 50.3 50.0 48.6\n49.8\n55.0 53.0\n53.0 51.6 53.1 51.1 55.3\n49.4\nWSC\nacc\ndev\n93.8\n32\n59.6\n56.7 65.4\n61.5 66.3 60.6 64.4 65.4\n58.7\n58.7 60.6\n62.5 66.3 60.6 66.3 69.2\n58.7\n60.6 54.8\n49.0 62.5 67.3 75.0 75.0\n80.1\nMultiRC\nacc\ndev\n62.3\n32\n4.72\n9.65 12.3\n13.6 14.3 18.4 24.2 27.6\n4.72\n9.65 12.3\n13.6 14.3 18.4 24.2 27.6\n6.09\n11.8 16.8\n20.8 24.7 23.8 25.0 32.5\n30.5\nMultiRC\nf1a\ndev\n88.2\n32\n57.0\n59.7 60.4\n59.9 60.0 64.5 71.4 72.9\n57.0\n59.7 60.4\n59.9 60.0 64.5 71.4 72.9\n45.0\n55.9 64.2\n65.4 69.5 66.4 69.3 74.8\n75.4\nReCoRD\nacc\ndev\n92.5\n32\n70.8\n78.5 82.1\n84.1 86.2 88.6 89.0 90.2\n69.8\n77.0 80.7\n83.0 85.9 88.0 88.8 90.2\n69.8\n77.2 81.3\n83.1 86.6 87.9 88.9 89.0\n90.2\nReCoRD\nf1\ndev\n93.3\n32\n71.9\n79.2 82.8\n85.2 87.3 89.5 90.4 91.0\n70.7\n77.8 81.6\n83.9 86.8 88.8 89.7 91.2\n70.7\n77.9 82.1\n84.0 87.5 88.8 89.8 90.1\n91.1\nSuperGLUE\naverage\ndev\n89.0\n40.6\n47.4 46.8\n49.6 50.1 52.3 54.4 58.2\n54.4\n55.1 56.7\n57.8 61.2 59.7 64.3 68.9\n50.2\n56.2 56.8\n60.0 64.3 63.6 66.9 73.2\n71.8"}
{"doc_id": "2005.14165", "para_id": 495, "text": "ANLI R1\nacc\ntest\n73.8\n50\n33.4\n34.2 33.4\n33.4 34.2 32.3 33.2 34.6\n32.1\n31.6 31.9\n34.6 30.6 31.6 32.7 32.0\n32.1\n32.5 30.9\n32.5 33.5 33.1 33.3 36.8\nANLI R2\nacc\ntest\n50.7\n50\n33.2\n31.9 33.3\n33.3 33.8 33.5 33.5 35.4\n35.7\n33.7 33.2\n32.7 32.7 33.9 33.9 33.9\n35.7\n33.8 32.1\n31.4 32.6 33.3 32.6 34.0\nANLI R3\nacc\ntest\n48.3\n50\n33.6\n34.0 33.8\n33.4 35.3 34.8 34.4 34.5\n35.0\n32.6 33.0\n33.9 34.1 33.1 32.5 35.1\n35.0\n34.4 35.1\n36.0 32.7 33.9 34.5 40.2"}
{"doc_id": "2005.14165", "para_id": 496, "text": "2D+\nacc\nn/a\n50\n0.70\n0.65 0.70\n0.85 1.10 2.54 15.4 76.9\n2.00\n0.55 3.15\n4.00 12.1 19.6 73.0 99.6\n2.00\n4.10 3.50\n4.50 8.90 11.9 55.5 100.0\n2D-\nacc\nn/a\n50\n1.25\n1.25 1.25\n1.25 1.60 7.60 12.6 58.0\n1.15\n0.95 1.45\n1.95 3.85 11.5 44.6 86.4\n1.15\n1.45 2.25\n2.70 7.35 13.6 52.4 98.9\n3D+\nacc\nn/a\n50\n0.10\n0.10 0.05\n0.10 0.10 0.25 1.40 34.2\n0.15\n0.00 0.10\n0.30 0.45 0.95 15.4 65.5\n0.15\n0.45 0.30\n0.55 0.75 0.90 8.40 80.4\n3D-\nacc\nn/a\n50\n0.05\n0.05 0.05\n0.05 0.05 0.45 1.35 48.3\n0.05\n0.15 0.25\n0.30 0.55 1.60 6.15 78.7\n0.05\n0.10 0.15\n0.35 0.65 1.05 9.20 94.2\n4D+\nacc\nn/a\n50\n0.05\n0.05 0.00\n0.00 0.05 0.05 0.15 4.00\n0.00\n0.00 0.10\n0.00 0.00 0.10 0.80 14.0\n0.00\n0.05 0.05\n0.00 0.15 0.15 0.40 25.5\n4D-\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.10 7.50\n0.00\n0.00 0.00\n0.00 0.05 0.00 0.50 14.0\n0.00\n0.05 0.00\n0.00 0.10 0.05 0.40 26.8\n5D+\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.65\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 3.45\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 9.30\n5D-\nacc\nn/a\n50\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 0.80\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.05 3.75\n0.00\n0.00 0.00\n0.00 0.00 0.00 0.00 9.90\n2Dx\nacc\nn/a\n50\n2.20\n2.25 2.65\n2.10 2.55 5.80 6.15 19.8\n1.35\n2.35 3.35\n2.35 4.75 9.15 11.0 27.4\n1.35\n2.90 2.70\n2.85 4.25 6.10 7.05 29.2\n1DC\nacc\nn/a\n50\n1.25\n2.95 2.75\n0.05 0.30 2.35 0.75 9.75\n1.90\n2.80 2.85\n3.65 6.45 9.15 8.20 14.3\n1.70\n2.15 3.90\n5.75 6.20 7.60 9.95 21.3"}
{"doc_id": "2005.14165", "para_id": 497, "text": "Cycled Letters\nacc\nn/a\n100\n0.62\n0.71 2.85\n0.00 0.63 1.35 2.58 3.66\n1.67\n4.36 5.68\n6.46 6.25 9.41 15.1 21.7\n4.63\n9.27 10.7\n14.5 16.7 21.9 27.7 37.9\nAnagrams 1\nacc\nn/a\n100\n0.10\n0.14 0.40\n0.00 0.27 0.69 1.16 2.28\n0.21\n0.61 1.12\n1.27 1.60 2.72 3.72 8.62\n0.50\n1.27 2.13\n3.05 3.81 5.49 8.38 15.1\nAnagrams 2\nacc\nn/a\n100\n0.81\n1.21 2.69\n0.01 1.71 3.75 4.53 8.91\n1.19\n2.62 4.70\n4.77 6.97 10.2 14.6 25.9\n1.94\n4.80 7.59\n9.87 12.6 18.9 25.6 39.7\nSymbol Insertion acc\nn/a\n100\n0.00\n0.00 0.10\n0.00 0.05 0.42 0.89 8.26\n0.03\n0.05 0.57\n1.18 1.67 3.46 6.62 45.4\n0.11\n0.28 2.19\n4.18 6.61 11.0 27.3 67.2\nReversed Words\nacc\nn/a\n100\n0.00\n0.01 0.01\n0.01 0.02 0.03 0.03 0.09\n0.02\n0.01 0.01\n0.00 0.05 0.07 0.11 0.48\n0.00\n0.05 0.00\n0.17 0.24 0.30 0.42 0.44"}
{"doc_id": "2005.14165", "para_id": 498, "text": "SAT Analogies\nacc\nn/a\n20\n35.6\n39.0 45.2\n44.1 50.0 49.2 52.7 53.7\n30.5\n41.2 43.1\n46.5 55.1 54.3 53.5 59.1\n30.5\n40.4 42.8\n40.6 48.4 51.9 53.5 65.2"}
{"doc_id": "2005.14165", "para_id": 499, "text": "Table H.1: Scores for every task, setting and model that we investigate in this paper."}
{"doc_id": "2005.14165", "para_id": 500, "text": "Figure H.2: Results for SAT task.\nFigure H.3: All results for all Winograd tasks."}
{"doc_id": "2005.14165", "para_id": 501, "text": "Figure H.5: All results for all Cloze and Completion tasks."}
{"doc_id": "2005.14165", "para_id": 502, "text": "Figure H.6: All results for all Common Sense Reasoning tasks."}
{"doc_id": "2005.14165", "para_id": 503, "text": "Figure H.8: All results for all Reading Comprehension tasks."}
{"doc_id": "2005.14165", "para_id": 504, "text": "Figure H.11: All results for all Translation tasks."}
{"doc_id": "2005.14165", "para_id": 505, "text": "[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent.\nIn Advances in neural information processing systems, pages 3981–3989, 2016."}
{"doc_id": "2005.14165", "para_id": 506, "text": "[AI19] WeChat AI. Tr-mt (ensemble), December 2019."}
{"doc_id": "2005.14165", "para_id": 507, "text": "[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019."}
{"doc_id": "2005.14165", "para_id": 508, "text": "[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum´e III, and Hanna Wallach. Language (technology) is power:\nA critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050, 2020."}
{"doc_id": "2005.14165", "para_id": 509, "text": "[BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language\nprocessing, pages 1533–1544, 2013."}
{"doc_id": "2005.14165", "para_id": 510, "text": "[BDD+09] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth\nPASCAL recognizing textual entailment challenge. 2009."}
{"doc_id": "2005.14165", "para_id": 511, "text": "[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical\nresource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200–2204, 2010."}
{"doc_id": "2005.14165", "para_id": 512, "text": "[BHDD+06] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The second PASCAL recognising textual entailment challenge. 2006."}
{"doc_id": "2005.14165", "para_id": 513, "text": "[BHT+20] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella\nLapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language.\narXiv preprint arXiv:2004.10151, 2020."}
{"doc_id": "2005.14165", "para_id": 514, "text": "[BLC13] Yoshua Bengio, Nicholas L´eonard, and Aaron C. Courville. Estimating or propagating gradients through\nstochastic neurons for conditional computation. Arxiv, 2013."}
{"doc_id": "2005.14165", "para_id": 515, "text": "[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. arXiv preprint arXiv:1911.11641, 2019."}
{"doc_id": "2005.14165", "para_id": 516, "text": "[Car97] Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "2005.14165", "para_id": 517, "text": "[CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language\nConference, 1978."}
{"doc_id": "2005.14165", "para_id": 518, "text": "[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457, 2018."}
{"doc_id": "2005.14165", "para_id": 519, "text": "[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers, 2019."}
{"doc_id": "2005.14165", "para_id": 520, "text": "[CHI+18] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. Quac : Question answering in context. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 521, "text": "[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019."}
{"doc_id": "2005.14165", "para_id": 522, "text": "[CLY+19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740,\n2019."}
{"doc_id": "2005.14165", "para_id": 523, "text": "[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote, 2017."}
{"doc_id": "2005.14165", "para_id": 524, "text": "[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."}
{"doc_id": "2005.14165", "para_id": 525, "text": "[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiﬁcation,\nand recognising textual entailment, pages 177–190. Springer, 2006."}
{"doc_id": "2005.14165", "para_id": 526, "text": "[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. Arxiv, 2018."}
{"doc_id": "2005.14165", "para_id": 527, "text": "[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine\ntranslation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation,\npages 97–104, 2014."}
{"doc_id": "2005.14165", "para_id": 528, "text": "[DL15] Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems, 2015."}
{"doc_id": "2005.14165", "para_id": 529, "text": "[DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat-\ning projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung\n23. Data can be found at https://github.com/mcdm/CommitmentBank/."}
{"doc_id": "2005.14165", "para_id": 530, "text": "[DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016."}
{"doc_id": "2005.14165", "para_id": 531, "text": "[DWD+19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDrop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161, 2019."}
{"doc_id": "2005.14165", "para_id": 532, "text": "[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. Arxiv, 2019."}
{"doc_id": "2005.14165", "para_id": 533, "text": "[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale.\narXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "2005.14165", "para_id": 534, "text": "[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. ArXiv, abs/1703.03400, 2017."}
{"doc_id": "2005.14165", "para_id": 535, "text": "[Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000."}
{"doc_id": "2005.14165", "para_id": 536, "text": "[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases\nin word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019."}
{"doc_id": "2005.14165", "para_id": 537, "text": "[GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020."}
{"doc_id": "2005.14165", "para_id": 538, "text": "[GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computational Linguistics, 2007."}
{"doc_id": "2005.14165", "para_id": 539, "text": "[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 540, "text": "[GSL+18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A\nSmith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018."}
{"doc_id": "2005.14165", "para_id": 541, "text": "[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza-\ntion of generated text. arXiv preprint arXiv: 1906.04043, 2019."}
{"doc_id": "2005.14165", "para_id": 542, "text": "[GWC+18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource\nneural machine translation. arXiv preprint arXiv:1808.08437, 2018."}
{"doc_id": "2005.14165", "para_id": 543, "text": "[HB20] Daniel Hernandez and Tom Brown. Ai and efﬁciency, May 2020."}
{"doc_id": "2005.14165", "para_id": 544, "text": "[HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\nCoRR, abs/1904.09751, 2019."}
{"doc_id": "2005.14165", "para_id": 545, "text": "[HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020."}
{"doc_id": "2005.14165", "para_id": 546, "text": "[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md.\nMostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.\narXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "2005.14165", "para_id": 547, "text": "[HR18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv\npreprint arXiv:1801.06146, 2018."}
{"doc_id": "2005.14165", "para_id": 548, "text": "[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015."}
{"doc_id": "2005.14165", "para_id": 549, "text": "[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent.\nIn International Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001."}
{"doc_id": "2005.14165", "para_id": 550, "text": "[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini,\nDani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual\nevaluation. arXiv preprint arXiv:1911.03064, 2019."}
{"doc_id": "2005.14165", "para_id": 551, "text": "[IBGC+14] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. A neural\nnetwork for factoid question answering over paragraphs. In Empirical Methods in Natural Language\nProcessing, 2014."}
{"doc_id": "2005.14165", "para_id": 552, "text": "[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of\ngenerated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019."}
{"doc_id": "2005.14165", "para_id": 553, "text": "[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017."}
{"doc_id": "2005.14165", "para_id": 554, "text": "[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020."}
{"doc_id": "2005.14165", "para_id": 555, "text": "[JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits\nof language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "2005.14165", "para_id": 556, "text": "[JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019."}
{"doc_id": "2005.14165", "para_id": 557, "text": "[JZC+19] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on\nconversational question answering. arXiv preprint arXiv:1909.10772, 2019."}
{"doc_id": "2005.14165", "para_id": 558, "text": "[KCR+18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North\nAmerican Chapter of the Association for Computational Linguistics (NAACL), 2018."}
{"doc_id": "2005.14165", "para_id": 559, "text": "[KKS+20] Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi.\nUniﬁedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020."}
{"doc_id": "2005.14165", "para_id": 560, "text": "[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s ﬁt to fabricate: Ai-generated\ntext as a tool of media misinformation, 2020."}
{"doc_id": "2005.14165", "para_id": 561, "text": "[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020."}
{"doc_id": "2005.14165", "para_id": 562, "text": "[KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-\ntions: a benchmark for question answering research. Transactions of the Association of Computational\nLinguistics, 2019."}
{"doc_id": "2005.14165", "para_id": 563, "text": "[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016."}
{"doc_id": "2005.14165", "para_id": 564, "text": "[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002."}
{"doc_id": "2005.14165", "para_id": 565, "text": "[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019."}
{"doc_id": "2005.14165", "para_id": 566, "text": "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019."}
{"doc_id": "2005.14165", "para_id": 567, "text": "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\nAdversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020."}
{"doc_id": "2005.14165", "para_id": 568, "text": "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint\narXiv:1905.07504, 2019."}
{"doc_id": "2005.14165", "para_id": 569, "text": "[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning, 2012."}
{"doc_id": "2005.14165", "para_id": 570, "text": "[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\narXiv:2001.08210, 2020."}
{"doc_id": "2005.14165", "para_id": 571, "text": "[LGH+15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation\nlearning using multi-task deep neural networks for semantic classiﬁcation and information retrieval. In\nProceedings of the 2015 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2015."}
{"doc_id": "2005.14165", "para_id": 572, "text": "[LH17] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017."}
{"doc_id": "2005.14165", "para_id": 573, "text": "[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\nnetworks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482,\n2019."}
{"doc_id": "2005.14165", "para_id": 574, "text": "[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\nnatural language understanding. arXiv preprint arXiv:1901.11504, 2019."}
{"doc_id": "2005.14165", "para_id": 575, "text": "[Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint\narXiv:2005.00955, 2020."}
{"doc_id": "2005.14165", "para_id": 576, "text": "[LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019."}
{"doc_id": "2005.14165", "para_id": 577, "text": "[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017."}
{"doc_id": "2005.14165", "para_id": 578, "text": "[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692, 2019."}
{"doc_id": "2005.14165", "para_id": 579, "text": "[LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Kiela Douwe.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401,\n2020."}
{"doc_id": "2005.14165", "para_id": 580, "text": "[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018."}
{"doc_id": "2005.14165", "para_id": 581, "text": "[LWS+20] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\nTrain large, then compress: Rethinking model size for efﬁcient training and inference of transformers,\n2020."}
{"doc_id": "2005.14165", "para_id": 582, "text": "[LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017."}
{"doc_id": "2005.14165", "para_id": 583, "text": "[LYN+20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy\nLin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020."}
{"doc_id": "2005.14165", "para_id": 584, "text": "[Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation,\n1992."}
{"doc_id": "2005.14165", "para_id": 585, "text": "[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305,\n2017."}
{"doc_id": "2005.14165", "para_id": 586, "text": "[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781, 2013."}
{"doc_id": "2005.14165", "para_id": 587, "text": "[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of\ncommonsense stories. arXiv preprint arXiv:1604.01696, 2016."}
{"doc_id": "2005.14165", "para_id": 588, "text": "[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?\na new dataset for open book question answering. ArXiv, abs/1809.02789, 2018."}
{"doc_id": "2005.14165", "para_id": 589, "text": "[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of\nlarge-batch training, 2018."}
{"doc_id": "2005.14165", "para_id": 590, "text": "[MKM+94] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,\nKaren Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure.\nIn Proceedings of the workshop on Human Language Technology, pages 114–119. Association for\nComputational Linguistics, 1994."}
{"doc_id": "2005.14165", "para_id": 591, "text": "[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2005.14165", "para_id": 592, "text": "[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019."}
{"doc_id": "2005.14165", "para_id": 593, "text": "[MWZ+18] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018."}
{"doc_id": "2005.14165", "para_id": 594, "text": "[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456, 2020."}
{"doc_id": "2005.14165", "para_id": 595, "text": "[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments.\narXiv preprint arXiv:1907.07355, 2019."}
{"doc_id": "2005.14165", "para_id": 596, "text": "[Nor09] Peter Norvig. Natural language corpus data, 2009."}
{"doc_id": "2005.14165", "para_id": 597, "text": "[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor\nas woman is to doctor. arXiv preprint arXiv:1905.09866, 2019."}
{"doc_id": "2005.14165", "para_id": 598, "text": "[NWD+19] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nnli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019."}
{"doc_id": "2005.14165", "para_id": 599, "text": "[PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating\ncontext-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "2005.14165", "para_id": 600, "text": "[PFB18] Jason Phang, Thibault F´evry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary\ntraining on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018."}
{"doc_id": "2005.14165", "para_id": 601, "text": "[PHR+18] Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. Collecting diverse natural language inference problems for sentence representation\nevaluation. In Proceedings of EMNLP, 2018."}
{"doc_id": "2005.14165", "para_id": 602, "text": "[PKL+16] Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern´andez. The lambada dataset: Word prediction\nrequiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016."}
{"doc_id": "2005.14165", "para_id": 603, "text": "[PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word\nembeddings: Architecture and representation, 2018."}
{"doc_id": "2005.14165", "para_id": 604, "text": "[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771, 2018."}
{"doc_id": "2005.14165", "para_id": 605, "text": "[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning.\nGloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), 2014."}
{"doc_id": "2005.14165", "para_id": 606, "text": "[QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020."}
{"doc_id": "2005.14165", "para_id": 607, "text": "[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801, 2019."}
{"doc_id": "2005.14165", "para_id": 608, "text": "[RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011."}
{"doc_id": "2005.14165", "para_id": 609, "text": "[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019."}
{"doc_id": "2005.14165", "para_id": 610, "text": "[RCP+17] Scott Reed, Yutian Chen, Thomas Paine, A¨aron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn\ndistributions. arXiv preprint arXiv:1710.10304, 2017."}
{"doc_id": "2005.14165", "para_id": 611, "text": "[RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for\nsquad. arXiv preprint arXiv:1806.03822, 2018."}
{"doc_id": "2005.14165", "para_id": 612, "text": "[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral),\n2016."}
{"doc_id": "2005.14165", "para_id": 613, "text": "[RLL+19] Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension\nwith numerical reasoning. In Proceedings of EMNLP, 2019."}
{"doc_id": "2005.14165", "para_id": 614, "text": "[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme.\nGender bias in\ncoreference resolution. arXiv preprint arXiv:1804.09301, 2018."}
{"doc_id": "2005.14165", "para_id": 615, "text": "[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training, 2018."}
{"doc_id": "2005.14165", "para_id": 616, "text": "[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication, 2012."}
{"doc_id": "2005.14165", "para_id": 617, "text": "[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of\nthe generalization error across scales, 2019."}
{"doc_id": "2005.14165", "para_id": 618, "text": "[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters\nof a language model? arXiv preprint arXiv:2002.08910, 2020."}
{"doc_id": "2005.14165", "para_id": 619, "text": "[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer, 2019."}
{"doc_id": "2005.14165", "para_id": 620, "text": "[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019."}
{"doc_id": "2005.14165", "para_id": 621, "text": "[SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale, 2019."}
{"doc_id": "2005.14165", "para_id": 622, "text": "[SBC+19] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris\nMcGufﬁe, and Jasmine Wang. Release strategies and the social impacts of language models, 2019."}
{"doc_id": "2005.14165", "para_id": 623, "text": "[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: On biases in language generation. arXiv preprint arXiv:1909.01326, 2019."}
{"doc_id": "2005.14165", "para_id": 624, "text": "[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."}
{"doc_id": "2005.14165", "para_id": 625, "text": "[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR, abs/1907.10597, 2019."}
{"doc_id": "2005.14165", "para_id": 626, "text": "[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709, 2015."}
{"doc_id": "2005.14165", "para_id": 627, "text": "[SMM+17] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017."}
{"doc_id": "2005.14165", "para_id": 628, "text": "[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2019."}
{"doc_id": "2005.14165", "para_id": 629, "text": "[SS20] Timo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation and natural\nlanguage inference. arXiv preprint arXiv:2001.07676, 2020."}
{"doc_id": "2005.14165", "para_id": 630, "text": "[STQ+19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence\npre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "2005.14165", "para_id": 631, "text": "[TFR+17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain\nrandomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017."}
{"doc_id": "2005.14165", "para_id": 632, "text": "[TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations.\nCoRR, abs/cs/0508103, 2005."}
{"doc_id": "2005.14165", "para_id": 633, "text": "[TL18] Trieu H. Trinh and Quoc V. Le.\nA simple method for commonsense reasoning.\narXiv preprint\narXiv:1806.02847, 2018."}
{"doc_id": "2005.14165", "para_id": 634, "text": "[TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent\nmodules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003."}
{"doc_id": "2005.14165", "para_id": 635, "text": "[Tur20] Project Turing. Microsoft research blog, Feb 2020."}
{"doc_id": "2005.14165", "para_id": 636, "text": "[VBL+16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One\nShot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016."}
{"doc_id": "2005.14165", "para_id": 637, "text": "[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems, 2017."}
{"doc_id": "2005.14165", "para_id": 638, "text": "[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand-\ning systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019."}
{"doc_id": "2005.14165", "para_id": 639, "text": "[WXH+18] Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent\ndual learning. ICLR 2019, 2018."}
{"doc_id": "2005.14165", "para_id": 640, "text": "[XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le.\nUnsupervised data\naugmentation for consistency training, 2019."}
{"doc_id": "2005.14165", "para_id": 641, "text": "[YdC+19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,\nLingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating\ngeneral linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019."}
{"doc_id": "2005.14165", "para_id": 642, "text": "[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237,\n2019."}
{"doc_id": "2005.14165", "para_id": 643, "text": "[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019."}
{"doc_id": "2005.14165", "para_id": 644, "text": "[ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin\nChoi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019."}
{"doc_id": "2005.14165", "para_id": 645, "text": "[ZLL+18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885, 2018."}
{"doc_id": "2005.14165", "para_id": 646, "text": "[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019."}
{"doc_id": "2005.14165", "para_id": 647, "text": "[ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593,\n2019."}
{"doc_id": "2010.11929", "para_id": 0, "text": "AN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"}
{"doc_id": "2010.11929", "para_id": 1, "text": "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†"}
{"doc_id": "2010.11929", "para_id": 2, "text": "∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com"}
{"doc_id": "2010.11929", "para_id": 3, "text": "While the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1"}
{"doc_id": "2010.11929", "para_id": 4, "text": "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance."}
{"doc_id": "2010.11929", "para_id": 5, "text": "In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020)."}
{"doc_id": "2010.11929", "para_id": 6, "text": "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classiﬁcation in supervised fashion."}
{"doc_id": "2010.11929", "para_id": 7, "text": "When trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases"}
{"doc_id": "2010.11929", "para_id": 8, "text": "1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer"}
{"doc_id": "2010.11929", "para_id": 9, "text": "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data."}
{"doc_id": "2010.11929", "para_id": 10, "text": "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks."}
{"doc_id": "2010.11929", "para_id": 11, "text": "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020)."}
{"doc_id": "2010.11929", "para_id": 12, "text": "Naive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefﬁciently on hardware accelerators."}
{"doc_id": "2010.11929", "para_id": 13, "text": "Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well."}
{"doc_id": "2010.11929", "para_id": 14, "text": "There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019)."}
{"doc_id": "2010.11929", "para_id": 15, "text": "Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet."}
{"doc_id": "2010.11929", "para_id": 16, "text": "Our work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works."}
{"doc_id": "2010.11929", "para_id": 17, "text": "Linear Projection of Flattened Patches\n* Extra learnable\n[ cl ass]  embedding"}
{"doc_id": "2010.11929", "para_id": 18, "text": "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017)."}
{"doc_id": "2010.11929", "para_id": 19, "text": "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\ntheir efﬁcient implementations – can be used almost out of the box."}
{"doc_id": "2010.11929", "para_id": 20, "text": "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings."}
{"doc_id": "2010.11929", "para_id": 21, "text": "Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time."}
{"doc_id": "2010.11929", "para_id": 22, "text": "Position embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder."}
{"doc_id": "2010.11929", "para_id": 23, "text": "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019)."}
{"doc_id": "2010.11929", "para_id": 24, "text": "The MLP contains two layers with a GELU non-linearity."}
{"doc_id": "2010.11929", "para_id": 25, "text": "z0 = [xclass; x1\npE; x2\npE; · · · ; xN\np E] + Epos,\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\n(1)"}
{"doc_id": "2010.11929", "para_id": 26, "text": "Inductive bias.\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch."}
{"doc_id": "2010.11929", "para_id": 27, "text": "Hybrid Architecture.\nAs an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classiﬁcation input embedding and position embeddings are added as described above."}
{"doc_id": "2010.11929", "para_id": 28, "text": "Typically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 29, "text": "We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future."}
{"doc_id": "2010.11929", "para_id": 30, "text": "Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 31, "text": "ViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M"}
{"doc_id": "2010.11929", "para_id": 32, "text": "Table 1: Details of Vision Transformer model variants."}
{"doc_id": "2010.11929", "para_id": 33, "text": "We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\nimagery, and Structured – tasks that require geometric understanding like localization."}
{"doc_id": "2010.11929", "para_id": 34, "text": "Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive."}
{"doc_id": "2010.11929", "para_id": 35, "text": "For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model."}
{"doc_id": "2010.11929", "para_id": 36, "text": "Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b)."}
{"doc_id": "2010.11929", "para_id": 37, "text": "Metrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\nwhere ﬁne-tuning would be too costly."}
{"doc_id": "2010.11929", "para_id": 38, "text": "We ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days."}
{"doc_id": "2010.11929", "para_id": 39, "text": "Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this"}
{"doc_id": "2010.11929", "para_id": 40, "text": "Ours-JFT\nOurs-JFT\nOurs-I21k\nBiT-L\nNoisy Student\n(ViT-H/14)\n(ViT-L/16)\n(ViT-L/16)\n(ResNet152x4)\n(EfﬁcientNet-L2)"}
{"doc_id": "2010.11929", "para_id": 41, "text": "ImageNet\n88.55 ± 0.04\n87.76 ± 0.03\n85.30 ± 0.02\n87.54 ± 0.02\n88.4/88.5∗\nImageNet ReaL\n90.72 ± 0.05\n90.54 ± 0.03\n88.62 ± 0.05\n90.54\n90.55\nCIFAR-10\n99.50 ± 0.06\n99.42 ± 0.03\n99.15 ± 0.03\n99.37 ± 0.06\n−\nCIFAR-100\n94.55 ± 0.04\n93.90 ± 0.05\n93.25 ± 0.05\n93.51 ± 0.08\n−\nOxford-IIIT Pets\n97.56 ± 0.03\n97.32 ± 0.11\n94.67 ± 0.15\n96.62 ± 0.23\n−\nOxford Flowers-102\n99.68 ± 0.02\n99.74 ± 0.00\n99.61 ± 0.02\n99.63 ± 0.03\n−\nVTAB (19 tasks)\n77.63 ± 0.23\n76.28 ± 0.46\n72.72 ± 0.21\n76.29 ± 1.70\n−"}
{"doc_id": "2010.11929", "para_id": 42, "text": "Table 2:\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\nin Touvron et al. (2020)."}
{"doc_id": "2010.11929", "para_id": 43, "text": "70\nViT-H/14\nBiT-L (R152x4)\nVIVI-Ex-100% (R50x3)\nS4L (R50x1)"}
{"doc_id": "2010.11929", "para_id": 44, "text": "Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups."}
{"doc_id": "2010.11929", "para_id": 45, "text": "model still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\nproximately 30 days."}
{"doc_id": "2010.11929", "para_id": 46, "text": "Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\nSpecialized the performance of the top two models is similar."}
{"doc_id": "2010.11929", "para_id": 47, "text": "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments."}
{"doc_id": "2010.11929", "para_id": 48, "text": "First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance"}
{"doc_id": "2010.11929", "para_id": 49, "text": "2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\nresolution increase during ﬁne-tuning improves the performance."}
{"doc_id": "2010.11929", "para_id": 50, "text": "ImageNet\nImageNet-21k\nJFT-300M\nPre-training dataset"}
{"doc_id": "2010.11929", "para_id": 51, "text": "10 M\n30 M\n100 M\n300 M\nNumber of JFT pre-training samples"}
{"doc_id": "2010.11929", "para_id": 52, "text": "Figure 3:\nTransfer to ImageNet.\nWhile\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows."}
{"doc_id": "2010.11929", "para_id": 53, "text": "Figure 4: Linear few-shot evaluation on Ima-\ngeNet versus pre-training size. ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved."}
{"doc_id": "2010.11929", "para_id": 54, "text": "Figure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models."}
{"doc_id": "2010.11929", "para_id": 55, "text": "region spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes."}
{"doc_id": "2010.11929", "para_id": 56, "text": "Second, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is sufﬁcient, even beneﬁcial."}
{"doc_id": "2010.11929", "para_id": 57, "text": "Overall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work."}
{"doc_id": "2010.11929", "para_id": 58, "text": "We perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone)."}
{"doc_id": "2010.11929", "para_id": 59, "text": "Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts."}
{"doc_id": "2010.11929", "para_id": 60, "text": "To begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The ﬁrst layer of\nthe Vision Transformer linearly projects the ﬂattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding ﬁlters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the ﬁne structure within each patch."}
{"doc_id": "2010.11929", "para_id": 61, "text": "After the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4)."}
{"doc_id": "2010.11929", "para_id": 62, "text": "Self-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speciﬁcally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6)."}
{"doc_id": "2010.11929", "para_id": 63, "text": "Figure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails."}
{"doc_id": "2010.11929", "para_id": 64, "text": "Transformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin"}
{"doc_id": "2010.11929", "para_id": 65, "text": "RGB embedding filters\n(first 28 principal components)"}
{"doc_id": "2010.11929", "para_id": 66, "text": "Figure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details."}
{"doc_id": "2010.11929", "para_id": 67, "text": "et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work."}
{"doc_id": "2010.11929", "para_id": 68, "text": "We have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\ndatasets, whilst being relatively cheap to pre-train."}
{"doc_id": "2010.11929", "para_id": 69, "text": "While these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\ntraining. Finally, further scaling of ViT would likely lead to improved performance."}
{"doc_id": "2010.11929", "para_id": 70, "text": "The work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions."}
{"doc_id": "2010.11929", "para_id": 71, "text": "Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020."}
{"doc_id": "2010.11929", "para_id": 72, "text": "Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 73, "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 74, "text": "I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nIn ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 75, "text": "Lucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\nwe done with imagenet? arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 76, "text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 77, "text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 78, "text": "Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a."}
{"doc_id": "2010.11929", "para_id": 79, "text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020b."}
{"doc_id": "2010.11929", "para_id": 80, "text": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c."}
{"doc_id": "2010.11929", "para_id": 81, "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 82, "text": "Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR, 2020."}
{"doc_id": "2010.11929", "para_id": 83, "text": "J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009."}
{"doc_id": "2010.11929", "para_id": 84, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019."}
{"doc_id": "2010.11929", "para_id": 85, "text": "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\nlutional neural networks. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 86, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016."}
{"doc_id": "2010.11929", "para_id": 87, "text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 88, "text": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv, 2019."}
{"doc_id": "2010.11929", "para_id": 89, "text": "Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 90, "text": "Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 91, "text": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020."}
{"doc_id": "2010.11929", "para_id": 92, "text": "Olivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\nICML, 2020."}
{"doc_id": "2010.11929", "para_id": 93, "text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015."}
{"doc_id": "2010.11929", "para_id": 94, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}
{"doc_id": "2010.11929", "para_id": 95, "text": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020."}
{"doc_id": "2010.11929", "para_id": 96, "text": "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."}
{"doc_id": "2010.11929", "para_id": 97, "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In NIPS, 2012."}
{"doc_id": "2010.11929", "para_id": 98, "text": "Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989."}
{"doc_id": "2010.11929", "para_id": 99, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 100, "text": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019."}
{"doc_id": "2010.11929", "para_id": 101, "text": "Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020."}
{"doc_id": "2010.11929", "para_id": 102, "text": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 103, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 104, "text": "M. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nICVGIP, 2008."}
{"doc_id": "2010.11929", "para_id": 105, "text": "Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\n2012."}
{"doc_id": "2010.11929", "para_id": 106, "text": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018."}
{"doc_id": "2010.11929", "para_id": 107, "text": "B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838–855, 1992.\ndoi: 10.1137/0330046.\nURL\nhttps://doi.org/10.1137/0330046."}
{"doc_id": "2010.11929", "para_id": 108, "text": "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520, 2019."}
{"doc_id": "2010.11929", "para_id": 109, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018."}
{"doc_id": "2010.11929", "para_id": 110, "text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019."}
{"doc_id": "2010.11929", "para_id": 111, "text": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019."}
{"doc_id": "2010.11929", "para_id": 112, "text": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, 2017."}
{"doc_id": "2010.11929", "para_id": 113, "text": "Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV, 2019."}
{"doc_id": "2010.11929", "para_id": 114, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS. 2019."}
{"doc_id": "2010.11929", "para_id": 115, "text": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020."}
{"doc_id": "2010.11929", "para_id": 116, "text": "Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020."}
{"doc_id": "2010.11929", "para_id": 117, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017."}
{"doc_id": "2010.11929", "para_id": 118, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a."}
{"doc_id": "2010.11929", "para_id": 119, "text": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\narXiv preprint\narXiv:2003.07853, 2020b."}
{"doc_id": "2010.11929", "para_id": 120, "text": "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019."}
{"doc_id": "2010.11929", "para_id": 121, "text": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018."}
{"doc_id": "2010.11929", "para_id": 122, "text": "Dirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR, 2019."}
{"doc_id": "2010.11929", "para_id": 123, "text": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020."}
{"doc_id": "2010.11929", "para_id": 124, "text": "Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018."}
{"doc_id": "2010.11929", "para_id": 125, "text": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\nimproves imagenet classiﬁcation. In CVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 126, "text": "Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\nSupervised Learning. In ICCV, 2019a."}
{"doc_id": "2010.11929", "para_id": 127, "text": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b."}
{"doc_id": "2010.11929", "para_id": 128, "text": "Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR, 2020."}
{"doc_id": "2010.11929", "para_id": 129, "text": "Models\nDataset\nEpochs\nBase LR\nLR decay\nWeight decay\nDropout"}
{"doc_id": "2010.11929", "para_id": 130, "text": "ViT-B/{16,32}\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nViT-L/32\nJFT-300M\n7\n6 · 10−4\nlinear\n0.1\n0.0\nViT-L/16\nJFT-300M\n7/14\n4 · 10−4\nlinear\n0.1\n0.0\nViT-H/14\nJFT-300M\n14\n3 · 10−4\nlinear\n0.1\n0.0\nR50x{1,2}\nJFT-300M\n7\n10−3\nlinear\n0.1\n0.0\nR101x1\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nR152x{1,2}\nJFT-300M\n7\n6 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-B/{16,32}\nJFT-300M\n7\n8 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-L/32\nJFT-300M\n7\n2 · 10−4\nlinear\n0.1\n0.0\nR50+ViT-L/16\nJFT-300M\n7/14\n4 · 10−4\nlinear\n0.1\n0.0\nViT-B/{16,32}\nImageNet-21k\n90\n10−3\nlinear\n0.03\n0.1\nViT-L/{16,32}\nImageNet-21k\n30/90\n10−3\nlinear\n0.03\n0.1\nViT-∗\nImageNet\n300\n3 · 10−3\ncosine\n0.3\n0.1"}
{"doc_id": "2010.11929", "para_id": 131, "text": "Table 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224."}
{"doc_id": "2010.11929", "para_id": 132, "text": "Standard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\ntwo elements of the sequence and their respective query qi and key kj representations."}
{"doc_id": "2010.11929", "para_id": 133, "text": "Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k."}
{"doc_id": "2010.11929", "para_id": 134, "text": "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\nUmsa ∈Rk·Dh×D\n(8)"}
{"doc_id": "2010.11929", "para_id": 135, "text": "Table 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224."}
{"doc_id": "2010.11929", "para_id": 136, "text": "We ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,"}
{"doc_id": "2010.11929", "para_id": 137, "text": "ImageNet\n20 000\n{0.003, 0.01, 0.03, 0.06}\nCIFAR100\n10 000\n{0.001, 0.003, 0.01, 0.03}\nCIFAR10\n10 000\n{0.001, 0.003, 0.01, 0.03}\nOxford-IIIT Pets\n500\n{0.001, 0.003, 0.01, 0.03}\nOxford Flowers-102\n500\n{0.001, 0.003, 0.01, 0.03}\nVTAB (19 tasks)\n2 500\n0.01"}
{"doc_id": "2010.11929", "para_id": 138, "text": "Table 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\nﬁne-tuning resolution is 384."}
{"doc_id": "2010.11929", "para_id": 139, "text": "for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020))."}
{"doc_id": "2010.11929", "para_id": 140, "text": "When transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer."}
{"doc_id": "2010.11929", "para_id": 141, "text": "For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks."}
{"doc_id": "2010.11929", "para_id": 142, "text": "We employ the masked patch prediction objective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations."}
{"doc_id": "2010.11929", "para_id": 143, "text": "We trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics."}
{"doc_id": "2010.11929", "para_id": 144, "text": "Lastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet."}
{"doc_id": "2010.11929", "para_id": 145, "text": "We report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to"}
{"doc_id": "2010.11929", "para_id": 146, "text": "ImageNet\nCIFAR-10\n98.13\n97.77\n97.86\n97.94\n-\nCIFAR-100\n87.13\n86.31\n86.35\n87.07\n-\nImageNet\n77.91\n73.38\n76.53\n71.16\n-\nImageNet ReaL\n83.57\n79.56\n82.19\n77.83\n-\nOxford Flowers-102\n89.49\n85.43\n89.66\n86.36\n-\nOxford-IIIT-Pets\n93.81\n92.04\n93.64\n91.35\n-"}
{"doc_id": "2010.11929", "para_id": 147, "text": "ImageNet-21k\nCIFAR-10\n98.95\n98.79\n99.16\n99.13\n99.27\nCIFAR-100\n91.67\n91.97\n93.44\n93.04\n93.82\nImageNet\n83.97\n81.28\n85.15\n80.99\n85.13\nImageNet ReaL\n88.35\n86.63\n88.40\n85.65\n88.70\nOxford Flowers-102\n99.38\n99.11\n99.61\n99.19\n99.51\nOxford-IIIT-Pets\n94.43\n93.02\n94.73\n93.09\n94.82"}
{"doc_id": "2010.11929", "para_id": 148, "text": "JFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.49\n94.04\n92.52\n94.55\nImageNet\n84.15\n80.73\n87.12\n84.37\n88.04\nImageNet ReaL\n88.85\n86.27\n89.99\n88.28\n90.33\nOxford Flowers-102\n99.56\n99.27\n99.56\n99.45\n99.68\nOxford-IIIT-Pets\n95.80\n93.40\n97.11\n95.83\n97.56"}
{"doc_id": "2010.11929", "para_id": 149, "text": "Table 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2."}
{"doc_id": "2010.11929", "para_id": 150, "text": "Epochs\nImageNet\nImageNet ReaL\nCIFAR-10\nCIFAR-100\nPets\nFlowers\nexaFLOPs\nname"}
{"doc_id": "2010.11929", "para_id": 151, "text": "ViT-B/32\n7\n80.73\n86.27\n98.61\n90.49\n93.40\n99.27\n55\nViT-B/16\n7\n84.15\n88.85\n99.00\n91.87\n95.80\n99.56\n224\nViT-L/32\n7\n84.37\n88.28\n99.19\n92.52\n95.83\n99.45\n196\nViT-L/16\n7\n86.30\n89.43\n99.38\n93.46\n96.81\n99.66\n783\nViT-L/16\n14\n87.12\n89.99\n99.38\n94.04\n97.11\n99.56\n1567\nViT-H/14\n14\n88.08\n90.36\n99.50\n94.71\n97.11\n99.71\n4262"}
{"doc_id": "2010.11929", "para_id": 152, "text": "ResNet50x1\n7\n77.54\n84.56\n97.67\n86.07\n91.11\n94.26\n50\nResNet50x2\n7\n82.12\n87.94\n98.29\n89.20\n93.43\n97.02\n199\nResNet101x1\n7\n80.67\n87.07\n98.48\n89.17\n94.08\n95.95\n96\nResNet152x1\n7\n81.88\n87.96\n98.82\n90.22\n94.17\n96.94\n141\nResNet152x2\n7\n84.97\n89.69\n99.06\n92.05\n95.37\n98.62\n563\nResNet152x2\n14\n85.56\n89.89\n99.24\n91.92\n95.75\n98.75\n1126\nResNet200x3\n14\n87.22\n90.15\n99.34\n93.53\n96.32\n99.04\n3306"}
{"doc_id": "2010.11929", "para_id": 153, "text": "R50x1+ViT-B/32\n7\n84.90\n89.15\n99.01\n92.24\n95.75\n99.46\n106\nR50x1+ViT-B/16\n7\n85.58\n89.65\n99.14\n92.63\n96.65\n99.40\n274\nR50x1+ViT-L/32\n7\n85.68\n89.04\n99.24\n92.93\n96.97\n99.43\n246\nR50x1+ViT-L/16\n7\n86.60\n89.72\n99.18\n93.64\n97.03\n99.40\n859\nR50x1+ViT-L/16\n14\n87.12\n89.76\n99.31\n93.89\n97.36\n99.11\n1668"}
{"doc_id": "2010.11929", "para_id": 154, "text": "Table 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\naFLOPs)."}
{"doc_id": "2010.11929", "para_id": 155, "text": "Figure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training."}
{"doc_id": "2010.11929", "para_id": 156, "text": "ResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning"}
{"doc_id": "2010.11929", "para_id": 157, "text": "ImageNet\n77.54\n78.24\n84.97\n84.37\nCIFAR10\n97.67\n97.46\n99.06\n99.07\nCIFAR100\n86.07\n85.17\n92.05\n91.06\nOxford-IIIT Pets\n91.11\n91.00\n95.37\n94.79\nOxford Flowers-102\n94.26\n92.06\n98.62\n99.32\nAverage\n89.33\n88.79\n94.01\n93.72"}
{"doc_id": "2010.11929", "para_id": 158, "text": "Table 7: Fine-tuning ResNet models pre-trained with Adam and SGD."}
{"doc_id": "2010.11929", "para_id": 159, "text": "Figure 8: Scaling different model dimensions of the Vision Transformer."}
{"doc_id": "2010.11929", "para_id": 160, "text": "performance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor 7 epochs, not 30."}
{"doc_id": "2010.11929", "para_id": 161, "text": "We ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\nall dimensions proportionally results in robust improvements."}
{"doc_id": "2010.11929", "para_id": 162, "text": "In order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then trans-\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer."}
{"doc_id": "2010.11929", "para_id": 163, "text": "This design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,"}
{"doc_id": "2010.11929", "para_id": 164, "text": "Figure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\nwell, but require different learning-rates."}
{"doc_id": "2010.11929", "para_id": 165, "text": "Pos. Emb.\nDefault/Stem\nEvery Layer\nEvery Layer-Shared"}
{"doc_id": "2010.11929", "para_id": 166, "text": "No Pos. Emb.\n0.61382\nN/A\nN/A\n1-D Pos. Emb.\n0.64206\n0.63964\n0.64292\n2-D Pos. Emb.\n0.64001\n0.64046\n0.64022\nRel. Pos. Emb.\n0.64032\nN/A\nN/A"}
{"doc_id": "2010.11929", "para_id": 167, "text": "Table 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear."}
{"doc_id": "2010.11929", "para_id": 168, "text": "the difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9."}
{"doc_id": "2010.11929", "para_id": 169, "text": "We ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:"}
{"doc_id": "2010.11929", "para_id": 170, "text": "• Providing no positional information: Considering the inputs as a bag of patches."}
{"doc_id": "2010.11929", "para_id": 171, "text": "• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper)."}
{"doc_id": "2010.11929", "para_id": 172, "text": "• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\nembedding for that patch."}
{"doc_id": "2010.11929", "para_id": 173, "text": "• Relative positional embeddings: Considering the relative distance between patches to en-\ncode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the log-\nits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax."}
{"doc_id": "2010.11929", "para_id": 174, "text": "In addition to different ways of encoding spatial information, we also tried different ways of in-\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after"}
{"doc_id": "2010.11929", "para_id": 175, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 176, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 177, "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column"}
{"doc_id": "2010.11929", "para_id": 178, "text": "Figure 10: Position embeddings of models trained with different hyperparameters."}
{"doc_id": "2010.11929", "para_id": 179, "text": "the stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers)."}
{"doc_id": "2010.11929", "para_id": 180, "text": "Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10)."}
{"doc_id": "2010.11929", "para_id": 181, "text": "Figure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels."}
{"doc_id": "2010.11929", "para_id": 182, "text": "We are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,"}
{"doc_id": "2010.11929", "para_id": 183, "text": "we perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor."}
{"doc_id": "2010.11929", "para_id": 184, "text": "Figure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions."}
{"doc_id": "2010.11929", "para_id": 185, "text": "Another quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\nmodels."}
{"doc_id": "2010.11929", "para_id": 186, "text": "Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient."}
{"doc_id": "2010.11929", "para_id": 187, "text": "Axial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3."}
{"doc_id": "2010.11929", "para_id": 188, "text": "Moreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP."}
{"doc_id": "2010.11929", "para_id": 189, "text": "Figure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at"}
{"doc_id": "2010.11929", "para_id": 190, "text": "3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation."}
{"doc_id": "2010.11929", "para_id": 191, "text": "Figure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left)."}
{"doc_id": "2010.11929", "para_id": 192, "text": "the cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right)."}
{"doc_id": "2010.11929", "para_id": 193, "text": "To understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens."}
{"doc_id": "2010.11929", "para_id": 194, "text": "To compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers."}
{"doc_id": "2010.11929", "para_id": 195, "text": "We also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy."}
{"doc_id": "2010.11929", "para_id": 196, "text": "Table 9 shows the scores attained on each of the VTAB-1k tasks."}
{"doc_id": "2010.11929", "para_id": 197, "text": "Figure 14: Further example attention maps as in Figure 6 (random selection)."}
{"doc_id": "2010.11929", "para_id": 198, "text": "Table 9: Breakdown of VTAB-1k performance across tasks."}
{"doc_id": "2010.11929", "para_id": 199, "text": "ViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7"}
{"doc_id": "2103.00020", "para_id": 0, "text": "Learning Transferable Visual Models From Natural Language Supervision"}
{"doc_id": "2103.00020", "para_id": 1, "text": "Alec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1"}
{"doc_id": "2103.00020", "para_id": 2, "text": "Girish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1 Ilya Sutskever 1"}
{"doc_id": "2103.00020", "para_id": 3, "text": "Task-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\nnitude in compute, model capacity, and data, steadily im-\nproving capabilities. The development of “text-to-text” as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task-\nagnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset speciﬁc customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspeciﬁc training data."}
{"doc_id": "2103.00020", "para_id": 4, "text": "State-of-the-art computer vision systems are\ntrained to predict a ﬁxed set of predetermined\nobject categories. This restricted form of super-\nvision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im-\nage is an efﬁcient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark-\ning on over 30 different existing computer vi-\nsion datasets, spanning tasks such as OCR, ac-\ntion recognition in videos, geo-localization, and\nmany types of ﬁne-grained object classiﬁcation.\nThe model transfers non-trivially to most tasks\nand is often competitive with a fully supervised\nbaseline without the need for any dataset spe-\nciﬁc training. For instance, we match the ac-\ncuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28\nmillion training examples it was trained on. We\nrelease our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP."}
{"doc_id": "2103.00020", "para_id": 5, "text": "These results suggest that the aggregate supervision acces-\nsible to modern pre-training methods within web-scale col-\nlections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other ﬁelds such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging."}
{"doc_id": "2103.00020", "para_id": 6, "text": "Over 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre-\ndict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi-\nble to learn more data efﬁcient image representations via\nmanifold learning in the weight space of classiﬁers trained\nto predict words in captions associated with images. Sri-\nvastava & Salakhutdinov (2012) explored deep represen-\ntation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag features.\nJoulin et al. (2016) modernized this line of work and demon-\nstrated that CNNs trained to predict words in image cap-\ntions learn useful image representations. They converted\nthe title, description, and hashtag metadata of images in the\nYFCC100M dataset (Thomee et al., 2016) into a bag-of-\nwords multi-label classiﬁcation task and showed that pre-\ntraining AlexNet (Krizhevsky et al., 2012) to predict these\nlabels learned representations which preformed similarly\nto ImageNet-based pre-training on transfer tasks. Li et al.\n(2017) then extended this approach to predicting phrase n-\ngrams in addition to individual words and demonstrated the\nability of their system to zero-shot transfer to other image"}
{"doc_id": "2103.00020", "para_id": 7, "text": "Pre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-\nford et al., 2018; Devlin et al., 2018; Raffel et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 8, "text": "*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <{alec, jongwook}@openai.com>."}
{"doc_id": "2103.00020", "para_id": 9, "text": "Learning Transferable Visual Models From Natural Language Supervision\n2"}
{"doc_id": "2103.00020", "para_id": 10, "text": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classiﬁer to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classiﬁer by embedding the names or descriptions of the\ntarget dataset’s classes."}
{"doc_id": "2103.00020", "para_id": 11, "text": "classiﬁcation datasets by scoring target classes based on\ntheir dictionary of learned visual n-grams and predicting the\none with the highest score. Adopting more recent architec-\ntures and pre-training approaches, VirTex (Desai & Johnson,\n2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-\nVIRT (Zhang et al., 2020) have recently demonstrated the\npotential of transformer-based language modeling, masked\nlanguage modeling, and contrastive objectives to learn im-\nage representations from text."}
{"doc_id": "2103.00020", "para_id": 12, "text": "mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general-\nity. Both approaches also use static softmax classiﬁers to\nperform prediction and lack a mechanism for dynamic out-\nputs. This severely curtails their ﬂexibility and limits their\n“zero-shot” capabilities."}
{"doc_id": "2103.00020", "para_id": 13, "text": "A crucial difference between these weakly supervised mod-\nels and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classiﬁers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im-\nage, text) pairs and demonstrate that a simpliﬁed version of\nConVIRT trained from scratch, which we call CLIP, for Con-\ntrastive Language-Image Pre-training, is an efﬁcient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob-\nserve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We ﬁnd that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and ﬁnd"}
{"doc_id": "2103.00020", "para_id": 14, "text": "While exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com-\nmon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com-\nputer vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet-related hashtags on Instagram im-\nages is an effective pre-training task. When ﬁne-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset."}
{"doc_id": "2103.00020", "para_id": 15, "text": "This line of work represents the current pragmatic middle\nground between learning from a limited amount of super-\nvised “gold-labels” and learning from practically unlimited\namounts of raw text. However, it is not without compro-"}
{"doc_id": "2103.00020", "para_id": 16, "text": "Learning Transferable Visual Models From Natural Language Supervision\n3"}
{"doc_id": "2103.00020", "para_id": 17, "text": "vision. Although early work wrestled with the complexity\nof natural language when using topic model and n-gram\nrepresentations, improvements in deep contextual represen-\ntation learning suggest we now have the tools to effectively\nleverage this abundant source of supervision (McCann et al.,\n2017)."}
{"doc_id": "2103.00020", "para_id": 18, "text": "Learning from natural language has several potential\nstrengths over other training methods. It’s much easier\nto scale natural language supervision compared to standard\ncrowd-sourced labeling for image classiﬁcation since it does\nnot require annotations to be in a classic “machine learning\ncompatible format” such as the canonical 1-of-N majority\nvote “gold label”. Instead, methods which work on natural\nlanguage can learn passively from the supervision contained\nin the vast amount of text on the internet. Learning from\nnatural language also has an important advantage over most\nunsupervised or self-supervised learning approaches in that\nit doesn’t “just” learn a representation but also connects that\nrepresentation to language which enables ﬂexible zero-shot\ntransfer. In the following subsections, we detail the speciﬁc\napproach we settled on."}
{"doc_id": "2103.00020", "para_id": 19, "text": "Bag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Language Model"}
{"doc_id": "2103.00020", "para_id": 20, "text": "Figure 2. CLIP is much more efﬁcient at zero-shot transfer\nthan our image caption baseline. Although highly expressive,\nwe found that transformer-based language models are relatively\nweak at zero-shot ImageNet classiﬁcation. Here, we see that it\nlearns 3x slower than a baseline which predicts a bag-of-words\n(BoW) encoding of the text (Joulin et al., 2016). Swapping the\nprediction objective for the contrastive objective of CLIP further\nimproves efﬁciency another 4x."}
{"doc_id": "2103.00020", "para_id": 21, "text": "it can be competitive with prior task-speciﬁc supervised\nmodels. We also conﬁrm these ﬁndings with linear-probe\nrepresentation learning analysis and show that CLIP out-\nperforms the best publicly available ImageNet model while\nalso being more computationally efﬁcient. We additionally\nﬁnd that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model’s capability. These re-\nsults have signiﬁcant policy and ethical implications, which\nwe consider in Section 7."}
{"doc_id": "2103.00020", "para_id": 22, "text": "Existing work has mainly used three datasets, MS-COCO\n(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and\nYFCC100M (Thomee et al., 2016). While MS-COCO and\nVisual Genome are high quality crowd-labeled datasets, they\nare small by modern standards with approximately 100,000\ntraining photos each. By comparison, other computer vision\nsystems are trained on up to 3.5 billion Instagram photos\n(Mahajan et al., 2018). YFCC100M, at 100 million photos,\nis a possible alternative, but the metadata for each image is\nsparse and of varying quality. Many images use automati-\ncally generated ﬁlenames like 20160716 113957.JPG\nas “titles” or contain “descriptions” of camera exposure\nsettings. After ﬁltering to keep only images with natural\nlanguage titles and/or descriptions in English, the dataset\nshrunk by a factor of 6 to only 15 million photos. This is\napproximately the same size as ImageNet."}
{"doc_id": "2103.00020", "para_id": 23, "text": "At the core of our approach is the idea of learning percep-\ntion from supervision contained in natural language. As\ndiscussed in the introduction, this is not at all a new idea,\nhowever terminology used to describe work in this space\nis varied, even seemingly contradictory, and stated motiva-\ntions are diverse. Zhang et al. (2020), Gomez et al. (2017),\nJoulin et al. (2016), and Desai & Johnson (2020) all intro-\nduce methods which learn visual representations from text\npaired with images but describe their approaches as unsuper-\nvised, self-supervised, weakly supervised, and supervised\nrespectively."}
{"doc_id": "2103.00020", "para_id": 24, "text": "A major motivation for natural language supervision is the\nlarge quantities of data of this form available publicly on the\ninternet. Since existing datasets do not adequately reﬂect\nthis possibility, considering results only on them would un-\nderestimate the potential of this line of research. To address\nthis, we constructed a new dataset of 400 million (image,\ntext) pairs collected form a variety of publicly available\nsources on the Internet. To attempt to cover as broad a set\nof visual concepts as possible, we search for (image, text)\npairs as part of the construction process whose text includes\none of a set of 500,000 queries.1 We approximately class"}
{"doc_id": "2103.00020", "para_id": 25, "text": "We emphasize that what is common across this line of work\nis not any of the details of the particular methods used but\nthe appreciation of natural language as a training signal. All\nthese approaches are learning from natural language super-"}
{"doc_id": "2103.00020", "para_id": 26, "text": "1The base query list is all words occurring at least 100 times in\nthe English version of Wikipedia. This is augmented with bi-grams"}
{"doc_id": "2103.00020", "para_id": 27, "text": "Learning Transferable Visual Models From Natural Language Supervision\n4"}
{"doc_id": "2103.00020", "para_id": 28, "text": "balance the results by including up to 20,000 (image, text)\npairs per query. The resulting dataset has a similar total\nword count as the WebText dataset used to train GPT-2. We\nrefer to this dataset as WIT for WebImageText."}
{"doc_id": "2103.00020", "para_id": 29, "text": "multi-modal embedding space by jointly training an image\nencoder and text encoder to maximize the cosine similar-\nity of the image and text embeddings of the N real pairs\nin the batch while minimizing the cosine similarity of the\nembeddings of the N 2 −N incorrect pairings. We opti-\nmize a symmetric cross entropy loss over these similarity\nscores. In Figure 3 we include pseudocode of the core of an\nimplementation of CLIP. To our knowledge this batch con-\nstruction technique and objective was ﬁrst introduced in the\narea of deep metric learning as the multi-class N-pair loss\nSohn (2016), was popularized for contrastive representation\nlearning by Oord et al. (2018) as the InfoNCE loss, and was\nrecently adapted for contrastive (text, image) representation\nlearning in the domain of medical imaging by Zhang et al.\n(2020)."}
{"doc_id": "2103.00020", "para_id": 30, "text": "State-of-the-art computer vision systems use very large\namounts of compute. Mahajan et al. (2018) required 19\nGPU years to train their ResNeXt101-32x48d and Xie et al.\n(2020) required 33 TPUv3 core-years to train their Noisy\nStudent EfﬁcientNet-L2. When considering that both these\nsystems were trained to predict only 1000 ImageNet classes,\nthe task of learning an open set of visual concepts from\nnatural language seems daunting. In the course of our ef-\nforts, we found training efﬁciency was key to successfully\nscaling natural language supervision and we selected our\nﬁnal pre-training method based on this metric."}
{"doc_id": "2103.00020", "para_id": 31, "text": "Due to the large size of our pre-training dataset, over-ﬁtting\nis not a major concern and the details of training CLIP are\nsimpliﬁed compared to the implementation of Zhang et al.\n(2020). We train CLIP from scratch without initializing the\nimage encoder with ImageNet weights or the text encoder\nwith pre-trained weights. We do not use the non-linear\nprojection between the representation and the contrastive\nembedding space, a change which was introduced by Bach-\nman et al. (2019) and popularized by Chen et al. (2020b).\nWe instead use only a linear projection to map from each en-\ncoder’s representation to the multi-modal embedding space.\nWe did not notice a difference in training efﬁciency between\nthe two versions and speculate that non-linear projections\nmay be co-adapted with details of current image only in\nself-supervised representation learning methods. We also\nremove the text transformation function tu from Zhang et al.\n(2020) which samples a single sentence at uniform from\nthe text since many of the (image, text) pairs in CLIP’s pre-\ntraining dataset are only a single sentence. We also simplify\nthe image transformation function tv. A random square\ncrop from resized images is the only data augmentation\nused during training. Finally, the temperature parameter\nwhich controls the range of the logits in the softmax, τ, is\ndirectly optimized during training as a log-parameterized\nmultiplicative scalar to avoid turning as a hyper-parameter."}
{"doc_id": "2103.00020", "para_id": 32, "text": "Our initial approach, similar to VirTex, jointly trained an\nimage CNN and text transformer from scratch to predict the\ncaption of an image. However, we encountered difﬁculties\nefﬁciently scaling this method. In Figure 2 we show that a\n63 million parameter transformer language model, which\nalready uses twice the compute of its ResNet-50 image\nencoder, learns to recognize ImageNet classes three times\nslower than a much simpler baseline that predicts a bag-of-\nwords encoding of the same text."}
{"doc_id": "2103.00020", "para_id": 33, "text": "Both these approaches share a key similarity. They try to pre-\ndict the exact words of the text accompanying each image.\nThis is a difﬁcult task due to the wide variety of descriptions,\ncomments, and related text that co-occur with images. Re-\ncent work in contrastive representation learning for images\nhas found that contrastive objectives can learn better repre-\nsentations than their equivalent predictive objective (Tian\net al., 2019). Other work has found that although generative\nmodels of images can learn high quality image representa-\ntions, they require over an order of magnitude more compute\nthan contrastive models with the same performance (Chen\net al., 2020a). Noting these ﬁndings, we explored training\na system to solve the potentially easier proxy task of pre-\ndicting only which text as a whole is paired with which\nimage and not the exact words of that text. Starting with\nthe same bag-of-words encoding baseline, we swapped the\npredictive objective for a contrastive objective in Figure 2\nand observed a further 4x efﬁciency improvement in the rate\nof zero-shot transfer to ImageNet."}
{"doc_id": "2103.00020", "para_id": 34, "text": "We consider two different architectures for the image en-\ncoder. For the ﬁrst, we use ResNet-50 (He et al., 2016a)\nas the base architecture for the image encoder due to its\nwidespread adoption and proven performance. We make sev-\neral modiﬁcations to the original version using the ResNet-\nD improvements from He et al. (2019) and the antialiased\nrect-2 blur pooling from Zhang (2019). We also replace\nthe global average pooling layer with an attention pooling\nmechanism. The attention pooling is implemented as a sin-\ngle layer of “transformer-style” multi-head QKV attention\nwhere the query is conditioned on the global average-pooled"}
{"doc_id": "2103.00020", "para_id": 35, "text": "Given a batch of N (image, text) pairs, CLIP is trained to\npredict which of the N × N possible (image, text) pairings\nacross a batch actually occurred. To do this, CLIP learns a"}
{"doc_id": "2103.00020", "para_id": 36, "text": "with high pointwise mutual information as well as the names of\nall Wikipedia articles above a certain search volume. Finally all\nWordNet synsets not already in the query list are added."}
{"doc_id": "2103.00020", "para_id": 37, "text": "Learning Transferable Visual Models From Natural Language Supervision\n5"}
{"doc_id": "2103.00020", "para_id": 38, "text": "# image_encoder - ResNet or Vision Transformer\n# text_encoder  - CBOW or Text Transformer\n# I[n, h, w, c] - minibatch of aligned images\n# T[n, l]       - minibatch of aligned texts\n# W_i[d_i, d_e] - learned proj of image to embed\n# W_t[d_t, d_e] - learned proj of text to embed\n# t             - learned temperature parameter"}
{"doc_id": "2103.00020", "para_id": 39, "text": "one dimension of the model. While Tan & Le (2019) tune\nthe ratio of compute allocated to each dimension for their\nEfﬁcientNet architecture, we use a simple baseline of allo-\ncating additional compute equally to increasing the width,\ndepth, and resolution of the model. For the text encoder, we\nonly scale the width of the model to be proportional to the\ncalculated increase in width of the ResNet and do not scale\nthe depth at all, as we found CLIP’s performance to be less\nsensitive to the capacity of the text encoder."}
{"doc_id": "2103.00020", "para_id": 40, "text": "# extract feature representations of each modality\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T)  #[n, d_t]"}
{"doc_id": "2103.00020", "para_id": 41, "text": "# joint multimodal embedding [n, d_e]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1)\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)"}
{"doc_id": "2103.00020", "para_id": 42, "text": "We train a series of 5 ResNets and 3 Vision Transformers.\nFor the ResNets we train a ResNet-50, a ResNet-101, and\nthen 3 more which follow EfﬁcientNet-style model scaling\nand use approximately 4x, 16x, and 64x the compute of a\nResNet-50. They are denoted as RN50x4, RN50x16, and\nRN50x64 respectively. For the Vision Transformers we\ntrain a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all\nmodels for 32 epochs. We use the Adam optimizer (Kingma\n& Ba, 2014) with decoupled weight decay regularization\n(Loshchilov & Hutter, 2017) applied to all weights that are\nnot gains or biases, and decay the learning rate using a\ncosine schedule (Loshchilov & Hutter, 2016). Initial hyper-\nparameters were set using a combination of grid searches,\nrandom search, and manual tuning on the baseline ResNet-\n50 model when trained for 1 epoch. Hyper-parameters were\nthen adapted heuristically for larger models due to compu-\ntational constraints. The learnable temperature parameter\nτ was initialized to the equivalent of 0.07 from (Wu et al.,\n2018) and clipped to prevent scaling the logits by more\nthan 100 which we found necessary to prevent training in-\nstability. We use a very large minibatch size of 32,768.\nMixed-precision (Micikevicius et al., 2017) was used to ac-\ncelerate training and save memory. To save additional mem-\nory, gradient checkpointing (Griewank & Walther, 2000;\nChen et al., 2016), half-precision Adam statistics (Dhariwal\net al., 2020), and half-precision stochastically rounded text\nencoder weights were used. The calculation of embedding\nsimilarities was also sharded with individual GPUs comput-\ning only the subset of the pairwise similarities necessary for\ntheir local batch of embeddings. The largest ResNet model,\nRN50x64, took 18 days to train on 592 V100 GPUs while\nthe largest Vision Transformer took 12 days on 256 V100\nGPUs. For the ViT-L/14 we also pre-train at a higher 336\npixel resolution for one additional epoch to boost perfor-\nmance similar to FixRes (Touvron et al., 2019). We denote\nthis model as ViT-L/14@336px. Unless otherwise speciﬁed,\nall results reported in this paper as “CLIP” use this model\nwhich we found to perform best."}
{"doc_id": "2103.00020", "para_id": 43, "text": "# scaled pairwise cosine similarities [n, n]\nlogits = np.dot(I_e, T_e.T) * np.exp(t)"}
{"doc_id": "2103.00020", "para_id": 44, "text": "# symmetric loss function\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss   = (loss_i + loss_t)/2"}
{"doc_id": "2103.00020", "para_id": 45, "text": "Figure 3. Numpy-like pseudocode for the core of an implementa-\ntion of CLIP."}
{"doc_id": "2103.00020", "para_id": 46, "text": "representation of the image. For the second architecture, we\nexperiment with the recently introduced Vision Transformer\n(ViT) (Dosovitskiy et al., 2020). We closely follow their\nimplementation with only the minor modiﬁcation of adding\nan additional layer normalization to the combined patch\nand position embeddings before the transformer and use a\nslightly different initialization scheme."}
{"doc_id": "2103.00020", "para_id": 47, "text": "The text encoder is a Transformer (Vaswani et al., 2017)\nwith the architecture modiﬁcations described in Radford\net al. (2019). As a base size we use a 63M-parameter 12-\nlayer 512-wide model with 8 attention heads. The trans-\nformer operates on a lower-cased byte pair encoding (BPE)\nrepresentation of the text with a 49,152 vocab size (Sen-\nnrich et al., 2015). For computational efﬁciency, the max\nsequence length was capped at 76. The text sequence is\nbracketed with [SOS] and [EOS] tokens and the activa-\ntions of the highest layer of the transformer at the [EOS]\ntoken are treated as the feature representation of the text\nwhich is layer normalized and then linearly projected into\nthe multi-modal embedding space. Masked self-attention\nwas used in the text encoder to preserve the ability to ini-\ntialize with a pre-trained language model or add language\nmodeling as an auxiliary objective, though exploration of\nthis is left as future work."}
{"doc_id": "2103.00020", "para_id": 48, "text": "While previous computer vision research has often scaled\nmodels by increasing the width (Mahajan et al., 2018) or\ndepth (He et al., 2016a) in isolation, for the ResNet image\nencoders we adapt the approach of Tan & Le (2019) which\nfound that allocating additional compute across all of width,\ndepth, and resolution outperforms only allocating it to only"}
{"doc_id": "2103.00020", "para_id": 49, "text": "Learning Transferable Visual Models From Natural Language Supervision\n6"}
{"doc_id": "2103.00020", "para_id": 50, "text": "training as a transfer learning method to improve supervised\nﬁne-tuning, it also included an ablation study demonstrat-\ning that the performance of four heuristic zero-shot transfer\nmethods improved steadily over the course of pre-training,\nwithout any supervised adaption. This analysis served as the\nbasis for GPT-2 (Radford et al., 2019) which focused exclu-\nsively on studying the task-learning capabilities of language\nmodels via zero-shot transfer."}
{"doc_id": "2103.00020", "para_id": 51, "text": "In computer vision, zero-shot learning usually refers to the\nstudy of generalizing to unseen object categories in image\nclassiﬁcation (Lampert et al., 2009). We instead use the\nterm in a broader sense and study generalization to unseen\ndatasets. We motivate this as a proxy for performing un-\nseen tasks, as aspired to in the zero-data learning paper of\nLarochelle et al. (2008). While much research in the ﬁeld of\nunsupervised learning focuses on the representation learn-\ning capabilities of machine learning systems, we motivate\nstudying zero-shot transfer as a way of measuring the task-\nlearning capabilities of machine learning systems. In this\nview, a dataset evaluates performance on a task on a spe-\nciﬁc distribution. However, many popular computer vision\ndatasets were created by the research community primarily\nas benchmarks to guide the development of generic image\nclassiﬁcation methods rather than measuring performance\non a speciﬁc task. While it is reasonable to say that the\nSVHN dataset measures the task of street number transcrip-\ntion on the distribution of Google Street View photos, it is\nunclear what “real” task the CIFAR-10 dataset measures.\nIt is clear, however, what distribution CIFAR-10 is drawn\nfrom - TinyImages (Torralba et al., 2008). On these kinds of\ndatasets, zero-shot transfer is more an evaluation of CLIP’s\nrobustness to distribution shift and domain generalization\nrather than task generalization. Please see Section 3.3 for\nanalysis focused on this."}
{"doc_id": "2103.00020", "para_id": 52, "text": "CLIP is pre-trained to predict if an image and a text snippet\nare paired together in its dataset. To perform zero-shot clas-\nsiﬁcation, we reuse this capability. For each dataset, we use\nthe names of all the classes in the dataset as the set of poten-\ntial text pairings and predict the most probable (image, text)\npair according to CLIP. In a bit more detail, we ﬁrst compute\nthe feature embedding of the image and the feature embed-\nding of the set of possible texts by their respective encoders.\nThe cosine similarity of these embeddings is then calculated,\nscaled by a temperature parameter τ, and normalized into a\nprobability distribution via a softmax. Note that this predic-\ntion layer is a multinomial logistic regression classiﬁer with\nL2-normalized inputs, L2-normalized weights, no bias, and\ntemperature scaling. When interpreted this way, the image\nencoder is the computer vision backbone which computes a\nfeature representation for the image and the text encoder is a\nhypernetwork (Ha et al., 2016) which generates the weights\nof a linear classiﬁer based on the text specifying the visual\nconcepts that the classes represent. Lei Ba et al. (2015) ﬁrst\nintroduced a zero-shot image classiﬁer of this form while\nthe idea of generating a classiﬁer from natural language\ndates back to at least Elhoseiny et al. (2013). Continuing\nwith this interpretation, every step of CLIP pre-training can\nbe viewed as optimizing the performance of a randomly\ncreated proxy to a computer vision dataset which contains 1\nexample per class and has 32,768 total classes deﬁned via\nnatural language descriptions. For zero-shot evaluation, we\ncache the zero-shot classiﬁer once it has been computed by\nthe text encoder and reuse it for all subsequent predictions.\nThis allows the cost of generating it to be amortized across\nall the predictions in a dataset."}
{"doc_id": "2103.00020", "para_id": 53, "text": "To our knowledge, Visual N-Grams (Li et al., 2017) ﬁrst\nstudied zero-shot transfer to existing image classiﬁcation\ndatasets in the manner described above. It is also the only\nother work we are aware of that has studied zero-shot trans-\nfer to standard image classiﬁcation datasets using a gener-\nically pre-trained model and serves as the best reference\npoint for contextualizing CLIP. Their approach learns the\nparameters of a dictionary of 142,806 visual n-grams (span-\nning 1- to 5- grams) and optimizes these n-grams using a\ndifferential version of Jelinek-Mercer smoothing to maxi-\nmize the probability of all text n-grams for a given image.\nIn order to perform zero-shot transfer, they ﬁrst convert the\ntext of each of the dataset’s class names into its n-gram\nrepresentation and then compute its probability according\nto their model, predicting the one with the highest score."}
{"doc_id": "2103.00020", "para_id": 54, "text": "In Table 1 we compare Visual N-Grams to CLIP. The best\nCLIP model improves accuracy on ImageNet from a proof\nof concept 11.5% to 76.2% and matches the performance\nof the original ResNet-50 despite using none of the 1.28\nmillion crowd-labeled training examples available for this\ndataset. Additionally, the top-5 accuracy of CLIP models\nare noticeably higher than their top-1, and this model has a\n95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,\n2016). The ability to match the performance of a strong,\nfully supervised baselines in a zero-shot setting suggests"}
{"doc_id": "2103.00020", "para_id": 55, "text": "Our focus on studying zero-shot transfer as an evaluation of\ntask learning is inspired by work demonstrating task learn-\ning in the ﬁeld of NLP. To our knowledge Liu et al. (2018)\nﬁrst identiﬁed task learning as an “unexpected side-effect”\nwhen a language model trained to generate Wikipedia ar-\nticles learned to reliably transliterate names between lan-\nguages. While GPT-1 (Radford et al., 2018) focused on pre-"}
{"doc_id": "2103.00020", "para_id": 56, "text": "Learning Transferable Visual Models From Natural Language Supervision\n7"}
{"doc_id": "2103.00020", "para_id": 57, "text": "Table 1. Comparing CLIP to prior zero-shot transfer image classi-\nﬁcation results. CLIP improves performance on all three datasets\nby a large amount. This improvement reﬂects many differences\nin the 4 years since the development of Visual N-Grams (Li et al.,\n2017)."}
{"doc_id": "2103.00020", "para_id": 58, "text": "CLIP is a signiﬁcant step towards ﬂexible and practical\nzero-shot computer vision classiﬁers. As mentioned above,\nthe comparison to Visual N-Grams is meant for contextu-\nalizing the performance of CLIP and should not be inter-\npreted as a direct methods comparison between CLIP and\nVisual N-Grams as many performance relevant differences\nbetween the two systems were not controlled for. For in-\nstance, we train on a dataset that is 10x larger, use a vision\nmodel that requires nearly 100x more compute per predic-\ntion, likely used over 1000x their training compute, and\nuse a transformer-based model which did not exist when\nVisual N-Grams was published. As a closer comparison, we\ntrained a CLIP ResNet-50 on the same YFCC100M dataset\nthat Visual N-Grams was trained on and found it matched\ntheir reported ImageNet performance within a V100 GPU\nday. This baseline was also trained from scratch instead of\nbeing initialized from pre-trained ImageNet weights as in\nVisual N-Grams."}
{"doc_id": "2103.00020", "para_id": 59, "text": "Prompt engineering and ensembling\nContextless class names (Li et al. 2017)"}
{"doc_id": "2103.00020", "para_id": 60, "text": "Figure 4. Prompt engineering and ensembling improve zero-\nshot performance. Compared to the baseline of using contextless\nclass names, prompt engineering and ensembling boost zero-shot\nclassiﬁcation performance by almost 5 points on average across\n36 datasets. This improvement is similar to the gain from using\n4 times more compute with the baseline zero-shot method but is\n“free” when amortized over many predictions."}
{"doc_id": "2103.00020", "para_id": 61, "text": "chosen somewhat haphazardly and do not anticipate issues\nrelated to zero-shot transfer which relies on task description\nin order to transfer successfully."}
{"doc_id": "2103.00020", "para_id": 62, "text": "CLIP also outperforms Visual N-Grams on the other 2 re-\nported datasets. On aYahoo, CLIP achieves a 95% reduction\nin the number of errors, and on SUN, CLIP more than dou-\nbles the accuracy of Visual N-Grams. To conduct a more\ncomprehensive analysis and stress test, we implement a\nmuch larger evaluation suite detailed in Appendix A. In\ntotal we expand from the 3 datasets reported in Visual N-\nGrams to include over 30 datasets and compare to over 50\nexisting computer vision systems to contextualize results."}
{"doc_id": "2103.00020", "para_id": 63, "text": "A common issue is polysemy. When the name of a class\nis the only information provided to CLIP’s text encoder it\nis unable to differentiate which word sense is meant due to\nthe lack of context. In some cases multiple meanings of the\nsame word might be included as different classes in the same\ndataset! This happens in ImageNet which contains both\nconstruction cranes and cranes that ﬂy. Another example is\nfound in classes of the Oxford-IIIT Pet dataset where the\nword boxer is, from context, clearly referring to a breed of\ndog, but to a text encoder lacking context could just as likely\nrefer to a type of athlete."}
{"doc_id": "2103.00020", "para_id": 64, "text": "Most standard image classiﬁcation datasets treat the infor-\nmation naming or describing classes which enables natural\nlanguage based zero-shot transfer as an afterthought. The\nvast majority of datasets annotate images with just a numeric\nid of the label and contain a ﬁle mapping these ids back to\ntheir names in English. Some datasets, such as Flowers102\nand GTSRB, don’t appear to include this mapping at all\nin their released versions preventing zero-shot transfer en-\ntirely.2 For many datasets, we observed these labels may be"}
{"doc_id": "2103.00020", "para_id": 65, "text": "Another issue we encountered is that it’s relatively rare in\nour pre-training dataset for the text paired with the image\nto be just a single word. Usually the text is a full sentence\ndescribing the image in some way. To help bridge this\ndistribution gap, we found that using the prompt template\n“A photo of a {label}.” to be a good default that\nhelps specify the text is about the content of the image. This\noften improves performance over the baseline of using only\nthe label text. For instance, just using this prompt improves\naccuracy on ImageNet by 1.3%."}
{"doc_id": "2103.00020", "para_id": 66, "text": "2Alec learned much more about ﬂower species and German\ntrafﬁc signs over the course of this project than he originally antic-\nipated."}
{"doc_id": "2103.00020", "para_id": 67, "text": "Learning Transferable Visual Models From Natural Language Supervision\n8"}
{"doc_id": "2103.00020", "para_id": 68, "text": "Similar to the “prompt engineering” discussion around GPT-\n3 (Brown et al., 2020; Gao et al., 2020), we have also\nobserved that zero-shot performance can be signiﬁcantly\nimproved by customizing the prompt text to each task. A\nfew, non exhaustive, examples follow. We found on several\nﬁne-grained image classiﬁcation datasets that it helped to\nspecify the category. For example on Oxford-IIIT Pets, us-\ning “A photo of a {label}, a type of pet.”\nto help provide context worked well. Likewise, on Food101\nspecifying a type of food and on FGVC Aircraft a type of\naircraft helped too. For OCR datasets, we found that putting\nquotes around the text or number to be recognized improved\nperformance. Finally, we found that on satellite image classi-\nﬁcation datasets it helped to specify that the images were of\nthis form and we use variants of “a satellite photo\nof a {label}.”."}
{"doc_id": "2103.00020", "para_id": 69, "text": "We also experimented with ensembling over multiple zero-\nshot classiﬁers as another way of improving performance.\nThese classiﬁers are computed by using different context\nprompts such as ‘A photo of a big {label}” and\n“A photo of a small {label}”. We construct the\nensemble over the embedding space instead of probability\nspace. This allows us to cache a single set of averaged text\nembeddings so that the compute cost of the ensemble is the\nsame as using a single classiﬁer when amortized over many\npredictions. We’ve observed ensembling across many gen-\nerated zero-shot classiﬁers to reliably improve performance\nand use it for the majority of datasets. On ImageNet, we\nensemble 80 different context prompts and this improves\nperformance by an additional 3.5% over the single default\nprompt discussed above. When considered together, prompt\nengineering and ensembling improve ImageNet accuracy\nby almost 5%. In Figure 4 we visualize how prompt engi-\nneering and ensembling change the performance of a set of\nCLIP models compared to the contextless baseline approach\nof directly embedding the class name as done in Li et al.\n(2017)."}
{"doc_id": "2103.00020", "para_id": 70, "text": "40\n30\n20\n10\n0\n10\n20\n30\n40\nScore (%)\nZero-Shot CLIP vs. Linear Probe on ResNet50"}
{"doc_id": "2103.00020", "para_id": 71, "text": "Figure 5. Zero-shot CLIP is competitive with a fully super-\nvised baseline. Across a 27 dataset eval suite, a zero-shot CLIP\nclassiﬁer outperforms a fully supervised linear classiﬁer ﬁtted on\nResNet-50 features on 16 datasets, including ImageNet."}
{"doc_id": "2103.00020", "para_id": 72, "text": "ten than not and wins on 16 of the 27 datasets. Looking at\nindividual datasets reveals some interesting behavior. On\nﬁne-grained classiﬁcation tasks, we observe a wide spread\nin performance. On two of these datasets, Stanford Cars and\nFood101, zero-shot CLIP outperforms logistic regression\non ResNet-50 features by over 20% while on two others,\nFlowers102 and FGVCAircraft, zero-shot CLIP underper-\nforms by over 10%. On OxfordPets and Birdsnap, per-\nformance is much closer. We suspect these difference are\nprimarily due to varying amounts of per-task supervision\nbetween WIT and ImageNet. On “general” object classiﬁca-\ntion datasets such as ImageNet, CIFAR10/100, STL10, and\nPascalVOC2007 performance is relatively similar with a\nslight advantage for zero-shot CLIP in all cases. On STL10,\nCLIP achieves 99.3% overall which appears to be a new\nstate of the art despite not using any training examples. Zero-\nshot CLIP signiﬁcantly outperforms a ResNet-50 on two\ndatasets measuring action recognition in videos. On Kinet-\nics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-\nshot CLIP also outperforms a ResNet-50’s features by 7.7%\non UCF101. We speculate this is due to natural language\nproviding wider supervision for visual concepts involving\nverbs, compared to the noun-centric object supervision in\nImageNet."}
{"doc_id": "2103.00020", "para_id": 73, "text": "Since task-agnostic zero-shot classiﬁers for computer vision\nhave been understudied, CLIP provides a promising oppor-\ntunity to gain a better understanding of this type of model.\nIn this section, we conduct a study of various properties of\nCLIP’s zero-shot classiﬁers. As a ﬁrst question, we look\nsimply at how well zero-shot classiﬁers perform. To con-\ntextualize this, we compare to the performance of a simple\noff-the-shelf baseline: ﬁtting a fully supervised, regularized,\nlogistic regression classiﬁer on the features of the canonical\nResNet-50. In Figure 5 we show this comparison across 27\ndatasets. Please see Appendix A for details of datasets and\nsetup."}
{"doc_id": "2103.00020", "para_id": 74, "text": "Zero-shot CLIP outperforms this baseline slightly more of-"}
{"doc_id": "2103.00020", "para_id": 75, "text": "Looking at where zero-shot CLIP notably underperforms,"}
{"doc_id": "2103.00020", "para_id": 76, "text": "Learning Transferable Visual Models From Natural Language Supervision\n9"}
{"doc_id": "2103.00020", "para_id": 77, "text": "expect zero-shot to underperform one-shot, we instead ﬁnd\nthat zero-shot CLIP matches the performance of 4-shot lo-\ngistic regression on the same feature space. This is likely\ndue to an important difference between the zero-shot and\nfew-shot approach. First, CLIP’s zero-shot classiﬁer is gen-\nerated via natural language which allows for visual concepts\nto be directly speciﬁed (“communicated”). By contrast,\n“normal” supervised learning must infer concepts indirectly\nfrom training examples. Context-less example-based learn-\ning has the drawback that many different hypotheses can\nbe consistent with the data, especially in the one-shot case.\nA single image often contains many different visual con-\ncepts. Although a capable learner is able to exploit visual\ncues and heuristics, such as assuming that the concept being\ndemonstrated is the primary object in an image, there is no\nguarantee."}
{"doc_id": "2103.00020", "para_id": 78, "text": "A potential resolution of this discrepancy between zero-\nshot and few-shot performance is to use CLIP’s zero-shot\nclassiﬁer as a prior for the weights of the few-shot classiﬁer.\nWhile adding an L2 penalty towards the generated weights\nis a straightforward implementation of this idea, we found\nthat hyperparameter optimization would often select for\nsuch a large value of this regularizer that the resulting few-\nshot classiﬁer was “just” the zero-shot classiﬁer. Research\ninto better methods of combining the strength of zero-shot\ntransfer with ﬂexibility of few-shot learning is a promising\ndirection for future work."}
{"doc_id": "2103.00020", "para_id": 79, "text": "0\n1\n2\n4\n8\n16\n# of labeled training examples per class"}
{"doc_id": "2103.00020", "para_id": 80, "text": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes.\nZero-shot CLIP matches the average performance of a 4-shot linear\nclassiﬁer trained on the same feature space and nearly matches the\nbest results of a 16-shot linear classiﬁer across publicly available\nmodels. For both BiT-M and SimCLRv2, the best performing\nmodel is highlighted. Light gray lines are other models in the eval\nsuite. The 20 datasets with at least 16 examples per class were\nused in this analysis."}
{"doc_id": "2103.00020", "para_id": 81, "text": "When comparing zero-shot CLIP to few-shot logistic re-\ngression on the features of other models, zero-shot CLIP\nroughly matches the performance of the best performing\n16-shot classiﬁer in our evaluation suite, which uses the fea-\ntures of a BiT-M ResNet-152x2 trained on ImageNet-21K.\nWe are certain that a BiT-L model trained on JFT-300M\nwould perform even better but these models have not been\npublicly released. That a BiT-M ResNet-152x2 performs\nbest in a 16-shot setting is somewhat surprising since, as\nanalyzed in Section 3.2, the Noisy Student EfﬁcientNet-L2\noutperforms it in a fully supervised setting by almost 5% on\naverage across 27 datasets."}
{"doc_id": "2103.00020", "para_id": 82, "text": "we see that zero-shot CLIP is quite weak on several spe-\ncialized, complex, or abstract tasks such as satellite image\nclassiﬁcation (EuroSAT and RESISC45), lymph node tumor\ndetection (PatchCamelyon), counting objects in synthetic\nscenes (CLEVRCounts), self-driving related tasks such as\nGerman trafﬁc sign recognition (GTSRB), recognizing dis-\ntance to the nearest car (KITTI Distance). These results\nhighlight the poor capability of zero-shot CLIP on more\ncomplex tasks. By contrast, non-expert humans can robustly\nperform several of these tasks, such as counting, satellite\nimage classiﬁcation, and trafﬁc sign recognition, suggesting\nsigniﬁcant room for improvement. However, we caution\nthat it is unclear whether measuring zero-shot transfer, as\nopposed to few-shot transfer, is a meaningful evaluation for\ndifﬁcult tasks that a learner has no prior experience with,\nsuch as lymph node tumor classiﬁcation for almost all hu-\nmans (and possibly CLIP)."}
{"doc_id": "2103.00020", "para_id": 83, "text": "In addition to studying the average performance of zero-shot\nCLIP and few-shot logistic regression, we also examine\nperformance on individual datasets. In Figure 7, we show\nestimates for the number of labeled examples per class that\na logistic regression classiﬁer on the same feature space\nrequires to match the performance of zero-shot CLIP. Since\nzero-shot CLIP is also a linear classiﬁer, this estimates the\neffective data efﬁciency of zero-shot transfer in this setting.\nIn order to avoid training thousands of linear classiﬁers,\nwe estimate the effective data efﬁciency based on a log-\nlinear interpolation of the performance of a 1, 2, 4, 8, 16-\nshot (when possible), and a fully supervised linear classiﬁer\ntrained on each dataset. We ﬁnd that zero-shot transfer can"}
{"doc_id": "2103.00020", "para_id": 84, "text": "While comparing zero-shot performance to fully supervised\nmodels contextualizes the task-learning capabilities of CLIP,\ncomparing to few-shot methods is a more direct compari-\nson, since zero-shot is its limit. In Figure 6, we visualize\nhow zero-shot CLIP compares to few-shot logistic regres-\nsion on the features of many image models including the\nbest publicly available ImageNet models, self-supervised\nlearning methods, and CLIP itself. While it is intuitive to"}
{"doc_id": "2103.00020", "para_id": 85, "text": "Learning Transferable Visual Models From Natural Language Supervision\n10"}
{"doc_id": "2103.00020", "para_id": 86, "text": "0\n25\n50\n75\n100\n125\n150\n175\n200\n# of labeled examples per class"}
{"doc_id": "2103.00020", "para_id": 87, "text": "20\n30\n40\n50\n60\n70\n80\n90\n100\nLinear Probe CLIP Performance"}
{"doc_id": "2103.00020", "para_id": 88, "text": "Figure 7. The data efﬁciency of zero-shot transfer varies\nwidely. Calculating the number of labeled examples per class\na linear classiﬁer on the same CLIP feature space requires to match\nthe performance of the zero-shot classiﬁer contextualizes the ef-\nfectiveness of zero-shot transfer. Values are estimated based on\nlog-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised\nresults. Performance varies widely from still underperforming a\none-shot classiﬁer on two datasets to matching an estimated 184\nlabeled examples per class."}
{"doc_id": "2103.00020", "para_id": 89, "text": "Figure 8. Zero-shot performance is correlated with linear\nprobe performance but still mostly sub-optimal. Comparing\nzero-shot and linear probe performance across datasets shows a\nstrong correlation with zero-shot performance mostly shifted 10 to\n25 points lower. On only 5 datasets does zero-shot performance\napproach linear probe performance (≤3 point difference)."}
{"doc_id": "2103.00020", "para_id": 90, "text": "mance, suggesting that CLIP is relatively consistent at con-\nnecting underlying representation and task learning to zero-\nshot transfer. However, zero-shot CLIP only approaches\nfully supervised performance on 5 datasets: STL10, CI-\nFAR10, Food101, OxfordPets, and Caltech101. On all 5\ndatasets, both zero-shot accuracy and fully supervised accu-\nracy are over 90%. This suggests that CLIP may be more\neffective at zero-shot transfer for tasks where its underly-\ning representations are also high quality. The slope of a\nlinear regression model predicting zero-shot performance\nas a function of fully supervised performance estimates that\nfor every 1% improvement in fully supervised performance,\nzero-shot performance improves by 1.28%. However, the\n95th-percentile conﬁdence intervals still include values of\nless than 1 (0.93-1.79)."}
{"doc_id": "2103.00020", "para_id": 91, "text": "have widely varying efﬁciency per dataset from less than 1\nlabeled example per class to 184. Two datasets, Flowers102\nand EuroSAT underperform one-shot models. Half of the\ndatasets require less than 5 examples per class with a median\nof 5.4. However, the mean estimated data efﬁciency is 20.8\nexamples per class. This is due to the 20% of datasets\nwhere supervised classiﬁers require many labeled examples\nper class in order to match performance. On ImageNet,\nzero-shot CLIP matches the performance of a 16-shot linear\nclassiﬁer trained on the same feature space."}
{"doc_id": "2103.00020", "para_id": 92, "text": "If we assume that evaluation datasets are large enough that\nthe parameters of linear classiﬁers trained on them are well\nestimated, then, because CLIP’s zero-shot classiﬁer is also\na linear classiﬁer, the performance of the fully supervised\nclassiﬁers roughly sets an upper bound for what zero-shot\ntransfer can achieve. In Figure 8 we compare CLIP’s zero-\nshot performance with fully supervised linear classiﬁers\nacross datasets. The dashed, y = x line represents an “op-\ntimal” zero-shot classiﬁer that matches the performance of\nits fully supervised equivalent. For most datasets, the per-\nformance of zero-shot classiﬁers still underperform fully su-\npervised classiﬁers by 10% to 25%, suggesting that there is\nstill plenty of headroom for improving CLIP’s task-learning\nand zero-shot transfer capabilities."}
{"doc_id": "2103.00020", "para_id": 93, "text": "Over the past few years, empirical studies of deep learning\nsystems have documented that performance is predictable as\na function of important quantities such as training compute\nand dataset size (Hestness et al., 2017; Kaplan et al., 2020).\nThe GPT family of models has so far demonstrated consis-\ntent improvements in zero-shot performance across a 1000x\nincrease in training compute. In Figure 9, we check whether\nthe zero-shot performance of CLIP follows a similar scaling\npattern. We plot the average error rate of the 5 ResNet CLIP\nmodels across 39 evaluations on 36 different datasets and\nﬁnd that a similar log-log linear scaling trend holds for CLIP\nacross a 44x increase in model compute. While the overall\ntrend is smooth, we found that performance on individual\nevaluations can be much noisier. We are unsure whether"}
{"doc_id": "2103.00020", "para_id": 94, "text": "There is a positive correlation of 0.82 (p-value < 10−6)\nbetween zero-shot performance and fully supervised perfor-"}
{"doc_id": "2103.00020", "para_id": 95, "text": "Learning Transferable Visual Models From Natural Language Supervision\n11"}
{"doc_id": "2103.00020", "para_id": 96, "text": "classiﬁers has the added beneﬁt of being very similar to the\napproach used for its zero-shot classiﬁers which enables\nextensive comparisons and analysis in Section 3.1. Finally,\nwe aim to compare CLIP to a comprehensive set of existing\nmodels across many tasks. Studying 66 different models on\n27 different datasets requires tuning 1782 different evalua-\ntions. Fine-tuning opens up a much larger design and hyper-\nparameter space, which makes it difﬁcult to fairly evaluate\nand computationally expensive to compare a diverse set of\ntechniques as discussed in other large scale empirical studies\n(Lucic et al., 2018; Choi et al., 2019). By comparison, linear\nclassiﬁers require minimal hyper-parameter tuning and have\nstandardized implementations and evaluation procedures.\nPlease see Appendix A for further details on evaluation."}
{"doc_id": "2103.00020", "para_id": 97, "text": "Figure 9. Zero-shot CLIP performance scales smoothly as a\nfunction of model compute. Across 39 evals on 36 different\ndatasets, average zero-shot error is well modeled by a log-log lin-\near trend across a 44x range of compute spanning 5 different CLIP\nmodels. Lightly shaded lines are performance on individual evals,\nshowing that performance is much more varied despite the smooth\noverall trend."}
{"doc_id": "2103.00020", "para_id": 98, "text": "Figure 10 summarizes our ﬁndings. To minimize selection\neffects that could raise concerns of conﬁrmation or reporting\nbias, we ﬁrst study performance on the 12 dataset evaluation\nsuite from Kornblith et al. (2019). While small CLIP mod-\nels such as a ResNet-50 and ResNet-101 outperform other\nResNets trained on ImageNet-1K (BiT-S and the originals),\nthey underperform ResNets trained on ImageNet-21K (BiT-\nM). These small CLIP models also underperform models\nin the EfﬁcientNet family with similar compute require-\nments. However, models trained with CLIP scale very well\nand the largest model we trained (ResNet-50x64) slightly\noutperforms the best performing existing model (a Noisy\nStudent EfﬁcientNet-L2) on both overall score and compute\nefﬁciency. We also ﬁnd that CLIP vision transformers are\nabout 3x more compute efﬁcient than CLIP ResNets, which\nallows us to reach higher overall performance within our\ncompute budget. These results qualitatively replicate the\nﬁndings of Dosovitskiy et al. (2020) which reported that\nvision transformers are more compute efﬁcient than con-\nvnets when trained on sufﬁciently large datasets. Our best\noverall model is a ViT-L/14 that is ﬁne-tuned at a higher res-\nolution of 336 pixels on our dataset for 1 additional epoch.\nThis model outperforms the best existing model across this\nevaluation suite by an average of 2.6%."}
{"doc_id": "2103.00020", "para_id": 99, "text": "this is caused by high variance between individual training\nruns on sub-tasks (as documented in D’Amour et al. (2020))\nmasking a steadily improving trend or whether performance\nis actually non-monotonic as a function of compute on some\ntasks."}
{"doc_id": "2103.00020", "para_id": 100, "text": "While we have extensively analyzed the task-learning ca-\npabilities of CLIP through zero-shot transfer in the previ-\nous section, it is more common to study the representation\nlearning capabilities of a model. There exist many ways to\nevaluate the quality of representations as well as disagree-\nments over what properties an “ideal” representation should\nhave (Locatello et al., 2020). Fitting a linear classiﬁer on\na representation extracted from the model and measuring\nits performance on various datasets is a common approach.\nAn alternative is measuring the performance of end-to-end\nﬁne-tuning of the model. This increases ﬂexibility, and\nprior work has convincingly demonstrated that ﬁne-tuning\noutperforms linear classiﬁcation on most image classiﬁ-\ncation datasets (Kornblith et al., 2019; Zhai et al., 2019).\nWhile the high performance of ﬁne-tuning motivates its\nstudy for practical reasons, we still opt for linear classiﬁer\nbased evaluation for several reasons. Our work is focused\non developing a high-performing task and dataset-agnostic\npre-training approach. Fine-tuning, because it adapts rep-\nresentations to each dataset during the ﬁne-tuning phase,\ncan compensate for and potentially mask failures to learn\ngeneral and robust representations during the pre-training\nphase. Linear classiﬁers, because of their limited ﬂexibility,\ninstead highlight these failures and provide clear feedback\nduring development. For CLIP, training supervised linear"}
{"doc_id": "2103.00020", "para_id": 101, "text": "As Figure 21 qualitatively shows, CLIP models learn a wider\nset of tasks than has previously been demonstrated in a sin-\ngle computer vision model trained end-to-end from random\ninitialization. These tasks include geo-localization, optical\ncharacter recognition, facial emotion recognition, and action\nrecognition. None of these tasks are measured in the evalua-\ntion suite of Kornblith et al. (2019). This could be argued\nto be a form of selection bias in Kornblith et al. (2019)’s\nstudy towards tasks that overlap with ImageNet. To address\nthis, we also measure performance on a broader 27 dataset\nevaluation suite. This evaluation suite, detailed in Appendix\nA includes datasets representing the aforementioned tasks,\nGerman Trafﬁc Signs Recognition Benchmark (Stallkamp\net al., 2011), as well as several other datasets adapted from\nVTAB (Zhai et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 102, "text": "Learning Transferable Visual Models From Natural Language Supervision\n12"}
{"doc_id": "2103.00020", "para_id": 103, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 104, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet"}
{"doc_id": "2103.00020", "para_id": 105, "text": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including\nEfﬁcientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;\nTouvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.,\n2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).\n(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models ﬁne-tuned or\nevaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset."}
{"doc_id": "2103.00020", "para_id": 106, "text": "On this broader evaluation suite, the beneﬁts of CLIP are\nmore clear. All CLIP models, regardless of scale, outper-\nform all evaluated systems in terms of compute efﬁciency.\nThe improvement in average score of the best model over\nprevious systems increases from 2.6% to 5%. We also ﬁnd\nthat self-supervised systems do noticeably better on our\nbroader evaluation suite. For instance, while SimCLRv2\nstill underperforms BiT-M on average on the 12 datasets\nof Kornblith et al. (2019), SimCLRv2 outperforms BiT-M\non our 27 dataset evaluation suite. These ﬁndings suggest\ncontinuing to expand task diversity and coverage in order\nto better understand the “general” performance of systems.\nWe suspect additional evaluation efforts along the lines of\nVTAB to be valuable."}
{"doc_id": "2103.00020", "para_id": 107, "text": "and HatefulMemes), geo-localization and scene recognition\n(Country211, SUN397), and activity recognition in videos\n(Kinetics700 and UCF101). In addition CLIP also does\nmuch better on ﬁne-grained car and trafﬁc sign recognition\n(Stanford Cars and GTSRB). This may reﬂect a problem\nwith overly narrow supervision in ImageNet. A result such\nas the 14.7% improvement on GTSRB could be indicative\nof an issue with ImageNet-1K, which has only a single la-\nbel for all trafﬁc and street signs. This could encourage\na supervised representation to collapse intra-class details\nand hurt accuracy on a ﬁne-grained downstream task. As\nmentioned, CLIP still underperforms the EfﬁcientNet on\nseveral datasets. Unsurprisingly, the dataset that the Efﬁ-\ncientNet does best relative to CLIP on is the one it was\ntrained on: ImageNet. The EffcientNet also slightly outper-\nforms CLIP on low-resolution datasets such as CIFAR10\nand CIFAR100. We suspect this is at least partly due to the\nlack of scale-based data augmentation in CLIP. The Efﬁ-\ncientNet also does slightly better on PatchCamelyon and\nCLEVRCounts, datasets where overall performance is still"}
{"doc_id": "2103.00020", "para_id": 108, "text": "In addition to the aggregate analysis above, we visualize\nper-dataset differences in the performance of the best CLIP\nmodel and the best model in our evaluation suite across\nall 27 datasets in Figure 11. CLIP outperforms the Noisy\nStudent EfﬁcientNet-L2 on 21 of the 27 datasets. CLIP\nimproves the most on tasks which require OCR (SST2"}
{"doc_id": "2103.00020", "para_id": 109, "text": "Learning Transferable Visual Models From Natural Language Supervision\n13"}
{"doc_id": "2103.00020", "para_id": 110, "text": "combination of the two? CLIP models, which are trained via\nnatural language supervision on a very large dataset and are\ncapable of high zero-shot performance, are an opportunity\nto investigate this question from a different angle."}
{"doc_id": "2103.00020", "para_id": 111, "text": "Taori et al. (2020) is a recent comprehensive study mov-\ning towards quantifying and understanding these behaviors\nfor ImageNet models. Taori et al. (2020) study how the\nperformance of ImageNet models change when evaluated\non natural distribution shifts. They measure performance\non a set of 7 distribution shifts: ImageNetV2 (Recht et al.,\n2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB\nand ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu\net al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),\nand ImageNet Rendition (Hendrycks et al., 2020a). They\ndistinguish these datasets, which all consist of novel images\ncollected from a variety of sources, from synthetic distri-\nbution shifts such as ImageNet-C (Hendrycks & Dietterich,\n2019), Stylized ImageNet (Geirhos et al., 2018), or adver-\nsarial attacks (Goodfellow et al., 2014) which are created by\nperturbing existing images in various ways. They propose\nthis distinction because in part because they ﬁnd that while\nseveral techniques have been demonstrated to improve per-\nformance on synthetic distribution shifts, they often fail to\nyield consistent improvements on natural distributions.3"}
{"doc_id": "2103.00020", "para_id": 112, "text": "10\n5\n0\n5\n10\n15\n20\n25\nScore (%)\nLogistic Regression on CLIP vs. EfficientNet L2 NS"}
{"doc_id": "2103.00020", "para_id": 113, "text": "Figure 11. CLIP’s features outperform the features of the best\nImageNet model on a wide variety of datasets. Fitting a linear\nclassiﬁer on CLIP’s features outperforms using the Noisy Student\nEfﬁcientNet-L2 on 21 out of 27 datasets."}
{"doc_id": "2103.00020", "para_id": 114, "text": "Across these collected datasets, the accuracy of ImageNet\nmodels drop well below the expectation set by the Ima-\ngeNet validation set. For the following summary discussion\nwe report average accuracy across all 7 natural distribution\nshift datasets and average accuracy across the correspond-\ning class subsets of ImageNet unless otherwise speciﬁed.\nAdditionally, for Youtube-BB and ImageNet-Vid, which\nhave two different evaluation settings, we use the average\nof pm-0 and pm-10 accuracy."}
{"doc_id": "2103.00020", "para_id": 115, "text": "In 2015, it was announced that a deep learning model ex-\nceeded human performance on the ImageNet test set (He\net al., 2015). However, research in the subsequent years\nhas repeatedly found that these models still make many sim-\nple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;\nAlcorn et al., 2019), and new benchmarks testing these sys-\ntems has often found their performance to be much lower\nthan both their ImageNet accuracy and human accuracy\n(Recht et al., 2019; Barbu et al., 2019). What explains this\ndiscrepancy? Various ideas have been suggested and stud-\nied (Ilyas et al., 2019; Geirhos et al., 2020). A common\ntheme of proposed explanations is that deep learning models\nare exceedingly adept at ﬁnding correlations and patterns\nwhich hold across their training dataset and thus improve\nin-distribution performance. However many of these corre-\nlations and patterns are actually spurious and do not hold for\nother distributions and result in large drops in performance\non other datasets."}
{"doc_id": "2103.00020", "para_id": 116, "text": "A ResNet-101 makes 5 times as many mistakes when eval-\nuated on these natural distribution shifts compared to the\nImageNet validation set. Encouragingly however, Taori et al.\n(2020) ﬁnd that accuracy under distribution shift increases\npredictably with ImageNet accuracy and is well modeled\nas a linear function of logit-transformed accuracy. Taori\net al. (2020) use this ﬁnding to propose that robustness\nanalysis should distinguish between effective and relative\nrobustness. Effective robustness measures improvements\nin accuracy under distribution shift above what is predicted\nby the documented relationship between in-distribution and\nout-of-distribution accuracy. Relative robustness captures\nany improvement in out-of-distribution accuracy. Taori et al.\n(2020) argue that robustness techniques should aim to im-\nprove both effective robustness and relative robustness."}
{"doc_id": "2103.00020", "para_id": 117, "text": "We caution that, to date, most of these studies limit their\nevaluation to models trained on ImageNet. Recalling the\ntopic of discussion, it may be a mistake to generalize too\nfar from these initial ﬁndings. To what degree are these\nfailures attributable to deep learning, ImageNet, or some"}
{"doc_id": "2103.00020", "para_id": 118, "text": "Almost all models studied in Taori et al. (2020) are trained"}
{"doc_id": "2103.00020", "para_id": 119, "text": "3We refer readers to Hendrycks et al. (2020a) for additional\nexperiments and discussion on this claim."}
{"doc_id": "2103.00020", "para_id": 120, "text": "Learning Transferable Visual Models From Natural Language Supervision\n14"}
{"doc_id": "2103.00020", "para_id": 121, "text": "Linear probe average over Kornblith et al.'s 12 datasets"}
{"doc_id": "2103.00020", "para_id": 122, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet"}
{"doc_id": "2103.00020", "para_id": 123, "text": "Figure 12. CLIP’s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset\nsplits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar\nImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overﬁt to their task."}
{"doc_id": "2103.00020", "para_id": 124, "text": "or ﬁne-tuned on the ImageNet dataset. Returning to the\ndiscussion in the introduction to this section - is training\nor adapting to the ImageNet dataset distribution the cause\nof the observed robustness gap? Intuitively, a zero-shot\nmodel should not be able to exploit spurious correlations\nor patterns that hold only on a speciﬁc distribution, since it\nis not trained on that distribution. 4 Thus it is reasonable\nto expect zero-shot models to have much higher effective\nrobustness. In Figure 13, we compare the performance of\nzero-shot CLIP with existing ImageNet models on natural\ndistribution shifts. All zero-shot CLIP models improve\neffective robustness by a large amount and reduce the size\nof the gap between ImageNet accuracy and accuracy under\ndistribution shift by up to 75%."}
{"doc_id": "2103.00020", "para_id": 125, "text": "in much more robust models regardless of whether they\nare zero-shot or ﬁne-tuned. As an initial experiment to\npotentially begin narrowing this down, we also measure\nhow the performance of CLIP models change after adapting\nto the ImageNet distribution via a L2 regularized logistic\nregression classiﬁer ﬁt to CLIP features on the ImageNet\ntraining set. We visualize how performance changes from\nthe zero-shot classiﬁer in Figure 14. Although adapting\nCLIP to the ImageNet distribution increases its ImageNet\naccuracy by 9.2% to 85.4% overall, and ties the accuracy\nof the 2018 SOTA from Mahajan et al. (2018), average\naccuracy under distribution shift slightly decreases."}
{"doc_id": "2103.00020", "para_id": 126, "text": "It is surprising to see a 9.2% increase in accuracy, which cor-\nresponds to roughly 3 years of improvement in SOTA, fail\nto translate into any improvement in average performance\nunder distribution shift. We also break down the differences\nbetween zero-shot accuracy and linear classiﬁer accuracy\nper dataset in Figure 14 and ﬁnd performance still increases\nsigniﬁcantly on one dataset, ImageNetV2. ImageNetV2\nclosely followed the creation process of the original Ima-\ngeNet dataset which suggests that gains in accuracy from\nsupervised adaptation are closely concentrated around the\nImageNet distribution. Performance decreases by 4.7% on"}
{"doc_id": "2103.00020", "para_id": 127, "text": "While these results show that zero-shot models can be much\nmore robust, they do not necessarily mean that supervised\nlearning on ImageNet causes a robustness gap. Other details\nof CLIP, such as its large and diverse pre-training dataset\nor use of natural language supervision could also result"}
{"doc_id": "2103.00020", "para_id": 128, "text": "4We caution that a zero-shot model can still exploit spurious\ncorrelations that are shared between the pre-training and evaluation\ndistributions."}
{"doc_id": "2103.00020", "para_id": 129, "text": "Learning Transferable Visual Models From Natural Language Supervision\n15"}
{"doc_id": "2103.00020", "para_id": 130, "text": "Ideal robust model (y = x)\nZero-Shot CLIP\nStandard ImageNet training\nExisiting robustness techniques\nImageNet"}
{"doc_id": "2103.00020", "para_id": 131, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 132, "text": "65\n70\n75\n80\n85\n90\n95\n100\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 133, "text": "Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model\n(dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink\nthis “robustness gap” by up to 75%. Linear ﬁts on logit transformed values are shown with bootstrap estimated 95% conﬁdence intervals.\n(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of\nthe best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation\nset, ResNet-101."}
{"doc_id": "2103.00020", "para_id": 134, "text": "pooling predictions across all sub-classes according to the\nImageNet class hierarchy. Sometimes this mapping is much\nless than perfect. For the person class in Youtube-BB, pre-\ndictions are made by pooling over the ImageNet classes for\na baseball player, a bridegroom, and a scuba diver. With\nCLIP we can instead generate a custom zero-shot classi-\nﬁer for each dataset directly based on its class names. In\nFigure 14 we see that this improves average effective ro-\nbustness by 5% but is concentrated in large improvements\non only a few datasets. Curiously, accuracy on ObjectNet\nalso increases by 2.3%. Although the dataset was designed\nto closely overlap with ImageNet classes, using the names\nprovided for each class by ObjectNet’s creators still helps a\nsmall amount compared to using ImageNet class names and\npooling predictions when necessary."}
{"doc_id": "2103.00020", "para_id": 135, "text": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,\nand 1.9% on ImageNet-A. The change in accuracy on the\ntwo other datasets, Youtube-BB and ImageNet Vid, is in-\nsigniﬁcant."}
{"doc_id": "2103.00020", "para_id": 136, "text": "How is it possible to improve accuracy by 9.2% on the Im-\nageNet dataset with little to no increase in accuracy under\ndistribution shift? Is the gain primarily from “exploiting\nspurious correlations”? Is this behavior unique to some com-\nbination of CLIP, the ImageNet datatset, and the distribution\nshifts studied, or a more general phenomena? Does it hold\nfor end-to-end ﬁnetuning as well as linear classiﬁers? We\ndo not have conﬁdent answers to these questions at this time.\nPrior work has also pre-trained models on distributions other\nthan ImageNet, but it is common to study and release mod-\nels only after they have been ﬁne-tuned to ImageNet. As a\nstep towards understanding whether pre-trained zero-shot\nmodels consistently have higher effective robustness than\nﬁne-tuned models, we encourage the authors of Mahajan\net al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.\n(2020) to, if possible, study these questions on their models\nas well."}
{"doc_id": "2103.00020", "para_id": 137, "text": "While zero-shot CLIP improves effective robustness, Figure\n14 shows that the beneﬁt is almost entirely gone in a fully\nsupervised setting. To better understand this difference, we\ninvestigate how effective robustness changes on the contin-\nuum from zero-shot to fully supervised. In Figure 15 we\nvisualize the performance of 0-shot, 1-shot, 2-shot, 4-shot\n..., 128-shot, and fully supervised logistic regression classi-\nﬁers on the best CLIP model’s features. We see that while\nfew-shot models also show higher effective robustness than\nexisting models, this beneﬁt fades as in-distribution per-\nformance increases with more training data and is mostly,\nthough not entirely, gone for the fully supervised model.\nAdditionally, zero-shot CLIP is notably more robust than\na few-shot model with equivalent ImageNet performance."}
{"doc_id": "2103.00020", "para_id": 138, "text": "We also investigate another robustness intervention enabled\nby ﬂexible zero-shot natural-language-based image classi-\nﬁers. The target classes across the 7 transfer datasets are\nnot always perfectly aligned with those of ImageNet. Two\ndatasets, Youtube-BB and ImageNet-Vid, consist of super-\nclasses of ImageNet. This presents a problem when trying\nto use the ﬁxed 1000-way classiﬁer of an ImageNet model\nto make predictions. Taori et al. (2020) handle this by max-"}
{"doc_id": "2103.00020", "para_id": 139, "text": "Learning Transferable Visual Models From Natural Language Supervision\n16"}
{"doc_id": "2103.00020", "para_id": 140, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 141, "text": "10\n5\n0\n5\n10\n15\n20\n25\n30\nChange from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 142, "text": "Ideal robust model (y = x)\nAdaptive Zero-Shot CLIP\nImageNet Zero-Shot CLIP\nLogistic Regression CLIP\nStandard ImageNet training\nRobustness intervention\nTrained with more data"}
{"doc_id": "2103.00020", "para_id": 143, "text": "10\n5\n0\n5\n10\n15\n20\n25\n30\nChange from zero-shot ImageNet classifier accuracy (%)"}
{"doc_id": "2103.00020", "para_id": 144, "text": "70\n75\n80\n85\n90\n95\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 145, "text": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness.\n(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classiﬁer\nand pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness\nas the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to\nImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset speciﬁc zero-shot\nclassiﬁers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don’t perfectly align\nwith ImageNet categories."}
{"doc_id": "2103.00020", "para_id": 146, "text": "humans on one of our tasks. We wanted to get a sense of\nhow strong human zero-shot performance is at these tasks,\nand how much human performance is improved if they are\nshown one or two image samples. This can help us to\ncompare task difﬁculty for humans and CLIP, and identify\ncorrelations and differences between them."}
{"doc_id": "2103.00020", "para_id": 147, "text": "Across our experiments, high effective robustness seems to\nresult from minimizing the amount of distribution speciﬁc\ntraining data a model has access to, but this comes at a cost\nof reducing dataset-speciﬁc performance."}
{"doc_id": "2103.00020", "para_id": 148, "text": "Taken together, these results suggest that the recent shift\ntowards large-scale task and dataset agnostic pre-training\ncombined with a reorientation towards zero-shot and few-\nshot benchmarking on broad evaluation suites (as advocated\nby Yogatama et al. (2019) and Linzen (2020)) promotes the\ndevelopment of more robust systems and provides a more\naccurate assessment of performance. We are curious to see\nif the same results hold for zero-shot models in the ﬁeld\nof NLP such as the GPT family. While Hendrycks et al.\n(2020b) has reported that pre-training improves relative ro-\nbustness on sentiment analysis, Miller et al. (2020)’s study\nof the robustness of question answering models under nat-\nural distribution shift ﬁnds, similar to Taori et al. (2020),\nlittle evidence of effective robustness improvements to date."}
{"doc_id": "2103.00020", "para_id": 149, "text": "We had ﬁve different humans look at each of 3669 images\nin the test split of the Oxford IIT Pets dataset (Parkhi et al.,\n2012) and select which of the 37 cat or dog breeds best\nmatched the image (or ‘I don’t know’ if they were com-\npletely uncertain). In the zero-shot case the humans were\ngiven no examples of the breeds and asked to label them\nto the best of their ability without an internet search. In\nthe one-shot experiment the humans were given one sample\nimage of each breed and in the two-shot experiment they\nwere given two sample images of each breed.5"}
{"doc_id": "2103.00020", "para_id": 150, "text": "One possible concern was that the human workers were not\nsufﬁciently motivated in the zero-shot task. High human\naccuracy of 94% on the STL-10 dataset (Coates et al., 2011)"}
{"doc_id": "2103.00020", "para_id": 151, "text": "5There is not a perfect correspondence between the human\nfew-shot tasks and the model’s few-shot performance since the\nmodel cannot refer to sample images in the way that the humans\ncan."}
{"doc_id": "2103.00020", "para_id": 152, "text": "How does CLIP compare to human performance and human\nlearning? To get a better understanding of how well humans\nperform in similar evaluation settings to CLIP, we evaluated"}
{"doc_id": "2103.00020", "para_id": 153, "text": "Learning Transferable Visual Models From Natural Language Supervision\n17"}
{"doc_id": "2103.00020", "para_id": 154, "text": "Average on 7 natural distribution shift datasets (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 155, "text": "Accuracy\nMajority Vote\non Full Dataset\nAccuracy\non Guesses"}
{"doc_id": "2103.00020", "para_id": 156, "text": "Zero-shot human\n53.7\n57.0\n69.7\n63.9\nZero-shot CLIP\n93.5\n93.5\n93.5\n93.5\nOne-shot human\n75.7\n80.3\n78.5\n81.2\nTwo-shot human\n75.7\n85.0\n79.2\n86.1"}
{"doc_id": "2103.00020", "para_id": 157, "text": "Table 2. Comparison of human performance on Oxford IIT Pets.\nAs in Parkhi et al. (2012), the metric is average per-class classiﬁca-\ntion accuracy. Most of the gain in performance when going from\nthe human zero shot case to the human one shot case is on images\nthat participants were highly uncertain on. “Guesses” refers to\nrestricting the dataset to where participants selected an answer\nother than “I don’t know”, the “majority vote” is taking the most\nfrequent (exclusive of ties) answer per image."}
{"doc_id": "2103.00020", "para_id": 158, "text": "Ideal robust model (y = x)\nFew-Shot CLIP (best model)\nZero-Shot CLIP (best model)\nStandard ImageNet training\nRobustness intervention\nTrained with more data"}
{"doc_id": "2103.00020", "para_id": 159, "text": "quality pre-trained model is near state-of-the-art for few\nshot learning (Tian et al., 2020), which suggests that there is\na gap between the best few-shot machine learning methods\nand human few-shot learning."}
{"doc_id": "2103.00020", "para_id": 160, "text": "65\n70\n75\n80\n85\n90\n95\nAverage on class subsampled ImageNet (top-1, %)"}
{"doc_id": "2103.00020", "para_id": 161, "text": "Figure 15. Few-shot CLIP also increases effective robustness\ncompared to existing ImageNet models but is less robust than\nzero-shot CLIP. Minimizing the amount of ImageNet training\ndata used for adaption increases effective robustness at the cost of\ndecreasing relative robustness. 16-shot logistic regression CLIP\nmatches zero-shot CLIP on ImageNet, as previously reported in\nFigure 7, but is less robust."}
{"doc_id": "2103.00020", "para_id": 162, "text": "If we plot human accuracy vs CLIP’s zero shot accuracy\n(Figure 16), we see that the hardest problems for CLIP are\nalso hard for humans. To the extent that errors are consistent,\nour hypothesis is that this is due to at least a two factors:\nnoise in the dataset (including mislabeled images) and out of\ndistribution images being hard for both humans and models."}
{"doc_id": "2103.00020", "para_id": 163, "text": "and 97-100% accuracy on the subset of attention check\nimages increased our trust in the human workers."}
{"doc_id": "2103.00020", "para_id": 164, "text": "A concern with pre-training on a very large internet dataset\nis unintentional overlap with downstream evals. This is\nimportant to investigate since, in a worst-case scenario, a\ncomplete copy of an evaluation dataset could leak into the\npre-training dataset and invalidate the evaluation as a mean-\ningful test of generalization. One option to prevent this is to\nidentify and remove all duplicates before training a model.\nWhile this guarantees reporting true hold-out performance,\nit requires knowing all possible data which a model might\nbe evaluated on ahead of time. This has the downside of\nlimiting the scope of benchmarking and analysis. Adding a\nnew evaluation would require an expensive re-train or risk\nreporting an un-quantiﬁed beneﬁt due to overlap."}
{"doc_id": "2103.00020", "para_id": 165, "text": "Interestingly, humans went from a performance average of\n54% to 76% with just one training example per class, and\nthe marginal gain from an additional training example is\nminimal. The gain in accuracy going from zero to one shot\nis almost entirely on images that humans were uncertain\nabout. This suggests that humans “know what they don’t\nknow” and are able to update their priors on the images they\nare most uncertain in based on a single example. Given this,\nit seems that while CLIP is a promising training strategy\nfor zero-shot performance (Figure 5) and does well on tests\nof natural distribution shift (Figure 13), there is a large\ndifference between how humans learn from a few examples\nand the few-shot methods in this paper."}
{"doc_id": "2103.00020", "para_id": 166, "text": "Instead, we document how much overlap occurs and how\nperformance changes due to these overlaps. To do this, we\nuse the following procedure:"}
{"doc_id": "2103.00020", "para_id": 167, "text": "This suggests that there are still algorithmic improvements\nwaiting to be made to decrease the gap between machine\nand human sample efﬁciency, as noted by Lake et al. (2016)\nand others. Because these few-shot evaluations of CLIP\ndon’t make effective use of prior knowledge and the humans\ndo, we speculate that ﬁnding a method to properly integrate\nprior knowledge into few-shot learning is an important step\nin algorithmic improvements to CLIP. To our knowledge,\nusing a linear classiﬁer on top of the features of a high-"}
{"doc_id": "2103.00020", "para_id": 168, "text": "1) For each evaluation dataset, we run a duplicate detector\n(see Appendix C) on its examples. We then manually inspect\nthe found nearest neighbors and set a per dataset threshold\nto keep high precision while maximizing recall. Using\nthis threshold, we then create two new subsets, Overlap,\nwhich contains all examples which have a similarity to a\ntraining example above the threshold, and Clean, which"}
{"doc_id": "2103.00020", "para_id": 169, "text": "Learning Transferable Visual Models From Natural Language Supervision\n18"}
{"doc_id": "2103.00020", "para_id": 170, "text": "our analysis. There is a median overlap of 2.2% and an av-\nerage overlap of 3.2%. Due to this small amount of overlap,\noverall accuracy is rarely shifted by more than 0.1% with\nonly 7 datasets above this threshold. Of these, only 2 are\nstatistically signiﬁcant after Bonferroni correction. The max\ndetected improvement is only 0.6% on Birdsnap which has\nthe second largest overlap at 12.1%. The largest overlap is\nfor Country211 at 21.5%. This is due to it being constructed\nout of YFCC100M, which our pre-training dataset contains\na ﬁltered subset of. Despite this large overlap there is only\na 0.2% increase in accuracy on Country211. This may be\nbecause the training text accompanying an example is often\nnot related to the speciﬁc task a downstream eval measures.\nCountry211 measures geo-localization ability, but inspect-\ning the training text for these duplicates showed they often\ndo not mention the location of the image."}
{"doc_id": "2103.00020", "para_id": 171, "text": "We are aware of two potential concerns with our analysis.\nFirst our detector is not perfect. While it achieves near\n100% accuracy on its proxy training task and manual in-\nspection + threshold tuning results in very high precision\nwith good recall among the found nearest-neighbors, we can\nnot tractably check its recall across 400 million examples.\nAnother potential confounder of our analysis is that the un-\nderlying data distribution may shift between the Overlap\nand Clean subsets. For example, on Kinetics-700 many\n“overlaps” are in fact all black transition frames. This ex-\nplains why Kinetics-700 has an apparent 20% accuracy drop\non Overlap. We suspect more subtle distribution shifts\nlikely exist. One possibility we noticed on CIFAR-100 is\nthat, due to the very low resolution of its images, many\nduplicates were false positives of small objects such as birds\nor planes. Changes in accuracy could instead be due to\nchanges in the class distribution or difﬁculty of the dupli-\ncates. Unfortunately, these distribution and difﬁculty shifts\ncould also mask the effects of over-ﬁtting."}
{"doc_id": "2103.00020", "para_id": 172, "text": "Figure 16. The hardest problems for CLIP also tend to be the hard-\nest problems for humans. Here we rank image categories by difﬁ-\nculty for CLIP as measured as probability of the correct label."}
{"doc_id": "2103.00020", "para_id": 173, "text": "contains all examples that are below this threshold. We\ndenote the unaltered full dataset All for reference. From\nthis we ﬁrst record the degree of data contamination as the\nratio of the number of examples in Overlap to the size of\nAll."}
{"doc_id": "2103.00020", "para_id": 174, "text": "2) We then compute the zero-shot accuracy of CLIP\nRN50x64 on the three splits and report All - Clean\nas our main metric. This is the difference in accuracy due\nto contamination. When positive it is our estimate of how\nmuch the overall reported accuracy on the dataset was in-\nﬂated by over-ﬁtting to overlapping data."}
{"doc_id": "2103.00020", "para_id": 175, "text": "However, these results closely follow the ﬁndings of simi-\nlar duplicate analysis in previous work on large scale pre-\ntraining. Mahajan et al. (2018) and Kolesnikov et al. (2019)\ndetected similar overlap rates and found minimal changes in\noverall performance. Importantly, Kolesnikov et al. (2019)\nalso compared the alternative de-duplication strategy dis-\ncussed in the introduction to this section with the approach\nwe settled on and observed little difference between the two\napproaches."}
{"doc_id": "2103.00020", "para_id": 176, "text": "3) The amount of overlap is often small so we also run a\nbinomial signiﬁcance test where we use the accuracy on\nClean as the null hypothesis and compute the one-tailed\n(greater) p-value for the Overlap subset. We also calculate\n99.5% Clopper-Pearson conﬁdence intervals on Dirty as\nanother check."}
{"doc_id": "2103.00020", "para_id": 177, "text": "A summary of this analysis is presented in Figure 17. Out\nof 35 datasets studied, 9 datasets have no detected overlap\nat all. Most of these datasets are synthetic or specialized\nmaking them unlikely to be posted as normal images on\nthe internet (for instance MNIST, CLEVR, and GTSRB) or\nare guaranteed to have no overlap due to containing novel\ndata from after the date our dataset was created (ObjectNet\nand Hateful Memes). This demonstrates our detector has\na low-false positive rate which is important as false posi-\ntives would under-estimate the effect of contamination in"}
{"doc_id": "2103.00020", "para_id": 178, "text": "There are still many limitations to CLIP. While several of\nthese are discussed as part of analysis in various sections,\nwe summarize and collect them here."}
{"doc_id": "2103.00020", "para_id": 179, "text": "On datasets with training splits, the performance of zero-\nshot CLIP is on average competitive with the simple su-"}
{"doc_id": "2103.00020", "para_id": 180, "text": "Learning Transferable Visual Models From Natural Language Supervision\n19"}
{"doc_id": "2103.00020", "para_id": 181, "text": "Difference in Accuracy on Overlapping vs. Clean Data (%)"}
{"doc_id": "2103.00020", "para_id": 182, "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nDetected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 183, "text": "0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nDetected Data Overlap (%)"}
{"doc_id": "2103.00020", "para_id": 184, "text": "Figure 17. Few statistically signiﬁcant improvements in accuracy due to detected data overlap. (Left) While several datasets have\nup to ±20% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have\n99.5% Clopper-Pearson conﬁdence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.\n(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to\noverlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy\nimprovements statistically signiﬁcant when calculated using a one-sided binomial test."}
{"doc_id": "2103.00020", "para_id": 185, "text": "pervised baseline of a linear classiﬁer on top of ResNet-50\nfeatures. On most of these datasets, the performance of\nthis baseline is now well below the overall state of the art.\nSigniﬁcant work is still needed to improve the task learning\nand transfer capabilities of CLIP. While scaling has so far\nsteadily improved performance and suggests a route for con-\ntinued improvement, we estimate around a 1000x increase\nin compute is required for zero-shot CLIP to reach overall\nstate-of-the-art performance. This is infeasible to train with\ncurrent hardware. Further research into improving upon the\ncomputational and data efﬁciency of CLIP will be necessary."}
{"doc_id": "2103.00020", "para_id": 186, "text": "CLIP learns a high quality semantic OCR representation that\nperforms well on digitally rendered text, which is common\nin its pre-training dataset, as evidenced by performance on\nRendered SST2. However, CLIP only achieves 88% accu-\nracy on the handwritten digits of MNIST. An embarrassingly\nsimple baseline of logistic regression on raw pixels outper-\nforms zero-shot CLIP. Both semantic and near-duplicate\nnearest-neighbor retrieval verify that there are almost no im-\nages that resemble MNIST digits in our pre-training dataset.\nThis suggests CLIP does little to address the underlying\nproblem of brittle generalization of deep learning models.\nInstead CLIP tries to circumvent the problem and hopes that\nby training on such a large and varied dataset that all data\nwill be effectively in-distribution. This is a naive assumption\nthat, as MNIST demonstrates, is easy to violate."}
{"doc_id": "2103.00020", "para_id": 187, "text": "Analysis in Section 3.1 found that CLIP’s zero-shot perfor-\nmance is still quite weak on several kinds of tasks. When\ncompared to task-speciﬁc models, the performance of CLIP\nis poor on several types of ﬁne-grained classiﬁcation such\nas differentiating models of cars, species of ﬂowers, and\nvariants of aircraft. CLIP also struggles with more abstract\nand systematic tasks such as counting the number of objects\nin an image. Finally for novel tasks which are unlikely to be\nincluded in CLIP’s pre-training dataset, such as classifying\nthe distance to the nearest car in a photo, CLIP’s perfor-\nmance can be near random. We are conﬁdent that there are\nstill many, many, tasks where CLIP’s zero-shot performance\nis near chance level."}
{"doc_id": "2103.00020", "para_id": 188, "text": "Although CLIP can ﬂexibly generate zero-shot classiﬁers\nfor a wide variety of tasks and datasets, CLIP is still limited\nto choosing from only those concepts in a given zero-shot\nclassiﬁer. This is a signiﬁcant restriction compared to a\ntruly ﬂexible approach like image captioning which could\ngenerate novel outputs. Unfortunately, as described in Sec-\ntion 2.3 we found the computational efﬁciency of the image\ncaption baseline we tried to be much lower than CLIP. A\nsimple idea worth trying is joint training of a contrastive\nand generative objective with the hope of combining the\nefﬁciency of CLIP with the ﬂexibility of a caption model.\nAs another alternative, search could be performed at infer-\nence time over many natural language explanations of a\ngiven image, similar to approach proposed in Learning with\nLatent Language Andreas et al. (2017)."}
{"doc_id": "2103.00020", "para_id": 189, "text": "While zero-shot CLIP generalizes well to many natural im-\nage distributions as investigated in Section 3.3, we’ve ob-\nserved that zero-shot CLIP still generalizes poorly to data\nthat is truly out-of-distribution for it. An illustrative exam-\nple occurs for the task of OCR as reported in Appendix E."}
{"doc_id": "2103.00020", "para_id": 190, "text": "Learning Transferable Visual Models From Natural Language Supervision\n20"}
{"doc_id": "2103.00020", "para_id": 191, "text": "CLIP also does not address the poor data efﬁciency of deep\nlearning. Instead CLIP compensates by using a source of\nsupervision that can be scaled to hundreds of millions of\ntraining examples. If every image seen during training of\na CLIP model was presented at a rate of one per second,\nit would take 405 years to iterate through the 12.8 billion\nimages seen over 32 training epochs. Combining CLIP\nwith self-supervision (Henaff, 2020; Chen et al., 2020c) and\nself-training (Lee; Xie et al., 2020) methods is a promising\ndirection given their demonstrated ability to improve data\nefﬁciency over standard supervised learning."}
{"doc_id": "2103.00020", "para_id": 192, "text": "CLIP has a wide range of capabilities due to its ability to\ncarry out arbitrary image classiﬁcation tasks. One can give\nit images of cats and dogs and ask it to classify cats, or give\nit images taken in a department store and ask it to classify\nshoplifters–a task with signiﬁcant social implications and\nfor which AI may be unﬁt. Like any image classiﬁcation\nsystem, CLIP’s performance and ﬁtness for purpose need to\nbe evaluated, and its broader impacts analyzed in context.\nCLIP also introduces a capability that will magnify and alter\nsuch issues: CLIP makes it possible to easily create your\nown classes for categorization (to ‘roll your own classiﬁer’)\nwithout a need for re-training. This capability introduces\nchallenges similar to those found in characterizing other,\nlarge-scale generative models like GPT-3 (Brown et al.,\n2020); models that exhibit non-trivial zero-shot (or few-\nshot) generalization can have a vast range of capabilities,\nmany of which are made clear only after testing for them."}
{"doc_id": "2103.00020", "para_id": 193, "text": "Our methodology has several signiﬁcant limitations. De-\nspite our focus on zero-shot transfer, we repeatedly queried\nperformance on full validation sets to guide the develop-\nment of CLIP. These validation sets often have thousands\nof examples, which is unrealistic for true zero-shot sce-\nnarios. Similar concerns have been raised in the ﬁeld of\nsemi-supervised learning (Oliver et al., 2018). Another po-\ntential issue is our selection of evaluation datasets. While\nwe have reported results on Kornblith et al. (2019)’s 12\ndataset evaluation suite as a standardized collection, our\nmain results use a somewhat haphazardly assembled col-\nlection of 27 datasets that is undeniably co-adapted with\nthe development and capabilities of CLIP. Creating a new\nbenchmark of tasks designed explicitly to evaluate broad\nzero-shot transfer capabilities, rather than re-using existing\nsupervised datasets, would help address these issues."}
{"doc_id": "2103.00020", "para_id": 194, "text": "Our studies of CLIP in a zero-shot setting show that the\nmodel displays signiﬁcant promise for widely-applicable\ntasks like image retrieval or search. For example, it can ﬁnd\nrelevant images in a database given text, or relevant text\ngiven an image. Further, the relative ease of steering CLIP\ntoward bespoke applications with little or no additional data\nor training could unlock a variety of novel applications that\nare hard for us to envision today, as has occurred with large\nlanguage models over the past few years."}
{"doc_id": "2103.00020", "para_id": 195, "text": "CLIP is trained on text paired with images on the internet.\nThese image-text pairs are unﬁltered and uncurated and\nresult in CLIP models learning many social biases. This\nhas been previously demonstrated for image caption models\n(Bhargava & Forsyth, 2019). We refer readers to Section 7\nfor detailed analysis and quantiﬁcation of these behaviors for\nCLIP as well as discussion of potential mitigation strategies."}
{"doc_id": "2103.00020", "para_id": 196, "text": "In addition to the more than 30 datasets studied in earlier\nsections of this paper, we evaluate CLIP’s performance on\nthe FairFace benchmark and undertake exploratory bias\nprobes. We then characterize the model’s performance in\na downstream task, surveillance, and discuss its usefulness\nas compared with other available systems. Many of CLIP’s\ncapabilities are omni-use in nature (e.g. OCR can be used\nto make scanned documents searchable, to power screen\nreading technologies, or to read license plates). Several\nof the capabilities measured, from action recognition, ob-\nject classiﬁcation, and geo-localization, to facial emotion\nrecognition, can be used in surveillance. Given its social\nimplications, we address this domain of use speciﬁcally in\nthe Surveillance section."}
{"doc_id": "2103.00020", "para_id": 197, "text": "While we have emphasized throughout this work that speci-\nfying image classiﬁers through natural language is a ﬂexible\nand general interface, it has its own limitations. Many com-\nplex tasks and visual concepts can be difﬁcult to specify\njust through text. Actual training examples are undeniably\nuseful but CLIP does not optimize for few-shot performance\ndirectly. In our work, we fall back to ﬁtting linear classiﬁers\non top of CLIP’s features. This results in a counter-intuitive\ndrop in performance when transitioning from a zero-shot\nto a few-shot setting. As discussed in Section 4, this is\nnotably different from human performance which shows a\nlarge increase from a zero to a one shot setting. Future work\nis needed to develop methods that combine CLIP’s strong\nzero-shot performance with efﬁcient few-shot learning."}
{"doc_id": "2103.00020", "para_id": 198, "text": "We have also sought to characterize the social biases inher-\nent to the model. Our bias tests represent our initial efforts\nto probe aspects of how the model responds in different sce-\nnarios, and are by nature limited in scope. CLIP and models\nlike it will need to be analyzed in relation to their speciﬁc\ndeployments to understand how bias manifests and iden-\ntify potential interventions. Further community exploration\nwill be required to develop broader, more contextual, and\nmore robust testing schemes so that AI developers can bet-\nter characterize biases in general purpose computer vision\nmodels."}
{"doc_id": "2103.00020", "para_id": 199, "text": "Learning Transferable Visual Models From Natural Language Supervision\n21"}
{"doc_id": "2103.00020", "para_id": 200, "text": "FairFace Model\n75.4\n94.4\n60.7\nLinear Probe CLIP\n92.8\n97.7\n63.1\nZero-Shot CLIP\n91.3\n97.2\n54.3\nLinear Probe Instagram\n87.2\n93.9\n54.1"}
{"doc_id": "2103.00020", "para_id": 201, "text": "FairFace Model\n93.7\n94.2\n59.7\nLinear Probe CLIP\n93.4\n96.5\n63.8\nZero-Shot CLIP\n58.3\n95.9\n57.1\nLinear Probe Instagram\n90.8\n93.2\n54.2"}
{"doc_id": "2103.00020", "para_id": 202, "text": "Table 4. Percent accuracy on Race, Gender, and Age classiﬁcation\nof images in FairFace categories ‘Black,’ ‘Indian,’ ‘East Asian,’\n‘Southeast Asian,’ ‘Middle Eastern,’ and ‘Latino’ (grouped to-\ngether as FairFace category ‘Non-White’)"}
{"doc_id": "2103.00020", "para_id": 203, "text": "Table 3. Percent accuracy on Race, Gender, and Age classiﬁcation\nof images in FairFace category ‘White’"}
{"doc_id": "2103.00020", "para_id": 204, "text": "Middle Southeast East\nModel\nGender Black White Indian Latino Eastern\nAsian\nAsian Average"}
{"doc_id": "2103.00020", "para_id": 205, "text": "Male\n96.9\n96.4\n98.7\n96.5\n98.9\n96.2\n96.9\n97.2\nLinear Probe CLIP\nFemale\n97.9\n96.7\n97.9\n99.2\n97.2\n98.5\n97.3\n97.8\n97.4\n96.5\n98.3\n97.8\n98.4\n97.3\n97.1\n97.5"}
{"doc_id": "2103.00020", "para_id": 206, "text": "Male\n96.3\n96.4\n97.7\n97.2\n98.3\n95.5\n96.8\n96.9\nZero-Shot CLIP\nFemale\n97.1\n95.3\n98.3\n97.8\n97.5\n97.2\n96.4\n97.0\n96.7\n95.9\n98.0\n97.5\n98.0\n96.3\n96.6"}
{"doc_id": "2103.00020", "para_id": 207, "text": "Male\n92.5\n94.8\n96.2\n93.1\n96.0\n92.7\n93.4\n94.1\nLinear Probe Instagram Female\n90.1\n91.4\n95.0\n94.8\n95.0\n94.1\n94.3\n93.4\n91.3\n93.2\n95.6\n94.0\n95.6\n93.4\n93.9"}
{"doc_id": "2103.00020", "para_id": 208, "text": "Table 5. Percent accuracy on gender classiﬁcation of images by FairFace race category"}
{"doc_id": "2103.00020", "para_id": 209, "text": "as an initial bias probe, then probe the model further to\nsurface additional biases and sources of biases, including\nclass design."}
{"doc_id": "2103.00020", "para_id": 210, "text": "Algorithmic decisions, training data, and choices about how\nclasses are deﬁned and taxonomized (which we refer to in-\nformally as “class design”) can all contribute to and amplify\nsocial biases and inequalities resulting from the use of AI\nsystems (Noble, 2018; Bechmann & Bowker, 2019; Bowker\n& Star, 2000). Class design is particularly relevant to mod-\nels like CLIP, since any developer can deﬁne a class and the\nmodel will provide some result."}
{"doc_id": "2103.00020", "para_id": 211, "text": "We evaluated two versions of CLIP on the FairFace dataset:\na zero-shot CLIP model (“ZS CLIP”), and a logistic regres-\nsion classiﬁer ﬁtted to FairFace’s dataset on top of CLIP’s\nfeatures (“LR CLIP”). We ﬁnd that LR CLIP gets higher\naccuracy on the FairFace dataset than both the ResNext-101\n32x48d Instagram model (“Linear Probe Instagram”) (Ma-\nhajan et al., 2018) and FairFace’s own model on most of the\nclassiﬁcation tests we ran7. ZS CLIP’s performance varies\nby category and is worse than that of FairFace’s model for a\nfew categories, and better for others. (See Table 3 and Table\n4)."}
{"doc_id": "2103.00020", "para_id": 212, "text": "In this section, we provide preliminary analysis of some\nof the biases in CLIP, using bias probes inspired by those\noutlined in Buolamwini & Gebru (2018) and K¨arkk¨ainen\n& Joo (2019). We also conduct exploratory bias research\nintended to ﬁnd speciﬁc examples of biases in the model,\nsimilar to that conducted by Solaiman et al. (2019)."}
{"doc_id": "2103.00020", "para_id": 213, "text": "and Keyes (2018) have shown. While FairFace’s dataset reduces\nthe proportion of White faces, it still lacks representation of entire\nlarge demographic groups, effectively erasing such categories. We\nuse the 2 gender categories and 7 race categories deﬁned in the\nFairFace dataset in a number of our experiments not in order to\nreinforce or endorse the use of such reductive categories, but in\norder to enable us to make comparisons to prior work.\n7One challenge with this comparison is that the FairFace model\nuses binary classes for race (“White” and “Non-White”), instead\nof breaking down races into ﬁner-grained sub-groups."}
{"doc_id": "2103.00020", "para_id": 214, "text": "We start by analyzing the performance of Zero-Shot CLIP on\nthe face image dataset FairFace (K¨arkk¨ainen & Joo, 2019)6"}
{"doc_id": "2103.00020", "para_id": 215, "text": "6FairFace is a face image dataset designed to balance age, gen-\nder, and race, in order to reduce asymmetries common in previous\nface datasets. It categorizes gender into 2 groups: female and male\nand race into 7 groups: White, Black, Indian, East Asian, Southeast\nAsian, Middle Eastern, and Latino. There are inherent problems\nwith race and gender classiﬁcations, as e.g. Bowker & Star (2000)"}
{"doc_id": "2103.00020", "para_id": 216, "text": "Learning Transferable Visual Models From Natural Language Supervision\n22"}
{"doc_id": "2103.00020", "para_id": 217, "text": "Middle\nSoutheast\nEast\nCategory\nBlack\nWhite\nIndian\nLatino\nEastern\nAsian\nAsian"}
{"doc_id": "2103.00020", "para_id": 218, "text": "Crime-related Categories\n16.4\n24.9\n24.4\n10.8\n19.7\n4.4\n1.3\nNon-human Categories\n14.4\n5.5\n7.6\n3.7\n2.0\n1.9\n0.0"}
{"doc_id": "2103.00020", "para_id": 219, "text": "Table 6. Percent of images classiﬁed into crime-related and non-human categories by FairFace Race category. The label set included 7\nFairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 220, "text": "Category Label Set\n0-2\n3-9\n10-19\n20-29\n30-39\n40-49\n50-59\n60-69\nover 70"}
{"doc_id": "2103.00020", "para_id": 221, "text": "Default Label Set\n30.3\n35.0\n29.5\n16.3\n13.9\n18.5\n19.1\n16.2\n10.4\nDefault Label Set + ‘child’ category\n2.3\n4.3\n14.7\n15.0\n13.4\n18.2\n18.6\n15.5\n9.4"}
{"doc_id": "2103.00020", "para_id": 222, "text": "Table 7. Percent of images classiﬁed into crime-related and non-human categories by FairFace Age category, showing comparison between\nresults obtained using a default label set and a label set to which the label ’child’ has been added. The default label set included 7 FairFace\nrace categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories."}
{"doc_id": "2103.00020", "para_id": 223, "text": "We found that 4.9% (conﬁdence intervals between 4.6%\nand 5.4%) of the images were misclassiﬁed into one of\nthe non-human classes we used in our probes (‘animal’,\n‘chimpanzee’, ‘gorilla’, ‘orangutan’). Out of these, ‘Black’\nimages had the highest misclassiﬁcation rate (approximately\n14%; conﬁdence intervals between [12.6% and 16.4%])\nwhile all other races had misclassiﬁcation rates under 8%.\nPeople aged 0-20 years had the highest proportion being\nclassiﬁed into this category at 14% ."}
{"doc_id": "2103.00020", "para_id": 224, "text": "Additionally, we test the performance of the LR CLIP and\nZS CLIP models across intersectional race and gender cate-\ngories as they are deﬁned in the FairFace dataset. We ﬁnd\nthat model performance on gender classiﬁcation is above\n95% for all race categories. Table 5 summarizes these re-\nsults."}
{"doc_id": "2103.00020", "para_id": 225, "text": "While LR CLIP achieves higher accuracy than the Linear\nProbe Instagram model on the FairFace benchmark dataset\nfor gender, race and age classiﬁcation of images by intersec-\ntional categories, accuracy on benchmarks offers only one\napproximation of algorithmic fairness, as Raji et al. (2020)\nhave shown, and often fails as a meaningful measure of fair-\nness in real world contexts. Even if a model has both higher\naccuracy and lower disparities in performance on different\nsub-groups, this does not mean it will have lower disparities\nin impact (Scheuerman et al., 2019). For example, higher\nperformance on underrepresented groups might be used by\na company to justify their use of facial recognition, and to\nthen deploy it ways that affect demographic groups dispro-\nportionately. Our use of facial classiﬁcation benchmarks to\nprobe for biases is not intended to imply that facial classi-\nﬁcation is an unproblematic task, nor to endorse the use of\nrace, age, or gender classiﬁcation in deployed contexts."}
{"doc_id": "2103.00020", "para_id": 226, "text": "We also found that 16.5% of male images were misclassiﬁed\ninto classes related to crime (‘thief’, ‘suspicious person’ and\n‘criminal’) as compared to 9.8% of female images. Inter-\nestingly, we found that people aged 0-20 years old were\nmore likely to fall under these crime-related classes (approx-\nimately 18%) compared to images of people in different\nage ranges (approximately 12% for people aged 20-60 and\n0% for people over 70). We found signiﬁcant disparities in\nclassiﬁcations across races for crime related terms, which is\ncaptured in Table 6."}
{"doc_id": "2103.00020", "para_id": 227, "text": "Given that we observed that people under 20 were the most\nlikely to be classiﬁed in both the crime-related and non-\nhuman animal categories, we carried out classiﬁcation for\nthe images with the same classes but with an additional\ncategory ‘child’ added to the categories. Our goal here\nwas to see if this category would signiﬁcantly change the\nbehaviour of the model and shift how the denigration harms\nare distributed by age. We found that this drastically reduced\nthe number of images of people under 20 classiﬁed in either\ncrime-related categories or non-human animal categories\n(Table 7). This points to how class design has the potential\nto be a key factor determining both the model performance\nand the unwanted biases or behaviour the model may exhibit\nwhile also asks overarching questions about the use of face"}
{"doc_id": "2103.00020", "para_id": 228, "text": "We also probed the model using classiﬁcation terms with\nhigh potential to cause representational harm, focusing on\ndenigration harms in particular (Crawford, 2017). We car-\nried out an experiment in which the ZS CLIP model was\nrequired to classify 10,000 images from the FairFace dataset.\nIn addition to the FairFace classes, we added in the follow-\ning classes: ‘animal’, ‘gorilla’, ‘chimpanzee’, ‘orangutan’,\n‘thief’, ‘criminal’ and ‘suspicious person’. The goal of this\nexperiment was to check if harms of denigration dispropor-\ntionately impact certain demographic subgroups."}
{"doc_id": "2103.00020", "para_id": 229, "text": "Learning Transferable Visual Models From Natural Language Supervision\n23"}
{"doc_id": "2103.00020", "para_id": 230, "text": "images to automatically classify people along such lines\n(y Arcas et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 231, "text": "When given the combined set of labels that Google Cloud\nVision (GCV), Amazon Rekognition and Microsoft returned\nfor all the images, similar to the biases Schwemmer et al.\n(2020) found in GCV systems, we found our system also\ndisproportionately attached labels to do with hair and ap-\npearance in general to women more than men. For ex-\nample, labels such as ‘brown hair’, ‘blonde’ and ‘blond’\nappeared signiﬁcantly more often for women. Additionally,\nCLIP attached some labels that described high status occu-\npations disproportionately more often to men such as ‘ex-\necutive’ and ‘doctor’. Out of the only four occupations that\nit attached more often to women, three were ‘newscaster’,\n‘television presenter’ and ‘newsreader’ and the fourth was\n‘Judge’. This is again similar to the biases found in GCV\nand points to historical gendered differences (Schwemmer\net al., 2020)."}
{"doc_id": "2103.00020", "para_id": 232, "text": "The results of these probes can change based on the class\ncategories one chooses to include as well as the speciﬁc\nlanguage one uses to describe each class. Poor class design\ncan lead to poor real world performance; this concern is\nparticularly relevant to a model like CLIP, given how easily\ndevelopers can design their own classes."}
{"doc_id": "2103.00020", "para_id": 233, "text": "We also carried out experiments similar to those outlined by\nSchwemmer et al. (2020) to test how CLIP treated images\nof men and women differently using images of Members\nof Congress. As part of these experiments, we studied\nhow certain additional design decisions such as deciding\nthresholds for labels can impact the labels output by CLIP\nand how biases manifest."}
{"doc_id": "2103.00020", "para_id": 234, "text": "We carried out three experiments - we tested for accuracy\non gender classiﬁcation and we tested for how labels were\ndifferentially distributed across two different label sets. For\nour ﬁrst label set, we used a label set of 300 occupations and\nfor our second label set we used a combined set of labels that\nGoogle Cloud Vision, Amazon Rekognition and Microsoft\nAzure Computer Vision returned for all the images."}
{"doc_id": "2103.00020", "para_id": 235, "text": "Interestingly, when we lowered the threshold to 0.5% for\nthis set of labels, we found that the labels disproportionately\ndescribing men also shifted to appearance oriented words\nsuch as ‘suit’, ‘tie’ and ‘necktie’ (Figure 18). Many occupa-\ntion oriented words such as ‘military person’ and ‘executive’\n- which were not used to describe images of women at the\nhigher 4% threshold - were used for both men and women\nat the lower 0.5% threshold, which could have caused the\nchange in labels for men. The reverse was not true. Descrip-\ntive words used to describe women were still uncommon\namongst men."}
{"doc_id": "2103.00020", "para_id": 236, "text": "We ﬁrst simply looked into gender prediction performance\nof the model on the images of Members of Congress, in\norder to check to see if the model correctly recognized\nmen as men and women as women given the image of a\nperson who appeared to be in an ofﬁcial setting/position of\npower. We found that the model got 100% accuracy on the\nimages. This is slightly better performance than the model’s\nperformance on the FairFace dataset. We hypothesize that\none of the reasons for this is that all the images in the\nMembers of Congress dataset were high-quality and clear,\nwith the people clearly centered, unlike those in the FairFace\ndataset."}
{"doc_id": "2103.00020", "para_id": 237, "text": "Design decisions at every stage of building a model impact\nhow biases manifest and this is especially true for CLIP\ngiven the ﬂexibility it offers. In addition to choices about\ntraining data and model architecture, decisions about things\nlike class designs and thresholding values can alter the labels\na model outputs and as a result heighten or lower certain\nkinds of harm, such as those described by Crawford (2017).\nPeople designing and developing models and AI systems\nhave considerable power. Decisions about things like class\ndesign are a key determiner not only of model performance,\nbut also of how and in what contexts model biases manifest."}
{"doc_id": "2103.00020", "para_id": 238, "text": "In order to study how the biases in returned labels depend on\nthe thresholds set for label probability, we did an experiment\nin which we set threshold values at 0.5% and 4.0%. We\nfound that the lower threshold led to lower quality of labels.\nHowever, even the differing distributions of labels under\nthis threshold can hold signals for bias. For example, we\nﬁnd that under the 0.5% threshold labels such as ‘nanny’\nand ‘housekeeper’ start appearing for women whereas labels\nsuch as ‘prisoner’ and ‘mobster’ start appearing for men.\nThis points to gendered associations similar to those that\nhave previously been found for occupations (Schwemmer\net al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016)."}
{"doc_id": "2103.00020", "para_id": 239, "text": "These experiments are not comprehensive.\nThey illus-\ntrate potential issues stemming from class design and other\nsources of bias, and are intended to spark inquiry."}
{"doc_id": "2103.00020", "para_id": 240, "text": "We next sought to characterize model performance in re-\nlation to a downstream task for which there is signiﬁcant\nsocietal sensitivity: surveillance. Our analysis aims to better\nembody the characterization approach described above and\nto help orient the research community towards the potential\nfuture impacts of increasingly general purpose computer\nvision models and aid the development of norms and checks"}
{"doc_id": "2103.00020", "para_id": 241, "text": "At the higher 4% threshold, the labels with the highest prob-\nability across both genders include “lawmaker”, “legislator”\nand “congressman”. However, the presence of these biases\namongst lower probability labels nonetheless point to larger\nquestions about what ‘sufﬁciently’ safe behaviour may look"}
{"doc_id": "2103.00020", "para_id": 242, "text": "Learning Transferable Visual Models From Natural Language Supervision\n24"}
{"doc_id": "2103.00020", "para_id": 243, "text": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google\nCloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were\nidentiﬁed with χ2 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a\ncertain label by gender."}
{"doc_id": "2103.00020", "para_id": 244, "text": "around such systems. Our inclusion of surveillance is not\nintended to indicate enthusiasm for this domain - rather, we\nthink surveillance is an important domain to try to make\npredictions about given its societal implications (Zuboff,\n2015; Browne, 2015)."}
{"doc_id": "2103.00020", "para_id": 245, "text": "the model to choose from. Additionally, we carried out a\n‘stress test’ where the class set included at least one more\ncaption for something that was ‘close’ to the image (for\nexample, ‘parking lot with white car’ vs. ‘parking lot with\nred car’). We found that the model had a top-1 accuracy\nof 91.8% on the CCTV images for the initial evaluation.\nThe accuracy dropped signiﬁcantly to 51.1% for the second\nevaluation, with the model incorrectly choosing the ‘close’\nanswer 40.7% of the time."}
{"doc_id": "2103.00020", "para_id": 246, "text": "We measure the model’s performance on classiﬁcation of\nimages from CCTV cameras and zero-shot celebrity identiﬁ-\ncation. We ﬁrst tested model performance on low-resolution\nimages captured from surveillance cameras (e.g. CCTV\ncameras). We used the VIRAT dataset (Oh et al., 2011) and\ndata captured by Varadarajan & Odobez (2009), which both\nconsist of real world outdoor scenes with non-actors."}
{"doc_id": "2103.00020", "para_id": 247, "text": "For ﬁne-grained detection, the zero-shot model performed\npoorly, with results near random. Note that this experiment\nwas targeted only towards detecting the presence or absence\nof small objects in image sequences."}
{"doc_id": "2103.00020", "para_id": 248, "text": "Given CLIP’s ﬂexible class construction, we tested 515\nsurveillance images captured from 12 different video se-\nquences on self-constructed general classes for coarse and\nﬁne grained classiﬁcation. Coarse classiﬁcation required the\nmodel to correctly identify the main subject of the image (i.e.\ndetermine if the image was a picture of an empty parking\nlot, school campus, etc.). For ﬁne-grained classiﬁcation, the\nmodel had to choose between two options constructed to\ndetermine if the model could identify the presence/absence\nof smaller features in the image such as a person standing\nin the corner."}
{"doc_id": "2103.00020", "para_id": 249, "text": "We also tested CLIP’s zero-shot performance for ‘in the\nwild’ identity detection using the CelebA dataset8. We did\nthis to evaluate the model’s performance for identity detec-\ntion using just the publicly available data it was pre-trained\non. While we tested this on a dataset of celebrities who have\na larger number of images on the internet, we hypothesize\nthat the number of images in the pre-training data needed\nfor the model to associate faces with names will keep de-\ncreasing as models get more powerful (see Table 8), which\nhas signiﬁcant societal implications (Garvie, 2019). This"}
{"doc_id": "2103.00020", "para_id": 250, "text": "For coarse classiﬁcation, we constructed the classes by hand-\ncaptioning the images ourselves to describe the contents\nof the image and there were always at least 6 options for"}
{"doc_id": "2103.00020", "para_id": 251, "text": "8Note: The CelebA dataset is more representative of faces with\nlighter skin tones. Due to the nature of the dataset, we were not\nable to control for race, gender, age, etc."}
{"doc_id": "2103.00020", "para_id": 252, "text": "Learning Transferable Visual Models From Natural Language Supervision\n25"}
{"doc_id": "2103.00020", "para_id": 253, "text": "We hope that this work motivates future research on the\ncharacterization of the capabilities, shortcomings, and biases\nof such models, and we are excited to engage with the\nresearch community on such questions."}
{"doc_id": "2103.00020", "para_id": 254, "text": "CLIP L/14\n59.2\n43.3\n42.2\nCLIP RN50x64\n56.4\n39.5\n38.4\nCLIP RN50x16\n52.7\n37.4\n36.3\nCLIP RN50x4\n52.8\n38.1\n37.3"}
{"doc_id": "2103.00020", "para_id": 255, "text": "We believe one good step forward is community exploration\nto further characterize the capabilities of models like CLIP\nand - crucially - identify application areas where they have\npromising performance and areas where they may have\nreduced performance9. This process of characterization can\nhelp researchers increase the likelihood models are used\nbeneﬁcially by:"}
{"doc_id": "2103.00020", "para_id": 256, "text": "Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy"}
{"doc_id": "2103.00020", "para_id": 257, "text": "mirrors recent developments in natural language processing,\nin which recent large language models trained on Internet\ndata often exhibit a surprising ability to provide informa-\ntion related to relatively minor public ﬁgures (Brown et al.,\n2020)."}
{"doc_id": "2103.00020", "para_id": 258, "text": "• Identifying potentially beneﬁcial downstream uses of\nmodels early in the research process, enabling other\nresearchers to think about applications."}
{"doc_id": "2103.00020", "para_id": 259, "text": "We found that the model had 59.2% top-1 accuracy out\nof 100 possible classes for ‘in the wild’ 8k celebrity im-\nages. However, this performance dropped to 43.3% when\nwe increased our class sizes to 1k celebrity names. This\nperformance is not competitive when compared to produc-\ntion level models such as Google’s Celebrity Recognition\n(Google). However, what makes these results noteworthy is\nthat this analysis was done using only zero-shot identiﬁca-\ntion capabilities based on names inferred from pre-training\ndata - we didn’t use any additional task-speciﬁc dataset, and\nso the (relatively) strong results further indicate that before\ndeploying multimodal models, people will need to carefully\nstudy them for behaviors in a given context and domain."}
{"doc_id": "2103.00020", "para_id": 260, "text": "• Surfacing tasks with signiﬁcant sensitivity and a large\nset of societal stakeholders, which may call for inter-\nvention by policymakers."}
{"doc_id": "2103.00020", "para_id": 261, "text": "• Better characterizing biases in models, alerting other\nresearchers to areas of concern and areas for interven-\ntions."}
{"doc_id": "2103.00020", "para_id": 262, "text": "• Creating suites of tests to evaluate systems like CLIP\non, so we can better characterize model capabilities\nearlier in the development cycle."}
{"doc_id": "2103.00020", "para_id": 263, "text": "• Identifying potential failure modes and areas for further\nwork."}
{"doc_id": "2103.00020", "para_id": 264, "text": "CLIP offers signiﬁcant beneﬁt for tasks that have relatively\nlittle data given its zero-shot capabilities. However, large\ndatasets and high performing supervised models exist for\nmany in-demand surveillance tasks such as facial recogni-\ntion. As a result, CLIP’s comparative appeal for such uses\nis low. Additionally, CLIP is not designed for common\nsurveillance-relevant tasks like object detection and seman-\ntic segmentation. This means it has limited use for certain\nsurveillance tasks when models that are designed with these\nuses in mind such as Detectron2 (Wu et al., 2019) are widely\navailable."}
{"doc_id": "2103.00020", "para_id": 265, "text": "We plan to contribute to this work, and hope this analysis\nprovides some motivating examples for subsequent research."}
{"doc_id": "2103.00020", "para_id": 266, "text": "Any model that leverages written, spoken, signed or any\nother form of human language as part of its training signal\nis arguably using natural language as a source of supervi-\nsion. This is an admittedly extremely broad area and covers\nmost work in the ﬁeld of distributional semantics including\ntopic models (Blei et al., 2003), word, sentence, and para-\ngraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &\nMikolov, 2014), and language models (Bengio et al., 2003).\nIt also includes much of the broader ﬁeld of NLP that deals\nwith predicting or modeling sequences of natural language\nin some way. Work in NLP intentionally leveraging natural\nlanguage supervision in the form of explanations, feedback,\ninstructions, and advice for tasks such as classiﬁcation (as\nopposed to the commonly used representation of supervision\nas a set of arbitrarily encoded discrete category labels) has"}
{"doc_id": "2103.00020", "para_id": 267, "text": "However, CLIP does unlock a certain aspect of usability\ngiven how it removes the need for training data. Thus, CLIP\nand similar models could enable bespoke, niche surveillance\nuse cases for which no well-tailored models or datasets exist,\nand could lower the skill requirements to build such appli-\ncations. As our experiments show, ZS CLIP displays non-\ntrivial, but not exceptional, performance on a few surveil-\nlance relevant tasks today."}
{"doc_id": "2103.00020", "para_id": 268, "text": "This preliminary analysis is intended to illustrate some of\nthe challenges that general purpose computer vision models\npose and to give a glimpse into their biases and impacts."}
{"doc_id": "2103.00020", "para_id": 269, "text": "9A model could be unﬁt for use due to inadequate performance\nor due to the inappropriateness of AI use in the application area\nitself."}
{"doc_id": "2103.00020", "para_id": 270, "text": "Learning Transferable Visual Models From Natural Language Supervision\n26"}
{"doc_id": "2103.00020", "para_id": 271, "text": "large scale representation learning by training a system to\npair descriptive text with videos instead of images. Several\nworks have explored using dense spoken natural language\nsupervision for videos (Miech et al., 2019; 2020b). When\nconsidered together with CLIP, these works suggest that\nlarge scale natural language supervision is a promising way\nto learn high quality perceptual systems for many domains.\nAlayrac et al. (2020) extended this line of work to an addi-\ntional modality by adding raw audio as an additional super-\nvision source and demonstrated beneﬁts from combining all\nthree sources of supervision."}
{"doc_id": "2103.00020", "para_id": 272, "text": "been explored in many creative and advanced ways. Dialog\nbased learning (Weston, 2016; Li et al., 2016; Hancock et al.,\n2019) develops techniques to learn from interactive natural\nlanguage feedback in dialog. Several papers have leveraged\nsemantic parsing to convert natural language explanations\ninto features (Srivastava et al., 2017) or additional training\nlabels (Hancock et al., 2018). More recently, ExpBERT\n(Murty et al., 2020) uses feature representations produced\nby conditioning a deep contextual language model on nat-\nural language explanations and descriptions of relations to\nimprove performance on the task of relation extraction."}
{"doc_id": "2103.00020", "para_id": 273, "text": "CLIP is an example of using natural language as a training\nsignal for learning about a domain other than language. In\nthis context, the earliest use of the term natural language\nsupervision that we are aware of is the work of Ramanathan\net al. (2013) which showed that natural language descrip-\ntions could be used along side other sources of supervision\nto improve performance on the task of video event under-\nstanding. However, as mentioned in the introduction and\napproach section, methods of leveraging natural language\ndescriptions in computer vision well predate the use of this\nspeciﬁc term, especially for image retrieval (Mori et al.,\n1999) and object classiﬁcation (Wang et al., 2009). Other\nearly work leveraged tags (but not natural language) asso-\nciated with images for the task of semantic segmentation\n(Barnard et al., 2003). More recently, He & Peng (2017)\nand Liang et al. (2020) demonstrated using natural language\ndescriptions and explanations to improve ﬁne-grained vi-\nsual classiﬁcation of birds. Others have investigated how\ngrounded language can be used to improve visual represen-\ntations and classiﬁers on the ShapeWorld dataset (Kuhnle\n& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).\nFinally, techniques which combine natural language with\nreinforcement learning environments (Narasimhan et al.,\n2015) have demonstrated exciting emergent behaviors such\nas systematically accomplishing zero-shot tasks (Hill et al.,\n2019)."}
{"doc_id": "2103.00020", "para_id": 274, "text": "As part of our work on CLIP we also construct a new dataset\nof image-text pairs. Modern work on image-text retrieval\nhas relied on a set of crowd-sourced sentence level im-\nage caption evaluation datasets like Pascal1K (Rashtchian\net al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K\n(Young et al., 2014). However, these datasets are still rel-\natively small and limit achievable performance. Several\nmethods have been proposed to create larger datasets au-\ntomatically with Ordonez et al. (2011) as a notable early\nexample. In the deep learning era, Mithun et al. (2018)\ndemonstrated an additional set of (image, text) pairs col-\nlected from the internet could improve retrieval performance\nand several new automatically constructed datasets such as\nConceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,\n2020), and OCR-CC (Yang et al., 2020) have been created.\nHowever, these datasets still use signiﬁcantly more aggres-\nsive ﬁltering or are designed for a speciﬁc task such as OCR\nand as a result are still much smaller than WIT with between\n1 and 10 million training examples."}
{"doc_id": "2103.00020", "para_id": 275, "text": "A related idea to CLIP is webly supervised learning. This\nline of work queries image search engines to build image\ndatasets by querying for terms and uses the queries as the\nlabels for the returned images (Fergus et al., 2005). Classi-\nﬁers trained on these large but noisily labeled datasets can\nbe competitive with those trained on smaller carefully la-\nbeled datasets. These image-query pairs are also often used\nto improve performance on standard datasets as additional\ntraining data (Chen & Gupta, 2015). CLIP also uses search\nqueries as part of its dataset creation process. However\nCLIP only uses full text sequences co-occuring with images\nas supervision rather than just the queries, which are often\nonly a single word or short n-gram. We also restrict this step\nin CLIP to text only querying for sub-string matches while\nmost webly supervised work uses standard image search\nengines which have their own complex retrieval and ﬁlter-\ning pipelines that often involve computer vision systems.\nOf this line of work, Learning Everything about Anything:\nWebly-Supervised Visual Concept Learning (Divvala et al.,"}
{"doc_id": "2103.00020", "para_id": 276, "text": "CLIP’s pre-training task optimizes for text-image retrieval.\nThis areas of research dates back to the mid-90s with the\npreviously mentioned Mori et al. (1999) as representative of\nearly work. While initial efforts focused primarily on predic-\ntive objectives over time research shifted towards learning\njoint multi-modal embedding spaces with techniques like\nkernel Canonical Correlation Analysis and various ranking\nobjectives (Weston et al., 2010; Socher & Fei-Fei, 2010;\nHodosh et al., 2013). Over time work explored many combi-\nnations of training objective, transfer, and more expressive\nmodels and steadily improved performance (Frome et al.,\n2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,\n2014; Faghri et al., 2017)."}
{"doc_id": "2103.00020", "para_id": 277, "text": "2014) has a notably similar ambition and goal as CLIP."}
{"doc_id": "2103.00020", "para_id": 278, "text": "Other work has leveraged natural language supervision for\ndomains other than images. Stroud et al. (2020) explores"}
{"doc_id": "2103.00020", "para_id": 279, "text": "Finally, CLIP is related to a recent burst of activity on learn-\ning joint models of vision and language (Lu et al., 2019; Tan"}
{"doc_id": "2103.00020", "para_id": 280, "text": "Learning Transferable Visual Models From Natural Language Supervision\n27"}
{"doc_id": "2103.00020", "para_id": 281, "text": "& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.,\n2020). This line of work focuses on richly connecting vision\nand language in order to solve complex downstream tasks\nsuch as visual question answering, visual commonsense\nreasoning, or multimodal entailment. These approaches\nleverage impressively engineered models which combine 3\n(or more) pre-trained subsystems, typically an image feature\nmodel, a region proposal / object detection model, and a\npre-trained masked language model such as BERT. These\nsystems are then jointly ﬁne-tuned via various training objec-\ntives on image-text pairs and applied to the aforementioned\ntasks and achieve impressive results. CLIP is instead fo-\ncused on learning visual models from scratch via natural\nlanguage supervision and does not densely connect the two\ndomains with a joint attention model. The only interaction\nin a CLIP model between the image and text domain is a\nsingle dot product in a learned joint embedding space. We\nare excited to see CLIP hybridized with this line of work."}
{"doc_id": "2103.00020", "para_id": 282, "text": "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorﬂow: A system for large-scale machine learning. In\n12th {USENIX} symposium on operating systems design\nand implementation ({OSDI} 16), pp. 265–283, 2016."}
{"doc_id": "2103.00020", "para_id": 283, "text": "Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi´c,\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\nand Zisserman, A. Self-supervised multimodal versatile\nnetworks. arXiv preprint arXiv:2006.16228, 2020."}
{"doc_id": "2103.00020", "para_id": 284, "text": "Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-\nS., and Nguyen, A. Strike (with) a pose: Neural networks\nare easily fooled by strange poses of familiar objects. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 4845–4854, 2019."}
{"doc_id": "2103.00020", "para_id": 285, "text": "Andreas, J., Klein, D., and Levine, S. Learning with latent\nlanguage. arXiv preprint arXiv:1711.00482, 2017."}
{"doc_id": "2103.00020", "para_id": 286, "text": "We have investigated whether it is possible to transfer the\nsuccess of task-agnostic web-scale pre-training in NLP to\nanother domain. We ﬁnd that adopting this formula re-\nsults in similar behaviors emerging in the ﬁeld of computer\nvision and discuss the social implications of this line of\nresearch. In order to optimize their training objective, CLIP\nmodels learn to perform a wide variety of tasks during pre-\ntraining. This task learning can then be leveraged via natural\nlanguage prompting to enable zero-shot transfer to many\nexisting datasets. At sufﬁcient scale, the performance of this\napproach can be competitive with task-speciﬁc supervised\nmodels although there is still room for much improvement."}
{"doc_id": "2103.00020", "para_id": 287, "text": "Assiri, Y. Stochastic optimization of plain convolutional\nneural networks with simple methods. arXiv preprint\narXiv:2001.08856, 2020."}
{"doc_id": "2103.00020", "para_id": 288, "text": "Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning\nrepresentations by maximizing mutual information across\nviews. In Advances in Neural Information Processing\nSystems, pp. 15535–15545, 2019."}
{"doc_id": "2103.00020", "para_id": 289, "text": "Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-\nfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A\nlarge-scale bias-controlled dataset for pushing the lim-\nits of object recognition models. In Advances in Neural\nInformation Processing Systems, pp. 9453–9463, 2019."}
{"doc_id": "2103.00020", "para_id": 290, "text": "We’d like to thank the millions of people involved in creating\nthe data CLIP is trained on. We’d also like to thank Susan\nZhang for her work on image conditional language models\nwhile at OpenAI, Ishaan Gulrajani for catching an error in\nthe pseudocode, and Irene Solaiman, Miles Brundage, and\nGillian Hadﬁeld for their thoughtful feedback on the broader\nimpacts section of the paper. We are also grateful to the\nAcceleration and Supercomputing teams at OpenAI for their\ncritical work on software and hardware infrastructure this\nproject used. Finally, we’d also like to thank the developers\nof the many software packages used throughout this project\nincluding, but not limited, to Numpy (Harris et al., 2020),\nSciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-\nFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),\npandas (pandas development team, 2020), and scikit-learn\n(Pedregosa et al., 2011)."}
{"doc_id": "2103.00020", "para_id": 291, "text": "Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,\nD. M., and Jordan, M. I. Matching words and pictures.\nJournal of machine learning research, 3(Feb):1107–1135,\n2003."}
{"doc_id": "2103.00020", "para_id": 292, "text": "Bechmann, A. and Bowker, G. C. Unsupervised by any\nother name: Hidden layers of knowledge production in\nartiﬁcial intelligence on social media. Big Data & Society,\n6(1):205395171881956, January 2019. doi: 10.1177/\n2053951718819569. URL https://doi.org/10.\n1177/2053951718819569."}
{"doc_id": "2103.00020", "para_id": 293, "text": "Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A\nneural probabilistic language model. Journal of machine\nlearning research, 3(Feb):1137–1155, 2003."}
{"doc_id": "2103.00020", "para_id": 294, "text": "Bhargava, S. and Forsyth, D. Exposing and correcting the\ngender bias in image captioning datasets and models.\narXiv preprint arXiv:1912.00578, 2019."}
{"doc_id": "2103.00020", "para_id": 295, "text": "Learning Transferable Visual Models From Natural Language Supervision\n28"}
{"doc_id": "2103.00020", "para_id": 296, "text": "Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet\nallocation. Journal of machine Learning research, 3(Jan):\n993–1022, 2003."}
{"doc_id": "2103.00020", "para_id": 297, "text": "Chen, X., Fan, H., Girshick, R., and He, K. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020d."}
{"doc_id": "2103.00020", "para_id": 298, "text": "Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\nCheng, Y., and Liu, J. Uniter: Learning universal image-\ntext representations. arXiv preprint arXiv:1909.11740,\n2019."}
{"doc_id": "2103.00020", "para_id": 299, "text": "Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and\nKalai, A. T. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings. Advances\nin neural information processing systems, 29:4349–4357,\n2016."}
{"doc_id": "2103.00020", "para_id": 300, "text": "Cheng, G., Han, J., and Lu, X. Remote sensing image scene\nclassiﬁcation: Benchmark and state of the art. Proceed-\nings of the IEEE, 105(10):1865–1883, 2017."}
{"doc_id": "2103.00020", "para_id": 301, "text": "Bowker, G. C. and Star, S. L. Sorting things out: Classiﬁca-\ntion and its consequences. MIT press, 2000."}
{"doc_id": "2103.00020", "para_id": 302, "text": "Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J.,\nand Dahl, G. E. On empirical comparisons of optimiz-\ners for deep learning. arXiv preprint arXiv:1910.05446,\n2019."}
{"doc_id": "2103.00020", "para_id": 303, "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020."}
{"doc_id": "2103.00020", "para_id": 304, "text": "Coates, A., Ng, A., and Lee, H. An analysis of single-\nlayer networks in unsupervised feature learning. In Pro-\nceedings of the fourteenth international conference on\nartiﬁcial intelligence and statistics, pp. 215–223, 2011."}
{"doc_id": "2103.00020", "para_id": 305, "text": "Browne, S. Dark Matters: Surveillance of Blackness. Duke\nUniversity Press, 2015."}
{"doc_id": "2103.00020", "para_id": 306, "text": "Bulent Sariyildiz, M., Perez, J., and Larlus, D. Learning\nvisual representations with caption annotations. arXiv\ne-prints, pp. arXiv–2008, 2020."}
{"doc_id": "2103.00020", "para_id": 307, "text": "Crawford, K.\nThe trouble with bias.\nNIPS 2017\nKeynote, 2017. URL https://www.youtube.com/\nwatch?v=fMym_BKWQzk."}
{"doc_id": "2103.00020", "para_id": 308, "text": "Buolamwini, J. and Gebru, T. Gender shades: Intersec-\ntional accuracy disparities in commercial gender classi-\nﬁcation. In Conference on fairness, accountability and\ntransparency, pp. 77–91, 2018."}
{"doc_id": "2103.00020", "para_id": 309, "text": "Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.\nIn Advances in neural information processing systems,\npp. 3079–3087, 2015."}
{"doc_id": "2103.00020", "para_id": 310, "text": "Carreira, J., Noland, E., Hillier, C., and Zisserman, A. A\nshort note on the kinetics-700 human action dataset. arXiv\npreprint arXiv:1907.06987, 2019."}
{"doc_id": "2103.00020", "para_id": 311, "text": "D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\nJ., Hoffman, M. D., et al. Underspeciﬁcation presents\nchallenges for credibility in modern machine learning.\narXiv preprint arXiv:2011.03395, 2020."}
{"doc_id": "2103.00020", "para_id": 312, "text": "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\nD., and Sutskever, I. Generative pretraining from pixels.\nIn International Conference on Machine Learning, pp.\n1691–1703. PMLR, 2020a."}
{"doc_id": "2103.00020", "para_id": 313, "text": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09, 2009."}
{"doc_id": "2103.00020", "para_id": 314, "text": "Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016."}
{"doc_id": "2103.00020", "para_id": 315, "text": "Deng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A.,\nand Fei-Fei, L. Ilsvrc 2012, 2012. URL http://www.\nimage-net.org/challenges/LSVRC/2012/."}
{"doc_id": "2103.00020", "para_id": 316, "text": "Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. arXiv preprint arXiv:2002.05709, 2020b."}
{"doc_id": "2103.00020", "para_id": 317, "text": "Desai, K. and Johnson, J. Virtex: Learning visual rep-\nresentations from textual annotations. arXiv preprint\narXiv:2006.06666, 2020."}
{"doc_id": "2103.00020", "para_id": 318, "text": "Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. Big self-supervised models are strong semi-\nsupervised learners. arXiv preprint arXiv:2006.10029,\n2020c."}
{"doc_id": "2103.00020", "para_id": 319, "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018."}
{"doc_id": "2103.00020", "para_id": 320, "text": "Chen, X. and Gupta, A.\nWebly supervised learning of\nconvolutional networks.\nIn Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 1431–\n1439, 2015."}
{"doc_id": "2103.00020", "para_id": 321, "text": "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv preprint arXiv:2005.00341, 2020."}
{"doc_id": "2103.00020", "para_id": 322, "text": "Learning Transferable Visual Models From Natural Language Supervision\n29"}
{"doc_id": "2103.00020", "para_id": 323, "text": "Divvala, S. K., Farhadi, A., and Guestrin, C. Learning\neverything about anything: Webly-supervised visual con-\ncept learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 3270–\n3277, 2014."}
{"doc_id": "2103.00020", "para_id": 324, "text": "biased towards texture; increasing shape bias improves ac-\ncuracy and robustness. arXiv preprint arXiv:1811.12231,\n2018."}
{"doc_id": "2103.00020", "para_id": 325, "text": "Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,\nBrendel, W., Bethge, M., and Wichmann, F. A. Short-\ncut learning in deep neural networks. arXiv preprint\narXiv:2004.07780, 2020."}
{"doc_id": "2103.00020", "para_id": 326, "text": "Dodge, S. and Karam, L. A study and comparison of human\nand deep learning recognition performance under visual\ndistortions. In 2017 26th international conference on\ncomputer communication and networks (ICCCN), pp. 1–\n7. IEEE, 2017."}
{"doc_id": "2103.00020", "para_id": 327, "text": "Gomez, L., Patel, Y., Rusi˜nol, M., Karatzas, D., and Jawahar,\nC. Self-supervised learning of visual features through\nembedding images into text topic spaces. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 4230–4239, 2017."}
{"doc_id": "2103.00020", "para_id": 328, "text": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020."}
{"doc_id": "2103.00020", "para_id": 329, "text": "Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572, 2014."}
{"doc_id": "2103.00020", "para_id": 330, "text": "Elhoseiny, M., Saleh, B., and Elgammal, A. Write a classi-\nﬁer: Zero-shot learning using purely textual descriptions.\nIn Proceedings of the IEEE International Conference on\nComputer Vision, pp. 2584–2591, 2013."}
{"doc_id": "2103.00020", "para_id": 331, "text": "Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\nD., Lee, D.-H., et al. Challenges in representation learn-\ning: A report on three machine learning contests. Neural\nNetworks, 64:59–63, 2015."}
{"doc_id": "2103.00020", "para_id": 332, "text": "Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\nproving visual-semantic embeddings with hard negatives.\narXiv preprint arXiv:1707.05612, 2017."}
{"doc_id": "2103.00020", "para_id": 333, "text": "Google. Google cloud api: Celebrity recognition. URL"}
{"doc_id": "2103.00020", "para_id": 334, "text": "https://cloud.google.com/vision/docs/\ncelebrity-recognition."}
{"doc_id": "2103.00020", "para_id": 335, "text": "Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learn-\ning object categories from google’s image search. In\nTenth IEEE International Conference on Computer Vision\n(ICCV’05) Volume 1, volume 2, pp. 1816–1823. IEEE,\n2005."}
{"doc_id": "2103.00020", "para_id": 336, "text": "Griewank, A. and Walther, A. Algorithm 799: revolve: an\nimplementation of checkpointing for the reverse or ad-\njoint mode of computational differentiation. ACM Trans-\nactions on Mathematical Software (TOMS), 26(1):19–45,\n2000."}
{"doc_id": "2103.00020", "para_id": 337, "text": "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\nRanzato, M., and Mikolov, T. Devise: A deep visual-\nsemantic embedding model. In Advances in neural infor-\nmation processing systems, pp. 2121–2129, 2013."}
{"doc_id": "2103.00020", "para_id": 338, "text": "Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020."}
{"doc_id": "2103.00020", "para_id": 339, "text": "Gan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J.\nLarge-scale adversarial training for vision-and-language\nrepresentation learning. arXiv preprint arXiv:2006.06195,\n2020."}
{"doc_id": "2103.00020", "para_id": 340, "text": "Ha, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv\npreprint arXiv:1609.09106, 2016."}
{"doc_id": "2103.00020", "para_id": 341, "text": "Hancock, B., Bringmann, M., Varma, P., Liang, P., Wang,\nS., and R´e, C. Training classiﬁers with natural language\nexplanations. In Proceedings of the conference. Associ-\nation for Computational Linguistics. Meeting, volume\n2018, pp. 1884. NIH Public Access, 2018."}
{"doc_id": "2103.00020", "para_id": 342, "text": "Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-\nguage models better few-shot learners. arXiv preprint\narXiv:2012.15723, 2020."}
{"doc_id": "2103.00020", "para_id": 343, "text": "Garvie,\nC.,\nMay\n2019.\nURL\nhttps://www.\nflawedfacedata.com/."}
{"doc_id": "2103.00020", "para_id": 344, "text": "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.\nLearning from dialogue after deployment: Feed yourself,\nchatbot! arXiv preprint arXiv:1901.05415, 2019."}
{"doc_id": "2103.00020", "para_id": 345, "text": "Geiger, A., Lenz, P., and Urtasun, R. Are we ready for\nautonomous driving? the kitti vision benchmark suite. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2012."}
{"doc_id": "2103.00020", "para_id": 346, "text": "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\nKerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del"}
{"doc_id": "2103.00020", "para_id": 347, "text": "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\nmann, F. A., and Brendel, W. Imagenet-trained cnns are"}
{"doc_id": "2103.00020", "para_id": 348, "text": "Learning Transferable Visual Models From Natural Language Supervision\n30"}
{"doc_id": "2103.00020", "para_id": 349, "text": "R´ıo, J., Wiebe, M., Peterson, P., G´erard-Marchant, P.,\nSheppard, K., Reddy, T., Weckesser, W., Abbasi, H.,\nGohlke, C., and Oliphant, T. E. Array programming\nwith NumPy. Nature, 585:357–362, 2020. doi: 10.1038/\ns41586-020-2649-2."}
{"doc_id": "2103.00020", "para_id": 350, "text": "Hendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016."}
{"doc_id": "2103.00020", "para_id": 351, "text": "Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\nSong, D. Natural adversarial examples. arXiv preprint\narXiv:1907.07174, 2019."}
{"doc_id": "2103.00020", "para_id": 352, "text": "Hays, J. and Efros, A. A. Im2gps: estimating geographic\ninformation from a single image. In 2008 ieee confer-\nence on computer vision and pattern recognition, pp. 1–8.\nIEEE, 2008."}
{"doc_id": "2103.00020", "para_id": 353, "text": "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\net al. The many faces of robustness: A critical analy-\nsis of out-of-distribution generalization. arXiv preprint\narXiv:2006.16241, 2020a."}
{"doc_id": "2103.00020", "para_id": 354, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectiﬁers: Surpassing human-level performance on\nimagenet classiﬁcation. In Proceedings of the IEEE inter-\nnational conference on computer vision, pp. 1026–1034,\n2015."}
{"doc_id": "2103.00020", "para_id": 355, "text": "Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,\nR., and Song, D. Pretrained transformers improve out-of-\ndistribution robustness. arXiv preprint arXiv:2004.06100,\n2020b."}
{"doc_id": "2103.00020", "para_id": 356, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016a."}
{"doc_id": "2103.00020", "para_id": 357, "text": "Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H.,\nKianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou,\nY. Deep learning scaling is predictable, empirically. arXiv\npreprint arXiv:1712.00409, 2017."}
{"doc_id": "2103.00020", "para_id": 358, "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016b."}
{"doc_id": "2103.00020", "para_id": 359, "text": "Hill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\nM., McClelland, J. L., and Santoro, A. Environmental\ndrivers of systematicity and generalization in a situated\nagent. In International Conference on Learning Repre-\nsentations, 2019."}
{"doc_id": "2103.00020", "para_id": 360, "text": "He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9729–\n9738, 2020."}
{"doc_id": "2103.00020", "para_id": 361, "text": "Hodosh, M., Young, P., and Hockenmaier, J. Framing image\ndescription as a ranking task: Data, models and evaluation\nmetrics. Journal of Artiﬁcial Intelligence Research, 47:\n853–899, 2013."}
{"doc_id": "2103.00020", "para_id": 362, "text": "Hongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet:\nEnhancing image geolocalization by combinatorial parti-\ntioning of maps. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pp. 536–551, 2018."}
{"doc_id": "2103.00020", "para_id": 363, "text": "He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.\nBag of tricks for image classiﬁcation with convolutional\nneural networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 558–\n567, 2019."}
{"doc_id": "2103.00020", "para_id": 364, "text": "Howard, J. and Ruder, S.\nUniversal language model\nﬁne-tuning for text classiﬁcation.\narXiv preprint\narXiv:1801.06146, 2018."}
{"doc_id": "2103.00020", "para_id": 365, "text": "He, X. and Peng, Y. Fine-grained image classiﬁcation via\ncombining vision and language. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 5994–6002, 2017."}
{"doc_id": "2103.00020", "para_id": 366, "text": "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran,\nB., and Madry, A. Adversarial examples are not bugs,\nthey are features. In Advances in Neural Information\nProcessing Systems, pp. 125–136, 2019."}
{"doc_id": "2103.00020", "para_id": 367, "text": "Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classiﬁcation. IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing, 12(7):2217–2226, 2019."}
{"doc_id": "2103.00020", "para_id": 368, "text": "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167, 2015."}
{"doc_id": "2103.00020", "para_id": 369, "text": "Henaff, O. Data-efﬁcient image recognition with contrastive\npredictive coding. In International Conference on Ma-\nchine Learning, pp. 4182–4192. PMLR, 2020."}
{"doc_id": "2103.00020", "para_id": 370, "text": "Jaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman,\nA. Deep structured output learning for unconstrained text\nrecognition. arXiv preprint arXiv:1412.5903, 2014."}
{"doc_id": "2103.00020", "para_id": 371, "text": "Hendrycks, D. and Dietterich, T. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261, 2019."}
{"doc_id": "2103.00020", "para_id": 372, "text": "Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial\ntransformer networks. Advances in neural information\nprocessing systems, 28:2017–2025, 2015."}
{"doc_id": "2103.00020", "para_id": 373, "text": "Learning Transferable Visual Models From Natural Language Supervision\n31"}
{"doc_id": "2103.00020", "para_id": 374, "text": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision, 123(1):32–73,\n2017."}
{"doc_id": "2103.00020", "para_id": 375, "text": "Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,\nLawrence Zitnick, C., and Girshick, R. Clevr: A diag-\nnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pp.\n2901–2910, 2017."}
{"doc_id": "2103.00020", "para_id": 376, "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Advances in neural information processing systems,\npp. 1097–1105, 2012."}
{"doc_id": "2103.00020", "para_id": 377, "text": "Joulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N.\nLearning visual features from large weakly supervised\ndata. In European Conference on Computer Vision, pp.\n67–84. Springer, 2016."}
{"doc_id": "2103.00020", "para_id": 378, "text": "Kuhnle, A. and Copestake, A.\nShapeworld-a new test\nmethodology for multimodal language understanding.\narXiv preprint arXiv:1704.04517, 2017."}
{"doc_id": "2103.00020", "para_id": 379, "text": "Kalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal\nmodeling in 3d cnn architectures with bert for action\nrecognition. arXiv preprint arXiv:2008.01232, 2020."}
{"doc_id": "2103.00020", "para_id": 380, "text": "K¨arkk¨ainen, K. and Joo, J. Fairface: Face attribute dataset\nfor balanced race, gender, and age, 2019."}
{"doc_id": "2103.00020", "para_id": 381, "text": "Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020."}
{"doc_id": "2103.00020", "para_id": 382, "text": "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-\nman, S. J. Building machines that learn and think like\npeople, 2016."}
{"doc_id": "2103.00020", "para_id": 383, "text": "Karpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment\nembeddings for bidirectional image sentence mapping.\nIn Advances in neural information processing systems,\npp. 1889–1897, 2014."}
{"doc_id": "2103.00020", "para_id": 384, "text": "Lampert, C. H., Nickisch, H., and Harmeling, S. Learning\nto detect unseen object classes by between-class attribute\ntransfer. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 951–958. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 385, "text": "Keyes, O. The misgendering machines: Trans/hci implica-\ntions of automatic gender recognition. Proceedings of the\nACM on Human-Computer Interaction, 2(CSCW):1–22,\n2018."}
{"doc_id": "2103.00020", "para_id": 386, "text": "Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning\nof new tasks. 2008."}
{"doc_id": "2103.00020", "para_id": 387, "text": "Le, Q. and Mikolov, T. Distributed representations of sen-\ntences and documents. In International conference on\nmachine learning, pp. 1188–1196, 2014."}
{"doc_id": "2103.00020", "para_id": 388, "text": "Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\nRingshia, P., and Testuggine, D. The hateful memes\nchallenge: Detecting hate speech in multimodal memes.\narXiv preprint arXiv:2005.04790, 2020."}
{"doc_id": "2103.00020", "para_id": 389, "text": "LeCun, Y.\nThe mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/."}
{"doc_id": "2103.00020", "para_id": 390, "text": "Kingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014."}
{"doc_id": "2103.00020", "para_id": 391, "text": "Lee, D.-H. Pseudo-label: The simple and efﬁcient semi-\nsupervised learning method for deep neural networks."}
{"doc_id": "2103.00020", "para_id": 392, "text": "Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. arXiv preprint arXiv:1411.2539, 2014."}
{"doc_id": "2103.00020", "para_id": 393, "text": "Lei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep\nzero-shot convolutional neural networks using textual\ndescriptions. In Proceedings of the IEEE International\nConference on Computer Vision, pp. 4247–4255, 2015."}
{"doc_id": "2103.00020", "para_id": 394, "text": "Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun,\nR., Torralba, A., and Fidler, S. Skip-thought vectors.\nAdvances in neural information processing systems, 28:\n3294–3302, 2015."}
{"doc_id": "2103.00020", "para_id": 395, "text": "Li, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\nvisual n-grams from web data. In Proceedings of the\nIEEE International Conference on Computer Vision, pp.\n4183–4192, 2017."}
{"doc_id": "2103.00020", "para_id": 396, "text": "Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\nJ., Gelly, S., and Houlsby, N. Large scale learning of\ngeneral visual representations for transfer. arXiv preprint\narXiv:1912.11370, 2019."}
{"doc_id": "2103.00020", "para_id": 397, "text": "Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D.\nUnicoder-vl: A universal encoder for vision and language\nby cross-modal pre-training. 2020a."}
{"doc_id": "2103.00020", "para_id": 398, "text": "Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet\nmodels transfer better?\nIn Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2661–2671, 2019."}
{"doc_id": "2103.00020", "para_id": 399, "text": "Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J.\nLearning through dialogue interactions by asking ques-\ntions. arXiv preprint arXiv:1612.04936, 2016."}
{"doc_id": "2103.00020", "para_id": 400, "text": "Learning Transferable Visual Models From Natural Language Supervision\n32"}
{"doc_id": "2103.00020", "para_id": 401, "text": "Proceedings of the European Conference on Computer\nVision (ECCV), pp. 181–196, 2018."}
{"doc_id": "2103.00020", "para_id": 402, "text": "Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., et al.\nOscar: Object-\nsemantics aligned pre-training for vision-language tasks.\narXiv preprint arXiv:2004.06165, 2020b."}
{"doc_id": "2103.00020", "para_id": 403, "text": "McCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors. In\nAdvances in neural information processing systems, pp.\n6294–6305, 2017."}
{"doc_id": "2103.00020", "para_id": 404, "text": "Liang, W., Zou, J., and Yu, Z. Alice: Active learning with\ncontrastive natural language explanations. arXiv preprint\narXiv:2009.10259, 2020."}
{"doc_id": "2103.00020", "para_id": 405, "text": "McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The\nnatural language decathlon: Multitask learning as ques-\ntion answering. arXiv preprint arXiv:1806.08730, 2018."}
{"doc_id": "2103.00020", "para_id": 406, "text": "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In European conference on\ncomputer vision, pp. 740–755. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 407, "text": "Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740, 2017."}
{"doc_id": "2103.00020", "para_id": 408, "text": "Linzen, T.\nHow can we accelerate progress towards\nhuman-like linguistic generalization?\narXiv preprint\narXiv:2005.00955, 2020."}
{"doc_id": "2103.00020", "para_id": 409, "text": "Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\nI., and Sivic, J. Howto100m: Learning a text-video em-\nbedding by watching hundred million narrated video clips.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pp. 2630–2640, 2019."}
{"doc_id": "2103.00020", "para_id": 410, "text": "Lippe, P., Holla, N., Chandra, S., Rajamanickam, S., An-\ntoniou, G., Shutova, E., and Yannakoudakis, H. A mul-\ntimodal framework for the detection of hateful memes.\narXiv preprint arXiv:2012.12871, 2020."}
{"doc_id": "2103.00020", "para_id": 411, "text": "Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\nssi, R., Kaiser, L., and Shazeer, N.\nGenerating\nwikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018."}
{"doc_id": "2103.00020", "para_id": 412, "text": "Miech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisser-\nman, A. Rareact: A video dataset of unusual interactions.\narXiv preprint arXiv:2008.01018, 2020a."}
{"doc_id": "2103.00020", "para_id": 413, "text": "Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\nand Zisserman, A. End-to-end learning of visual represen-\ntations from uncurated instructional videos. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 9879–9889, 2020b."}
{"doc_id": "2103.00020", "para_id": 414, "text": "Locatello, F., Bauer, S., Lucic, M., R¨atsch, G., Gelly, S.,\nSch¨olkopf, B., and Bachem, O.\nA sober look at the\nunsupervised learning of disentangled representations\nand their evaluation. arXiv preprint arXiv:2010.14766,\n2020."}
{"doc_id": "2103.00020", "para_id": 415, "text": "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. Advances in neural informa-\ntion processing systems, 26:3111–3119, 2013."}
{"doc_id": "2103.00020", "para_id": 416, "text": "Loshchilov, I. and Hutter, F.\nSgdr:\nStochastic gra-\ndient descent with warm restarts.\narXiv preprint\narXiv:1608.03983, 2016."}
{"doc_id": "2103.00020", "para_id": 417, "text": "Loshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017."}
{"doc_id": "2103.00020", "para_id": 418, "text": "Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect\nof natural distribution shift on question answering models.\narXiv preprint arXiv:2004.14444, 2020."}
{"doc_id": "2103.00020", "para_id": 419, "text": "Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. In Advances in Neural Information\nProcessing Systems, pp. 13–23, 2019."}
{"doc_id": "2103.00020", "para_id": 420, "text": "Mishra, A., Alahari, K., and Jawahar, C. Scene text recogni-\ntion using higher order language priors. 2012."}
{"doc_id": "2103.00020", "para_id": 421, "text": "Mithun, N. C., Panda, R., Papalexakis, E. E., and Roy-\nChowdhury, A. K. Webly supervised joint embedding for\ncross-modal image-text retrieval. In Proceedings of the\n26th ACM international conference on Multimedia, pp.\n1856–1864, 2018."}
{"doc_id": "2103.00020", "para_id": 422, "text": "Lu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging\nweakly supervised data and pose representation for action\nrecognition, 2020.\nURL https://www.youtube.\ncom/watch?v=KOQFxbPPLOE&t=1390s."}
{"doc_id": "2103.00020", "para_id": 423, "text": "Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bous-\nquet, O. Are gans created equal? a large-scale study.\nAdvances in neural information processing systems, 31:\n700–709, 2018."}
{"doc_id": "2103.00020", "para_id": 424, "text": "Mori, Y., Takahashi, H., and Oka, R. Image-to-word trans-\nformation based on dividing and vector quantizing images\nwith words. Citeseer, 1999."}
{"doc_id": "2103.00020", "para_id": 425, "text": "Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri,\nM., Li, Y., Bharambe, A., and van der Maaten, L. Ex-\nploring the limits of weakly supervised pretraining. In"}
{"doc_id": "2103.00020", "para_id": 426, "text": "Mu, J., Liang, P., and Goodman, N. Shaping visual represen-\ntations with language for few-shot classiﬁcation. arXiv\npreprint arXiv:1911.02683, 2019."}
{"doc_id": "2103.00020", "para_id": 427, "text": "Learning Transferable Visual Models From Natural Language Supervision\n33"}
{"doc_id": "2103.00020", "para_id": 428, "text": "Muller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolo-\ncation estimation of photos using a hierarchical model\nand scene classiﬁcation. In Proceedings of the European\nConference on Computer Vision (ECCV), pp. 563–579,\n2018."}
{"doc_id": "2103.00020", "para_id": 429, "text": "Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances\nin Neural Information Processing Systems 32, pp. 8024–\n8035, 2019."}
{"doc_id": "2103.00020", "para_id": 430, "text": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in Python. Journal of\nMachine Learning Research, 12:2825–2830, 2011."}
{"doc_id": "2103.00020", "para_id": 431, "text": "Murty, S., Koh, P. W., and Liang, P. Expbert: Representation\nengineering with natural language explanations. arXiv\npreprint arXiv:2005.01932, 2020."}
{"doc_id": "2103.00020", "para_id": 432, "text": "Narasimhan, K., Kulkarni, T., and Barzilay, R. Language\nunderstanding for text-based games using deep reinforce-\nment learning. arXiv preprint arXiv:1506.08941, 2015."}
{"doc_id": "2103.00020", "para_id": 433, "text": "Pennington, J., Socher, R., and Manning, C. D. Glove:\nGlobal vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pp. 1532–1543, 2014."}
{"doc_id": "2103.00020", "para_id": 434, "text": "Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y. Reading digits in natural images with\nunsupervised feature learning. 2011."}
{"doc_id": "2103.00020", "para_id": 435, "text": "Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. arXiv preprint arXiv:1802.05365,\n2018."}
{"doc_id": "2103.00020", "para_id": 436, "text": "Noble, S. U. Algorithms of oppression: How search engines\nreinforce racism. 2018."}
{"doc_id": "2103.00020", "para_id": 437, "text": "Nosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvest-\ning implicit group attitudes and beliefs from a demonstra-\ntion web site. Group Dynamics: Theory, Research, and\nPractice, 6(1):101, 2002."}
{"doc_id": "2103.00020", "para_id": 438, "text": "Qi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\nA.\nImagebert: Cross-modal pre-training with large-\nscale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966, 2020."}
{"doc_id": "2103.00020", "para_id": 439, "text": "Oh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee,\nJ. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al.\nA large-scale benchmark dataset for event recognition in\nsurveillance video. In CVPR 2011, pp. 3153–3160. IEEE,\n2011."}
{"doc_id": "2103.00020", "para_id": 440, "text": "Quattoni, A., Collins, M., and Darrell, T. Learning visual\nrepresentations using images with captions. In 2007 IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 1–8. IEEE, 2007."}
{"doc_id": "2103.00020", "para_id": 441, "text": "Radford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining, 2018."}
{"doc_id": "2103.00020", "para_id": 442, "text": "Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good-\nfellow, I. Realistic evaluation of deep semi-supervised\nlearning algorithms. Advances in neural information pro-\ncessing systems, 31:3235–3246, 2018."}
{"doc_id": "2103.00020", "para_id": 443, "text": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019."}
{"doc_id": "2103.00020", "para_id": 444, "text": "Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018."}
{"doc_id": "2103.00020", "para_id": 445, "text": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019."}
{"doc_id": "2103.00020", "para_id": 446, "text": "Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing\nimages using 1 million captioned photographs. Advances\nin neural information processing systems, 24:1143–1151,\n2011."}
{"doc_id": "2103.00020", "para_id": 447, "text": "Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee,\nJ., and Denton, E. Saving face: Investigating the ethical\nconcerns of facial recognition auditing, 2020."}
{"doc_id": "2103.00020", "para_id": 448, "text": "pandas development team, T.\npandas-dev/pandas: Pan-\ndas, February 2020. URL https://doi.org/10.\n5281/zenodo.3509134."}
{"doc_id": "2103.00020", "para_id": 449, "text": "Ramanathan, V., Liang, P., and Fei-Fei, L. Video event\nunderstanding using natural language descriptions. In\nProceedings of the IEEE International Conference on\nComputer Vision, pp. 905–912, 2013."}
{"doc_id": "2103.00020", "para_id": 450, "text": "Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\nC. V. Cats and dogs. In IEEE Conference on Computer\nVision and Pattern Recognition, 2012."}
{"doc_id": "2103.00020", "para_id": 451, "text": "Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J.\nCollecting image annotations using amazon’s mechanical\nturk. In Proceedings of the NAACL HLT 2010 Workshop\non Creating Speech and Language Data with Amazon’s\nMechanical Turk, pp. 139–147, 2010."}
{"doc_id": "2103.00020", "para_id": 452, "text": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,"}
{"doc_id": "2103.00020", "para_id": 453, "text": "Learning Transferable Visual Models From Natural Language Supervision\n34"}
{"doc_id": "2103.00020", "para_id": 454, "text": "Sohn, K. Improved deep metric learning with multi-class\nn-pair loss objective. In Advances in neural information\nprocessing systems, pp. 1857–1865, 2016."}
{"doc_id": "2103.00020", "para_id": 455, "text": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-\nagenet classiﬁers generalize to imagenet? arXiv preprint\narXiv:1902.10811, 2019."}
{"doc_id": "2103.00020", "para_id": 456, "text": "Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-\nVoss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W.,\nKreps, S., McCain, M., Newhouse, A., Blazakis, J.,\nMcGufﬁe, K., and Wang, J. Release strategies and the\nsocial impacts of language models, 2019."}
{"doc_id": "2103.00020", "para_id": 457, "text": "Salimans, T. and Kingma, D. P. Weight normalization: A\nsimple reparameterization to accelerate training of deep\nneural networks. In Advances in neural information pro-\ncessing systems, pp. 901–909, 2016."}
{"doc_id": "2103.00020", "para_id": 458, "text": "Scheuerman, M. K., Paul, J. M., and Brubaker, J. R. How\ncomputers see gender: An evaluation of gender classiﬁca-\ntion in commercial facial analysis services. Proceedings\nof the ACM on Human-Computer Interaction, 3(CSCW):\n1–33, 2019."}
{"doc_id": "2103.00020", "para_id": 459, "text": "Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset\nof 101 human actions classes from videos in the wild.\narXiv preprint arXiv:1212.0402, 2012."}
{"doc_id": "2103.00020", "para_id": 460, "text": "Speer, R. ftfy. Zenodo, 2019. URL https://doi.org/\n10.5281/zenodo.2591652. Version 5.5."}
{"doc_id": "2103.00020", "para_id": 461, "text": "Schwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija,\nS., Schoonvelde, M., and Lockhart, J. W. Diagnosing\ngender bias in image recognition systems. Socius, 6:\n2378023120967171, 2020."}
{"doc_id": "2103.00020", "para_id": 462, "text": "Srivastava, N. and Salakhutdinov, R. Multimodal learning\nwith deep boltzmann machines. In NIPS, 2012."}
{"doc_id": "2103.00020", "para_id": 463, "text": "Srivastava, S., Labutov, I., and Mitchell, T. Joint concept\nlearning and semantic parsing from natural language ex-\nplanations. In Proceedings of the 2017 conference on\nempirical methods in natural language processing, pp.\n1527–1536, 2017."}
{"doc_id": "2103.00020", "para_id": 464, "text": "Sennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units.\narXiv\npreprint arXiv:1508.07909, 2015."}
{"doc_id": "2103.00020", "para_id": 465, "text": "Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B.,\nand Schmidt, L. Do image classiﬁers generalize across\ntime? arXiv preprint arXiv:1906.02168, 2019."}
{"doc_id": "2103.00020", "para_id": 466, "text": "Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Trafﬁc Sign Recognition Benchmark: A multi-\nclass classiﬁcation competition. In IEEE International\nJoint Conference on Neural Networks, pp. 1453–1460,\n2011."}
{"doc_id": "2103.00020", "para_id": 467, "text": "Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\n2565, 2018."}
{"doc_id": "2103.00020", "para_id": 468, "text": "Stroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937,\n2020."}
{"doc_id": "2103.00020", "para_id": 469, "text": "Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pp.\n8317–8326, 2019."}
{"doc_id": "2103.00020", "para_id": 470, "text": "Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi,\nA.\nInception-v4, inception-resnet and the impact\nof residual connections on learning.\narXiv preprint\narXiv:1602.07261, 2016."}
{"doc_id": "2103.00020", "para_id": 471, "text": "Socher, R. and Fei-Fei, L. Connecting modalities: Semi-\nsupervised segmentation and annotation of images using\nunaligned text corpora. In 2010 IEEE Computer Society\nConference on Computer Vision and Pattern Recognition,\npp. 966–973. IEEE, 2010."}
{"doc_id": "2103.00020", "para_id": 472, "text": "Tan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490, 2019."}
{"doc_id": "2103.00020", "para_id": 473, "text": "Tan, M. and Le, Q. V. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946, 2019."}
{"doc_id": "2103.00020", "para_id": 474, "text": "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A. Y., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing, pp. 1631–1642, 2013."}
{"doc_id": "2103.00020", "para_id": 475, "text": "Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classiﬁcation. arXiv preprint\narXiv:2007.00644, 2020."}
{"doc_id": "2103.00020", "para_id": 476, "text": "Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng,\nA. Y. Grounded compositional semantics for ﬁnding and\ndescribing images with sentences. Transactions of the\nAssociation for Computational Linguistics, 2:207–218,\n2014."}
{"doc_id": "2103.00020", "para_id": 477, "text": "Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM, 59(2):64–73, 2016."}
{"doc_id": "2103.00020", "para_id": 478, "text": "Learning Transferable Visual Models From Natural Language Supervision\n35"}
{"doc_id": "2103.00020", "para_id": 479, "text": "Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849, 2019."}
{"doc_id": "2103.00020", "para_id": 480, "text": "Wang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He,\nM., Wang, Y., and Liu, W. All you need is boundary: To-\nward arbitrary-shaped text spotting. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34,\npp. 12160–12167, 2020."}
{"doc_id": "2103.00020", "para_id": 481, "text": "Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classiﬁcation: a\ngood embedding is all you need?\narXiv preprint\narXiv:2003.11539, 2020."}
{"doc_id": "2103.00020", "para_id": 482, "text": "Wang, J., Markert, K., and Everingham, M. Learning mod-\nels for object recognition from natural language descrip-\ntions. In BMVC, volume 1, pp. 2, 2009."}
{"doc_id": "2103.00020", "para_id": 483, "text": "Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 30(11):1958–1970, 2008."}
{"doc_id": "2103.00020", "para_id": 484, "text": "Weston, J., Bengio, S., and Usunier, N. Large scale im-\nage annotation: learning to rank with joint word-image\nembeddings. Machine learning, 81(1):21–35, 2010."}
{"doc_id": "2103.00020", "para_id": 485, "text": "Touvron, H., Vedaldi, A., Douze, M., and J´egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems, pp. 8252–8262,\n2019."}
{"doc_id": "2103.00020", "para_id": 486, "text": "Weston, J. E. Dialog-based language learning. In Advances\nin Neural Information Processing Systems, pp. 829–837,\n2016."}
{"doc_id": "2103.00020", "para_id": 487, "text": "Varadarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops, pp. 1338–1345. IEEE, 2009."}
{"doc_id": "2103.00020", "para_id": 488, "text": "Weyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolo-\ncation with convolutional neural networks. In European\nConference on Computer Vision, pp. 37–55. Springer,\n2016."}
{"doc_id": "2103.00020", "para_id": 489, "text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017."}
{"doc_id": "2103.00020", "para_id": 490, "text": "Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-\nshick, R.\nDetectron2.\nhttps://github.com/\nfacebookresearch/detectron2, 2019."}
{"doc_id": "2103.00020", "para_id": 491, "text": "Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018."}
{"doc_id": "2103.00020", "para_id": 492, "text": "Wu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature\nlearning via non-parametric instance-level discrimination.\narXiv preprint arXiv:1805.01978, 2018."}
{"doc_id": "2103.00020", "para_id": 493, "text": "Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,\nFeng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scientiﬁc Computing\nin Python. Nature Methods, 17:261–272, 2020. doi:\n10.1038/s41592-019-0686-2."}
{"doc_id": "2103.00020", "para_id": 494, "text": "Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training\nwith noisy student improves imagenet classiﬁcation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10687–10698, 2020."}
{"doc_id": "2103.00020", "para_id": 495, "text": "y\nArcas,\nB.\nA.,\nMitchell,\nM.,\nand\nTodorov,\nA.\nPhysiognomy’s\nnew\nclothes.\n2017.\nURL\nhttps://medium.com/@blaisea/\nphysiognomys-new-clothes-f2d4b59fdd6a."}
{"doc_id": "2103.00020", "para_id": 496, "text": "Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang,\nL., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware\npre-training for text-vqa and text-caption. arXiv preprint\narXiv:2012.04638, 2020."}
{"doc_id": "2103.00020", "para_id": 497, "text": "Vo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the\ndeep learning era. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pp. 2621–2630,\n2017."}
{"doc_id": "2103.00020", "para_id": 498, "text": "Yogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\nYu, L., Dyer, C., et al. Learning and evaluating general\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\n2019."}
{"doc_id": "2103.00020", "para_id": 499, "text": "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018."}
{"doc_id": "2103.00020", "para_id": 500, "text": "Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\nimage descriptions to visual denotations: New similarity\nmetrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Lin-\nguistics, 2:67–78, 2014."}
{"doc_id": "2103.00020", "para_id": 501, "text": "Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro-\nbust global representations by penalizing local predictive\npower. In Advances in Neural Information Processing\nSystems, pp. 10506–10518, 2019."}
{"doc_id": "2103.00020", "para_id": 502, "text": "Learning Transferable Visual Models From Natural Language Supervision\n36"}
{"doc_id": "2103.00020", "para_id": 503, "text": "Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H.,\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv\npreprint arXiv:2006.16934, 2020."}
{"doc_id": "2103.00020", "para_id": 504, "text": "Zeiler, M. D. and Fergus, R. Visualizing and understand-\ning convolutional networks. In European conference on\ncomputer vision, pp. 818–833. Springer, 2014."}
{"doc_id": "2103.00020", "para_id": 505, "text": "Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P.,\nRiquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neu-\nmann, M., Dosovitskiy, A., et al. A large-scale study of\nrepresentation learning with the visual task adaptation\nbenchmark. arXiv preprint arXiv:1910.04867, 2019."}
{"doc_id": "2103.00020", "para_id": 506, "text": "Zhang, R. Making convolutional networks shift-invariant\nagain. arXiv preprint arXiv:1904.11486, 2019."}
{"doc_id": "2103.00020", "para_id": 507, "text": "Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Lan-\nglotz, C. P. Contrastive learning of medical visual repre-\nsentations from paired images and text. arXiv preprint\narXiv:2010.00747, 2020."}
{"doc_id": "2103.00020", "para_id": 508, "text": "Zuboff, S.\nBig other: surveillance capitalism and the\nprospects of an information civilization. Journal of Infor-\nmation Technology, 30(1):75–89, 2015."}
{"doc_id": "2103.00020", "para_id": 509, "text": "Learning Transferable Visual Models From Natural Language Supervision\n37"}
{"doc_id": "2103.00020", "para_id": 510, "text": "the ResNet-50 architecture as in the smallest contrastive\nmodel. To do so, the output from the CNN is projected into\nfour tokens, which are then fed as a preﬁx to a language\nmodel autoregressively predicting the text tokens. Apart\nfrom the training objective, the model was trained on the\nsame dataset for the same number of epochs as other CLIP\nmodels."}
{"doc_id": "2103.00020", "para_id": 511, "text": "We provide additional details for linear probe experiments\npresented in this paper, including the list of the datasets and\nmodels used for evaluation."}
{"doc_id": "2103.00020", "para_id": 512, "text": "We use the 12 datasets from the well-studied evaluation\nsuite introduced by (Kornblith et al., 2019) and add 15\nadditional datasets in order to assess the performance of\nmodels on a wider variety of distributions and tasks. These\ndatasets include MNIST, the Facial Expression Recognition\n2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\net al., 2011), EuroSAT (Helber et al., 2019), the NWPU-\nRESISC45 dataset (Cheng et al., 2017), the German Traf-\nﬁc Sign Recognition Benchmark (GTSRB) dataset (Stal-\nlkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\nPatchCamelyon (Veeling et al., 2018), the UCF101 action\nrecognition dataset (Soomro et al., 2012), Kinetics 700 (Car-\nreira et al., 2019), 2,500 random samples of the CLEVR\ndataset (Johnson et al., 2017), the Hateful Memes dataset\n(Kiela et al., 2020), and the ImageNet-1k dataset (Deng\net al., 2012). For the two video datasets (UCF101 and Ki-\nnetics700), we use the middle frame of each video clip as\nthe input image. STL-10 and UCF101 have multiple pre-\ndeﬁned train/validation/test splits, 10 and 3 respectively, and\nwe report the average over all splits. Details on each dataset\nand the corresponding evaluation metrics are provided in\nTable 9."}
{"doc_id": "2103.00020", "para_id": 513, "text": "CLIP-RN\nFive ResNet-based contrastive CLIP models\nare included. As discussed in the paper, the ﬁrst two models\nfollow ResNet-50 and ResNet-101, and we use EfﬁcientNet-\nstyle (Tan & Le, 2019) scaling for the next three models\nwhich simultaneously scale the model width, the number\nof layers, and the input resolution to obtain models with\nroughly 4x, 16x, and 64x computation."}
{"doc_id": "2103.00020", "para_id": 514, "text": "CLIP-ViT\nWe include four CLIP models that use the Vi-\nsion Transformer (Dosovitskiy et al., 2020) architecture as\nthe image encoder. We include three models trained on 224-\nby-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and\nthe ViT-L/14 model ﬁne-tuned on 336-by-336 pixel input\nimages."}
{"doc_id": "2103.00020", "para_id": 515, "text": "EfﬁcietNet\nWe use the nine models (B0-B8) from the\noriginal EfﬁcientNet paper (Tan & Le, 2019), as well as\nthe noisy-student variants (B0-B7, L2-475, and L2-800)\n(Tan & Le, 2019). The largest models (L2-475 and L2-800)\ntake the input resolutions of 475x475 and 800x800 pixels,\nrespectively."}
{"doc_id": "2103.00020", "para_id": 516, "text": "Additionally, we created two datasets that we call Coun-\ntry211 and Rendered SST2. The Country211 dataset is\ndesigned to assess the geolocation capability of visual rep-\nresentations. We ﬁltered the YFCC100m dataset (Thomee\net al., 2016) to ﬁnd 211 countries (deﬁned as having an\nISO-3166 country code) that have at least 300 photos with\nGPS coordinates, and we built a balanced dataset with 211\ncategories, by sampling 200 photos for training and 100\nphotos for testing, for each country."}
{"doc_id": "2103.00020", "para_id": 517, "text": "Instagram-pretrained ResNeXt\nWe use the four models\n(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan\net al., 2018), as well as their two FixRes variants which use\nhigher input resolutions (Touvron et al., 2019)."}
{"doc_id": "2103.00020", "para_id": 518, "text": "Big Transfer (BiT)\nWe use BiT-S and BiT-M models\n(Kolesnikov et al., 2019), trained on the ImageNet-1k and\nImageNet-21k datasets. The model weights for BiT-L is not\npublicly available."}
{"doc_id": "2103.00020", "para_id": 519, "text": "The Rendered SST2 dataset is designed to measure the opti-\ncal character recognition capability of visual representations.\nTo do so, we used the sentences from the Stanford Sentiment\nTreebank dataset (Socher et al., 2013) and rendered them\ninto images, with black texts on a white background, in a\n448×448 resolution. Two example images from this dataset\nare shown in Figure 19."}
{"doc_id": "2103.00020", "para_id": 520, "text": "Vision Transformer (ViT)\nWe also include four ViT\n(Dosovitskiy et al., 2020) checkpoints pretrained on the\nImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-\nL/16, and ViT-H/14. We note that their best-performing\nmodels, trained on the JFT-300M dataset, are not available\npublicly."}
{"doc_id": "2103.00020", "para_id": 521, "text": "SimCLRv2\nThe SimCLRv2 (Chen et al., 2020c) project\nreleased pre-trained and ﬁne-tuned models in various set-\ntings. We use the seven pretrain-only checkpoints with\nselective kernels."}
{"doc_id": "2103.00020", "para_id": 522, "text": "In combination with the datasets listed above, we evaluate\nthe following series of models using linear probes."}
{"doc_id": "2103.00020", "para_id": 523, "text": "LM RN50\nThis is a multimodal model that uses an au-\ntoregressive loss instead of a contrastive loss, while using"}
{"doc_id": "2103.00020", "para_id": 524, "text": "BYOL\nWe use the recently released model weights of\nBYOL (Grill et al., 2020), speciﬁcally their 50x1 and 200x2"}
{"doc_id": "2103.00020", "para_id": 525, "text": "Learning Transferable Visual Models From Natural Language Supervision\n38"}
{"doc_id": "2103.00020", "para_id": 526, "text": "Figure 19. Two example images from the Rendered SST2 dataset"}
{"doc_id": "2103.00020", "para_id": 527, "text": "a test split, we use the provided validation set to perform\nthe hyperparameter search, and for the datasets that do not\nprovide a validation split or have not published labels for\nthe test data, we split the training dataset to perform the\nhyperparameter search. For the ﬁnal result, we combine the\nvalidation split back with the training split and report the\nperformance on the unused split."}
{"doc_id": "2103.00020", "para_id": 528, "text": "Momentum Contrast (MoCo)\nWe include the MoCo-v1\n(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)\ncheckpoints."}
{"doc_id": "2103.00020", "para_id": 529, "text": "VirTex\nWe use the pretrained model of VirTex (Desai &\nJohnson, 2020). We note that VirTex has a similar model\ndesign to CLIP-AR but is trained on a 1000x smaller dataset\nof high-quality captions from MSCOCO."}
{"doc_id": "2103.00020", "para_id": 530, "text": "The individual linear probe scores are provided in Table 10\nand plotted in Figure 20. The best-performing CLIP model,\nusing ViT-L/14 archiecture and 336-by-336 pixel images,\nachieved the state of the art in 21 of the 27 datasets, i.e.\nincluded in the Clopper-Pearson 99.5% conﬁdence interval\naround each dataset’s top score. For many datasets, CLIP\nperforms signiﬁcantly better than other models, demonstrat-\ning the advantage of natural language supervision over tradi-\ntional pre-training approaches based on image classiﬁcation.\nSee Section 3.2 for more discussions on the linear probe\nresults."}
{"doc_id": "2103.00020", "para_id": 531, "text": "ResNet\nWe add the original ResNet checkpoints released\nby (He et al., 2016b), namely ResNet-50, ResNet-101, and\nResNet152."}
{"doc_id": "2103.00020", "para_id": 532, "text": "We use image features taken from the penultimate layer of\neach model, ignoring any classiﬁcation layer provided. For\nCLIP-ViT models, we used the features before the linear\nprojection to the embedding space, which corresponds to\nI f in Figure 3. We train a logistic regression classiﬁer\nusing scikit-learn’s L-BFGS implementation, with maxi-\nmum 1,000 iterations, and report the corresponding met-\nric for each dataset. We determine the L2 regularization\nstrength λ using a hyperparameter sweep on the validation\nsets over the range between 10−6 and 106, with 96 log-\narithmically spaced steps. To save compute required for\nthe sweeps, we perform a parametric binary search that\nstarts with λ = [10−6, 10−4, 10−2, 1, 102, 104, 106] and it-\neratively halves the interval around the peak until it reaches\na resolution of 8 steps per decade. The hyperparameter\nsweeps are performed on a validation split of each dataset.\nFor the datasets that contain a validation split in addition to"}
{"doc_id": "2103.00020", "para_id": 533, "text": "Learning Transferable Visual Models From Natural Language Supervision\n39"}
{"doc_id": "2103.00020", "para_id": 534, "text": "Dataset\nClasses\nTrain size\nTest size\nEvaluation metric"}
{"doc_id": "2103.00020", "para_id": 535, "text": "Food-101\n102\n75,750\n25,250\naccuracy\nCIFAR-10\n10\n50,000\n10,000\naccuracy\nCIFAR-100\n100\n50,000\n10,000\naccuracy\nBirdsnap\n500\n42,283\n2,149\naccuracy\nSUN397\n397\n19,850\n19,850\naccuracy\nStanford Cars\n196\n8,144\n8,041\naccuracy\nFGVC Aircraft\n100\n6,667\n3,333\nmean per class\nPascal VOC 2007 Classiﬁcation\n20\n5,011\n4,952\n11-point mAP\nDescribable Textures\n47\n3,760\n1,880\naccuracy\nOxford-IIIT Pets\n37\n3,680\n3,669\nmean per class\nCaltech-101\n102\n3,060\n6,085\nmean-per-class\nOxford Flowers 102\n102\n2,040\n6,149\nmean per class"}
{"doc_id": "2103.00020", "para_id": 536, "text": "MNIST\n10\n60,000\n10,000\naccuracy\nFacial Emotion Recognition 2013\n8\n32,140\n3,574\naccuracy\nSTL-10\n10\n1000\n8000\naccuracy\nEuroSAT\n10\n10,000\n5,000\naccuracy\nRESISC45\n45\n3,150\n25,200\naccuracy\nGTSRB\n43\n26,640\n12,630\naccuracy\nKITTI\n4\n6,770\n711\naccuracy\nCountry211\n211\n43,200\n21,100\naccuracy\nPatchCamelyon\n2\n294,912\n32,768\naccuracy\nUCF101\n101\n9,537\n1,794\naccuracy\nKinetics700\n700\n494,801\n31,669\nmean(top1, top5)\nCLEVR Counts\n8\n2,000\n500\naccuracy\nHateful Memes\n2\n8,500\n500\nROC AUC\nRendered SST2\n2\n7,792\n1,821\naccuracy\nImageNet\n1000\n1,281,167\n50,000\naccuracy"}
{"doc_id": "2103.00020", "para_id": 537, "text": "Table 9. Datasets examined for linear probes. We note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are\navailable online at the time of this writing."}
{"doc_id": "2103.00020", "para_id": 538, "text": "Learning Transferable Visual Models From Natural Language Supervision\n40"}
{"doc_id": "2103.00020", "para_id": 539, "text": "LM RN50\n81.3 82.8 61.7 44.2 69.6 74.9 44.9 85.5 71.5 82.8 85.5 91.1 96.6 60.1 95.3 93.4 84.0 73.8 70.2 19.0 82.9 76.4 51.9 51.2 65.2 76.8 65.2"}
{"doc_id": "2103.00020", "para_id": 540, "text": "50\n86.4 88.7 70.3 56.4 73.3 78.3 49.1 87.1 76.4 88.2 89.6 96.1 98.3 64.2 96.6 95.2 87.5 82.4 70.2 25.3 82.7 81.6 57.2 53.6 65.7 72.6 73.3\n101\n88.9 91.1 73.5 58.6 75.1 84.0 50.7 88.0 76.3 91.0 92.0 96.4 98.4 65.2 97.8 95.9 89.3 82.4 73.6 26.6 82.8 84.0 60.3 50.3 68.2 73.3 75.7\n50x4\n91.3 90.5 73.0 65.7 77.0 85.9 57.3 88.4 79.5 91.9 92.5 97.8 98.5 68.1 97.8 96.4 89.7 85.5 59.4 30.3 83.0 85.7 62.6 52.5 68.0 76.6 78.2\n50x16\n93.3 92.2 74.9 72.8 79.2 88.7 62.7 89.0 79.1 93.5 93.7 98.3 98.9 68.7 98.6 97.0 91.4 89.0 69.2 34.8 83.5 88.0 66.3 53.8 71.1 80.0 81.5\n50x64\n94.8 94.1 78.6 77.2 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9 98.9 71.3 99.1 97.1 92.8 90.2 69.2 40.7 83.7 89.5 69.1 55.0 75.0 81.2 83.6"}
{"doc_id": "2103.00020", "para_id": 541, "text": "B/32\n88.8 95.1 80.5 58.5 76.6 81.8 52.0 87.7 76.5 90.0 93.0 96.9 99.0 69.2 98.3 97.0 90.5 85.3 66.2 27.8 83.9 85.5 61.7 52.1 66.7 70.8 76.1\nB/16\n92.8 96.2 83.1 67.8 78.4 86.7 59.5 89.2 79.2 93.1 94.7 98.1 99.0 69.5 99.0 97.1 92.7 86.6 67.8 33.3 83.5 88.4 66.1 57.1 70.3 75.5 80.2\nL/14\n95.2 98.0 87.5 77.0 81.8 90.9 69.4 89.6 82.1 95.1 96.5 99.2 99.2 72.2 99.7 98.2 94.1 92.5 64.7 42.9 85.8 91.5 72.0 57.8 76.2 80.8 83.9\nL/14-336px\n95.9 97.9 87.4 79.9 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2 99.2 72.9 99.7 98.1 94.9 92.4 69.2 46.4 85.6 92.0 73.0 60.3 77.3 80.5 85.4"}
{"doc_id": "2103.00020", "para_id": 542, "text": "B0\n74.3 92.5 76.5 59.7 62.0 62.5 55.7 84.4 71.2 93.0 93.3 91.7 98.2 57.2 97.1 97.3 85.5 80.0 73.8 12.4 83.1 74.4 47.6 47.9 55.7 53.4 76.9\nB1\n74.2 93.2 77.2 61.3 62.6 62.5 56.1 84.7 74.2 93.4 93.6 92.4 98.3 57.0 97.5 96.8 84.5 75.9 75.5 12.5 82.7 74.7 48.5 44.3 54.5 54.4 78.6\nB2\n75.8 93.6 77.9 64.4 64.0 63.2 57.0 85.3 73.5 93.9 93.5 92.9 98.5 56.6 97.7 96.9 84.4 76.4 73.1 12.6 84.3 75.1 49.4 42.6 55.4 55.2 79.7\nB3\n77.4 94.0 78.0 66.5 64.4 66.0 59.3 85.8 73.1 94.1 93.7 93.3 98.5 57.1 98.2 97.3 85.0 75.8 76.1 13.4 83.3 78.1 50.9 45.1 53.8 54.8 81.0\nB4\n79.7 94.1 78.7 70.1 65.4 66.4 60.4 86.5 73.4 94.7 93.5 93.2 98.8 57.9 98.6 96.8 85.0 78.3 72.3 13.9 83.1 79.1 52.5 46.5 54.4 55.4 82.9\nB5\n81.5 93.6 77.9 72.4 67.1 72.7 68.9 86.7 73.9 95.0 94.7 94.5 98.4 58.5 98.7 96.8 86.0 78.5 69.6 14.9 84.7 80.9 54.5 46.6 53.3 56.3 83.7\nB6\n82.4 94.0 78.0 73.5 65.8 71.1 68.2 87.6 73.9 95.0 94.1 93.7 98.4 60.2 98.7 96.8 85.4 78.1 72.7 15.3 84.2 80.0 54.1 51.1 53.3 57.0 84.0\nB7\n84.5 94.9 80.1 74.7 69.0 77.1 72.3 87.2 76.8 95.2 94.7 95.9 98.6 61.3 99.1 96.3 86.8 80.8 75.8 16.4 85.2 81.9 56.8 51.9 54.4 57.8 84.8\nB8\n84.5 95.0 80.7 75.2 69.6 76.8 71.5 87.4 77.1 94.9 95.2 96.3 98.6 61.4 99.2 97.0 87.4 80.4 70.9 17.4 85.2 82.4 57.7 51.4 51.7 55.8 85.3"}
{"doc_id": "2103.00020", "para_id": 543, "text": "B0\n78.1 94.0 78.6 63.5 65.5 57.2 53.7 85.6 75.6 93.8 93.1 94.5 98.1 55.6 98.2 97.0 84.3 74.0 71.6 14.0 83.1 76.7 51.7 47.3 55.7 55.0 78.5\nB1\n80.4 95.1 80.2 66.6 67.6 59.6 53.7 86.2 77.0 94.6 94.4 95.1 98.0 56.1 98.6 96.9 84.3 73.1 67.1 14.5 83.9 79.9 54.5 46.1 54.3 54.9 81.1\nB2\n80.9 95.3 81.3 67.6 67.9 60.9 55.2 86.3 77.7 95.0 94.7 94.4 98.0 55.5 98.8 97.3 84.6 71.7 70.0 14.6 82.9 80.1 55.1 46.1 54.1 55.3 82.2\nB3\n82.6 95.9 82.1 68.6 68.8 60.6 55.4 86.5 77.2 95.0 94.8 95.2 98.1 56.0 99.1 96.5 85.0 70.5 69.5 15.1 83.1 81.8 56.8 45.1 55.7 52.0 83.8\nB4\n85.2 95.6 81.0 72.5 69.7 56.1 52.6 87.0 78.7 94.8 95.2 95.3 98.2 56.0 99.3 95.3 84.8 61.9 64.8 16.0 82.8 83.4 59.8 43.2 55.3 53.0 85.4\nB5\n87.6 96.3 82.4 75.3 71.6 64.7 64.8 87.8 79.6 95.5 95.6 96.6 98.8 60.9 99.4 96.1 87.0 68.5 73.7 16.4 83.5 86.4 61.6 46.3 53.4 55.8 85.8\nB6\n87.3 97.0 83.9 75.8 71.4 67.6 65.6 87.3 78.5 95.2 96.4 97.2 98.6 61.9 99.5 96.6 86.1 70.7 72.4 17.6 84.2 85.5 61.0 49.6 54.6 55.7 86.4\nB7\n88.4 96.0 82.0 76.9 72.6 72.2 71.2 88.1 80.5 95.5 95.5 96.6 98.5 62.7 99.4 96.2 88.5 73.4 73.0 18.5 83.8 86.6 63.2 50.5 57.2 56.7 87.0\nL2-475\n91.6 99.0 91.0 74.8 76.4 75.1 66.8 89.5 81.9 95.6 96.5 97.7 98.9 67.5 99.6 97.0 89.5 73.4 68.9 22.2 86.3 89.4 68.2 58.3 58.6 55.2 88.3\nL2-800\n92.0 98.7 89.0 78.5 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9 98.5 68.4 99.7 97.2 89.9 77.7 66.9 23.7 86.8 88.9 66.7 62.7 58.4 56.9 88.4"}
{"doc_id": "2103.00020", "para_id": 544, "text": "32x8d\n84.8 95.9 80.9 63.8 69.0 74.2 56.0 88.0 75.4 95.4 93.9 91.7 97.4 60.7 99.1 95.7 82.1 72.3 69.2 16.7 82.3 80.1 56.8 42.2 53.3 55.2 83.3\n32x16d\n85.7 96.5 80.9 64.8 70.5 77.5 56.7 87.9 76.2 95.6 94.9 92.5 97.4 61.6 99.3 95.5 82.8 73.8 66.1 17.5 83.4 81.1 58.2 41.3 54.2 56.1 84.4\n32x32d\n86.7 96.8 82.7 67.1 71.5 77.5 55.4 88.3 78.5 95.8 95.3 94.4 97.9 62.4 99.3 95.7 85.4 71.2 66.8 18.0 83.7 82.1 58.8 39.7 55.3 56.7 85.0\n32x48d\n86.9 96.8 83.4 65.9 72.2 76.6 53.2 88.0 77.2 95.5 95.8 93.6 98.1 63.7 99.4 95.3 85.4 73.0 67.2 18.5 82.7 82.8 59.2 41.3 55.5 56.7 85.2\nFixRes-v1\n88.5 95.7 81.1 67.4 72.9 80.5 57.6 88.0 77.9 95.8 96.1 94.5 97.9 62.2 99.4 96.2 86.6 76.5 64.8 19.3 82.5 83.4 59.8 43.5 56.6 59.0 86.0\nFixRes-v2\n88.5 95.7 81.1 67.3 72.9 80.7 57.5 88.0 77.9 95.0 96.0 94.5 98.0 62.1 99.4 96.5 86.6 76.3 64.8 19.5 82.3 83.5 59.8 44.2 56.6 59.0 86.0"}
{"doc_id": "2103.00020", "para_id": 545, "text": "R50x1\n72.5 91.7 74.8 57.7 61.1 53.5 52.5 83.7 72.4 92.3 91.2 92.0 98.4 56.1 96.4 97.4 85.0 70.0 66.0 12.5 83.0 72.3 47.5 48.3 54.1 55.3 75.2\nR50x3\n75.1 93.7 79.0 61.1 63.7 55.2 54.1 84.8 74.6 92.5 91.6 92.8 98.8 58.7 97.0 97.8 86.4 73.1 73.8 14.0 84.2 76.4 50.0 49.2 54.7 54.2 77.2\nR101x1\n73.5 92.8 77.4 58.4 61.3 54.0 52.4 84.4 73.5 92.5 91.8 90.6 98.3 56.5 96.8 97.3 84.6 69.4 68.9 12.6 82.0 73.5 48.6 45.4 52.6 55.5 76.0\nR101x3\n74.7 93.9 79.8 57.8 62.9 54.7 53.3 84.7 75.5 92.3 91.2 92.6 98.8 59.7 97.3 98.0 85.5 71.8 60.2 14.1 83.1 75.9 50.4 49.7 54.1 54.6 77.4\nR152x2\n74.9 94.3 79.7 58.7 62.7 55.9 53.6 85.3 74.9 93.0 92.0 91.7 98.6 58.3 97.1 97.8 86.2 71.8 71.6 13.9 84.1 76.2 49.9 48.2 53.8 55.9 77.1\nR152x4\n74.7 94.2 79.2 57.8 62.9 51.2 50.8 85.4 75.4 93.1 91.2 91.4 98.9 61.4 97.2 98.0 85.5 72.8 67.9 14.9 83.1 76.0 50.3 42.9 53.6 56.0 78.5"}
{"doc_id": "2103.00020", "para_id": 546, "text": "R50x1\n83.3 94.9 82.2 70.9 69.9 59.0 55.6 86.8 77.3 91.5 93.9 99.4 98.0 60.6 98.4 97.5 87.4 68.6 68.2 16.6 82.5 79.4 53.2 49.4 54.5 53.4 76.7\nR50x3\n86.9 96.7 86.2 75.7 74.6 60.6 54.2 87.7 78.5 93.2 95.3 99.4 98.6 64.6 99.3 98.0 88.1 69.9 59.6 19.6 83.4 83.5 57.8 51.3 55.8 55.6 80.7\nR101x1\n85.5 95.7 84.4 73.0 72.5 59.8 55.0 87.3 78.1 92.2 95.0 99.5 98.1 62.5 99.0 97.6 87.8 68.7 67.7 18.0 84.0 82.3 55.9 53.4 54.8 53.1 79.4\nR101x3\n87.2 97.4 87.5 72.4 75.0 57.4 47.4 87.5 79.6 93.2 95.4 99.6 98.6 64.3 99.4 98.2 87.7 68.8 64.1 20.7 80.4 84.0 58.7 52.6 54.9 54.3 81.2\nR152x2\n88.0 97.5 87.8 75.8 75.9 61.5 55.3 88.1 79.8 93.6 95.9 99.5 98.5 64.3 99.5 97.9 89.0 70.0 70.3 20.7 82.6 85.5 59.6 50.8 54.9 55.1 81.9\nR152x4\n87.2 97.6 88.2 72.4 75.0 49.1 43.4 87.1 79.9 92.4 95.4 99.3 98.5 65.7 99.5 97.8 87.7 68.2 57.1 20.6 80.4 84.6 59.0 49.7 57.2 55.1 81.5"}
{"doc_id": "2103.00020", "para_id": 547, "text": "B/32\n81.8 96.7 86.3 65.2 70.7 49.1 42.7 85.3 73.1 90.4 94.5 98.7 97.8 59.0 99.0 96.3 83.0 68.1 65.1 15.7 82.6 79.1 51.7 38.9 57.1 54.6 76.6\nB/16\n86.7 96.9 86.4 74.0 74.2 54.7 46.0 86.7 74.3 92.7 94.1 99.2 97.4 61.3 99.5 96.4 84.5 63.1 61.5 17.5 85.4 82.7 56.6 40.0 57.0 56.1 80.9\nL/16\n87.4 97.9 89.0 76.5 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3 98.0 64.0 99.6 96.5 85.7 70.4 58.8 17.7 85.7 84.1 58.0 38.4 58.4 52.8 81.9\nH/14\n83.4 95.8 84.5 70.2 69.2 62.3 54.8 84.7 75.4 91.7 93.7 98.9 98.5 62.4 98.4 97.3 87.0 73.9 63.4 15.4 87.0 79.4 52.1 41.1 55.9 54.1 75.9"}
{"doc_id": "2103.00020", "para_id": 548, "text": "R50x1\n76.4 93.2 77.9 48.6 64.1 56.3 51.7 84.4 77.0 88.3 91.8 92.9 97.6 59.7 96.7 97.5 85.8 71.1 69.1 15.8 84.8 78.4 51.0 56.2 53.9 53.8 73.8\nR50x3\n81.0 95.6 82.4 56.5 67.0 65.6 61.1 85.9 78.8 90.9 94.1 95.4 98.7 62.6 98.2 97.9 88.2 78.2 74.7 17.6 85.4 82.6 54.6 55.4 54.2 55.2 77.3\nR101x1\n77.9 94.8 79.9 51.9 65.2 57.1 52.0 85.4 77.2 90.0 91.6 92.7 97.2 59.4 97.6 96.8 84.6 65.7 70.6 16.1 84.3 78.8 52.4 53.6 55.1 55.7 76.1\nR101x3\n82.2 96.4 83.4 57.5 68.2 64.6 60.0 86.2 78.9 91.8 95.0 95.4 98.4 63.0 98.5 97.9 88.0 77.5 69.1 18.3 85.5 82.9 55.9 52.2 54.5 56.3 78.8\nR152x1\n78.6 95.0 79.9 50.3 65.6 55.6 52.2 85.8 77.3 90.1 92.5 91.8 97.6 59.8 98.1 96.6 84.3 64.8 70.3 16.6 83.9 79.4 53.1 57.2 55.8 54.8 76.9\nR152x2\n82.3 96.7 83.9 58.1 68.5 64.9 58.7 86.6 79.1 92.2 94.1 96.0 98.2 64.1 98.5 98.0 88.1 77.0 69.8 18.4 85.3 82.7 56.2 53.6 56.0 56.5 79.2\nR152x3\n83.6 96.8 84.5 60.3 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3 98.7 65.4 98.8 98.1 89.5 78.4 68.5 19.4 85.2 83.5 57.0 54.4 54.6 54.2 80.0"}
{"doc_id": "2103.00020", "para_id": 549, "text": "50x1\n74.0 93.6 79.1 47.6 63.7 61.6 62.3 82.6 77.0 88.3 93.7 94.3 98.7 58.8 96.4 97.6 88.2 80.1 71.4 14.1 84.8 77.3 49.3 56.1 53.8 54.4 73.3\n200x2\n78.5 96.2 83.3 53.4 68.5 61.7 55.4 86.6 77.4 91.9 95.5 93.9 98.7 62.6 98.6 97.7 87.4 77.1 76.4 16.4 84.0 82.6 55.1 54.1 52.5 52.4 79.2"}
{"doc_id": "2103.00020", "para_id": 550, "text": "v1\n65.9 85.0 63.1 27.5 52.6 35.9 43.5 75.7 70.0 70.4 78.1 85.4 97.6 54.3 85.6 97.1 82.9 62.6 60.2 12.6 85.7 64.2 40.7 54.7 55.6 53.5 57.2\nv2\n72.2 93.4 76.3 39.6 60.2 48.3 51.1 82.6 75.1 84.4 89.9 90.7 98.4 58.3 95.7 97.2 85.4 75.7 75.4 13.2 85.6 72.7 47.8 56.9 53.9 53.8 69.1"}
{"doc_id": "2103.00020", "para_id": 551, "text": "VirTex\n57.9 83.9 57.5 17.0 49.8 22.4 34.5 83.8 58.2 53.6 70.6 74.7 98.1 56.5 86.7 94.8 74.1 69.5 71.3 8.7 83.1 61.5 39.9 45.5 53.5 55.8 50.7"}
{"doc_id": "2103.00020", "para_id": 552, "text": "50\n71.3 91.8 74.5 52.7 60.5 49.9 48.5 83.8 72.3 92.4 90.8 90.8 98.3 54.9 96.4 96.7 83.6 70.6 67.1 11.7 82.5 71.2 46.8 43.0 56.5 55.5 74.3\n101\n72.7 93.0 77.2 53.7 60.8 50.1 47.0 84.4 71.6 92.3 91.9 90.4 98.5 56.6 97.0 97.1 83.4 72.5 63.6 11.9 83.3 72.7 48.3 43.2 53.0 54.7 75.8\n152\n73.7 93.5 78.0 55.1 61.6 52.8 48.4 84.5 71.9 93.0 92.1 89.6 98.2 57.0 97.6 97.0 83.1 70.1 70.2 12.3 82.9 75.3 49.2 42.4 53.2 53.9 77.1"}
{"doc_id": "2103.00020", "para_id": 553, "text": "Table 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson conﬁdence\ninterval of each dataset’s top score are shown in bold."}
{"doc_id": "2103.00020", "para_id": 554, "text": "⋆We updated the STL10 scores from the previous version of this paper after ﬁxing a CUDA-related bug."}
{"doc_id": "2103.00020", "para_id": 555, "text": "Learning Transferable Visual Models From Natural Language Supervision\n41"}
{"doc_id": "2103.00020", "para_id": 556, "text": "CLIP-ViT\nCLIP-ResNet\nEfficientNet-NoisyStudent\nEfficientNet\nInstagram-pretrained\nSimCLRv2\nBYOL\nMoCo\nViT (ImageNet-21k)\nBiT-M\nBiT-S\nResNet"}
{"doc_id": "2103.00020", "para_id": 557, "text": "Figure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10."}
{"doc_id": "2103.00020", "para_id": 558, "text": "Learning Transferable Visual Models From Natural Language Supervision\n42"}
{"doc_id": "2103.00020", "para_id": 559, "text": "correct rank: 1/101    correct probability: 90.15%"}
{"doc_id": "2103.00020", "para_id": 560, "text": "correct rank: 1/397    correct probability: 90.22%"}
{"doc_id": "2103.00020", "para_id": 561, "text": "a centered satellite photo of permanent crop land."}
{"doc_id": "2103.00020", "para_id": 562, "text": "a centered satellite photo of brushland or shrubland."}
{"doc_id": "2103.00020", "para_id": 563, "text": "correct rank: 1/101    correct probability: 99.30%"}
{"doc_id": "2103.00020", "para_id": 564, "text": "correct rank: 1/102    correct probability: 99.81%"}
{"doc_id": "2103.00020", "para_id": 565, "text": "correct rank: 1/200    correct probability: 76.02%"}
{"doc_id": "2103.00020", "para_id": 566, "text": "correct rank: 1/100    correct probability: 38.02%"}
{"doc_id": "2103.00020", "para_id": 567, "text": "correct rank: 1/1000    correct probability: 88.27%"}
{"doc_id": "2103.00020", "para_id": 568, "text": "a photo of a mcdonnell douglas md-90, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 569, "text": "a photo of a mcdonnell douglas dc-9-30, a type of aircraft."}
{"doc_id": "2103.00020", "para_id": 570, "text": "correct rank: 1/196    correct probability: 63.30%"}
{"doc_id": "2103.00020", "para_id": 571, "text": "correct rank: 1/723    correct probability: 98.63%"}
{"doc_id": "2103.00020", "para_id": 572, "text": "correct rank: 1/700    correct probability: 98.98%"}
{"doc_id": "2103.00020", "para_id": 573, "text": "correct rank: 1/102    correct probability: 74.25%"}
{"doc_id": "2103.00020", "para_id": 574, "text": "correct rank: 1/1000    correct probability: 91.61%"}
{"doc_id": "2103.00020", "para_id": 575, "text": "correct rank: 4/500    correct probability: 12.00%"}
{"doc_id": "2103.00020", "para_id": 576, "text": "a photo of a broad tailed hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 577, "text": "a photo of a bishop of llandaff, a type of flower."}
{"doc_id": "2103.00020", "para_id": 578, "text": "a photo of a calliope hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 579, "text": "a photo of a black chinned hummingbird, a type of bird."}
{"doc_id": "2103.00020", "para_id": 580, "text": "a photo of a prince of wales feathers, a type of flower."}
{"doc_id": "2103.00020", "para_id": 581, "text": "correct rank: 1/113    correct probability: 98.34%"}
{"doc_id": "2103.00020", "para_id": 582, "text": "correct rank: 1/1000    correct probability: 79.54%"}
{"doc_id": "2103.00020", "para_id": 583, "text": "correct rank: 83/2000    correct probability: 0.27%"}
{"doc_id": "2103.00020", "para_id": 584, "text": "correct rank: 1/1000    correct probability: 79.56%"}
{"doc_id": "2103.00020", "para_id": 585, "text": "correct label: red and white triangle with exclamation mark warning"}
{"doc_id": "2103.00020", "para_id": 586, "text": "a zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 587, "text": "a zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 588, "text": "a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 589, "text": "a zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 590, "text": "a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign."}
{"doc_id": "2103.00020", "para_id": 591, "text": "Figure 21. Visualization of predictions from 36 CLIP zero-shot classiﬁers. All examples are random with the exception of reselecting\nHateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent\nthe class. When more than one template is used, the ﬁrst template is shown. The ground truth label is colored green while an incorrect\nprediction is colored orange."}
{"doc_id": "2103.00020", "para_id": 592, "text": "Learning Transferable Visual Models From Natural Language Supervision\n43"}
{"doc_id": "2103.00020", "para_id": 593, "text": "RN50\n81.1 75.6 41.6 32.6 59.6 55.8 19.3 82.1 41.7 85.4 82.1 65.9 66.6 42.2 94.3 41.1 54.2 35.2 42.2 16.1 57.6 63.6 43.5 20.3 59.7 56.9 59.6\nRN101\n83.9 81.0 49.0 37.2 59.9 62.3 19.5 82.4 43.9 86.2 85.1 65.7 59.3 45.6 96.7 33.1 58.5 38.3 33.3 16.9 55.2 62.2 46.7 28.1 61.1 64.2 62.2\nRN50x4\n86.8 79.2 48.9 41.6 62.7 67.9 24.6 83.0 49.3 88.1 86.0 68.0 75.2 51.1 96.4 35.0 59.2 35.7 26.0 20.2 57.5 65.5 49.0 17.0 58.3 66.6 65.8\nRN50x16\n90.5 82.2 54.2 45.9 65.0 72.3 30.3 82.9 52.8 89.7 87.6 71.9 80.0 56.0 97.8 40.3 64.4 39.6 33.9 24.0 62.5 68.7 53.4 17.6 58.9 67.6 70.5\nRN50x64\n91.8 86.8 61.3 48.9 66.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 90.8 61.0 98.3 59.4 69.7 47.9 33.2 29.6 65.0 74.1 56.8 27.5 62.1 70.7 73.6"}
{"doc_id": "2103.00020", "para_id": 594, "text": "B/32\n84.4 91.3 65.1 37.8 63.2 59.4 21.2 83.1 44.5 87.0 87.9 66.7 51.9 47.3 97.2 49.4 60.3 32.2 39.4 17.8 58.4 64.5 47.8 24.8 57.6 59.6 63.2\nB/16\n89.2 91.6 68.7 39.1 65.2 65.6 27.1 83.9 46.0 88.9 89.3 70.4 56.0 52.7 98.2 54.1 65.5 43.3 44.0 23.3 48.1 69.8 52.4 23.4 61.7 59.8 68.6\nL/14\n92.9 96.2 77.9 48.3 67.7 77.3 36.1 84.1 55.3 93.5 92.6 78.7 87.2 57.5 99.3 59.9 71.6 50.3 23.1 32.7 58.8 76.2 60.3 24.3 63.3 64.0 75.3\nL/14-336px\n93.8 95.7 77.5 49.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 88.3 57.7 99.4 59.6 71.7 52.3 21.9 34.9 63.0 76.9 61.3 24.8 63.3 67.9 76.2"}
{"doc_id": "2103.00020", "para_id": 595, "text": "Table 11. Zero-shot performance of CLIP models over 27 datasets."}
{"doc_id": "2103.00020", "para_id": 596, "text": "Figure 22. CLIP’s zero-shot performance compared to linear-probe ResNet performance"}
{"doc_id": "2103.00020", "para_id": 597, "text": "Learning Transferable Visual Models From Natural Language Supervision\n44"}
{"doc_id": "2103.00020", "para_id": 598, "text": "Linear Classiﬁer\nZero Shot\nDataset\nYFCC\nWIT\n∆\nYFCC\nWIT\n∆"}
{"doc_id": "2103.00020", "para_id": 599, "text": "To provide a qualitative summary / overview of CLIP’s zero-\nshot performance we visualize a randomly selected predic-\ntion for 36 different zero-shot CLIP classiﬁers in Figure\n21. In addition, Table 11 and Figure 22 show the individual\nzero-shot performance scores for each dataset."}
{"doc_id": "2103.00020", "para_id": 600, "text": "Birdsnap\n47.4\n35.3\n+12.1\n19.9\n4.5\n+15.4\nCountry211\n23.1\n17.3\n+5.8\n5.2\n5.3\n+0.1\nFlowers102\n94.4\n89.8\n+4.6\n48.6\n21.7\n+26.9\nGTSRB\n66.8\n72.5\n−5.7\n6.9\n7.0\n−0.1\nUCF101\n69.2\n74.9\n−5.7\n22.9\n32.0\n−9.1\nStanford Cars\n31.4\n50.3\n−18.9\n3.8\n10.9\n−7.1"}
{"doc_id": "2103.00020", "para_id": 601, "text": "ImageNet\n62.0\n60.8\n+1.2\n31.3\n27.6\n+3.7\nDataset Average\n65.5\n66.6\n−1.1\n29.6\n30.0\n−0.4\nDataset “Wins”\n10\n15\n−5\n19\n18\n+1"}
{"doc_id": "2103.00020", "para_id": 602, "text": "Our early attempts at duplicate detection and analysis used\nnearest neighbors in the model’s learned embedding space.\nWhile it is intuitive to use a model’s own notion of similar-\nity, we encountered issues. We found the model’s feature\nspace is weighted very heavily towards semantic similar-\nity. Many false positives occurred due to distinct objects\nthat would be described similarly (soccer balls, ﬂowers of\nthe same species, etc...) having almost perfect similarity.\nWe also observed the model was quite poor at assigning\ncertain kinds of near-duplicates high similarity scores. We\nnoticed repeatedly that images with high-frequency textures\n(such as fur or stripe patterns) pre-processed by different\nresizing algorithms (nearest neighbor vs bi-linear) could\nhave surprisingly low similarity. This resulted in many false\nnegatives."}
{"doc_id": "2103.00020", "para_id": 603, "text": "Table 12. CLIP performs similarly when trained on only\nYFCC100M.\nComparing\na\nResNet-50\ntrained\non\nonly\nYFCC100M with a same sized subset of WIT shows simi-\nlar average performance and number of wins on zero shot and\nlinear classiﬁer evals.\nHowever, large differences in dataset\nspeciﬁc performance occur. We include performance on the 3\ndatasets where YFCC does best and worst compared to WIT\naccording to a linear probe in order to highlight this as well as\naggregate performance across all linear and zero-shot evals and\nthe canonical ImageNet dataset."}
{"doc_id": "2103.00020", "para_id": 604, "text": "To study whether our custom dataset is critical to the perfor-\nmance of CLIP, we trained a model on a ﬁltered subset of\nthe YFCC100M dataset (details described in Section 2.2)\nand compared its performance to the same model trained\non an equally sized subset of WIT. We train each model for\n32 epochs at which point transfer performance begins to\nplateau due to overﬁtting. Results are shown in Table 12.\nAcross our whole eval suite, YFCC and WIT perform simi-\nlarly on average for both zero-shot and linear probe settings.\nHowever, performance on speciﬁc ﬁne-grained classiﬁca-\ntion datasets can vary widely - sometimes by over 10%.\nOur speculation is that these differences in performance re-\nﬂect the relative density of relevant data in each pre-training\ndataset. For instance, pre-training on YFCC100M, which\nmight contain many photos of birds and ﬂowers (common\nsubjects for photographers), results in better performance on\nBirdsnap and Flowers102, while pre-training on WIT results\nin better car and pet classiﬁers (which appear common in\nour dataset)."}
{"doc_id": "2103.00020", "para_id": 605, "text": "We built our own near-duplicate detector to ﬁx this issue.\nWe created a synthetic data augmentation pipeline that com-\nbined a variety of common image manipulations. The aug-\nmentation pipeline combines random cropping and zooming,\naspect ratio distortion, downsizing and upscaling to different\nresolutions, minor rotations, jpeg compression, and HSV\ncolor jitter. The pipeline also randomly selects from differ-\nent interpolation algorithms for all relevant steps. We then\ntrained a model to maximize the similarity of an image and\nits transformed variant while minimizing similarity to all\nother images in a training batch. We used the same n-pair /\nInfoNCE loss as CLIP but with a ﬁxed temperature of 0.07."}
{"doc_id": "2103.00020", "para_id": 606, "text": "We selected a ResNet-50 as the model architecture. We\nmodiﬁed the base ResNet-50 with the anti-alias improve-\nments from (Zhang, 2019) and used weight norm (Sali-\nmans & Kingma, 2016) instead of batch norm (Ioffe &\nSzegedy, 2015) to avoid leaking information about dupli-\ncates via batch statistics - a problem previously noted in\n(Henaff, 2020). We also found the GELU activation func-\ntion (Hendrycks & Gimpel, 2016) to perform better for this\ntask. We trained the model with a total batch size of 1,712\nfor approximately 30 million images sampled from our pre-\ntraining dataset. At the end of training it achieves nearly\n100% accuracy on its proxy training task."}
{"doc_id": "2103.00020", "para_id": 607, "text": "Overall, these results are encouraging as they suggest our\napproach can use any reasonably ﬁltered collection of paired\n(text, image) data. This mirrors recent work which reported\npositive results using the same contrastive pre-training ob-\njective on the relatively different domain of medical imaging\n(Zhang et al., 2020). It also is similar to the ﬁndings of noisy\nstudent self-training which reported only slight improve-\nments when using their JFT300M dataset over YFCC100M\n(Xie et al., 2020). We suspect the major advantage of our\ndataset over the already existing YFCC100M is its much\nlarger size."}
{"doc_id": "2103.00020", "para_id": 608, "text": "Learning Transferable Visual Models From Natural Language Supervision\n45"}
{"doc_id": "2103.00020", "para_id": 609, "text": "Finally, we caution that WIT includes this ﬁltered subset\nof YFCC100M. This could result in our ablation under-\nestimating the size of performance differences between\nYFCC100M and the rest of WIT. We do not think this is\nlikely as YFCC100M is only 3.7% of the overall WIT data\nblend and it did not noticeably change the performance of\nmodels when it was added to the existing data blend during\nthe creation of WIT."}
{"doc_id": "2103.00020", "para_id": 610, "text": "on 5 datasets requiring the direct and indirect use of OCR.\nThree of these datasets MNIST (LeCun), SVHN (Netzer\net al., 2011), and IIIT5K (Mishra et al., 2012) directly check\nthe ability of a model to perform low-level character and\nword recognition, while Hateful Memes (Kiela et al., 2020)\nand SST-2 (Socher et al., 2013) check the ability of a model\nto use OCR to perform a semantic task. Results are reported\nin Table 14."}
{"doc_id": "2103.00020", "para_id": 611, "text": "CLIP’s performance is still highly variable and appears to\nbe sensitive to some combination of the domain (rendered or\nnatural images) and the type of text to be recognized (num-\nbers or words). CLIP’s OCR performance is strongest Hate-\nful Memes and SST-2 - datasets where the text is digitally\nrendered and consists mostly of words. On IIIT5K, which\nis natural images of individually cropped words, zero-shot\nCLIP performs a bit more respectively and its performance\nis similar to Jaderberg et al. (2014) early work combining\ndeep learning and structured prediction to perform open-\nvocabulary OCR. However, performance is noticeably lower\non two datasets involving recognition of hand written and\nstreet view numbers. CLIP’s 51% accuracy on full number\nSVHN is well below any published results. Inspection sug-\ngests CLIP struggles with repeated characters as well as the\nlow resolution and blurry images of SVHN. CLIP’s zero-\nshot MNIST performance is also poor and is outperformed\nby supervised logistic regression on raw pixels, one of the\nsimplest possible machine learning baselines."}
{"doc_id": "2103.00020", "para_id": 612, "text": "Due to the large variety of datasets and experiments consid-\nered in this work, the main body focuses on summarizing\nand analyzing overall results. In the following subsections\nwe report details of performance for speciﬁc groups of tasks,\ndatasets, and evaluation settings."}
{"doc_id": "2103.00020", "para_id": 613, "text": "CLIP pre-trains for the task of image-text retrieval on our\nnoisy web-scale dataset. Although the focus of this paper\nis on representation learning and task learning for the pur-\npose of transfer to a wide variety of downstream datasets,\nvalidating that CLIP is able to achieve high transfer perfor-\nmance transfer on exactly what it is pre-trained for is an\nimportant sanity check / proof of concept. In Table 13 we\ncheck the zero-shot transfer performance of CLIP for both\ntext and image retrieval on the Flickr30k and MSCOCO\ndatsets. Zero-shot CLIP matches or outperforms all prior\nzero-shot results on these two datasets. Zero-shot CLIP is\nalso competitive with the current overall SOTA for the task\nof text retrieval on Flickr30k. On image retrieval, CLIP’s\nperformance relative to the overall state of the art is notice-\nably lower. However, zero-shot CLIP is still competitive\nwith a ﬁne-tuned Unicoder-VL. On the larger MS-COCO\ndataset ﬁne-tuning improves performance signiﬁcantly and\nzero-shot CLIP is not competitive with the most recent work.\nFor both these datasets we prepend the prompt “a photo\nof” to the description of each image which we found boosts\nCLIP’s zero-shot R@1 performance between 1 and 2 points."}
{"doc_id": "2103.00020", "para_id": 614, "text": "SST-2 is a sentence level NLP dataset which we render into\nimages. We include SST-2 in order to check whether CLIP\nis able to convert low level OCR capability into a higher\nlevel representation. Fitting a linear classiﬁer on CLIP’s rep-\nresentation of rendered sentences achives 80.5% accuracy.\nThis is on par with the 80% accuracy of a continuous bag\nof words baseline using GloVe word vectors pre-trained on\n840 billion tokens (Pennington et al., 2014). While this is a\nsimple NLP baseline by today’s standard, and well below\nthe 97.5% of the current SOTA, it is encouraging to see\nthat CLIP is able to turn an image of rendered text into a\nnon-trivial sentence level representation. Fully supervised\nCLIP is also surprisingly strong on Hateful Meme detec-\ntion, where CLIP is only 0.7 points behind the current single\nmodel SOTA and several points above the best baseline from\nthe original paper. Similar to SST-2, these other results on\nHateful Memes use the ground truth text which CLIP does\nnot have access to. Finally, we note that zero-shot CLIP\noutperforms the best results using fully supervised linear\nprobes across all other 56 models included in our evaluation\nsuite. This suggests CLIP’s OCR capability is at least some-\nwhat unique compared to existing work on self-supervised\nand supervised representation learning."}
{"doc_id": "2103.00020", "para_id": 615, "text": "Although visualizations have shown that ImageNet models\ncontain features that respond to the presence of text in an\nimage (Zeiler & Fergus, 2014), these representations are\nnot sufﬁciently ﬁne-grained to use for the task of optical\ncharacter recognition (OCR). To compensate, models are\naugmented with the outputs of custom OCR engines and\nfeatures to boost performance on tasks where this capability\nis required (Singh et al., 2019; Yang et al., 2020). Early dur-\ning the development of CLIP, we noticed that CLIP began to\nlearn primitive OCR capabilities which appeared to steadily\nimprove over the course of the project. To evaluate this\nqualitatively noticed behavior, we measured performance"}
{"doc_id": "2103.00020", "para_id": 616, "text": "Learning Transferable Visual Models From Natural Language Supervision\n46"}
{"doc_id": "2103.00020", "para_id": 617, "text": "Text Retrieval\nImage Retrieval\nFlickr30k\nMSCOCO\nFlickr30k\nMSCOCO\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10"}
{"doc_id": "2103.00020", "para_id": 618, "text": "Unicoder-VLa\n86.2\n96.3\n99.0\n62.3\n87.1\n92.8\n71.5\n90.9\n94.9\n46.7\n76.0\n85.3\nUniterb\n87.3\n98.0\n99.2\n65.7\n88.6\n93.8\n75.6\n94.1\n96.8\n52.9\n79.9\n88.0\nVILLAc\n87.9\n97.5\n98.8\n-\n-\n-\n76.3\n94.2\n96.8\n-\n-\n-\nOscard\n-\n-\n-\n73.5\n92.2\n96.0\n-\n-\n-\n57.5\n82.8\n89.8\nERNIE-ViLe\n88.7\n98.0\n99.2\n-\n-\n-\n76.7\n93.6\n96.4\n-\n-\n-"}
{"doc_id": "2103.00020", "para_id": 619, "text": "Visual N-Gramsf\n15.4\n35.7\n45.1\n8.7\n23.1\n33.3\n8.8\n21.2\n29.9\n5.0\n14.5\n21.9\nImageBERTg\n-\n-\n-\n44.0\n71.2\n80.4\n-\n-\n-\n32.3\n59.0\n70.2\nUnicoder-VLa\n64.3\n86.8\n92.3\n-\n-\n-\n48.4\n76.0\n85.2\n-\n-\n-\nUniterb\n83.6\n95.7\n97.7\n-\n-\n-\n68.7\n89.2\n93.9\n-\n-\n-\nCLIP\n88.0\n98.7\n99.4\n58.4\n81.5\n88.1\n68.7\n90.6\n95.2\n37.8\n62.4\n72.2"}
{"doc_id": "2103.00020", "para_id": 620, "text": "Table 13. CLIP improves zero-shot retrieval and is competitive with the best ﬁne-tuned result on Flickr30k text retrieval. Bold\nindicates best overall performance while an underline indicates best in category performance (zero-shot or ﬁne-tuned). For all other\nmodels, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\na(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f(Li et al., 2017) g(Qi et al., 2020)"}
{"doc_id": "2103.00020", "para_id": 621, "text": "R(2+1)D-BERTa\n98.7\n-\n-\n-\nNS ENet-L2b\n-\n84.8\n-\n-\nHT100M S3Dd\n91.3\n-\n-\n-\nBaseline I3De\n-\n70.2\n-\n-"}
{"doc_id": "2103.00020", "para_id": 622, "text": "Raw Pixels\n92.5\n-\n-\n-\n-\nES Best\n98.9h\n-\n-\n58.6h\n59.0i"}
{"doc_id": "2103.00020", "para_id": 623, "text": "MMV FACf\n91.8\n-\n-\n-\nNS ENet-L2c\n89.4c\n68.2c\n-\n-\nCLIP\n92.0\n73.0\n-\n-"}
{"doc_id": "2103.00020", "para_id": 624, "text": "HT100M S3Dd\n-\n-\n30.5\n34.8\nCLIP\n80.3\n69.6\n40.7\n44.8"}
{"doc_id": "2103.00020", "para_id": 625, "text": "Table 14. OCR performance on 5 datasets. All metrics are accuracy\non the test set except for Hateful Memes which reports ROC AUC\non the dev set. Single model SOTA reported to best of knowledge.\nES Best reports the best performance across the 56 non-CLIP\nmodels in our evaluation suite. a(Assiri, 2020) b(Jaderberg et al.,\n2015) c(Wang et al., 2020) d(Lippe et al., 2020) f(Jaderberg et al.,\n2014) g(Wang et al., 2018) h(Xie et al., 2020) i(Mahajan et al.,\n2018)"}
{"doc_id": "2103.00020", "para_id": 626, "text": "Table 15. Action recognition performance on 3 video datasets. Sin-\ngle model SOTA reported to best of knowledge. Note that linear\nCLIP and linear NS ENet-L2 are trained and evaluated on a single\nframe subsampled version of each dataset and not directly compa-\nrable to prior work. On Kinetics-700, we report the ActivityNet\ncompetition metric which is the average of top-1 and top-5 per-\nformance. a(Kalfaoglu et al., 2020) b(Lu et al., 2020) c(Xie et al.,\n2020) d(Miech et al., 2020b) e(Carreira et al., 2019) f(Alayrac\net al., 2020)"}
{"doc_id": "2103.00020", "para_id": 627, "text": "For the purpose of learning, a potentially important aspect\nof natural language is its ability to express, and therefore su-\npervise, an extremely wide set of concepts. A CLIP model,\nsince it is trained to pair semi-arbitrary text with images, is\nlikely to receive supervision for a wide range of visual con-\ncepts involving both common and proper nouns, verbs, and\nadjectives. ImageNet-1K, by contrast, only labels common\nnouns. Does the lack of broader supervision in ImageNet\nresult in weaker transfer of ImageNet models to tasks involv-\ning the recognition of visual concepts that are not nouns?"}
{"doc_id": "2103.00020", "para_id": 628, "text": "action classiﬁcation datasets which measure the ability of a\nmodel to recognize verbs. In Table 15 we report results on\nUCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira\net al., 2019), two common datasets for the task. Unfortu-\nnately, our CPU based linear classiﬁer takes a prohibitively\nlong time to evaluate on a video dataset due to the very large\nnumber of training frames. To deal with this, we aggres-\nsively sub-sample each video to only a single center frame,\neffectively turning it into an image classiﬁcation dataset.\nAs a result, our reported performance in a linear evaluation\nsetting likely under estimates performance by a moderate\namount."}
{"doc_id": "2103.00020", "para_id": 629, "text": "To investigate this, we measure and compare the perfor-\nmance of CLIP and ImageNet models on several video"}
{"doc_id": "2103.00020", "para_id": 630, "text": "Learning Transferable Visual Models From Natural Language Supervision\n47"}
{"doc_id": "2103.00020", "para_id": 631, "text": "IN\nIN-V2\nIN-A\nIN-R\nObjectNet\nIN-Sketch\nIN-Vid\nYTBB\nTop-1\nTop-1\nTop-1\nTop-1\nTop-1\nTop-1\nPM0\nPM10\nPM0\nPM10"}
{"doc_id": "2103.00020", "para_id": 632, "text": "NS EfﬁcientNet-L2a\n88.3\n80.2\n84.9\n74.7\n68.5\n47.6\n88.0\n82.1\n67.7\n63.5\nFixResNeXt101-32x48d V2b\n86.4\n78.0\n68.4\n80.0\n57.8\n59.1\n85.8\n72.2\n68.9\n57.7\nLinear Probe CLIP\n85.4\n75.9\n75.3\n84.2\n66.2\n57.4\n89.1\n77.2\n68.7\n63.1\nZero-Shot CLIP\n76.2\n70.1\n77.2\n88.9\n72.3\n60.2\n95.3\n89.2\n95.2\n88.5"}
{"doc_id": "2103.00020", "para_id": 633, "text": "Table 16. Detailed ImageNet robustness performance. IN is used to abbreviate for ImageNet. a(Xie et al., 2020) b(Touvron et al., 2019)"}
{"doc_id": "2103.00020", "para_id": 634, "text": "Despite this handicap, CLIP features transfer surprisingly\nwell to this task. CLIP matches the best prior result on UCF-\n101 in a linear probe evaluation setting and also outperforms\nall other models in our evaluation suite. On Kinetics-700,\nCLIP also outperforms the ﬁne-tuned I3D baseline from the\noriginal paper. Since it does not require a training stage,\nwe report CLIP’s zero-shot performance when averaging\npredictions across all frames. CLIP also performs well in\nthis setting and on Kinetics-700 its performance is within\n1% of the fully supervised I3D baseline which is trained\non 545000 labeled videos. Encouraged by these results, we\nalso measure CLIP’s performance on the recently introduced\nRareAct dataset (Miech et al., 2020a) which was designed\nto measure zero-shot recognition of unusual actions like\n“hammering a phone” and “drilling an egg”. CLIP improves\nover the prior state of the art, a S3D model trained on auto-\nmatically extracted captions from 100 million instructional\nvideos, by 10 points."}
{"doc_id": "2103.00020", "para_id": 635, "text": "Another behavior we noticed during the development of\nCLIP was its ability to recognize many places and locations.\nTo quantify this we created the Country211 dataset as de-\nscribed in Appendix A and report results on it throughout\nthe paper. However it is a new benchmark so to compare\nwith prior work on geolocalization we also report results\non the IM2GPS test set from Hays & Efros (2008) in Table\n17. Since IM2GPS is a regression benchmark, we guess the\nGPS coordinates of the nearest image in a set of reference\nimages using CLIP’s embedding space. This is not a zero-\nshot result since it uses nearest-neighbor regression. Despite\nquerying only 1 million images, which is much less than\nprior work, CLIP performs similarly to several task speciﬁc\nmodels. It is not, however, competitive with the current state\nof the art."}
{"doc_id": "2103.00020", "para_id": 636, "text": "While CLIP has encouragingly strong performance on the\ntask of action recognition, we note that there are many differ-\nences between the models being compared beyond just their\nform of supervision such as model architecture, training\ndata distribution, dataset size, and compute used. Further\nwork is needed to more precisely determine what speciﬁc\ndesign decisions contribute to achieving high performance\non this task."}
{"doc_id": "2103.00020", "para_id": 637, "text": "Section 3.3 provides a high level summary and analysis of\nImageNet-related robustness results. We brieﬂy provide\nsome additional numerical details in this appendix. Per-\nformance results per dataset are provided in Table 16 and\ncompared with the current state of the art results reported\nin Taori et al. (2020)’s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP’s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its ﬂexible zero-shot capability and on\nImageNet-R, which likely reﬂects CLIP’s pre-training dis-\ntribution including signiﬁcant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020)."}
{"doc_id": "2103.00020", "para_id": 638, "text": "ISNsa\n16.9\n43.0\n51.9\n66.7\n80.2\nCPlaNetb\n16.5\n37.1\n46.4\n62.0\n78.5\nCLIP\n13.9\n32.9\n43.0\n62.0\n79.3\nDeep-Ret+c\n14.4\n33.3\n47.7\n61.6\n73.4\nPlaNetd\n8.4\n24.5\n37.6\n53.6\n71.3"}
{"doc_id": "2103.00020", "para_id": 639, "text": "Table 17. Geolocalization performance on the IM2GPS test set.\nMetric is percent of images localized within a given radius. Models\nare ordered by average performance. a(Muller-Budack et al., 2018)\nb(Hongsuck Seo et al., 2018) c(Vo et al., 2017) c(Weyand et al.,\n2016)"}
{"doc_id": "2103.00020", "para_id": 640, "text": "Learning Transferable Visual Models From Natural Language Supervision\n48"}
{"doc_id": "2103.00020", "para_id": 641, "text": "Batch size\n32768\nVocabulary size\n49408\nTraining epochs\n32\nMaximum temperature\n100.0\nWeight decay\n0.2\nWarm-up iterations\n2000\nAdam β1\n0.9\nAdam β2\n0.999 (ResNet), 0.98 (ViT)\nAdam ϵ\n10−8 (ResNet), 10−6 (ViT)"}
{"doc_id": "2103.00020", "para_id": 642, "text": "Learning\nEmbedding\nInput\nResNet\nText Transformer\nModel\nrate\ndimension\nresolution\nblocks\nwidth\nlayers\nwidth\nheads"}
{"doc_id": "2103.00020", "para_id": 643, "text": "RN50\n5 × 10−4\n1024\n224\n(3, 4, 6, 3)\n2048\n12\n512\n8\nRN101\n5 × 10−4\n512\n224\n(3, 4, 23, 3)\n2048\n12\n512\n8\nRN50x4\n5 × 10−4\n640\n288\n(4, 6, 10, 6)\n2560\n12\n640\n10\nRN50x16\n4 × 10−4\n768\n384\n(6, 8, 18, 8)\n3072\n12\n768\n12\nRN50x64\n3.6 × 10−4\n1024\n448\n(3, 15, 36, 10)\n4096\n12\n1024\n16"}
{"doc_id": "2103.00020", "para_id": 644, "text": "Learning\nEmbedding\nInput\nVision Transformer\nText Transformer\nModel\nrate\ndimension\nresolution\nlayers\nwidth\nheads\nlayers\nwidth\nheads"}
{"doc_id": "2103.00020", "para_id": 645, "text": "ViT-B/32\n5 × 10−4\n512\n224\n12\n768\n12\n12\n512\n8\nViT-B/16\n5 × 10−4\n512\n224\n12\n768\n12\n12\n512\n8\nViT-L/14\n4 × 10−4\n768\n224\n24\n1024\n16\n12\n768\n12\nViT-L/14-336px\n2 × 10−5\n768\n336\n24\n1024\n16\n12\n768\n12"}
{"doc_id": "1910.10683", "para_id": 0, "text": "Journal of Machine Learning Research 21 (2020) 1-67\nSubmitted 1/20; Revised 6/20; Published 6/20"}
{"doc_id": "1910.10683", "para_id": 1, "text": "Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer"}
{"doc_id": "1910.10683", "para_id": 2, "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-\ntuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.1"}
{"doc_id": "1910.10683", "para_id": 3, "text": "Keywords:\ntransfer learning, natural language processing, multi-task learning, attention-\nbased models, deep learning"}
{"doc_id": "1910.10683", "para_id": 4, "text": "Training a machine learning model to perform natural language processing (NLP) tasks\noften requires that the model can process text in a way that is amenable to downstream\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\nthe model to “understand” text. This knowledge can range from low-level (e.g. the spelling"}
{"doc_id": "1910.10683", "para_id": 5, "text": "∗. Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence\nto craffel@gmail.com.\n1. https://github.com/google-research/text-to-text-transfer-transformer"}
{"doc_id": "1910.10683", "para_id": 6, "text": "©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu."}
{"doc_id": "1910.10683", "para_id": 7, "text": "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v21/20-074.html."}
{"doc_id": "1910.10683", "para_id": 8, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 9, "text": "or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks).\nIn modern machine learning practice, providing this knowledge is rarely done explicitly;\ninstead, it is often learned as part of an auxiliary task. For example, a historically common\napproach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map\nword identities to a continuous representation where, ideally, similar words map to similar\nvectors. These vectors are often learned through an objective that, for example, encourages\nco-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\nRecently, it has become increasingly common to pre-train the entire model on a data-rich\ntask. Ideally, this pre-training causes the model to develop general-purpose abilities and\nknowledge that can then be transferred to downstream tasks. In applications of transfer\nlearning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014), pre-training is typically done via supervised learning on a large labeled data set\nlike ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques\nfor transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.\nThis approach has recently been used to obtain state-of-the-art results in many of the most\ncommon NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu\net al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training\nfor NLP is particularly attractive because unlabeled text data is available en masse thanks\nto the Internet—for example, the Common Crawl project2 produces about 20TB of text\ndata extracted from web pages each month. This is a natural fit for neural networks, which\nhave been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better\nperformance simply by training a larger model on a larger data set (Hestness et al., 2017;\nShazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019;\nShazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).\nThis synergy has resulted in a great deal of recent work developing transfer learning\nmethodology for NLP, which has produced a wide landscape of pre-training objectives\n(Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled\ndata sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al.,\n2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018;\nHoulsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity\nof techniques in this burgeoning field can make it difficult to compare different algorithms,\ntease apart the effects of new contributions, and understand the space of existing methods for\ntransfer learning. Motivated by a need for more rigorous understanding, we leverage a unified\napproach to transfer learning that allows us to systematically study different approaches\nand push the current limits of the field.\nThe basic idea underlying our work is to treat every text processing problem as a\n“text-to-text” problem, i.e. taking text as input and producing new text as output. This\napproach is inspired by previous unifying frameworks for NLP tasks, including casting all text\nproblems as question answering (McCann et al., 2018), language modeling (Radford et al.,\n2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework\nallows us to directly apply the same model, objective, training procedure, and decoding\nprocess to every task we consider. We leverage this flexibility by evaluating performance\non a wide variety of English-based NLP problems, including question answering, document"}
{"doc_id": "1910.10683", "para_id": 10, "text": "\"summarize: state authorities\ndispatched emergency crews tuesday to"}
{"doc_id": "1910.10683", "para_id": 11, "text": "Figure 1: A diagram of our text-to-text framework. Every task we consider—including\ntranslation, question answering, and classification—is cast as feeding our model\ntext as input and training it to generate some target text. This allows us to use the\nsame model, loss function, hyperparameters, etc. across our diverse set of tasks. It\nalso provides a standard testbed for the methods included in our empirical survey.\n“T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”."}
{"doc_id": "1910.10683", "para_id": 12, "text": "summarization, and sentiment classification, to name a few. With this unified approach,\nwe can compare the effectiveness of different transfer learning objectives, unlabeled data\nsets, and other factors, while exploring the limits of transfer learning for NLP by scaling up\nmodels and data sets beyond what has previously been considered."}
{"doc_id": "1910.10683", "para_id": 13, "text": "We emphasize that our goal is not to propose new methods but instead to provide a\ncomprehensive perspective on where the field stands. As such, our work primarily comprises\na survey, exploration, and empirical comparison of existing techniques. We also explore the\nlimits of current approaches by scaling up the insights from our systematic study (training\nmodels up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks\nwe consider. In order to perform experiments at this scale, we introduce the “Colossal Clean\nCrawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text\nscraped from the web. Recognizing that the main utility of transfer learning is the possibility\nof leveraging pre-trained models in data-scarce settings, we release our code, data sets, and\npre-trained models.1"}
{"doc_id": "1910.10683", "para_id": 14, "text": "The remainder of the paper is structured as follows: In the following section, we discuss\nour base model and its implementation, our procedure for formulating every text processing\nproblem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a\nlarge set of experiments that explore the field of transfer learning for NLP. At the end of the\nsection (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art\nresults on a wide variety of benchmarks. Finally, we provide a summary of our results and\nwrap up with a look towards the future in Section 4."}
{"doc_id": "1910.10683", "para_id": 15, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 16, "text": "Before presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our “Colossal Clean Crawled\nCorpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the “Text-to-Text Transfer Transformer”\n(T5)."}
{"doc_id": "1910.10683", "para_id": 17, "text": "Early results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the “Transformer” architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention"}
{"doc_id": "1910.10683", "para_id": 18, "text": "3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/"}
{"doc_id": "1910.10683", "para_id": 19, "text": "mechanism after each self-attention layer that attends to the output of the encoder. The\nself-attention mechanism in the decoder also uses a form of autoregressive or causal self-\nattention, which only allows the model to attend to past outputs. The output of the final\ndecoder block is fed into a dense layer with a softmax output, whose weights are shared with\nthe input embedding matrix. All attention mechanisms in the Transformer are split up into\nindependent “heads” whose outputs are concatenated before being further processed.\nSince self-attention is order-independent (i.e. it is an operation on sets), it is common\nto provide an explicit position signal to the Transformer. While the original Transformer\nused a sinusoidal position signal or learned position embeddings, it has recently become\nmore common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\nInstead of using a fixed embedding for each position, relative position embeddings produce\na different learned embedding according to the offset between the “key” and “query” being\ncompared in the self-attention mechanism. We use a simplified form of position embeddings\nwhere each “embedding” is simply a scalar that is added to the corresponding logit used\nfor computing the attention weights. For efficiency, we also share the position embedding\nparameters across all layers in our model, though within a given layer each attention head\nuses a different learned position embedding. Typically, a fixed number of embeddings are\nlearned, each corresponding to a range of possible key-query offsets. In this work, we use 32\nembeddings for all of our models with ranges that increase in size logarithmically up to an\noffset of 128 beyond which we assign all relative positions to the same embedding. Note\nthat a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\ncan build a sensitivity to larger offsets by combining local information from previous layers.\nTo summarize, our model is roughly equivalent to the original Transformer proposed by\nVaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer\nnormalization outside the residual path, and using a different position embedding scheme.\nSince these architectural changes are orthogonal to the experimental factors we consider in\nour empirical survey of transfer learning, we leave the ablation of their impact for future\nwork.\nAs part of our study, we experiment with the scalability of these models, i.e. how their\nperformance changes as they are made to have more parameters or layers. Training large\nmodels can be non-trivial since they might not fit on a single machine and require a great deal\nof computation. As a result, we use a combination of model and data parallelism and train\nmodels on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers\nthat contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with\nsupporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,\n2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky,\n2014)."}
{"doc_id": "1910.10683", "para_id": 20, "text": "Much of the previous work on transfer learning for NLP makes use of large unlabeled data\nsets for unsupervised learning. In this paper, we are interested in measuring the effect of the\nquality, characteristics, and size of this unlabeled data. To generate data sets that satisfy\nour needs, we leverage Common Crawl as a source of text scraped from the web. Common"}
{"doc_id": "1910.10683", "para_id": 21, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 22, "text": "Crawl has previously been used as a source of text data for NLP, for example to train an\nn-gram language model (Buck et al., 2014), as training data for commonsense reasoning\n(Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013),\nas a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and\neven simply as a giant text corpus for testing optimizers (Anil et al., 2019).\nCommon Crawl is a publicly-available web archive that provides “web extracted text”\nby removing markup and other non-text content from the scraped HTML files. This process\nproduces around 20TB of scraped text data each month. Unfortunately, the majority of the\nresulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate\ntext like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped\ntext contains content that is unlikely to be helpful for any of the tasks we consider (offensive\nlanguage, placeholder text, source code, etc.). To address these issues, we used the following\nheuristics for cleaning up Common Crawl’s web extracted text:"}
{"doc_id": "1910.10683", "para_id": 23, "text": "• We only retained lines that ended in a terminal punctuation mark (i.e. a period,\nexclamation mark, question mark, or end quotation mark)."}
{"doc_id": "1910.10683", "para_id": 24, "text": "• We discarded any page with fewer than 3 sentences and only retained lines that\ncontained at least 5 words."}
{"doc_id": "1910.10683", "para_id": 25, "text": "• We removed any page that contained any word on the “List of Dirty, Naughty, Obscene\nor Otherwise Bad Words”.6"}
{"doc_id": "1910.10683", "para_id": 26, "text": "• Many of the scraped pages contained warnings stating that Javascript should be\nenabled so we removed any line with the word Javascript."}
{"doc_id": "1910.10683", "para_id": 27, "text": "• Some pages had placeholder “lorem ipsum” text; we removed any page where the\nphrase “lorem ipsum” appeared."}
{"doc_id": "1910.10683", "para_id": 28, "text": "• Some pages inadvertently contained code. Since the curly bracket “{” appears in\nmany programming languages (such as Javascript, widely used on the web) but not in\nnatural text, we removed any pages that contained a curly bracket."}
{"doc_id": "1910.10683", "para_id": 29, "text": "• Since some of the scraped pages were sourced from Wikipedia and had citation markers\n(e.g. [1], [citation needed], etc.), we removed any such markers."}
{"doc_id": "1910.10683", "para_id": 30, "text": "• Many pages had boilerplate policy notices, so we removed any lines containing the\nstrings “terms of use”, “privacy policy”, “cookie policy”, “uses cookies”, “use of\ncookies”, or “use cookies”."}
{"doc_id": "1910.10683", "para_id": 31, "text": "• To deduplicate the data set, we discarded all but one of any three-sentence span\noccurring more than once in the data set."}
{"doc_id": "1910.10683", "para_id": 32, "text": "Additionally, since most of our downstream tasks are focused on English-language text,\nwe used langdetect7 to filter out any pages that were not classified as English with a\nprobability of at least 0.99. Our heuristics are inspired by past work on using Common"}
{"doc_id": "1910.10683", "para_id": 33, "text": "6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n7. https://pypi.org/project/langdetect/"}
{"doc_id": "1910.10683", "para_id": 34, "text": "Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an\nautomatic language detector and discard short lines and Smith et al. (2013); Grave et al.\n(2018) both perform line-level deduplication. However, we opted to create a new data set\nbecause prior data sets use a more limited set of filtering heuristics, are not publicly available,\nand/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,\n2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on\nparallel training data for machine translation (Smith et al., 2013)).\nTo assemble our base data set, we downloaded the web extracted text from April 2019\nand applied the aforementioned filtering. This produces a collection of text that is not only\norders of magnitude larger than most data sets used for pre-training (about 750 GB) but also\ncomprises reasonably clean and natural English text. We dub this data set the “Colossal\nClean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8"}
{"doc_id": "1910.10683", "para_id": 35, "text": "We consider the impact of using various alternative versions of this data set in Section 3.4."}
{"doc_id": "1910.10683", "para_id": 36, "text": "Our goal in this paper is to measure general language learning abilities. As such, we study\ndownstream performance on a diverse set of benchmarks, including machine translation,\nquestion answering, abstractive summarization, and text classification. Specifically, we\nmeasure performance on the GLUE and SuperGLUE text classification meta-benchmarks;\nCNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English\nto German, French, and Romanian translation. All data was sourced from TensorFlow\nDatasets.9"}
{"doc_id": "1910.10683", "para_id": 37, "text": "GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a\ncollection of text classification tasks meant to test general language understanding abilities:"}
{"doc_id": "1910.10683", "para_id": 38, "text": "• Sentence acceptability judgment (CoLA (Warstadt et al., 2018))"}
{"doc_id": "1910.10683", "para_id": 39, "text": "• Sentiment analysis (SST-2 (Socher et al., 2013))"}
{"doc_id": "1910.10683", "para_id": 40, "text": "• Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer\net al., 2017), QQP (Iyer et al., 2017))"}
{"doc_id": "1910.10683", "para_id": 41, "text": "• Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,"}
{"doc_id": "1910.10683", "para_id": 42, "text": "2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 43, "text": "• Coreference resolution (WNLI and WSC (Levesque et al., 2012))"}
{"doc_id": "1910.10683", "para_id": 44, "text": "• Sentence completion (COPA (Roemmele et al., 2011))"}
{"doc_id": "1910.10683", "para_id": 45, "text": "• Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))"}
{"doc_id": "1910.10683", "para_id": 46, "text": "• Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018),\nBoolQ (Clark et al., 2019))"}
{"doc_id": "1910.10683", "para_id": 47, "text": "8. https://www.tensorflow.org/datasets/catalog/c4\n9. https://www.tensorflow.org/datasets"}
{"doc_id": "1910.10683", "para_id": 48, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 49, "text": "We use the data sets as distributed by the GLUE and SuperGLUE benchmarks.\nFor\nsimplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly\nfor SuperGLUE) as a single task by concatenating all of the constituent data sets. As\nsuggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR)\ndata set (Rahman and Ng, 2012) in the combined SuperGLUE task.\nThe CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question-\nanswering task but was adapted for text summarization by Nallapati et al. (2016); we\nuse the non-anonymized version from See et al. (2017) as an abstractive summarization\ntask. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our\nexperiments, the model is fed the question and its context and asked to generate the answer\ntoken-by-token. For WMT English to German, we use the same training data as (Vaswani\net al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013\nas a validation set (Bojar et al., 2014). For English to French, we use the standard training\ndata from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to\nRomanian, which is a standard lower-resource machine translation benchmark, we use the\ntrain and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train\non English data, so in order to learn to translate a given model will need to learn to generate\ntext in a new language."}
{"doc_id": "1910.10683", "para_id": 50, "text": "In order to train a single model on the diverse set of tasks described above, we cast all of\nthe tasks we consider into a “text-to-text” format—that is, a task where the model is fed\nsome text for context or conditioning and is then asked to produce some output text. This\nframework provides a consistent training objective both for pre-training and fine-tuning.\nSpecifically, the model is trained with a maximum likelihood objective (using “teacher forcing”\n(Williams and Zipser, 1989)) regardless of the task. To specify which task the model should\nperform, we add a task-specific (text) prefix to the original input sequence before feeding it\nto the model."}
{"doc_id": "1910.10683", "para_id": 51, "text": "As an example, to ask the model to translate the sentence “That is good.” from English\nto German, the model would be fed the sequence “translate English to German: That is\ngood.” and would be trained to output “Das ist gut.” For text classification tasks, the\nmodel simply predicts a single word corresponding to the target label. For example, on the\nMNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies\n(“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With\nour preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis:\nMy feelings towards pigeons are filled with animosity.” with the corresponding target word\n“entailment”. Note that an issue arises if our model outputs text on a text classification\ntask that does not correspond to any of the possible labels (for example if the model\noutputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”,\nor “contradiction”). In this case, we always count the model’s output as wrong, though we\nnever observed this behavior in any of our trained models. Note that the choice of text prefix\nused for a given task is essentially a hyperparameter; we found that changing the exact\nwording of the prefix had limited impact and so did not perform extensive experiments into\ndifferent prefix choices. A diagram of our text-to-text framework with a few input/output"}
{"doc_id": "1910.10683", "para_id": 52, "text": "examples is shown in Figure 1. We provide full examples of preprocessed inputs for every\ntask we studied in Appendix D.\nOur text-to-text framework follows previous work that casts multiple NLP tasks into\na common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a\nbenchmark that uses a consistent question-answering format for a suite of ten NLP tasks.\nThe Natural Language Decathlon also stipulates that all models must be multi-task, i.e.\nare able to simultaneously tackle all of the tasks at once. We instead allow for separately\nfine-tuning the model on each individual task and use short task prefixes instead of an explicit\nquestion-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of\nlanguage models by feeding some input to the model as a prefix and then autoregressively\nsampling an output. For example, automatic summarization is done by feeding in a document\nfollowed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation)\nand then the summary is predicted via autoregressive decoding. We mainly consider models\nthat explicitly process an input with an encoder before generating an output with a separate\ndecoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar\net al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to\npossible output choices are appended to the input and the model is trained to extract the\ninput span corresponding to the correct choice. In contrast, our framework also allows for\ngenerative tasks like machine translation and abstractive summarization where it is not\npossible to enumerate all possible output choices.\nWe were able to straightforwardly cast all of the tasks we considered into a text-to-text\nformat with the exception of STS-B, which is a regression task where the goal is to predict\na similarity score between 1 and 5. We found that most of these scores were annotated\nin increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and\nconverted the result to a literal string representation of the number (e.g. the floating-point\nvalue 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a\nstring corresponding to a number between 1 and 5, we convert it to a floating-point value;\notherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B\nregression problem as a 21-class classification problem.\nSeparately, we also convert the Winograd tasks (WNLI from GLUE, WSC from Super-\nGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more\namenable to the text-to-text framework. Examples from the Winograd tasks consist of a\ntext passage containing an ambiguous pronoun that could refer to more than one of the noun\nphrases in the passage. For example, the passage might be “The city councilmen refused\nthe demonstrators a permit because they feared violence.”, which contains the ambiguous\npronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI,\nWSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in\nthe text passage and asking the model to predict the noun that it refers to. The example\nmentioned above would be transformed to the input “The city councilmen refused the\ndemonstrators a permit because *they* feared violence.” and the model would be trained to\npredict the target text “The city councilmen”.\nFor WSC, examples contain the passage, the ambiguous pronoun, a candidate noun,\nand a True/False label reflecting whether the candidate matches the pronoun (ignoring any\narticles). We only train on examples with a “True” label since we do not know the correct\nnoun targets for examples with a “False” label. For evaluation, we assign a “True” label if"}
{"doc_id": "1910.10683", "para_id": 53, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 54, "text": "the words in the model’s output are a subset of the words in the candidate noun phrase\n(or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC\ntraining set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples\nfrom DPR are annotated with the correct referent noun, making it easy to use this data set\nin the format listed above."}
{"doc_id": "1910.10683", "para_id": 55, "text": "The WNLI training and validation sets have a significant overlap with the WSC training\nset. To avoid leaking validation examples into our training data (a particular issue in the\nmulti-task experiments of Section 3.5.2), we therefore never train on WNLI and never report\nresults on the WNLI validation set. Omitting results on the WNLI validation set is standard\npractice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the\ntraining set, i.e. validation examples are all slightly-perturbed versions of training examples\nwith the opposite label. As such, we do not include WNLI in the average GLUE score\nwhenever we report on the validation set (all sections except Section 3.7 where results\nare presented on the test sets). Converting examples from WNLI to the “referent noun\nprediction” variant described above is a little more involved; we describe this process in\nAppendix B."}
{"doc_id": "1910.10683", "para_id": 56, "text": "Recent advances in transfer learning for NLP have come from a wide variety of developments,\nsuch as new pre-training objectives, model architectures, unlabeled data sets, and more.\nIn this section, we carry out an empirical survey of these techniques in hopes of teasing\napart their contribution and significance. We then combine the insights gained to attain\nstate-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly\ngrowing area of research, it is not feasible for us to cover every possible technique or idea\nin our empirical study. For a broader literature review, we recommend a recent survey by\nRuder et al. (2019)."}
{"doc_id": "1910.10683", "para_id": 57, "text": "We systematically study these contributions by taking a reasonable baseline (described\nin Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3\nwe measure the performance of different unsupervised objectives while keeping the rest of\nour experimental pipeline fixed. This “coordinate ascent” approach might miss second-order\neffects (for example, some particular unsupervised objective may work best on a model\nlarger than our baseline setting), but performing a combinatorial exploration of all of the\nfactors in our study would be prohibitively expensive. In future work, we expect it could be\nfruitful to more thoroughly consider combinations of the approaches we study."}
{"doc_id": "1910.10683", "para_id": 58, "text": "Our goal is to compare a variety of different approaches on a diverse set of tasks while\nkeeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do\nnot exactly replicate existing approaches. For example, “encoder-only” models like BERT\n(Devlin et al., 2018) are designed to produce a single prediction per input token or a single\nprediction for an entire input sequence. This makes them applicable for classification or span\nprediction tasks but not for generative tasks like translation or abstractive summarization.\nAs such, none of the model architectures we consider are identical to BERT or consist of an\nencoder-only structure. Instead, we test approaches that are similar in spirit—for example,\nwe consider an analogous objective to BERT’s “masked language modeling” objective in"}
{"doc_id": "1910.10683", "para_id": 59, "text": "Section 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7)."}
{"doc_id": "1910.10683", "para_id": 60, "text": "Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections."}
{"doc_id": "1910.10683", "para_id": 61, "text": "For our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single “stack” (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of 12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention\nmechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\nthis results in a model with about 220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied\nin the model."}
{"doc_id": "1910.10683", "para_id": 62, "text": "As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,"}
{"doc_id": "1910.10683", "para_id": 63, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 64, "text": "we “pack” multiple sequences into each entry of the batch10 so that our batches contain\nroughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds\nto pre-training on 235 ≈34B tokens. This is considerably less than BERT (Devlin et al.,\n2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly\n2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still\nproviding a sufficient amount of pre-training for acceptable performance. We consider the\neffect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\nDuring pre-training, we use an “inverse square root” learning rate schedule: 1\n\u000ep"}
{"doc_id": "1910.10683", "para_id": 65, "text": "max(n, k)\nwhere n is the current training iteration and k is the number of warm-up steps (set to 104"}
{"doc_id": "1910.10683", "para_id": 66, "text": "in all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps,\nthen exponentially decays the learning rate until pre-training is over. We also experimented\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\nbetter results but requires knowing the total number of training steps ahead of time. Since\nwe will be varying the number of training steps in some of our experiments, we opt for the\nmore generic inverse square root schedule.\nOur models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen\nas a trade-off between the high-resource tasks (i.e. those with large data sets), which\nbenefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit\nquickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.\n216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save\na checkpoint every 5,000 steps and report results on the model checkpoint corresponding\nto the highest validation performance. For models fine-tuned on multiple tasks, we choose\nthe best checkpoint for each task independently. For all of the experiments except those in\nSection 3.7, we report results in the validation set to avoid performing model selection on\nthe test set."}
{"doc_id": "1910.10683", "para_id": 67, "text": "We use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens\n(Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000\nwordpieces. Since we ultimately fine-tune our model on English to German, French, and\nRomanian translation, we also require that our vocabulary covers these non-English languages.\nTo address this, we classified pages from the Common Crawl scrape used in C4 as German,\nFrench, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts\nof English C4 data with 1 part each of data classified as German, French or Romanian.\nThis vocabulary was shared across both the input and output of our model. Note that\nour vocabulary makes it so that our model can only process a predetermined, fixed set of\nlanguages."}
{"doc_id": "1910.10683", "para_id": 68, "text": "Leveraging unlabeled data to pre-train our model necessitates an objective that does not\nrequire labels but (loosely speaking) teaches the model generalizable knowledge that will be"}
{"doc_id": "1910.10683", "para_id": 69, "text": "10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\nindex.html#data_generators.generator_utils.pack_examples"}
{"doc_id": "1910.10683", "para_id": 70, "text": "Figure 2: Schematic of the objective we use in our baseline model. In this example, we\nprocess the sentence “Thank you for inviting me to your party last week.” The\nwords “for”, “inviting” and “last” (marked with an ×) are randomly chosen for\ncorruption. Each consecutive span of corrupted tokens is replaced by a sentinel\ntoken (shown as <X> and <Y>) that is unique over the example. Since “for” and\n“inviting” occur consecutively, they are replaced by a single sentinel <X>. The\noutput sequence then consists of the dropped-out spans, delimited by the sentinel\ntokens used to replace them in the input plus a final sentinel token <Z>."}
{"doc_id": "1910.10683", "para_id": 71, "text": "useful in downstream tasks. Preliminary work that applied the transfer learning paradigm\nof pre-training and fine-tuning all of the model’s parameters to NLP problems used a\ncausal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that\n“denoising” objectives (Devlin et al., 2018; Taylor, 1953) (also called “masked language\nmodeling”) produce better performance and as a result they have quickly become standard.\nIn a denoising objective, the model is trained to predict missing or otherwise corrupted\ntokens in the input. Inspired by BERT’s “masked language modeling” objective and the\n“word dropout” regularization technique (Bowman et al., 2015), we design an objective that\nrandomly samples and then drops out 15% of tokens in the input sequence. All consecutive\nspans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token\nis assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens\nwhich are added to our vocabulary and do not correspond to any wordpiece. The target\nthen corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel\ntokens used in the input sequence plus a final sentinel token to mark the end of the target\nsequence. Our choices to mask consecutive spans of tokens and only predict dropped-out\ntokens were made to reduce the computational cost of pre-training. We perform thorough\ninvestigation into pre-training objectives in Section 3.3. An example of the transformation\nresulting from applying this objective is shown in Figure 2. We empirically compare this\nobjective to many other variants in Section 3.3."}
{"doc_id": "1910.10683", "para_id": 72, "text": "In this section, we present results using the baseline experimental procedure described above\nto get a sense of what kind of performance to expect on our suite of downstream tasks.\nIdeally, we would repeat every experiment in our study multiple times to get a confidence\ninterval on our results. Unfortunately, this would be prohibitively expensive due to the large"}
{"doc_id": "1910.10683", "para_id": 73, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 74, "text": "⋆Baseline average\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline standard deviation\n0.235\n0.065\n0.343\n0.416\n0.112\n0.090\n0.108\nNo pre-training\n66.22\n17.60\n50.31\n53.04\n25.86\n39.77\n24.04"}
{"doc_id": "1910.10683", "para_id": 75, "text": "Table 1: Average and standard deviation of scores achieved by our baseline model and\ntraining procedure. For comparison, we also report performance when training on\neach task from scratch (i.e. without any pre-training) for the same number of steps\nused to fine-tune the baseline model. All scores in this table (and every table in\nour paper except Table 14) are reported on the validation sets of each data set."}
{"doc_id": "1910.10683", "para_id": 76, "text": "number of experiments we run. As a cheaper alternative, we train our baseline model 10\ntimes from scratch (i.e. with different random initializations and data set shuffling) and\nassume that the variance over these runs of the base model also applies to each experimental\nvariant. We don’t expect most of the changes we make to have a dramatic effect on the\ninter-run variance, so this should provide a reasonable indication of the significance of\ndifferent changes. Separately, we also measure the performance of training our model for 218"}
{"doc_id": "1910.10683", "para_id": 77, "text": "steps (the same number we use for fine-tuning) on all downstream tasks without pre-training.\nThis gives us an idea of how much pre-training benefits our model in the baseline setting.\nWhen reporting results in the main text, we only report a subset of the scores across all\nthe benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we\nreport the average score across all subtasks (as stipulated by the official benchmarks) under\nthe headings “GLUE” and “SGLUE”. For all translation tasks, we report the BLEU score\n(Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with “exp” smoothing\nand “intl” tokenization. We refer to scores for WMT English to German, English to French,\nand English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail,\nwe find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F\nmetrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under\nthe heading “CNNDM”. Similarly, for SQuAD we find the performance of the “exact match”\nand “F1” scores to be highly correlated so we report the “exact match” score alone. We\nprovide every score achieved on every task for all experiments in Table 16, Appendix E.\nOur results tables are all formatted so that each row corresponds to a particular experi-\nmental configuration with columns giving the scores for each benchmark. We will include\nthe mean performance of the baseline configuration in most tables. Wherever a baseline\nconfiguration appears, we will mark it with a ⋆(as in the first row of Table 1). We also\nwill boldface any score that is within two standard deviations of the maximum (best) in a\ngiven experiment.\nOur baseline results are shown in Table 1. Overall, our results are comparable to existing\nmodels of similar size. For example, BERTBASE achieved an exact match score of 80.8\non SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and\n84.24, respectively (see Table 16). Note that we cannot directly compare our baseline to\nBERTBASE because ours is an encoder-decoder model and was pre-trained for roughly 1⁄4\nas many steps. Unsurprisingly, we find that pre-training provides significant gains across\nalmost all benchmarks. The only exception is WMT English to French, which is a large"}
{"doc_id": "1910.10683", "para_id": 78, "text": "enough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and “no pre-training” emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data efficiency in this paper, we emphasize that this is one of the primary benefits of the\ntransfer learning paradigm.\nAs for inter-run variance, we find that for most tasks the standard deviation across runs\nis smaller than 1% of the task’s baseline score. Exceptions to this rule include CoLA, CB,\nand COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks.\nFor example, on CB our baseline model had an average F1 score of 91.22 with a standard\ndeviation of 3.237 (see Table 16), which may be partly due to the fact that CB’s validation\nset contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed\nas the average of scores across the tasks comprising each benchmark. As a result, we caution\nthat the high inter-run variance of CoLA, CB, and COPA can make it harder to compare\nmodels using the GLUE and SuperGLUE scores alone."}
{"doc_id": "1910.10683", "para_id": 79, "text": "While the Transformer was originally introduced with an encoder-decoder architecture, much\nmodern work on transfer learning for NLP uses alternative architectures. In this section, we\nreview and compare these architectural variants."}
{"doc_id": "1910.10683", "para_id": 80, "text": "A major distinguishing factor for different architectures is the “mask” used by different\nattention mechanisms in the model. Recall that the self-attention operation in a Transformer\ntakes a sequence as input and outputs a new sequence of the same length. Each entry of\nthe output sequence is produced by computing a weighted average of entries of the input\nsequence. Specifically, let yi refer to the ith element of the output sequence and xj refer to\nthe jth entry of the input sequence. yi is computed as P\nj wi,jxj, where wi,j is the scalar\nweight produced by the self-attention mechanism as a function of xi and xj. The attention\nmask is then used to zero out certain weights in order to constrain which entries of the input\ncan be attended to at a given output timestep. Diagrams of the masks we will consider are\nshown in Figure 3. For example, the causal mask (Figure 3, middle) sets any wi,j to zero if\nj > i.\nThe first model structure we consider is an an encoder-decoder Transformer, which\nconsists of two layer stacks: The encoder, which is fed an input sequence, and the decoder,\nwhich produces a new output sequence. A schematic of this architectural variant is shown\nin the left panel of Figure 4.\nThe encoder uses a “fully-visible” attention mask. Fully-visible masking allows a self-\nattention mechanism to attend to any entry of the input when producing each entry of\nits output. We visualize this masking pattern in Figure 3, left. This form of masking is\nappropriate when attending over a “prefix”, i.e. some context provided to the model that\nis later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible\nmasking pattern and appends a special “classification” token to the input. BERT’s output"}
{"doc_id": "1910.10683", "para_id": 81, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 82, "text": "Figure 3: Matrices representing different attention mask patterns. The input and output\nof the self-attention mechanism are denoted x and y respectively. A dark cell\nat row i and column j indicates that the self-attention mechanism is allowed to\nattend to input element j at output timestep i. A light cell indicates that the\nself-attention mechanism is not allowed to attend to the corresponding i and j\ncombination. Left: A fully-visible mask allows the self-attention mechanism to\nattend to the full input at every output timestep. Middle: A causal mask prevents\nthe ith output element from depending on any input elements from “the future”.\nRight: Causal masking with a prefix allows the self-attention mechanism to use\nfully-visible masking on a portion of the input sequence."}
{"doc_id": "1910.10683", "para_id": 83, "text": "at the timestep corresponding to the classification token is then used to make a prediction\nfor classifying the input sequence.\nThe self-attention operations in the Transformer’s decoder use a “causal” masking pattern.\nWhen producing the ith entry of the output sequence, causal masking prevents the model\nfrom attending to the jth entry of the input sequence for j > i. This is used during training\nso that the model can’t “see into the future” as it produces its output. An attention matrix\nfor this masking pattern is shown in Figure 3, middle.\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an\noutput sequence. That is, at each output timestep, a token is sampled from the model’s\npredicted distribution and the sample is fed back into the model to produce a prediction for\nthe next output timestep, and so on. As such, a Transformer decoder (without an encoder)\ncan be used as a language model (LM), i.e. a model trained solely for next-step prediction\n(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second\nmodel structure we consider. A schematic of this architecture is shown in Figure 4, middle.\nIn fact, early work on transfer learning for NLP used this architecture with a language\nmodeling objective as a pre-training method (Radford et al., 2018).\nLanguage models are typically used for compression or sequence generation (Graves,\n2013). However, they can also be used in the text-to-text framework simply by concatenating\nthe inputs and targets. As an example, consider the case of English to German translation:\nIf we have a training datapoint with input sentence “That is good.” and target “Das ist\ngut.”, we would simply train the model on next-step prediction over the concatenated input\nsequence “translate English to German: That is good. target: Das ist gut.” If we wanted to"}
{"doc_id": "1910.10683", "para_id": 84, "text": "Figure 4: Schematics of the Transformer architecture variants we consider. In this diagram,\nblocks represent elements of a sequence and lines represent attention visibility.\nDifferent colored groups of blocks indicate different Transformer layer stacks. Dark\ngrey lines correspond to fully-visible masking and light grey lines correspond\nto causal masking. We use “.” to denote a special end-of-sequence token that\nrepresents the end of a prediction. The input and output sequences are represented\nas x and y respectively. Left: A standard encoder-decoder architecture uses fully-\nvisible masking in the encoder and the encoder-decoder attention, with causal\nmasking in the decoder. Middle: A language model consists of a single Transformer\nlayer stack and is fed the concatenation of the input and target, using a causal\nmask throughout. Right: Adding a prefix to a language model corresponds to\nallowing fully-visible masking over the input."}
{"doc_id": "1910.10683", "para_id": 85, "text": "obtain the model’s prediction for this example, the model would be fed the prefix “translate\nEnglish to German: That is good. target:” and would be asked to generate the remainder\nof the sequence autoregressively. In this way, the model can predict an output sequence\ngiven an input, which satisfies the needs of text-to-text tasks. This approach was recently\nused to show that language models can learn to perform some text-to-text tasks without\nsupervision (Radford et al., 2019)."}
{"doc_id": "1910.10683", "para_id": 86, "text": "A fundamental and frequently cited drawback of using a language model in the text-\nto-text setting is that causal masking forces the model’s representation of the ith entry of\nthe input sequence to only depend on the entries up until i. To see why this is potentially\ndisadvantageous, consider the text-to-text framework where the model is provided with a\nprefix/context before being asked to make predictions (e.g., the prefix is an English sentence\nand the model is asked to predict the German translation). With fully causal masking, the\nmodel’s representation of a prefix state can only depend on prior entries of the prefix. So,\nwhen predicting an entry of the output, the model will attend to a representation of the\nprefix that is unnecessarily limited. Similar arguments have been made against using a\nunidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau\net al., 2015)."}
{"doc_id": "1910.10683", "para_id": 87, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 88, "text": "This issue can be avoided in a Transformer-based language model simply by changing\nthe masking pattern. Instead of using a causal mask, we use fully-visible masking during\nthe prefix portion of the sequence. This masking pattern and a schematic of the resulting\n“prefix LM” (the third model structure we consider) are illustrated in the rightmost panels of\nFigures 3 and 4, respectively. In the English to German translation example mentioned above,\nfully-visible masking would be applied to the prefix “translate English to German: That is\ngood. target:” and causal masking would be used during training for predicting the target\n“Das ist gut.” Using a prefix LM in the text-to-text framework was originally proposed by"}
{"doc_id": "1910.10683", "para_id": 89, "text": "Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective\non a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder\nmodel with parameters shared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target sequence."}
{"doc_id": "1910.10683", "para_id": 90, "text": "We note that when following our text-to-text framework, the prefix LM architecture\nclosely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an\nexample from the MNLI benchmark where the premise is “I hate pigeons.”, the hypothesis is\n“My feelings towards pigeons are filled with animosity.” and the correct label is “entailment”.\nTo feed this example into a language model, we would transform it into the sequence “mnli\npremise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\ntarget: entailment”. In this case, the fully-visible prefix would correspond to the entire input\nsequence up to the word “target:”, which can be seen as being analogous to the “classification”\ntoken used in BERT. So, our model would have full visibility over the entire input, and then\nwould be tasked with making a classification by outputting the word “entailment”. It is easy\nfor the model to learn to output one of the valid class labels given the task prefix (“mnli” in\nthis case). As such, the main difference between a prefix LM and the BERT architecture is\nthat the classifier is simply integrated into the output layer of the Transformer decoder in\nthe prefix LM."}
{"doc_id": "1910.10683", "para_id": 91, "text": "In the interest of experimentally comparing these architectural variants, we would like each\nmodel we consider to be equivalent in some meaningful way. We might say that two models\nare equivalent if they either have the same number of parameters or they require roughly\nthe same amount of computation to process a given (input-sequence, target-sequence) pair.\nUnfortunately, it is not possible to compare an encoder-decoder model to a language model\narchitecture (comprising a single Transformer stack) according to both of these criteria\nat the same time. To see why, first note an encoder-decoder model with L layers in the\nencoder and L layers in the decoder has approximately the same number of parameters as a\nlanguage model with 2L layers. However, the same L + L encoder-decoder model will have\napproximately the same computational cost as a language model with only L layers. This\nis a consequence of the fact that the L layers in the language model must be applied to\nboth the input and output sequence, while the encoder is only applied to the input sequence\nand the decoder is only applied to the output sequence. Note that these equivalences are\napproximate—there are some extra parameters in the decoder due to the encoder-decoder\nattention and there are also some computational costs in the attention layers that are\nquadratic in the sequence lengths. In practice, however, we observed nearly identical step"}
{"doc_id": "1910.10683", "para_id": 92, "text": "times for L-layer language models versus L + L-layer encoder-decoder models, suggesting a\nroughly equivalent computational cost. Further, for the model sizes we consider, the number\nof parameters in the encoder-decoder attention layers is about 10% of the total parameter\ncount, so we make the simplifying assumption that an L + L-layer encoder-decoder model\nhas the same number of parameters as an 2L-layer language model.\nTo provide a reasonable means of comparison, we consider multiple configurations for\nour encoder-decoder model. We will refer to the number of layers and parameters in a\nBERTBASE-sized layer stack as L and P, respectively. We will use M to refer to the number\nof FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model\nto process a given input-target pair. In total, we will compare:"}
{"doc_id": "1910.10683", "para_id": 93, "text": "• An encoder-decoder model with L layers in the encoder and L layers in the decoder.\nThis model has 2P parameters and a computation cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 94, "text": "• An equivalent model, but with parameters shared across the encoder and decoder,\nresulting in P parameters and an M-FLOP computational cost."}
{"doc_id": "1910.10683", "para_id": 95, "text": "• An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P\nparameters and an M/2-FLOP cost."}
{"doc_id": "1910.10683", "para_id": 96, "text": "• A decoder-only language model with L layers and P parameters and a resulting\ncomputational cost of M FLOPs."}
{"doc_id": "1910.10683", "para_id": 97, "text": "• A decoder-only prefix LM with the same architecture (and thus the same number\nof parameters and computational cost), but with fully-visible self-attention over the\ninput."}
{"doc_id": "1910.10683", "para_id": 98, "text": "As an unsupervised objective, we will consider both a basic language modeling objective as\nwell as our baseline denoising objective described in Section 3.1.4. We include the language\nmodeling objective due to its historic use as a pre-training objective (Dai and Le, 2015;\nRamachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,\n2018) as well as its natural fit for the language model architectures we consider. For models\nthat ingest a prefix before making predictions (the encoder-decoder model and prefix LM),\nwe sample a span of text from our unlabeled data set and choose a random point to split\nit into prefix and target portions. For the standard language model, we train the model\nto predict the entire span from beginning to end. Our unsupervised denoising objective is\ndesigned for text-to-text models; to adapt it for use with a language model we concatenate\nthe inputs and targets as described in Section 3.2.1."}
{"doc_id": "1910.10683", "para_id": 99, "text": "The scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count (2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in"}
{"doc_id": "1910.10683", "para_id": 100, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 101, "text": "Architecture\nObjective\nParams\nCost\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 102, "text": "⋆Encoder-decoder\nDenoising\n2P\nM\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nEnc-dec, shared\nDenoising\nP\nM\n82.81\n18.78\n80.63\n70.73\n26.72\n39.03\n27.46\nEnc-dec, 6 layers\nDenoising\nP\nM/2\n80.88\n18.97\n77.59\n68.42\n26.38\n38.40\n26.95\nLanguage model\nDenoising\nP\nM\n74.70\n17.93\n61.14\n55.02\n25.09\n35.28\n25.86\nPrefix LM\nDenoising\nP\nM\n81.82\n18.61\n78.94\n68.11\n26.43\n37.98\n27.39"}
{"doc_id": "1910.10683", "para_id": 103, "text": "Encoder-decoder\nLM\n2P\nM\n79.56\n18.59\n76.02\n64.29\n26.27\n39.17\n26.86\nEnc-dec, shared\nLM\nP\nM\n79.60\n18.13\n76.35\n63.50\n26.62\n39.17\n27.05\nEnc-dec, 6 layers\nLM\nP\nM/2\n78.67\n18.26\n75.32\n64.06\n26.13\n38.42\n26.89\nLanguage model\nLM\nP\nM\n73.78\n17.54\n53.81\n56.51\n25.23\n34.31\n25.38\nPrefix LM\nLM\nP\nM\n79.68\n17.84\n76.87\n64.86\n26.28\n37.51\n26.76"}
{"doc_id": "1910.10683", "para_id": 104, "text": "Table 2: Performance of the different architectural variants described in Section 3.2.2. We\nuse P to refer to the number of parameters in a 12-layer base Transformer layer\nstack and M to refer to the FLOPs required to process a sequence using the encoder-\ndecoder model. We evaluate each architectural variant using a denoising objective\n(described in Section 3.1.4) and an autoregressive objective (as is commonly used\nto train language models)."}
{"doc_id": "1910.10683", "para_id": 105, "text": "the encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al.,\n2019) also found that sharing parameters across Transformer blocks can be an effective means\nof lowering the total parameter count without sacrificing much performance. XLNet also\nbears some resemblance to the shared encoder-decoder approach with a denoising objective\n(Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms\nthe decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder\nattention is beneficial. Finally, we confirm the widely-held conception that using a denoising\nobjective always results in better downstream task performance compared to a language\nmodeling objective. This observation has been previously made by Devlin et al. (2018),\nVoita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more\ndetailed exploration of unsupervised objectives in the following section."}
{"doc_id": "1910.10683", "para_id": 106, "text": "The choice of unsupervised objective is of central importance as it provides the mechanism\nthrough which the model gains general-purpose knowledge to apply to downstream tasks.\nThis has led to the development of a wide variety of pre-training objectives (Dai and Le,\n2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019;\nLiu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019).\nIn this section, we perform a procedural exploration of the space of unsupervised objectives.\nIn many cases, we will not replicate an existing objective exactly—some will be modified to\nfit our text-to-text encoder-decoder framework and, in other cases, we will use objectives\nthat combine concepts from multiple common approaches.\nOverall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized\nspan of text from our unlabeled text data set. The token sequence is processed to produce a\n(corrupted) input sequence and a corresponding target. Then, the model is trained as usual"}
{"doc_id": "1910.10683", "para_id": 107, "text": "Prefix language modeling\nThank you for inviting\nme to your party last week .\nBERT-style Devlin et al. (2018)\nThank you <M> <M> me to your party apple week .\n(original text)\nDeshuffling\nparty me for your to . last fun you inviting week Thank\n(original text)\nMASS-style Song et al. (2019)\nThank you <M> <M> me to your party <M> week .\n(original text)\nI.i.d. noise, replace spans\nThank you <X> me to your party <Y> week .\n<X> for inviting <Y> last <Z>\nI.i.d. noise, drop tokens\nThank you me to your party week .\nfor inviting last\nRandom spans\nThank you <X> to <Y> week .\n<X> for inviting me <Y> your party last <Z>"}
{"doc_id": "1910.10683", "para_id": 108, "text": "Table 3: Examples of inputs and targets produced by some of the unsupervised objectives\nwe consider applied to the input text “Thank you for inviting me to your party last\nweek .” Note that all of our objectives process tokenized text. For this particular\nsentence, all words were mapped to a single token by our vocabulary. We write\n(original text) as a target to denote that the model is tasked with reconstructing the\nentire input text. <M> denotes a shared mask token and <X>, <Y>, and <Z> denote\nsentinel tokens that are assigned unique token IDs. The BERT-style objective\n(second row) includes a corruption where some tokens are replaced by a random\ntoken ID; we show this via the greyed-out word apple."}
{"doc_id": "1910.10683", "para_id": 109, "text": "with maximum likelihood to predict the target sequence. We provide illustrative examples\nof many of the objectives we consider in Table 3."}
{"doc_id": "1910.10683", "para_id": 110, "text": "To begin with, we compare three techniques that are inspired by commonly-used objectives\nbut differ significantly in their approach. First, we include a basic “prefix language modeling”\nobjective as was used in Section 3.2.3.\nThis technique splits a span of text into two\ncomponents, one to use as inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder. Second, we consider an objective inspired by the “masked\nlanguage modeling” (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a\nspan of text and corrupts 15% of the tokens. 90% of the corrupted tokens are replaced\nwith a special mask token and 10% are replaced with a random token. Since BERT is an\nencoder-only model, its goal during pre-training is to reconstruct masked tokens at the\noutput of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted\nsequence as the target. Note that this differs from our baseline objective, which uses only\nthe corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally,\nwe also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was\napplied to a denoising sequential autoencoder. This approach takes a sequence of tokens,\nshuffles it, and then uses the original deshuffled sequence as a target. We provide examples\nof the inputs and targets for these three methods in the first three rows of Table 3.\nThe performance of these three objectives is shown in Table 4. Overall, we find that the\nBERT-style objective performs best, though the prefix language modeling objective attains\nsimilar performance on the translation tasks. Indeed, the motivation for the BERT objective\nwas to outperform language model-based pre-training. The deshuffling objective performs\nconsiderably worse than both prefix language modeling and the BERT-style objective."}
{"doc_id": "1910.10683", "para_id": 111, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 112, "text": "Prefix language modeling\n80.69\n18.94\n77.99\n65.27\n26.86\n39.73\n27.49\nBERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nDeshuffling\n73.17\n18.59\n67.61\n58.47\n26.11\n39.30\n25.62"}
{"doc_id": "1910.10683", "para_id": 113, "text": "Table 4: Performance of the three disparate pre-training objectives described in Section 3.3.1."}
{"doc_id": "1910.10683", "para_id": 114, "text": "BERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nMASS-style (Song et al., 2019)\n82.32\n19.16\n80.10\n69.28\n26.79\n39.89\n27.55\n⋆Replace corrupted spans\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nDrop corrupted tokens\n84.44\n19.31\n80.52\n68.67\n27.07\n39.76\n27.82"}
{"doc_id": "1910.10683", "para_id": 115, "text": "Table 5: Comparison of variants of the BERT-style pre-training objective. In the first two\nvariants, the model is trained to reconstruct the original uncorrupted text segment.\nIn the latter two, the model only predicts the sequence of corrupted tokens."}
{"doc_id": "1910.10683", "para_id": 116, "text": "Based on the results in the prior section, we will now focus on exploring modifications to\nthe BERT-style denoising objective. This objective was originally proposed as a pre-training\ntechnique for an encoder-only model trained for classification and span prediction. As\nsuch, it may be possible to modify it so that it performs better or is more efficient in our\nencoder-decoder text-to-text setup.\nFirst, we consider a simple variant of the BERT-style objective where we don’t include the\nrandom token swapping step. The resulting objective simply replaces 15% of the tokens in\nthe input with a mask token and the model is trained to reconstruct the original uncorrupted\nsequence. A similar masking objective was used by Song et al. (2019) where it was referred to\nas “MASS”, so we call this variant the “MASS-style” objective. Second, we were interested\nto see if it was possible to avoid predicting the entire uncorrupted text span since this\nrequires self-attention over long sequences in the decoder. We consider two strategies to\nachieve this: First, instead of replacing each corrupted token with a mask token, we replace\nthe entirety of each consecutive span of corrupted tokens with a unique mask token. Then,\nthe target sequence becomes the concatenation of the “corrupted” spans, each prefixed by\nthe mask token used to replace it in the input. This is the pre-training objective we use in\nour baseline, described in Section 3.1.4. Second, we also consider a variant where we simply\ndrop the corrupted tokens from the input sequence completely and task the model with\nreconstructing the dropped tokens in order. Examples of these approaches are shown in the\nfifth and sixth rows of Table 3.\nAn empirical comparison of the original BERT-style objective to these three alternatives\nis shown in Table 5. We find that in our setting, all of these variants perform similarly. The\nonly exception was that dropping corrupted tokens completely produced a small improvement\nin the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our"}
{"doc_id": "1910.10683", "para_id": 117, "text": "Corruption rate\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 118, "text": "10%\n82.82\n19.00\n80.38\n69.55\n26.87\n39.28\n27.44\n⋆15%\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n25%\n83.00\n19.54\n80.96\n70.48\n27.04\n39.83\n27.47\n50%\n81.27\n19.32\n79.80\n70.33\n27.01\n39.90\n27.49"}
{"doc_id": "1910.10683", "para_id": 119, "text": "Table 6:\nPerformance of the i.i.d. corruption objective with different corruption rates."}
{"doc_id": "1910.10683", "para_id": 120, "text": "baseline average of 53.84, see Table 16). This may be due to the fact that CoLA involves\nclassifying whether a given sentence is grammatically and syntactically acceptable, and\nbeing able to determine when tokens are missing is closely related to detecting acceptability.\nHowever, dropping tokens completely performed worse than replacing them with sentinel\ntokens on SuperGLUE. The two variants that do not require predicting the full original\nsequence (“replace corrupted spans” and “drop corrupted spans”) are both potentially\nattractive since they make the target sequences shorter and consequently make training\nfaster. Going forward, we will explore variants where we replace corrupted spans with\nsentinel tokens and only predict the corrupted tokens (as in our baseline objective)."}
{"doc_id": "1910.10683", "para_id": 121, "text": "So far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al.,\n2018). Again, since our text-to-text framework differs from BERT’s, we are interested to\nsee if a different corruption rate works better for us. We compare corruption rates of 10%,\n15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited\neffect on the model’s performance. The only exception is that the largest corruption rate we\nconsider (50%) results in a significant degradation of performance on GLUE and SQuAD.\nUsing a larger corruption rate also results in longer targets, which can potentially slow down\ntraining. Based on these results and the historical precedent set by BERT, we will use a\ncorruption rate of 15% going forward."}
{"doc_id": "1910.10683", "para_id": 122, "text": "We now turn towards the goal of speeding up training by predicting shorter targets. The\napproach we have used so far makes an i.i.d. decision for each input token as to whether\nto corrupt it or not. When multiple consecutive tokens have been corrupted, they are\ntreated as a “span” and a single unique mask token is used to replace the entire span.\nReplacing entire spans with a single token results in unlabeled text data being processed into\nshorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case\nthat a significant number of corrupted tokens appear consecutively. As a result, we might\nobtain additional speedup by specifically corrupting spans of tokens rather than corrupting\nindividual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a\npre-training objective for BERT, where it was found to improve performance (Joshi et al.,\n2019).\nTo test this idea, we consider an objective that specifically corrupts contiguous, randomly-\nspaced spans of tokens. This objective can be parametrized by the proportion of tokens to\nbe corrupted and the total number of corrupted spans. The span lengths are then chosen"}
{"doc_id": "1910.10683", "para_id": 123, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 124, "text": "⋆Baseline (i.i.d.)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n2\n83.54\n19.39\n82.09\n72.20\n26.76\n39.99\n27.63\n3\n83.49\n19.62\n81.84\n72.53\n26.86\n39.65\n27.62\n5\n83.40\n19.24\n82.05\n72.23\n26.88\n39.40\n27.53\n10\n82.85\n19.33\n81.84\n70.44\n26.79\n39.49\n27.69"}
{"doc_id": "1910.10683", "para_id": 125, "text": "Table 7: Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for\ndifferent average span lengths. In all cases, we corrupt 15% of the original text\nsequence."}
{"doc_id": "1910.10683", "para_id": 126, "text": "randomly to satisfy these specified parameters. For example, if we are processing a sequence\nof 500 tokens and we have specified that 15% of tokens should be corrupted and that there\nshould be 25 total spans, then the total number of corrupted tokens would be 500×0.15 = 75\nand the average span length would be 75/25 = 3. Note that given the original sequence\nlength and corruption rate, we can equivalently parametrize this objective either by the\naverage span length or the total number of spans.\nWe compare the span-corruption objective to the i.i.d-corruption objective in Table 7.\nWe use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3,\n5 and 10. Again, we find a limited difference between these objectives, though the version\nwith an average span length of 10 slightly underperforms the other values in some cases.\nWe also find in particular that using an average span length of 3 slightly (but significantly)\noutperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the\nspan-corruption objective also provides some speedup during training compared to the i.i.d.\nnoise approach because span corruption produces shorter sequences on average."}
{"doc_id": "1910.10683", "para_id": 127, "text": "Figure 5 shows a flow chart of the choices made during our exploration of unsupervised\nobjectives. Overall, the most significant difference in performance we observed was that\ndenoising objectives outperformed language modeling and deshuffling for pre-training. We\ndid not observe a remarkable difference across the many variants of the denoising objectives\nwe explored. However, different objectives (or parameterizations of objectives) can lead to\ndifferent sequence lengths and thus different training speeds. This implies that choosing\namong the denoising objectives we considered here should mainly be done according to\ntheir computational cost. Our results also suggest that additional exploration of objectives\nsimilar to the ones we consider here may not lead to significant gains for the tasks and model\nwe consider. Instead, it may be fortuitous to explore entirely different ways of leveraging\nunlabeled data."}
{"doc_id": "1910.10683", "para_id": 128, "text": "Like the unsupervised objective, the pre-training data set itself is a crucial component of\nthe transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training\ndata sets are usually not treated as significant contributions on their own and are often not"}
{"doc_id": "1910.10683", "para_id": 129, "text": "Figure 5: A flow chart of our exploration of unsupervised objectives. We first consider a\nfew disparate approaches in Section 3.3.1 and find that a BERT-style denoising\nobjective performs best. Then, we consider various methods for simplifying the\nBERT objective so that it produces shorter target sequences in Section 3.3.2.\nGiven that replacing dropped-out spans with sentinel tokens performs well and\nresults in short target sequences, in Section 3.3.3 we experiment with different\ncorruption rates. Finally, we evaluate an objective that intentionally corrupts\ncontiguous spans of tokens in Section 3.3.4."}
{"doc_id": "1910.10683", "para_id": 130, "text": "released alongside pre-trained models and code. Instead, they are typically introduced in\nthe course of presenting a new method or model. As a result, there has been relatively little\ncomparison of different pre-training data sets as well as a lack of a “standard” data set used\nfor pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c;\nYang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced)\ndata set to using a smaller preexisting data set (often Wikipedia). To probe more deeply\ninto the impact of the pre-training data set on performance, in this section we compare\nvariants of our C4 data set and other potential sources of pre-training data. We release all\nof the C4 data set variants we consider as part of TensorFlow Datasets.11"}
{"doc_id": "1910.10683", "para_id": 131, "text": "In creating C4, we developed various heuristics to filter the web-extracted text from Common\nCrawl (see Section 2.2 for a description). We are interested in measuring whether this\nfiltering results in improved performance on downstream tasks, in addition to comparing\nit to other filtering approaches and common pre-training data sets. Towards this end, we\ncompare the performance of our baseline model after pre-training on the following data sets:"}
{"doc_id": "1910.10683", "para_id": 132, "text": "C4 As a baseline, we first consider pre-training on our proposed unlabeled data set as\ndescribed in Section 2.2."}
{"doc_id": "1910.10683", "para_id": 133, "text": "Unfiltered C4 To measure the effect of the heuristic filtering we used in creating C4\n(deduplication, removing bad words, only retaining sentences, etc.), we also generate\nan alternate version of C4 that forgoes this filtering. Note that we still use langdetect"}
{"doc_id": "1910.10683", "para_id": 134, "text": "11. https://www.tensorflow.org/datasets/catalog/c4"}
{"doc_id": "1910.10683", "para_id": 135, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 136, "text": "to extract English text. As a result, our “unfiltered” variant still includes some filtering\nbecause langdetect sometimes assigns a low probability to non-natural English text."}
{"doc_id": "1910.10683", "para_id": 137, "text": "RealNews-like Recent work has used text data extracted from news websites (Zellers\net al., 2019; Baevski et al., 2019). To compare to this approach, we generate another\nunlabeled data set by additionally filtering C4 to only include content from one of the\ndomains used in the “RealNews” data set (Zellers et al., 2019). Note that for ease of\ncomparison, we retain the heuristic filtering methods used in C4; the only difference is\nthat we have ostensibly omitted any non-news content."}
{"doc_id": "1910.10683", "para_id": 138, "text": "WebText-like Similarly, the WebText data set (Radford et al., 2019) only uses content\nfrom webpages that were submitted to the content aggregation website Reddit and\nreceived a “score” of at least 3. The score for a webpage submitted to Reddit is\ncomputed based on the proportion of users who endorse (upvote) or oppose (downvote)\nthe webpage. The idea behind using the Reddit score as a quality signal is that users\nof the site would only upvote high-quality text content. To generate a comparable data\nset, we first tried removing all content from C4 that did not originate from a URL that\nappeared in the list prepared by the OpenWebText effort.12 However, this resulted in\ncomparatively little content—only about 2 GB—because most pages never appear on\nReddit. Recall that C4 was created based on a single month of Common Crawl data.\nTo avoid using a prohibitively small data set, we therefore downloaded 12 months\nof data from Common Crawl from August 2018 to July 2019, applied our heuristic\nfiltering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like\ndata set, which is of comparable size to the original 40GB WebText data set (Radford\net al., 2019)."}
{"doc_id": "1910.10683", "para_id": 139, "text": "Wikipedia The website Wikipedia consists of millions of encyclopedia articles written\ncollaboratively. The content on the site is subject to strict quality guidelines and\ntherefore has been used as a reliable source of clean and natural text. We use the\nEnglish Wikipedia text data from TensorFlow Datasets,13 which omits any markup or\nreference sections from the articles."}
{"doc_id": "1910.10683", "para_id": 140, "text": "Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia\nis that it represents only one possible domain of natural text (encyclopedia articles).\nTo mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the\nToronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from\neBooks, which represents a different domain of natural language. BERT’s popularity\nhas led to the Wikipedia + TBC combination being used in many subsequent works."}
{"doc_id": "1910.10683", "para_id": 141, "text": "The results achieved after pre-training on each of these data sets is shown in Table 8. A\nfirst obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades\nperformance and makes the unfiltered variant perform the worst in every task. Beyond\nthis, we found that in some cases a pre-training data set with a more constrained domain\noutperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus"}
{"doc_id": "1910.10683", "para_id": 142, "text": "12. https://github.com/jcpeterson/openwebtext\n13. https://www.tensorflow.org/datasets/catalog/wikipedia"}
{"doc_id": "1910.10683", "para_id": 143, "text": "Data set\nSize\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 144, "text": "⋆C4\n745GB\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nC4, unfiltered\n6.1TB\n81.46\n19.14\n78.78\n68.04\n26.55\n39.34\n27.21\nRealNews-like\n35GB\n83.83\n19.23\n80.39\n72.38\n26.75\n39.90\n27.48\nWebText-like\n17GB\n84.03\n19.31\n81.42\n71.40\n26.80\n39.74\n27.59\nWikipedia\n16GB\n81.85\n19.31\n81.29\n68.01\n26.94\n39.69\n27.67\nWikipedia + TBC\n20GB\n83.65\n19.28\n82.08\n73.24\n26.77\n39.63\n27.57"}
{"doc_id": "1910.10683", "para_id": 145, "text": "Table 8: Performance resulting from pre-training on different data sets. The first four\nvariants are based on our new C4 data set."}
{"doc_id": "1910.10683", "para_id": 146, "text": "produced a SuperGLUE score of 73.24, beating our baseline’s score (using C4) of 71.36.\nThis is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to\n50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC\nis a reading comprehension data set whose largest source of data comes from fiction books,\nwhich is exactly the domain covered by TBC. Similarly, using the RealNews-like data set\nfor pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for\nReCoRD, a data set that measures reading comprehension on news articles. As a final\nexample, using data from Wikipedia produced significant (but less dramatic) gains on\nSQuAD, which is a question-answering data set with passages sourced from Wikipedia.\nSimilar observations have been made in prior work, e.g. Beltagy et al. (2019) found that\npre-training BERT on text from research papers improved its performance on scientific tasks.\nThe main lesson behind these findings is that pre-training on in-domain unlabeled data can\nimprove performance on downstream tasks. This is unsurprising but also unsatisfying if\nour goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary\ndomains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded\nimprovements on downstream tasks. This observation also motivates the parallel line of\nresearch on domain adaptation for natural language processing; for surveys of this field see\ne.g. Ruder (2019); Li (2012).\nA drawback to only pre-training on a single domain is that the resulting data sets are\noften substantially smaller. Similarly, while the WebText-like variant performed as well or\nbetter than the C4 data set in our baseline setting, the Reddit-based filtering produced a\ndata set that was about 40× smaller than C4 despite being based on 12× more data from\nCommon Crawl. Note, however, that in our baseline setup we only pre-train on 235 ≈34B\ntokens, which is only about 8 times larger than the smallest pre-training data set we consider.\nWe investigate at what point using a smaller pre-training data sets poses an issue in the\nfollowing section."}
{"doc_id": "1910.10683", "para_id": 147, "text": "The pipeline we use to create C4 was designed to be able to create extremely large pre-\ntraining data sets. The access to so much data allows us to pre-train our models without\nrepeating examples. It is not clear whether repeating examples during pre-training would\nbe helpful or harmful to downstream performance because our pre-training objective is itself\nstochastic and can help prevent the model from seeing the same exact data multiple times."}
{"doc_id": "1910.10683", "para_id": 148, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 149, "text": "Number of tokens\nRepeats\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 150, "text": "⋆Full data set\n0\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n229\n64\n82.87\n19.19\n80.97\n72.03\n26.83\n39.74\n27.63\n227\n256\n82.62\n19.20\n79.78\n69.97\n27.02\n39.71\n27.33\n225\n1,024\n79.55\n18.57\n76.27\n64.76\n26.38\n39.56\n26.80\n223\n4,096\n76.34\n18.33\n70.92\n59.29\n26.37\n38.84\n25.81"}
{"doc_id": "1910.10683", "para_id": 151, "text": "Table 9: Measuring the effect of repeating data during pre-training. In these experiments,\nwe only use the first N tokens from C4 (with varying values of N shown in the\nfirst column) but still pre-train over 235 tokens. This results in the data set being\nrepeated over the course of pre-training (with the number of repeats for each\nexperiment shown in the second column), which may result in memorization (see\nFigure 6)."}
{"doc_id": "1910.10683", "para_id": 152, "text": "To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model\non artificially truncated versions of C4. Recall that we pre-train our baseline model on\n235 ≈34B tokens (a small fraction of the total size of C4). We consider training on truncated\nvariants of C4 consisting of 229, 227, 225 and 223 tokens. These sizes correspond to repeating\nthe data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training."}
{"doc_id": "1910.10683", "para_id": 153, "text": "The resulting downstream performance is shown in Table 9. As expected, performance\ndegrades as the data set size shrinks. We suspect this may be due to the fact that the model\nbegins to memorize the pre-training data set. To measure if this is true, we plot the training\nloss for each of these data set sizes in Figure 6. Indeed, the model attains significantly\nsmaller training losses as the size of the pre-training data set shrinks, suggesting possible\nmemorization. Baevski et al. (2019) similarly observed that truncating the pre-training data\nset size can degrade downstream task performance."}
{"doc_id": "1910.10683", "para_id": 154, "text": "We note that these effects are limited when the pre-training data set is repeated only\n64 times. This suggests that some amount of repetition of pre-training data might not be\nharmful. However, given that additional pre-training can be beneficial (as we will show in\nSection 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest\nusing large pre-training data sets whenever possible. We also note that this effect may be\nmore pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting\nto a smaller pre-training data set."}
{"doc_id": "1910.10683", "para_id": 155, "text": "So far we have considered the setting where all parameters of a model are pre-trained on\nan unsupervised task before being fine-tuned on individual supervised tasks. While this\napproach is straightforward, various alternative methods for training the model on down-\nstream/supervised tasks have been proposed. In this section, we compare different schemes\nfor fine-tuning the model in addition to the approach of training the model simultaneously\non multiple tasks."}
{"doc_id": "1910.10683", "para_id": 156, "text": "Figure 6: Pre-training loss for our original C4 data set as well as 4 artificially truncated\nversions. The sizes listed refer to the number of tokens in each data set. The four\nsizes considered correspond to repeating the data set between 64 and 4,096 times\nover the course of pre-training. Using a smaller data set size results in smaller\ntraining loss values, which may suggest some memorization of the unlabeled data\nset."}
{"doc_id": "1910.10683", "para_id": 157, "text": "It has been argued that fine-tuning all of the model’s parameters can lead to suboptimal\nresults, particularly on low-resource tasks (Peters et al., 2019). Early results on transfer\nlearning for text classification tasks advocated fine-tuning only the parameters of a small\nclassifier that was fed sentence embeddings produced by a fixed pre-trained model (Subra-\nmanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau\net al., 2017). This approach is less applicable to our encoder-decoder model because the\nentire decoder must be trained to output the target sequences for a given task. Instead, we\nfocus on two alternative fine-tuning approaches that update only a subset of the parameters\nof our encoder-decoder model.\nThe first, “adapter layers” (Houlsby et al., 2019; Bapna et al., 2019), is motivated by\nthe goal of keeping most of the original model fixed while fine-tuning. Adapter layers are\nadditional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward\nnetworks in each block of the Transformer. These new feed-forward networks are designed\nso that their output dimensionality matches their input. This allows them to be inserted\ninto the network with no additional changes to the structure or parameters. When fine-\ntuning, only the adapter layer and layer normalization parameters are updated. The main\nhyperparameter of this approach is the inner dimensionality d of the feed-forward network,\nwhich changes the number of new parameters added to the model. We experiment with\nvarious values for d.\nThe second alternative fine-tuning method we consider is “gradual unfreezing” (Howard\nand Ruder, 2018). In gradual unfreezing, more and more of the model’s parameters are fine-\ntuned over time. Gradual unfreezing was originally applied to a language model architecture\nconsisting of a single stack of layers. In this setting, at the start of fine-tuning only the"}
{"doc_id": "1910.10683", "para_id": 158, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 159, "text": "Fine-tuning method\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 160, "text": "⋆All parameters\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nAdapter layers, d = 32\n80.52\n15.08\n79.32\n60.40\n13.84\n17.88\n15.54\nAdapter layers, d = 128\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nAdapter layers, d = 512\n81.54\n17.78\n79.18\n64.30\n23.45\n33.98\n25.81\nAdapter layers, d = 2048\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nGradual unfreezing\n82.50\n18.95\n79.17\n70.79\n26.71\n39.02\n26.93"}
{"doc_id": "1910.10683", "para_id": 161, "text": "Table 10: Comparison of different alternative fine-tuning methods that only update a subset\nof the model’s parameters. For adapter layers, d refers to the inner dimensionality\nof the adapters."}
{"doc_id": "1910.10683", "para_id": 162, "text": "parameters of the final layer are updated, then after training for a certain number of updates\nthe parameters of the second-to-last layer are also included, and so on until the entire\nnetwork’s parameters are being fine-tuned. To adapt this approach to our encoder-decoder\nmodel, we gradually unfreeze layers in the encoder and decoder in parallel, starting from\nthe top in both cases. Since the parameters of our input embedding matrix and output\nclassification matrix are shared, we update them throughout fine-tuning. Recall that our\nbaseline model consists of 12 layers each in the encoder and decoder and is fine-tuned for\n218 steps. As such, we subdivide the fine-tuning process into 12 episodes of 218/12 steps each\nand train from layers 12 −n to 12 in the nth episode. We note that Howard and Ruder\n(2018) suggested fine-tuning an additional layer after each epoch of training. However, since\nour supervised data sets vary so much in size and since some of our downstream tasks are\nactually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler\nstrategy of fine-tuning an additional layer after every 218/12 steps.\nA comparison of the performance of these fine-tuning approaches is shown in Table 10.\nFor adapter layers, we report the performance using an inner dimensionality d of 32, 128,\n512, 2048. Pursuant with past results (Houlsby et al., 2019; Bapna et al., 2019) we find that\nlower-resource tasks like SQuAD work well with a small value of d whereas higher resource\ntasks require a large dimensionality to achieve reasonable performance. This suggests that\nadapter layers could be a promising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size. Note that in our case we treat\nGLUE and SuperGLUE each as a single “task” by concatenating their constituent data\nsets, so although they comprise some low-resource data sets the combined data set is large\nenough that it necessitates a large value of d. We found that gradual unfreezing caused\na minor degradation in performance across all tasks, though it did provide some speedup\nduring fine-tuning. Better results may be attainable by more carefully tuning the unfreezing\nschedule."}
{"doc_id": "1910.10683", "para_id": 163, "text": "So far, we have been pre-training our model on a single unsupervised learning task before\nfine-tuning it individually on each downstream task. An alternative approach, called “multi-\ntask learning” (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a\ntime. This approach typically has the goal of training a single model that can simultaneously"}
{"doc_id": "1910.10683", "para_id": 164, "text": "perform many tasks at once, i.e. the model and most of its parameters are shared across all\ntasks. We relax this goal somewhat and instead investigate methods for training on multiple\ntasks at once in order to eventually produce separate parameter settings that perform well\non each individual task. For example, we might train a single model on many tasks, but\nwhen reporting performance we are allowed to select a different checkpoint for each task.\nThis loosens the multi-task learning framework and puts it on more even footing compared\nto the pre-train-then-fine-tune approach we have considered so far. We also note that in our\nunified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets\ntogether. It follows that we can still train on unlabeled data when using multi-task learning\nby treating the unsupervised task as one of the tasks being mixed together. In contrast,\nmost applications of multi-task learning to NLP add task-specific classification networks or\nuse different loss functions for each task (Liu et al., 2019b).\nAs pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task\nlearning is how much data from each task the model should be trained on. Our goal is to not\nunder- or over-train the model—that is, we want the model to see enough data from a given\ntask that it can perform the task well, but not to see so much data that it memorizes the\ntraining set. How exactly to set the proportion of data coming from each task can depend on\nvarious factors including data set sizes, the “difficulty” of learning the task (i.e. how much\ndata the model must see before being able to perform the task effectively), regularization,\netc. An additional issue is the potential for “task interference” or “negative transfer”, where\nachieving good performance on one task can hinder performance on another. Given these\nconcerns, we begin by exploring various strategies for setting the proportion of data coming\nfrom each task. A similar exploration was performed by Wang et al. (2019a)."}
{"doc_id": "1910.10683", "para_id": 165, "text": "Examples-proportional mixing A major factor in how quickly a model will overfit to\na given task is the task’s data set size. As such, a natural way to set the mixing\nproportions is to sample in proportion to the size of each task’s data set. This is\nequivalent to concatenating the data sets for all tasks and randomly sampling examples\nfrom the combined data set. Note, however, that we are including our unsupervised\ndenoising task, which uses a data set that is orders of magnitude larger than every\nother task’s. It follows that if we simply sample in proportion to each data set’s size,\nthe vast majority of the data the model sees will be unlabeled, and it will undertrain\non all of the supervised tasks. Even without the unsupervised task, some tasks (e.g.\nWMT English to French) are so large that they would similarly crowd out most of\nthe batches. To get around this issue, we set an artificial “limit” on the data set sizes\nbefore computing the proportions. Specifically, if the number of examples in each of\nour N task’s data sets is en, n ∈{1, . . . , N} then we set probability of sampling an\nexample from the mth task during training to rm = min(em, K)/ P min(en, K) where\nK is the artificial data set size limit."}
{"doc_id": "1910.10683", "para_id": 166, "text": "Temperature-scaled mixing An alternative way of mitigating the huge disparity between\ndata set sizes is to adjust the “temperature” of the mixing rates. This approach was\nused by multilingual BERT to ensure that the model was sufficiently trained on low-\nresource languages.14 To implement temperature scaling with temperature T, we raise"}
{"doc_id": "1910.10683", "para_id": 167, "text": "14. https://github.com/google-research/bert/blob/master/multilingual.md"}
{"doc_id": "1910.10683", "para_id": 168, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 169, "text": "each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they\nsum to 1. When T = 1, this approach is equivalent to examples-proportional mixing\nand as T increases the proportions become closer to equal mixing. We retain the data\nset size limit K (applied to obtain rm before temperature scaling) but set it to a large\nvalue of K = 221. We use a large value of K because increasing the temperature will\ndecrease the mixing rate of the largest data sets."}
{"doc_id": "1910.10683", "para_id": 170, "text": "Equal mixing In this case, we sample examples from each task with equal probability.\nSpecifically, each example in each batch is sampled uniformly at random from one of\nthe data sets we train on. This is most likely a suboptimal strategy, as the model will\noverfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly\ninclude it as a point of reference of what might go wrong when the proportions are set\nsuboptimally."}
{"doc_id": "1910.10683", "para_id": 171, "text": "To compare these mixing strategies on equal footing with our baseline pre-train-then-\nfine-tune results, we train multi-task models for the same total number of steps: 219 + 218 =\n786,432. The results are shown in Table 11.\nIn general, we find that multi-task training underperforms pre-training followed by\nfine-tuning on most tasks. The “equal” mixing strategy in particular results in dramatically\ndegraded performance, which may be because the low-resource tasks have overfit, the high-\nresource tasks have not seen enough data, or the model has not seen enough unlabeled data to\nlearn general-purpose language capabilities. For examples-proportional mixing, we find that\nfor most tasks there is a “sweet spot” for K where the model obtains the best performance,\nand larger or smaller values of K tend to result in worse performance. The exception (for the\nrange of K values we considered) was WMT English to French translation, which is such a\nhigh-resource task that it always benefits from a higher mixing proportion. Finally, we note\nthat temperature-scaled mixing also provides a means of obtaining reasonable performance\nfrom most tasks, with T = 2 performing the best in most cases. The finding that a multi-task\nmodel is outperformed by separate models trained on each individual task has previously\nbeen observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has\nbeen shown that the multi-task setup can confer benefits across very similar tasks Liu et al.\n(2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap\nbetween multi-task training and the pre-train-then-fine-tune approach."}
{"doc_id": "1910.10683", "para_id": 172, "text": "3.5.3 Combining Multi-Task Learning with Fine-Tuning"}
{"doc_id": "1910.10683", "para_id": 173, "text": "Recall that we are studying a relaxed version of multi-task learning where we train a single\nmodel on a mixture of tasks but are allowed to evaluate performance using different parameter\nsettings (checkpoints) for the model. We can extend this approach by considering the case\nwhere the model is pre-trained on all tasks at once but is then fine-tuned on the individual\nsupervised tasks. This is the method used by the “MT-DNN” (Liu et al., 2015, 2019b),\nwhich achieved state-of-the-art performance on GLUE and other benchmarks when it was\nintroduced. We consider three variants of this approach: In the first, we simply pre-train the\nmodel on an examples-proportional mixture with an artificial data set size limit of K = 219"}
{"doc_id": "1910.10683", "para_id": 174, "text": "before fine-tuning it on each individual downstream task. This helps us measure whether\nincluding the supervised tasks alongside the unsupervised objective during pre-training"}
{"doc_id": "1910.10683", "para_id": 175, "text": "Mixing strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 176, "text": "⋆Baseline (pre-train/fine-tune)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nEqual\n76.13\n19.02\n76.51\n63.37\n23.89\n34.31\n26.78\nExamples-proportional, K = 216\n80.45\n19.04\n77.25\n69.95\n24.35\n34.99\n27.10\nExamples-proportional, K = 217\n81.56\n19.12\n77.00\n67.91\n24.36\n35.00\n27.25\nExamples-proportional, K = 218\n81.67\n19.07\n78.17\n67.94\n24.57\n35.19\n27.39\nExamples-proportional, K = 219\n81.42\n19.24\n79.78\n67.30\n25.21\n36.30\n27.76\nExamples-proportional, K = 220\n80.80\n19.24\n80.36\n67.38\n25.66\n36.93\n27.68\nExamples-proportional, K = 221\n79.83\n18.79\n79.50\n65.10\n25.82\n37.22\n27.13\nTemperature-scaled, T = 2\n81.90\n19.28\n79.42\n69.92\n25.42\n36.72\n27.20\nTemperature-scaled, T = 4\n80.56\n19.22\n77.99\n69.54\n25.04\n35.82\n27.45\nTemperature-scaled, T = 8\n77.21\n19.10\n77.14\n66.07\n24.55\n35.35\n27.17"}
{"doc_id": "1910.10683", "para_id": 177, "text": "Table 11: Comparison of multi-task training using different mixing strategies. Examples-\nproportional mixing refers to sampling examples from each data set according to\nthe total size of each data set, with an artificial limit (K) on the maximum data set\nsize. Temperature-scaled mixing re-scales the sampling rates by a temperature T.\nFor temperature-scaled mixing, we use an artificial data set size limit of K = 221."}
{"doc_id": "1910.10683", "para_id": 178, "text": "gives the model some beneficial early exposure to the downstream tasks. We might also\nhope that mixing in many sources of supervision could help the pre-trained model obtain a\nmore general set of “skills” (loosely speaking) before it is adapted to an individual task. To\nmeasure this directly, we consider a second variant where we pre-train the model on the same\nexamples-proportional mixture (with K = 219) except that we omit one of the downstream\ntasks from this pre-training mixture. Then, we fine-tune the model on the task that was\nleft out during pre-training. We repeat this for each of the downstream tasks we consider.\nWe call this approach “leave-one-out” multi-task training. This simulates the real-world\nsetting where a pre-trained model is fine-tuned on a task it had not seen during pre-training.\nNote that multi-task pre-training provides a diverse mixture of supervised tasks. Since other\nfields (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014)) use a supervised data set for pre-training, we were interested to see whether\nomitting the unsupervised task from the multi-task pre-training mixture still produced good\nresults. For our third variant we therefore pre-train on an examples-proportional mixture of\nall of the supervised tasks we consider with K = 219. In all of these variants, we follow our\nstandard procedure of pre-training for 219 steps before fine-tuning for 218 steps.\nWe compare the results of these approaches in Table 12. For comparison, we also include\nresults for our baseline (pre-train then fine-tune) and for standard multi-task learning\n(without fine-tuning) on an examples-proportional mixture with K = 219. We find that\nfine-tuning after multi-task pre-training results in comparable performance to our baseline.\nThis suggests that using fine-tuning after multi-task learning can help mitigate some of\nthe trade-offs between different mixing rates described in Section 3.5.2. Interestingly, the\nperformance of “leave-one-out” training was only slightly worse, suggesting that a model\nthat was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pre-\ntraining might not result in a dramatic task interference). Finally, supervised multi-task\npre-training performed significantly worse in every case except for the translation tasks. This"}
{"doc_id": "1910.10683", "para_id": 179, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 180, "text": "Training strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 181, "text": "⋆Unsupervised pre-training + fine-tuning\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nMulti-task training\n81.42\n19.24\n79.78\n67.30\n25.21\n36.30\n27.76\nMulti-task pre-training + fine-tuning\n83.11\n19.12\n80.26\n71.03\n27.08\n39.80\n28.07\nLeave-one-out multi-task training\n81.98\n19.05\n79.97\n71.68\n26.93\n39.79\n27.87\nSupervised multi-task pre-training\n79.93\n18.96\n77.38\n65.36\n26.81\n40.13\n28.04"}
{"doc_id": "1910.10683", "para_id": 182, "text": "Table 12: Comparison of unsupervised pre-training, multi-task learning, and various forms\nof multi-task pre-training."}
{"doc_id": "1910.10683", "para_id": 183, "text": "could suggest that the translation tasks benefit less from (English) pre-training, whereas\nunsupervised pre-training is an important factor in the other tasks."}
{"doc_id": "1910.10683", "para_id": 184, "text": "The “bitter lesson” of machine learning research argues that general methods that can\nleverage additional computation ultimately win out against methods that rely on human\nexpertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016;\nMahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a).\nRecent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c;\nRadford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown\nthat scaling up produces improved performance compared to more carefully-engineered\nmethods. However, there are a variety of possible ways to scale, including using a bigger\nmodel, training the model for more steps, and ensembling. In this section, we compare these\ndifferent approaches by addressing the following premise: “You were just given 4× more\ncompute. How should you use it?”"}
{"doc_id": "1910.10683", "para_id": 185, "text": "We start with our baseline model, which has 220M parameters and is pre-trained and\nfine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized\nsimilarly to “BERTBASE”. To experiment with increased model size, we follow the guidelines\nof “BERTLARGE” Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and\n16-head attention mechanisms. We then generate two variants with 16 and 32 layers each in\nthe encoder and decoder, producing models with 2× and 4× as many parameters as our\noriginal model. These two variants also have a roughly 2× and 4× the computational cost.\nUsing our baseline and these two larger models, we consider three ways of using 4× as much\ncomputation: Training for 4× as many steps, training for 2× as many steps with the 2×\nbigger model, and training the 4× bigger model for the “baseline” number of training steps.\nWhen we increase the training steps, we scale both the pre-train and fine-tune steps for\nsimplicity. Note that when increasing the number of pre-training steps, we are effectively\nincluding more pre-training data as C4 is so large that we do not complete one pass over\nthe data even when training for 223 steps."}
{"doc_id": "1910.10683", "para_id": 186, "text": "An alternative way for the model to see 4× as much data is to increase the batch size by a\nfactor of 4. This can potentially result in faster training due to more efficient parallelization.\nHowever, training with a 4× larger batch size can yield a different outcome than training"}
{"doc_id": "1910.10683", "para_id": 187, "text": "Scaling strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo"}
{"doc_id": "1910.10683", "para_id": 188, "text": "⋆Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n1× size, 4× training steps\n85.33\n19.33\n82.45\n74.72\n27.08\n40.66\n27.93\n1× size, 4× batch size\n84.60\n19.42\n82.52\n74.64\n27.07\n40.60\n27.84\n2× size, 2× training steps\n86.18\n19.66\n84.18\n77.18\n27.52\n41.03\n28.19\n4× size, 1× training steps\n85.91\n19.73\n83.86\n78.04\n27.47\n40.71\n28.10\n4× ensembled\n84.77\n20.10\n83.09\n71.74\n28.05\n40.53\n28.57\n4× ensembled, fine-tune only\n84.05\n19.57\n82.36\n71.55\n27.55\n40.22\n28.09"}
{"doc_id": "1910.10683", "para_id": 189, "text": "Table 13: Comparison of different methods of scaling up our baseline model. All methods\nexcept ensembling fine-tuned models use 4× the computation as the baseline.\n“Size” refers to the number of parameters in the model and “training time” refers\nto the number of steps used for both pre-training and fine-tuning."}
{"doc_id": "1910.10683", "para_id": 190, "text": "for 4× as many steps (Shallue et al., 2018). We include an additional experiment where we\ntrain our baseline model with a 4× larger batch size to compare these two cases.\nIt is common practice on many of the benchmarks we consider to eke out additional\nperformance by training and evaluating using an ensemble of models. This provides an\northogonal way of using additional computation. To compare other scaling methods to\nensembling, we also measure the performance of an ensemble of 4 separately pre-trained and\nfine-tuned models. We average the logits across the ensemble before feeding them into the\noutput softmax nonlinearity to obtain an aggregate prediction. Instead of pre-training 4\nseparate models, a cheaper alternative is to take a single pre-trained model and produce 4\nseparate fine-tuned versions. While this does not use our entire 4× computational budget,\nwe also include this method to see if it produces competitive performance to the other scaling\nmethods.\nThe performance achieved after applying these various scaling methods is shown in\nTable 13.\nUnsurprisingly, increasing the training time and/or model size consistently\nimproves the baseline. There was no clear winner between training for 4× as many steps\nor using a 4× larger batch size, though both were beneficial. In general, increasing the\nmodel size resulted in an additional bump in performance compared to solely increasing\nthe training time or batch size. We did not observe a large difference between training a\n2× bigger model for 2× as long and training a 4× bigger model on any of the tasks we\nstudied. This suggests that increasing the training time and increasing the model size can be\ncomplementary means of improving performance. Our results also suggest that ensembling\nprovides an orthogonal and effective means of improving performance through scale. In some\ntasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling 4\ncompletely separately trained models significantly outperformed every other scaling approach.\nEnsembling models that were pre-trained together but fine-tuned separately also gave a\nsubstantial performance increase over the baseline, which suggests a cheaper means of\nimproving performance. The only exception was SuperGLUE, where neither ensembling\napproach significantly improved over the baseline.\nWe note that different scaling methods have different trade-offs that are separate from\ntheir performance. For example, using a larger model can make downstream fine-tuning and"}
{"doc_id": "1910.10683", "para_id": 191, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 192, "text": "inference more expensive. In contrast, the cost of pre-training a small model for longer is\neffectively amortized if it is applied to many downstream tasks. Separately, we note that\nensembling N separate models has a similar cost to using a model that has an N× higher\ncomputational cost. As a result, some consideration for the eventual use of the model is\nimportant when choosing between scaling methods."}
{"doc_id": "1910.10683", "para_id": 193, "text": "We now leverage the insights from our systematic study to determine how far we can push\nperformance on popular NLP benchmarks. We are also interested in exploring the current\nlimits of transfer learning for NLP by training larger models on large amounts of data. We\nstart with our baseline training approach and make the following changes:"}
{"doc_id": "1910.10683", "para_id": 194, "text": "Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption\nobjective described in Section 3.3.4, which was loosely inspired by SpanBERT (Joshi\net al., 2019). Specifically, we use a mean span length of 3 and corrupt 15% of the\noriginal sequence. We found that this objective produced marginally better performance\n(Table 7) while being slightly more computationally efficient due to shorter target\nsequence lengths."}
{"doc_id": "1910.10683", "para_id": 195, "text": "Longer training Our baseline model uses a relatively small amount of pre-training (1⁄4 as\nmuch as BERT (Devlin et al., 2018), 1⁄16 as much as XLNet (Yang et al., 2019), 1⁄64 as\nmuch as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we\ncan train for substantially longer without repeating data (which can be detrimental,\nas shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can\nindeed be helpful, and that both increasing the batch size and increasing the number of\ntraining steps can confer this benefit. We therefore pre-train our models for 1 million\nsteps on a batch size of 211 sequences of length 512, corresponding to a total of about\n1 trillion pre-training tokens (about 32× as many as our baseline). In Section 3.4.1, we\nshowed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC\ndata sets outperformed pre-training on C4 on a few downstream tasks. However, these\ndata set variants are sufficiently small that they would be repeated hundreds of times\nover the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2\nthat this repetition could be harmful, we opted instead to continue using the C4 data\nset."}
{"doc_id": "1910.10683", "para_id": 196, "text": "Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved\nperformance. However, using smaller models can be helpful in settings where limited\ncomputational resources are available for fine-tuning or inference. Based on these\nfactors, we train models with a wide range of sizes:"}
{"doc_id": "1910.10683", "para_id": 197, "text": "• Base.\nThis is our baseline model, whose hyperparameters are described in\nSection 3.1.1. It has roughly 220 million parameters."}
{"doc_id": "1910.10683", "para_id": 198, "text": "• Small. We consider a smaller model, which scales the baseline down by using\ndmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the\nencoder and decoder. This variant has about 60 million parameters."}
{"doc_id": "1910.10683", "para_id": 199, "text": "• Large. Since our baseline uses a BERTBASE-sized encoder and decoder, we\nalso consider a variant where the encoder and decoder are both similar in size\nand structure to BERTLARGE. Specifically, this variant uses dmodel = 1,024,\ndff = 4,096, dkv = 64, 16-headed attention, and 24 layers each in the encoder and\ndecoder, resulting in around 770 million parameters."}
{"doc_id": "1910.10683", "para_id": 200, "text": "• 3B and 11B. To further explore what kind of performance is possible when\nusing larger models, we consider two additional variants. In both cases, we use\ndmodel = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B”\nvariant, we use dff = 16,384 with 32-headed attention, which results in around\n2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention\nproducing a model with about 11 billion parameters. We chose to scale up dff\nspecifically because modern accelerators (such as the TPUs we train our models\non) are most efficient for large dense matrix multiplications like those in the\nTransformer’s feed-forward networks."}
{"doc_id": "1910.10683", "para_id": 201, "text": "Multi-task pre-training In Section 3.5.3, we showed that pre-training on a multi-task\nmixture of unsupervised and supervised tasks before fine-tuning worked as well as\npre-training on the unsupervised task alone. This is the approach advocated by the\n“MT-DNN” (Liu et al., 2015, 2019b). It also has the practical benefit of being able to\nmonitor “downstream” performance for the entire duration of training, rather than\njust during fine-tuning. We therefore used multi-task pre-training in our final set of\nexperiments. We hypothesize that larger models trained for longer might benefit from\na larger proportion of unlabeled data because they are more likely to overfit to smaller\ntraining data sets. However, we also note that the results of Section 3.5.3 suggest that\nfine-tuning after multi-task pre-training can mitigate some of the issues that might\narise from choosing a suboptimal proportion of unlabeled data. Based on these ideas,\nwe substitute the following artificial data set sizes for our unlabeled data before using\nstandard example-proportional mixing (described in Section 3.5.2): 710,000 for Small,\n2,620,000 for Base, 8,660,000 for Large, 33,500,000 for 3B, and 133,000,000 for 11B.\nFor all model variants, we also capped the effective data set size of the WMT English\nto French and WMT English to German data sets to 1M examples during pre-training."}
{"doc_id": "1910.10683", "para_id": 202, "text": "Fine-tuning on individual GLUE and SuperGLUE tasks So far, when fine-tuning\non GLUE and SuperGLUE, we have concatenated all of the data sets in each benchmark\nso that we only fine-tune models once for GLUE and once for SuperGLUE. This\napproach makes our study logistically simpler, but we found that this sacrifices a small\namount of performance on some tasks compared to fine-tuning on the task separately. A\npotential issue with fine-tuning on individual tasks, which would otherwise be mitigated\nby training on all tasks at once, is that we might overfit quickly to low-resource tasks.\nFor example, our large batch size of 211 length-512 sequences would result in the entire\ndata set appearing multiple times in each batch for many of the low-resource GLUE\nand SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences\nduring fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints\nevery 1,000 steps rather than every 5,000 steps to ensure we have access to the model’s\nparameters before it overfits."}
{"doc_id": "1910.10683", "para_id": 203, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 204, "text": "Beam search All of our previous results were reported using greedy decoding. For tasks\nwith long output sequences, we found improved performance from using beam search\n(Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty\nof α = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization\ntasks."}
{"doc_id": "1910.10683", "para_id": 205, "text": "Test set Since this is our final set of experiments, we report results on the test set rather\nthan the validation set. For CNN/Daily Mail, we use the standard test set distributed\nwith the data set. For the WMT tasks, this corresponds to using newstest2014 for\nEnglish-German, newstest2015 for English-French, and newstest2016 for English-\nRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to\ncompute official test set scores.15,16 For SQuAD, evaluating on the test set requires\nrunning inference on a benchmark server. Unfortunately, the computational resources\non this server are insufficient for obtaining predictions from our largest models. As\na result, we instead continue to report performance on the SQuAD validation set.\nFortunately, the model with the highest performance on the SQuAD test set also\nreported results on the validation set, so we can still compare to what is ostensibly\nthe state-of-the-art."}
{"doc_id": "1910.10683", "para_id": 206, "text": "Apart from those changes mentioned above, we use the same training procedure and\nhyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate\nschedule for pre-training, constant learning rate for fine-tuning, dropout regularization,\nvocabulary, etc.). For reference, these details are described in Section 2.\nThe results of this final set of experiments are shown in Table 14. Overall, we achieved\nstate-of-the-art performance on 18 out of the 24 tasks we consider. As expected, our largest\n(11 billion parameter) model performed best among our model size variants across all tasks.\nOur T5-3B model variant did beat the previous state of the art in a few tasks, but scaling\nthe model size to 11 billion parameters was the most important ingredient for achieving our\nbest performance. We now analyze the results for each individual benchmark.\nWe achieved a state-of-the-art average GLUE score of 90.3. Notably, our performance was\nsubstantially better than the previous state-of-the-art for the natural language inference tasks\nMNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine performance\nhas historically lagged behind human performance, which is 93.6 and 95.9 respectively (Wang\net al., 2018). In terms of parameter count, our 11B model variant is the largest model that\nhas been submitted to the GLUE benchmark. However, most of the best-scoring submissions\nuse a large amount of ensembling and computation to produce predictions. For example,\nthe best-performing variant of ALBERT (Lan et al., 2019) uses a model similar in size and\narchitecture to our 3B variant (though it has dramatically fewer parameters due to clever\nparameter sharing). To produce its impressive performance on GLUE, the ALBERT authors\nensembled “from 6 to 17” models depending on the task. This likely results in it being more\ncomputationally expensive to produce predictions with the ALBERT ensemble than it is\nwith T5-11B.\nFor SQuAD, we outperformed the previous state-of-the-art (ALBERT (Lan et al., 2019))\nby over one point on the Exact Match score. SQuAD is a long-standing benchmark that"}
{"doc_id": "1910.10683", "para_id": 207, "text": "15. http://gluebenchmark.com\n16. http://super.gluebenchmark.com"}
{"doc_id": "1910.10683", "para_id": 208, "text": "GLUE\nCoLA\nSST-2\nMRPC\nMRPC\nSTS-B\nSTS-B\nModel\nAverage\nMatthew’s\nAccuracy\nF1\nAccuracy\nPearson\nSpearman"}
{"doc_id": "1910.10683", "para_id": 209, "text": "Previous best\n89.4a\n69.2b\n97.1a\n93.6b\n91.5b\n92.7b\n92.3b"}
{"doc_id": "1910.10683", "para_id": 210, "text": "T5-Small\n77.4\n41.0\n91.8\n89.7\n86.6\n85.6\n85.0\nT5-Base\n82.7\n51.1\n95.2\n90.7\n87.5\n89.4\n88.6\nT5-Large\n86.4\n61.2\n96.3\n92.4\n89.9\n89.9\n89.2\nT5-3B\n88.5\n67.1\n97.4\n92.5\n90.0\n90.6\n89.8\nT5-11B\n90.3\n71.6\n97.5\n92.8\n90.4\n93.1\n92.8"}
{"doc_id": "1910.10683", "para_id": 211, "text": "QQP\nQQP\nMNLI-m\nMNLI-mm\nQNLI\nRTE\nWNLI\nModel\nF1\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 212, "text": "Previous best\n74.8c\n90.7b\n91.3a\n91.0a\n99.2a\n89.2a\n91.8a"}
{"doc_id": "1910.10683", "para_id": 213, "text": "T5-Small\n70.0\n88.0\n82.4\n82.3\n90.3\n69.9\n69.2\nT5-Base\n72.6\n89.4\n87.1\n86.2\n93.7\n80.1\n78.8\nT5-Large\n73.9\n89.9\n89.9\n89.6\n94.8\n87.2\n85.6\nT5-3B\n74.4\n89.7\n91.4\n91.2\n96.3\n91.1\n89.7\nT5-11B\n75.1\n90.6\n92.2\n91.9\n96.9\n92.8\n94.5"}
{"doc_id": "1910.10683", "para_id": 214, "text": "SQuAD\nSQuAD\nSuperGLUE\nBoolQ\nCB\nCB\nCOPA\nModel\nEM\nF1\nAverage\nAccuracy\nF1\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 215, "text": "Previous best\n90.1a\n95.5a\n84.6d\n87.1d\n90.5d\n95.2d\n90.6d"}
{"doc_id": "1910.10683", "para_id": 216, "text": "T5-Small\n79.10\n87.24\n63.3\n76.4\n56.9\n81.6\n46.0\nT5-Base\n85.44\n92.08\n76.2\n81.4\n86.2\n94.0\n71.2\nT5-Large\n86.66\n93.79\n82.3\n85.4\n91.6\n94.8\n83.4\nT5-3B\n88.53\n94.95\n86.4\n89.9\n90.3\n94.4\n92.0\nT5-11B\n91.26\n96.22\n88.9\n91.2\n93.9\n96.8\n94.8"}
{"doc_id": "1910.10683", "para_id": 217, "text": "MultiRC\nMultiRC\nReCoRD\nReCoRD\nRTE\nWiC\nWSC\nModel\nF1a\nEM\nF1\nAccuracy\nAccuracy\nAccuracy\nAccuracy"}
{"doc_id": "1910.10683", "para_id": 218, "text": "Previous best\n84.4d\n52.5d\n90.6d\n90.0d\n88.2d\n69.9d\n89.0d"}
{"doc_id": "1910.10683", "para_id": 219, "text": "T5-Small\n69.3\n26.3\n56.3\n55.4\n73.3\n66.9\n70.5\nT5-Base\n79.7\n43.1\n75.0\n74.2\n81.5\n68.3\n80.8\nT5-Large\n83.3\n50.7\n86.8\n85.9\n87.8\n69.3\n86.3\nT5-3B\n86.8\n58.3\n91.2\n90.4\n90.7\n72.1\n90.4\nT5-11B\n88.1\n63.3\n94.1\n93.4\n92.5\n76.9\n93.8"}
{"doc_id": "1910.10683", "para_id": 220, "text": "WMT EnDe\nWMT EnFr\nWMT EnRo\nCNN/DM\nCNN/DM\nCNN/DM\nModel\nBLEU\nBLEU\nBLEU\nROUGE-1\nROUGE-2\nROUGE-L"}
{"doc_id": "1910.10683", "para_id": 221, "text": "Previous best\n33.8e\n43.8e\n38.5f\n43.47g\n20.30g\n40.63g"}
{"doc_id": "1910.10683", "para_id": 222, "text": "T5-Small\n26.7\n36.0\n26.8\n41.12\n19.56\n38.35\nT5-Base\n30.9\n41.2\n28.0\n42.05\n20.34\n39.40\nT5-Large\n32.0\n41.5\n28.1\n42.50\n20.68\n39.75\nT5-3B\n31.8\n42.6\n28.2\n42.72\n21.02\n39.94\nT5-11B\n32.1\n43.4\n28.1\n43.52\n21.55\n40.69"}
{"doc_id": "1910.10683", "para_id": 223, "text": "Table 14: Performance of our T5 variants on every task we study. Small, Base, Large, 3B,\nand 11B refer to model configurations with 60 million, 220 million, 770 million,\n3 billion, and 11 billion parameters, respectively. In the first row of each table,\nwe report the state-of-the-art for the task (as of October 24th, 2019), with the\nsuperscript denoting its source with references listed at the end of this caption. All\nresults are reported on the test set except for SQuAD where we use the validation\nset.\na(Lan et al., 2019) b(Wang et al., 2019c) c(Zhu et al., 2019) d(Liu et al.,\n2019c) e(Edunov et al., 2018) f(Lample and Conneau, 2019) g(Dong et al., 2019)"}
{"doc_id": "1910.10683", "para_id": 224, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 225, "text": "was created over three years ago, and most recent improvements have only increased the\nstate-of-the-art by a fraction of a percentage point. We note that when results are reported\non the test set, they are typically based on an ensemble of models and/or leverage external\ndata sets (e.g. TriviaQA (Joshi et al., 2017) or NewsQA (Trischler et al., 2016)) to augment\nthe small SQuAD training set. Human performance on SQuAD is estimated at 82.30 and\n91.22 for the Exact Match and F1 metric respectively (Rajpurkar et al., 2016), so it is not\nclear if further improvements on this benchmark are meaningful.\nFor SuperGLUE, we improved upon the state-of-the-art by a large margin (from an\naverage score of 84.6 (Liu et al., 2019c) to 88.9). SuperGLUE was designed to include\ntasks that were “beyond the scope of current state-of-the-art systems, but solvable by most\ncollege-educated English speakers” (Wang et al., 2019b). We nearly match the human\nperformance of 89.8 (Wang et al., 2019b). Interestingly, on the reading comprehension tasks\n(MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the\nevaluation metrics used for these tasks may be biased towards machine-made predictions.\nOn the other hand, humans achieve 100% accuracy on both COPA and WSC, which is\nsignificantly better than our model’s performance. This suggests that there remain linguistic\ntasks that are hard for our model to perfect, particularly in the low-resource setting.\nWe did not achieve state-of-the-art performance on any of the WMT translation tasks.\nThis may be in part due to our use of an English-only unlabeled data set. We also note that\nmost of the best results on these tasks use backtranslation (Edunov et al., 2018; Lample and\nConneau, 2019), which is a sophisticated data augmentation scheme. The state of the art on\nthe low-resource English to Romanian benchmark also uses additional forms of cross-lingual\nunsupervised training (Lample and Conneau, 2019). Our results suggest that scale and\nEnglish-language pre-training may be insufficient to match the performance of these more\nsophisticated methods. On a more specific note, the best results on English to German\nnewstest2014 set use the much larger training set from WMT 2018 (Edunov et al., 2018),\nmaking direct comparison to our results difficult.\nFinally, on CNN/Daily Mail we attain state-of-the-art performance, though only by\na significant amount on the ROUGE-2-F score. It has been shown that improvements\nto the ROUGE score do not necessarily correspond to more coherent summaries (Paulus\net al., 2017). Furthermore, while CNN/Daily Mail is posed as an abstractive summarization\nbenchmark, purely extractive approaches have been shown to work well (Liu, 2019). It has\nalso been argued that generative models trained with maximum likelihood are prone to\nproducing repetitive summaries (See et al., 2017). Despite these potential issues, we find\nthat our models do generate coherent and largely correct summaries. We provide some\nnon-cherry-picked validation set examples in Appendix C.\nTo achieve its strong results, T5 combines insights from our experimental study with\nunprecedented scale. Note that in Section 3.6 we found that scaling up the pre-training\namount or size of our baseline model produced substantial gains. Given this, we were\ninterested to measure how much the “non-scaling” changes we introduced into T5 contributed\nto its strong performance. We therefore carried out a final experiment where we compared\nthe following three configurations: First, the standard baseline model, which was pre-trained\non 235 ≈34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e.\nthe same amount of pre-training used for T5), which we refer to as “baseline-1T”; and\nthird, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the"}
{"doc_id": "1910.10683", "para_id": 226, "text": "⋆Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98"}
{"doc_id": "1910.10683", "para_id": 227, "text": "Table 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. “Baseline-1T”\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 ≈34B\ntokens (as was used for the baseline)."}
{"doc_id": "1910.10683", "para_id": 228, "text": "“non-scaling” changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5’s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors."}
{"doc_id": "1910.10683", "para_id": 229, "text": "Having completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field."}
{"doc_id": "1910.10683", "para_id": 230, "text": "Text-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale."}
{"doc_id": "1910.10683", "para_id": 231, "text": "Architectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count."}
{"doc_id": "1910.10683", "para_id": 232, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 233, "text": "Unsupervised objectives Overall, we found that most “denoising” objectives, which train\nthe model to reconstruct randomly corrupted text, performed similarly in the text-to-\ntext setup. As a result, we suggest using objectives that produce short target sequences\nso that unsupervised pre-training is more computationally efficient."}
{"doc_id": "1910.10683", "para_id": 234, "text": "Data sets We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises\nheuristically-cleaned text from the Common Crawl web dump. When comparing C4 to\ndata sets that use additional filtering, we found that training on in-domain unlabeled\ndata could boost performance in a few downstream tasks. However, constraining to\na single domain typically results in a smaller data set. We separately showed that\nperformance can degrade when an unlabeled data set is small enough that it is repeated\nmany times over the course of pre-training. This motivates the use of a large and\ndiverse data set like C4 for generic language understanding tasks."}
{"doc_id": "1910.10683", "para_id": 235, "text": "Training strategies We found that the basic approach of updating all of a pre-trained\nmodel’s parameters during fine-tuning outperformed methods that are designed to\nupdate fewer parameters, although updating all parameters is most expensive. We also\nexperimented with various approaches for training the model on multiple tasks at once,\nwhich in our text-to-text setting simply corresponds to mixing examples from different\ndata sets when constructing batches. The primary concern in multi-task learning is\nsetting the proportion of each task to train on. We ultimately did not find a strategy\nfor setting mixing proportions that matched the performance of the basic approach of\nunsupervised pre-training followed by supervised fine-tuning. However, we found that\nfine-tuning after pre-training on a mixture of tasks produced comparable performance\nto unsupervised pre-training."}
{"doc_id": "1910.10683", "para_id": 236, "text": "Scaling We compared various strategies for taking advantage of additional compute, includ-\ning training the model on more data, training a larger model, and using an ensemble\nof models. We found each approach conferred a significant boost in performance,\nthough training a smaller model on more data was often outperformed by training\na larger model for fewer steps. We also showed an ensemble of models can provide\nsubstantially better results than a single model, which provides an orthogonal means\nof leveraging additional computation. Ensembling models that were fine-tuned from\nthe same base pre-trained model performed worse than pre-training and fine-tuning\nall models completely separately, though fine-tune-only ensembling still substantially\noutperformed a single model."}
{"doc_id": "1910.10683", "para_id": 237, "text": "Pushing the limits We combined our above insights and trained substantially larger\nmodels (up to 11 billion parameters) to achieve state-of-the-art results across many of\nthe benchmarks we considered. For unsupervised training, we extracted text from our\nC4 data set and applied a denoising objective that corrupts contiguous spans of tokens.\nWe pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,\nour models were trained on over 1 trillion tokens. In the interest of facilitating the\nreplication, extension, and application of our results, we release our code, the C4 data\nset, and pre-trained model weights for each T5 variant.1"}
{"doc_id": "1910.10683", "para_id": 238, "text": "The inconvenience of large models An unsurprising but important result from our\nstudy is that larger models tend to perform better. The fact that the hardware used for\nrunning these models is continually getting cheaper and more powerful suggests that\nscaling up may continue to be a promising way to achieve better performance (Sutton,\n2019). However, it will always be the case that there are applications and scenarios\nwhere using a smaller or less expensive model is helpful, for example when performing\nclient-side inference or federated learning (Konečn`y et al., 2015, 2016). Relatedly, one\nbeneficial use of transfer learning is the possibility of attaining good performance on\nlow-resource tasks. Low-resource tasks often occur (by definition) in settings where\none lacks the assets to label more data. It follows that low-resource applications often\nalso have limited access to computational resources which can incur additional costs.\nAs a result, we advocate for research on methods that achieve stronger performance\nwith cheaper models so that transfer learning can be applied where it will have the\nmost impact. Some current work along these lines include distillation (Hinton et al.,\n2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and\nconditional computation (Shazeer et al., 2017)."}
{"doc_id": "1910.10683", "para_id": 239, "text": "More efficient knowledge extraction Recall that one of the goals of pre-training is\n(loosely speaking) to provide the model with general-purpose “knowledge” that improves\nits performance on downstream tasks. The method we use in this work, which is\ncurrently common practice, is to train the model to denoise corrupted spans of text.\nWe suspect that this simplistic technique may not be a very efficient way to teach the\nmodel general-purpose knowledge. More concretely, it would be useful to be able to\nattain good fine-tuning performance without needing to train our models on 1 trillion\ntokens of text first. Some concurrent work along these lines improves efficiency by\npre-training a model to distinguish between real and machine-generated text (Clark\net al., 2020)."}
{"doc_id": "1910.10683", "para_id": 240, "text": "Formalizing the similarity between tasks We observed that pre-training on unlabeled\nin-domain data can improve performance on downstream tasks (Section 3.4). This\nfinding mostly relies on basic observations like the fact that SQuAD was created using\ndata from Wikipedia. It would be useful to formulate a more rigorous notion of the\n“similarity” between the pre-training and downstream tasks, so that we could make\nmore principled choices about what source of unlabeled data to use. There is some\nearly empirical work along these lines in the field of computer vision (Huh et al., 2016;\nKornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could\nalso help choose supervised pre-training tasks, which has been shown to be helpful for\nthe GLUE benchmark (Phang et al., 2018)."}
{"doc_id": "1910.10683", "para_id": 241, "text": "Language-agnostic models We were disappointed to find that English-only pre-training\ndid not achieve state-of-the-art results on the translation tasks we studied. We also\nare interested in avoiding the logistical difficulty of needing to specify which languages\na vocabulary can encode ahead of time. To address these issues, we are interested in\nfurther investigating language-agnostic models, i.e. models that can perform a given\nNLP task with good performance regardless of the text’s language. This is an especially"}
{"doc_id": "1910.10683", "para_id": 242, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 243, "text": "pertinent issue given that English is not the native language for the majority of the\nworld’s population."}
{"doc_id": "1910.10683", "para_id": 244, "text": "The motivation for this paper was the flurry of recent work on transfer learning for\nNLP. Before we began this work, these advances had already enabled breakthroughs in\nsettings where learning-based methods had not yet been shown to be effective. We are\nhappy to be able to continue this trend, for example by nearly matching human-level\nperformance on the SuperGLUE benchmark, a task specifically designed to be difficult\nfor modern transfer-learning pipelines. Our results stem from the combination of a\nstraightforward and unified text-to-text framework, our new C4 data set, and insights\nfrom our systematic study. Additionally, we provided an empirical overview of the\nfield and a perspective on where it stands. We are excited to see continued work using\ntransfer learning towards the goal of general language understanding."}
{"doc_id": "1910.10683", "para_id": 245, "text": "We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito,\nNoah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on\nthis manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for\nhis guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff\nKlingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil\nHoulsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield,\nYi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and\nPierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our\ndownload pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on\nSentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and\nmany other members of the Google Brain team for their discussion and insight."}
{"doc_id": "1910.10683", "para_id": 246, "text": "Colin designed the scope of this project and wrote this paper, ran all the experiments in\nSections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed\nmany of the ideas, including the text-to-text framework, unsupervised objectives, and\ndata set mixing strategies; implemented our base Transformer model and its architectural\nvariants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects\nfor this project, created the C4 data set, implemented our data set pipeline, and added\nvarious benchmark data sets.\nKatherine coordinated experiments, wrote and updated\ndocumentation, ran experiments to help design our baseline, and contributed to many parts\nof our codebase. Sharan contributed some of the required data sets and preprocessors, and\nran assorted preliminary experiments, in addition to co-leading the open-sourcing of our\ncodebase. Michael owned all aspects of the Winograd data sets, ingested many of the data\nsets we used, contributed various improvements and fixes to our infrastructure, and ran some\npreliminary experiments. Yanqi ran experiments and implemented methods to help settle on\na reasonable baseline and helped with the final fine-tuning of the models in Section 3.7. Wei\nalso helped with final fine-tuning and improved some of our preprocessors. Peter prototyped\nan early version of the pre-training data set and resolved issues pertaining to the SQuAD\nand CNN/DM tasks. All authors helped set the scope and research direction we followed in\nthis work."}
{"doc_id": "1910.10683", "para_id": 247, "text": "Appendix B. Converting WNLI to Our Text-to-Text Format"}
{"doc_id": "1910.10683", "para_id": 248, "text": "Note that as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead,\nwhen evaluating on the WNLI test set (for the results in Section 3.7), we convert the WNLI\ntest set to the “referent noun prediction” text-to-text format so that we can evaluate using a\nmodel trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed\nby He et al. (2019). Recall that examples from WNLI consist of a premise, a hypothesis,\nand a label that indicates whether the hypothesis is True or False. Using the example from\nSection 2.4, the hypothesis would be “The city councilmen refused the demonstrators a\npermit because they feared violence.” with the premise “The demonstrators feared violence.”\nand the label False. We first find the location of all pronouns in the premise (“they” in\nour example). Then, we find the maximum number of words that precede or follow each\npronoun that are a substring in the hypothesis (“feared violence” in our example), ignoring\ncase and punctuation. When the premise contains multiple candidate pronouns, we choose\nthe pronoun that is preceded or followed by the largest substring of the hypothesis. We then\nhighlight the pronoun in the premise by surrounding it with asterisks. For the candidate\nnoun (which is compared to our model’s prediction to obtain a True or False label), we\nremove the matching substring from the hypothesis and optionally make it non-possessive\n(resulting in “the demonstrators”)."}
{"doc_id": "1910.10683", "para_id": 249, "text": "To show that our model is generating fluent summaries, we include a few example decodes\nfrom our best model (T5-11B) on the validation set along with the ground-truth summaries.\nThese examples selected at random and were not cherry-picked."}
{"doc_id": "1910.10683", "para_id": 250, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 251, "text": "1. Ground-truth: leopard gave up after spiky creature refused to back down\nin fight in kruger national park, south africa .\nwildlife enthusiast lisl\nmoolman, 41, caught the bizarre battle while out on the road and armed with\nher camera .\nthe leopard and porcupine tumbled out of the bushes and began\nto fight by roadside - watched by ms moolman ."}
{"doc_id": "1910.10683", "para_id": 252, "text": "Prediction: leopard tried to make lunch out of a plucky porcupine in kruger\nnational park, south africa .\nbut the predator was put firmly in its place\nafter the spiky creature refused to back down during a fight .\nwildlife\nenthusiast lisl moolman, 41, caught the bizarre battle while out on the\nroad and armed with her camera ."}
{"doc_id": "1910.10683", "para_id": 253, "text": "2. Ground-truth: researchers say homes are a hotbed of undiscovered species\n.\nstudy of 40 north carolina homes found 8,000 bacterial and archaeal taxa\n.\n11 houses in california revealed a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 254, "text": "Prediction: researchers estimate that the indoor biome makes up about 0.5\npercent of ice-free land, or about 247,000 square miles, almost the size\nof texas .\na study of 40 houses in north carolina used genomic technologies\nto document more than 8,000 bacterial and archaeal taxa .\nanother study\nof 50 houses in the same state described more than 750 types of arthropods,\nincluding insects and spiders .\nan examination of 11 houses in california\nfound a great variety of fungus among us ."}
{"doc_id": "1910.10683", "para_id": 255, "text": "3. Ground-truth: executions have been temporarily halted in the state after\nthe drug used for lethal injections appeared faulty on monday night .\nkelly\ngissendaner had her death delayed for a second time when it was found that\nthe lethal pentobarbital dose appeared cloudy .\nstate officials dithered\nover the decision , changing their mind three times before deciding not\nto take the risk that it wouldn’t work .\nit is ’essential that executions\nare carried out in a constitutional manner,’ said georgia attorney general\nsam olens ."}
{"doc_id": "1910.10683", "para_id": 256, "text": "Prediction: the execution of kelly gissendaner, the only woman on georgia’s\ndeath row, was delayed for a second time on monday .\nthe execution team\nfound that the lethal pentobarbital dose appeared cloudy .\nthe cloudy drug\nbolstered death penalty opponents, who have been vocal in their opposition\nafter three botched executions in other parts of the country ."}
{"doc_id": "1910.10683", "para_id": 257, "text": "4. Ground-truth: dani alves was not selected for the brazil squad to face\nfrance and chile .\nbarcelona defender was filmed serving up hot dogs to\npeople on saturday .\nthis week alves released a charity single with former\nteam-mat jose pinto .\nalves looks set to leave barcelona on a free transfer\nthis summer .\nreports in spanish press claim he will sign for paris saint-germain\n."}
{"doc_id": "1910.10683", "para_id": 258, "text": "Prediction: dani alves was not selected for brazil’s friendlies against\nfrance and chile .\nthe barcelona right back has released a charity single\nwith jose pinto .\nalves was filmed serving up snacks from behind the counter"}
{"doc_id": "1910.10683", "para_id": 259, "text": "of a hot dog bar on saturday .\nalves is yet to be offered a new deal at\nthe nou camp .\nclick here for all the latest barcelona news ."}
{"doc_id": "1910.10683", "para_id": 260, "text": "In this section, we provide examples of our preprocessing for each of the data sets we consider."}
{"doc_id": "1910.10683", "para_id": 261, "text": "Processed input: cola sentence: John made Bill master of himself."}
{"doc_id": "1910.10683", "para_id": 262, "text": "Sentence 1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia\n(at the 1991 national census, some 3000 inhabitants of Slovenia declared\nthemselves as ethnic Italians)."}
{"doc_id": "1910.10683", "para_id": 263, "text": "Processed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians\nwere settled in Slovenia (at the 1991 national census, some 3000 inhabitants\nof Slovenia declared themselves as ethnic Italians).\nsentence2: Slovenia\nhas 3,000 inhabitants."}
{"doc_id": "1910.10683", "para_id": 264, "text": "Hypothesis: The St. Louis Cardinals have always won."}
{"doc_id": "1910.10683", "para_id": 265, "text": "Premise: yeah well losing is i mean i’m i’m originally from Saint Louis and\nSaint Louis Cardinals when they were there were uh a mostly a losing team\nbut"}
{"doc_id": "1910.10683", "para_id": 266, "text": "Processed input: mnli hypothesis: The St.\nLouis Cardinals have always won.\npremise:\nyeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis\nCardinals when they were there were uh a mostly a losing team but"}
{"doc_id": "1910.10683", "para_id": 267, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 268, "text": "Sentence 1: We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2: Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" ."}
{"doc_id": "1910.10683", "para_id": 269, "text": "Processed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid .\nsentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" ."}
{"doc_id": "1910.10683", "para_id": 270, "text": "Question: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand."}
{"doc_id": "1910.10683", "para_id": 271, "text": "Processed input: qnli question: Where did Jebe die?\nsentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand."}
{"doc_id": "1910.10683", "para_id": 272, "text": "Question 1: What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?"}
{"doc_id": "1910.10683", "para_id": 273, "text": "Processed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome?\nquestion2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?"}
{"doc_id": "1910.10683", "para_id": 274, "text": "Sentence: it confirms fincher ’s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight ."}
{"doc_id": "1910.10683", "para_id": 275, "text": "Processed input: sst2 sentence: it confirms fincher ’s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n."}
{"doc_id": "1910.10683", "para_id": 276, "text": "Sentence 1: Representatives for Puretunes could not immediately be reached\nfor comment Wednesday."}
{"doc_id": "1910.10683", "para_id": 277, "text": "Sentence 2: Puretunes representatives could not be located Thursday to comment\non the suit."}
{"doc_id": "1910.10683", "para_id": 278, "text": "Processed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday.\nsentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit."}
{"doc_id": "1910.10683", "para_id": 279, "text": "Premise: Valence the void-brain, Valence the virtuous valet.\nWhy couldn’t\nthe figger choose his own portion of titanic anatomy to shaft?\nDid he think\nhe was helping?"}
{"doc_id": "1910.10683", "para_id": 280, "text": "Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet.\nWhy couldn’t the figger choose his own portion\nof titanic anatomy to shaft?\nDid he think he was helping?"}
{"doc_id": "1910.10683", "para_id": 281, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 282, "text": "Premise: Political violence broke out in the nation."}
{"doc_id": "1910.10683", "para_id": 283, "text": "Choice 2: Many citizens took refuge in other territories."}
{"doc_id": "1910.10683", "para_id": 284, "text": "Processed input: copa choice1: Many citizens relocated to the capitol.\nchoice2:\nMany citizens took refuge in other territories.\npremise: Political violence\nbroke out in the nation.\nquestion: effect"}
{"doc_id": "1910.10683", "para_id": 285, "text": "Answer: There was only pie to eat, rather than traditional breakfast foods"}
{"doc_id": "1910.10683", "para_id": 286, "text": "Paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent\n2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent\n3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent\n4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s\npond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food\nbefore they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except\nfor pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),\nor oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went\nto the pond.<br><b>Sent 9: </b>On their way there they saw their friend\nJack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several\nhours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\n</b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went\noff to bed.<br>"}
{"doc_id": "1910.10683", "para_id": 287, "text": "Question: Why was Joey surprised the morning he woke up for breakfast?"}
{"doc_id": "1910.10683", "para_id": 288, "text": "Processed input: multirc question: Why was Joey surprised the morning he woke\nup for breakfast?\nanswer: There was only pie to eat, rather than traditional\nbreakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel\nnamed Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin\nJimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were\nalways laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together"}
{"doc_id": "1910.10683", "para_id": 289, "text": "at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning\nto eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything\nto eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit\n(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and\nJimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their\nfriend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for\nseveral hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The\ntwo squirrels ate some food that Joey’s mom, Jasmine, made and went off to\nbed.<br>"}
{"doc_id": "1910.10683", "para_id": 290, "text": "Sentence 1: It was the deliberation of his act that was insulting ."}
{"doc_id": "1910.10683", "para_id": 291, "text": "Processed input: wic pos: N sentence1: It was the deliberation of his act that\nwas insulting .\nsentence2: The deliberations of the jury .\nword: deliberation"}
{"doc_id": "1910.10683", "para_id": 292, "text": "Text: The stable was very roomy, with four good stalls; a large swinging window\nopened into the yard , which made it pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 293, "text": "Processed input: wsc:\nThe stable was very roomy, with four good stalls; a large\nswinging window opened into the yard , which made *it* pleasant and airy."}
{"doc_id": "1910.10683", "para_id": 294, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 295, "text": "Original input: marouane fellaini and adnan januzaj continue to show the world\nthey are not just teammates but also best mates.\nthe manchester united and\nbelgium duo both posted pictures of themselves out at a restaurant on monday\nnight ahead of their game against newcastle on wednesday .\njanuzaj poses\nin the middle of fellaini and a friend looking like somebody who failed to\nreceive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom\nfrom manchester united’s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame ."}
{"doc_id": "1910.10683", "para_id": 296, "text": "Processed input: summarize: marouane fellaini and adnan januzaj continue to show\nthe world they are not just teammates but also best mates.\nthe manchester\nunited and belgium duo both posted pictures of themselves out at a restaurant\non monday night ahead of their game against newcastle on wednesday .\njanuzaj\nposes in the middle of fellaini and a friend looking like somebody who failed\nto receive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom"}
{"doc_id": "1910.10683", "para_id": 297, "text": "from manchester united’s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame ."}
{"doc_id": "1910.10683", "para_id": 298, "text": "Original target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal’s side currently sit two points clear of liverpool\nin fourth ."}
{"doc_id": "1910.10683", "para_id": 299, "text": "Processed target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal’s side currently sit two points clear of liverpool\nin fourth ."}
{"doc_id": "1910.10683", "para_id": 300, "text": "Question: What does increased oxygen concentrations in the patient’s lungs\ndisplace?"}
{"doc_id": "1910.10683", "para_id": 301, "text": "Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers\nto increase the partial pressure of O 2 around the patient and, when needed,\nthe medical staff.\nCarbon monoxide poisoning, gas gangrene, and decompression\nsickness (the ’bends’) are sometimes treated using these devices.\nIncreased\nO 2 concentration in the lungs helps to displace carbon monoxide from the\nheme group of hemoglobin.\nOxygen gas is poisonous to the anaerobic bacteria\nthat cause gas gangrene, so increasing its partial pressure helps kill them.\nDecompression sickness occurs in divers who decompress too quickly after\na dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming\nin their blood.\nIncreasing the pressure of O 2 as soon as possible is part\nof the treatment."}
{"doc_id": "1910.10683", "para_id": 302, "text": "Processed input: question: What does increased oxygen concentrations in the patient’s\nlungs displace?\ncontext: Hyperbaric (high-pressure) medicine uses special\noxygen chambers to increase the partial pressure of O 2 around the patient\nand, when needed, the medical staff.\nCarbon monoxide poisoning, gas gangrene,\nand decompression sickness (the ’bends’) are sometimes treated using these\ndevices.\nIncreased O 2 concentration in the lungs helps to displace carbon\nmonoxide from the heme group of hemoglobin.\nOxygen gas is poisonous to the\nanaerobic bacteria that cause gas gangrene, so increasing its partial pressure\nhelps kill them.\nDecompression sickness occurs in divers who decompress too\nquickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and\nhelium, forming in their blood.\nIncreasing the pressure of O 2 as soon as\npossible is part of the treatment."}
{"doc_id": "1910.10683", "para_id": 303, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 304, "text": "Original input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 305, "text": "Processed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote."}
{"doc_id": "1910.10683", "para_id": 306, "text": "Original target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 307, "text": "Processed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie."}
{"doc_id": "1910.10683", "para_id": 308, "text": "Original input: This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 309, "text": "Processed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room."}
{"doc_id": "1910.10683", "para_id": 310, "text": "Original target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 311, "text": "Processed target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers."}
{"doc_id": "1910.10683", "para_id": 312, "text": "Original input: Taco Bell said it plans to add 2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 313, "text": "Processed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022."}
{"doc_id": "1910.10683", "para_id": 314, "text": "Original target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă\n2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 315, "text": "Processed target: Taco Bell a afirmat că, până în 2022, intent,ionează să deschidă\n2000 de restaurante în SUA."}
{"doc_id": "1910.10683", "para_id": 316, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 317, "text": "Appendix E. Scores on Every Task for All Experiments"}
{"doc_id": "1910.10683", "para_id": 318, "text": "The following table lists the scores achieved on every task in the experiments described in\nSections 3.2 to 3.6."}
{"doc_id": "1910.10683", "para_id": 319, "text": "GLUE\nSuperGLUE\nWMT\nScore\nCoLA\nSST-2\nMRPC\nMRPC\nSTSB\nSTSB\nQQP\nQQP\nMNLIm\nMNLImm\nQNLI\nRTE\nCNN/DM\nSQuAD\nScore\nBoolQ\nCB\nCB\nCOPA\nMultiRC\nMultiRC\nReCoRD\nReCoRD\nRTE\nWiC\nWSC\nEnDe\nEnFr\nEnRo\nTable\nExperiment\nAverage\nMCC\nAcc\nF1\nAcc\nPCC\nSCC\nF1\nAcc\nAcc\nAcc\nAcc\nAcc\nR-1-F\nR-2-F\nR-L-F\nEM\nF1\nAverage\nAcc\nF1\nAcc\nAcc\nF1\nEM\nF1\nEM\nAcc\nAcc\nAcc\nBLEU\nBLEU\nBLEU"}
{"doc_id": "1910.10683", "para_id": 320, "text": "1\n⋆Baseline average\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n1\nBaseline standard deviation\n0.235\n1.111\n0.569\n0.729\n1.019\n0.374\n0.418\n0.108\n0.070\n0.291\n0.231\n0.361\n1.393\n0.065\n0.065\n0.058\n0.343\n0.226\n0.416\n0.365\n3.237\n2.560\n2.741\n0.716\n1.011\n0.370\n0.379\n1.228\n0.850\n2.029\n0.112\n0.090\n0.108\n1\nNo pre-training\n66.22\n12.29\n80.62\n81.42\n73.04\n72.58\n72.97\n81.94\n86.62\n68.02\n67.98\n75.69\n58.84\n39.19\n17.60\n36.69\n50.31\n61.97\n53.04\n65.38\n71.61\n76.79\n62.00\n59.10\n0.84\n20.33\n17.95\n54.15\n54.08\n65.38\n25.86\n39.77\n24.04"}
{"doc_id": "1910.10683", "para_id": 321, "text": "2\n⋆Enc/dec, denoising\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n2\nEnc/dec, shared, denoising\n82.81\n55.24\n91.86\n91.58\n88.24\n87.43\n87.58\n88.69\n91.60\n83.88\n84.01\n90.23\n73.65\n41.11\n18.78\n38.48\n80.63\n88.49\n70.73\n77.13\n95.04\n96.43\n65.00\n66.16\n22.98\n68.95\n68.09\n70.76\n68.18\n75.96\n26.72\n39.03\n27.46\n2\nEnc/dec, 6 layers, denoising\n80.88\n46.26\n92.09\n91.51\n87.99\n87.01\n86.76\n87.93\n90.97\n82.20\n82.41\n88.83\n71.48\n40.83\n18.97\n38.31\n77.59\n86.07\n68.42\n73.79\n91.70\n92.86\n67.00\n61.02\n19.62\n61.26\n60.33\n72.20\n65.99\n75.00\n26.38\n38.40\n26.95\n2\nLanguage model, denoising\n74.70\n24.50\n90.60\n86.08\n78.92\n85.22\n85.42\n85.40\n88.99\n76.72\n77.05\n86.02\n64.62\n39.49\n17.93\n36.91\n61.14\n71.37\n55.02\n65.47\n60.08\n71.43\n58.00\n43.03\n2.94\n53.35\n52.31\n53.07\n58.62\n63.46\n25.09\n35.28\n25.86\n2\nPrefix LM, denoising\n81.82\n49.99\n92.43\n91.43\n88.24\n87.20\n86.98\n88.41\n91.39\n82.32\n82.93\n88.71\n74.01\n40.46\n18.61\n37.90\n78.94\n87.31\n68.11\n75.50\n93.37\n91.07\n60.00\n63.43\n21.20\n65.03\n64.11\n71.48\n65.67\n73.08\n26.43\n37.98\n27.39\n2\nEnc/dec, LM\n79.56\n42.03\n91.86\n91.64\n88.24\n87.13\n87.00\n88.21\n91.15\n81.68\n81.66\n88.54\n65.70\n40.67\n18.59\n38.13\n76.02\n84.85\n64.29\n72.23\n85.74\n89.29\n57.00\n60.53\n16.26\n59.28\n58.30\n65.34\n64.89\n70.19\n26.27\n39.17\n26.86\n2\nEnc/dec, shared, LM\n79.60\n44.83\n92.09\n90.20\n85.78\n86.03\n85.87\n87.77\n91.02\n81.74\n82.29\n89.16\n65.34\n40.16\n18.13\n37.59\n76.35\n84.86\n63.50\n70.49\n91.41\n87.50\n55.00\n60.21\n16.89\n57.83\n56.73\n63.54\n63.48\n70.19\n26.62\n39.17\n27.05\n2\nEnc/dec, 6 layers, LM\n78.67\n38.72\n91.40\n90.40\n86.52\n86.82\n86.49\n87.87\n91.03\n80.99\n80.92\n88.05\n65.70\n40.29\n18.26\n37.70\n75.32\n84.06\n64.06\n71.38\n85.25\n89.29\n60.00\n57.56\n16.79\n55.22\n54.30\n66.79\n63.95\n71.15\n26.13\n38.42\n26.89\n2\nLanguage model, LM\n73.78\n28.53\n89.79\n85.23\n78.68\n84.22\n84.00\n84.88\n88.70\n74.94\n75.77\n84.84\n58.84\n38.97\n17.54\n36.37\n53.81\n64.55\n56.51\n64.22\n59.92\n71.43\n64.00\n53.04\n1.05\n46.81\n45.78\n58.84\n56.74\n69.23\n25.23\n34.31\n25.38\n2\nPrefix LM, LM\n79.68\n41.26\n92.09\n90.11\n86.27\n86.82\n86.32\n88.35\n91.35\n81.71\n82.02\n89.04\n68.59\n39.66\n17.84\n37.13\n76.87\n85.39\n64.86\n71.47\n93.37\n91.07\n57.00\n58.67\n16.89\n59.25\n58.16\n64.26\n66.30\n71.15\n26.28\n37.51\n26.76"}
{"doc_id": "1910.10683", "para_id": 322, "text": "4\nLanguage modeling with prefix\n80.69\n44.22\n93.00\n91.68\n88.48\n87.20\n87.18\n88.39\n91.41\n82.66\n83.09\n89.29\n68.95\n40.71\n18.94\n38.15\n77.99\n86.43\n65.27\n73.55\n83.95\n87.50\n55.00\n59.65\n18.89\n61.76\n60.76\n68.59\n65.67\n73.08\n26.86\n39.73\n27.49\n4\nBERT-style (Devlin et al., 2018)\n82.96\n52.49\n92.55\n92.79\n89.95\n87.68\n87.66\n88.47\n91.44\n83.60\n84.05\n90.33\n75.45\n41.27\n19.17\n38.72\n80.65\n88.24\n69.85\n76.48\n94.37\n94.64\n61.00\n63.29\n25.08\n66.76\n65.85\n72.20\n69.12\n75.00\n26.78\n40.03\n27.41\n4\nDeshuffling\n73.17\n22.82\n87.16\n86.88\n81.13\n84.03\n83.82\n86.38\n89.90\n76.30\n76.34\n84.18\n58.84\n40.75\n18.59\n38.10\n67.61\n76.76\n58.47\n69.17\n63.70\n78.57\n56.00\n59.85\n12.70\n45.52\n44.36\n57.04\n64.89\n68.27\n26.11\n39.30\n25.62"}
{"doc_id": "1910.10683", "para_id": 323, "text": "5\nBERT-style (Devlin et al., 2018)\n82.96\n52.49\n92.55\n92.79\n89.95\n87.68\n87.66\n88.47\n91.44\n83.60\n84.05\n90.33\n75.45\n41.27\n19.17\n38.72\n80.65\n88.24\n69.85\n76.48\n94.37\n94.64\n61.00\n63.29\n25.08\n66.76\n65.85\n72.20\n69.12\n75.00\n26.78\n40.03\n27.41\n5\nMASS-style (Song et al., 2019)\n82.32\n47.01\n91.63\n92.53\n89.71\n88.21\n88.18\n88.58\n91.44\n82.96\n83.67\n90.02\n77.26\n41.16\n19.16\n38.55\n80.10\n88.07\n69.28\n75.08\n84.98\n89.29\n63.00\n64.46\n23.50\n66.71\n65.91\n72.20\n67.71\n78.85\n26.79\n39.89\n27.55\n5\n⋆Replace corrupted spans\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n5\nDrop corrupted tokens\n84.44\n60.04\n92.89\n92.79\n89.95\n87.28\n86.85\n88.56\n91.54\n83.94\n83.92\n90.74\n79.42\n41.27\n19.31\n38.70\n80.52\n88.28\n68.67\n75.90\n96.02\n94.64\n56.00\n65.06\n23.92\n65.54\n64.60\n71.12\n67.40\n74.04\n27.07\n39.76\n27.82"}
{"doc_id": "1910.10683", "para_id": 324, "text": "6\nCorruption rate = 10%\n82.82\n52.71\n92.09\n91.55\n88.24\n88.19\n88.15\n88.47\n91.40\n83.50\n84.51\n90.33\n75.45\n41.05\n19.00\n38.53\n80.38\n88.36\n69.55\n74.98\n92.37\n92.86\n62.00\n66.04\n24.66\n67.93\n67.09\n70.76\n67.24\n75.96\n26.87\n39.28\n27.44\n6\n⋆Corruption rate = 15%\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n6\nCorruption rate = 25%\n83.00\n53.47\n93.00\n92.44\n89.46\n87.36\n87.36\n88.68\n91.53\n84.44\n84.15\n90.77\n74.01\n41.69\n19.54\n39.14\n80.96\n88.61\n70.48\n76.39\n93.02\n92.86\n68.00\n65.46\n24.66\n68.20\n67.39\n73.65\n67.87\n72.12\n27.04\n39.83\n27.47\n6\nCorruption rate = 50%\n81.27\n46.26\n91.63\n91.11\n87.99\n87.87\n87.64\n88.70\n91.57\n83.64\n84.10\n90.24\n70.76\n41.51\n19.32\n38.89\n79.80\n87.76\n70.33\n75.02\n93.05\n92.86\n68.00\n62.97\n24.13\n64.94\n64.13\n72.20\n68.50\n77.88\n27.01\n39.90\n27.49"}
{"doc_id": "1910.10683", "para_id": 325, "text": "7\n⋆Baseline (i.i.d.)\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n7\nAverage span length = 2\n83.54\n53.82\n92.20\n93.05\n90.44\n87.85\n87.71\n88.42\n91.40\n84.28\n84.46\n90.88\n77.62\n41.23\n19.39\n38.69\n82.09\n89.69\n72.20\n77.06\n90.43\n91.07\n70.00\n66.28\n26.13\n71.34\n70.61\n75.45\n68.34\n78.85\n26.76\n39.99\n27.63\n7\nAverage span length = 3\n83.49\n53.90\n92.43\n92.25\n89.46\n87.49\n87.53\n88.72\n91.51\n84.85\n84.84\n90.99\n77.26\n41.50\n19.62\n38.94\n81.84\n89.66\n72.53\n76.85\n94.37\n94.64\n70.00\n67.64\n28.75\n70.84\n69.90\n74.73\n67.71\n77.88\n26.86\n39.65\n27.62\n7\nAverage span length = 5\n83.40\n52.12\n93.12\n92.63\n89.71\n88.70\n88.47\n88.84\n91.64\n84.32\n84.29\n90.79\n76.90\n41.39\n19.24\n38.82\n82.05\n89.79\n72.23\n77.06\n83.06\n89.29\n69.00\n68.16\n30.12\n71.36\n70.53\n75.81\n69.91\n79.81\n26.88\n39.40\n27.53\n7\nAverage span length = 10\n82.85\n50.11\n92.09\n91.95\n88.97\n88.45\n88.22\n88.86\n91.63\n84.34\n84.28\n91.07\n76.17\n41.38\n19.33\n38.80\n81.84\n89.39\n70.44\n76.45\n87.40\n89.29\n65.00\n66.87\n29.59\n69.82\n68.94\n72.56\n67.55\n75.96\n26.79\n39.49\n27.69"}
{"doc_id": "1910.10683", "para_id": 326, "text": "8\n⋆C4\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n8\nC4, unfiltered\n81.46\n48.01\n91.63\n92.72\n89.95\n87.79\n87.60\n88.31\n91.27\n82.30\n82.34\n88.71\n72.20\n41.09\n19.14\n38.54\n78.78\n87.04\n68.04\n75.75\n89.17\n91.07\n62.00\n65.52\n25.60\n62.42\n61.58\n69.68\n67.08\n72.12\n26.55\n39.34\n27.21\n8\nRealNews-like\n83.83\n56.55\n92.66\n92.06\n88.97\n87.71\n87.37\n88.51\n91.49\n84.35\n84.46\n90.61\n78.34\n41.38\n19.23\n38.84\n80.39\n88.50\n72.38\n77.00\n93.09\n94.64\n66.00\n65.92\n23.82\n74.56\n73.72\n75.81\n66.61\n80.77\n26.75\n39.90\n27.48\n8\nWebText-like\n84.03\n56.38\n93.12\n92.31\n89.22\n88.69\n88.68\n88.65\n91.56\n84.70\n84.84\n90.83\n77.62\n41.23\n19.31\n38.70\n81.42\n89.15\n71.40\n76.88\n83.08\n89.29\n66.00\n64.10\n24.24\n72.24\n71.36\n75.45\n68.03\n82.69\n26.80\n39.74\n27.59\n8\nWikipedia\n81.85\n45.53\n92.32\n91.67\n88.24\n85.62\n86.40\n88.37\n91.34\n82.61\n83.25\n90.96\n77.26\n41.39\n19.31\n38.81\n81.29\n89.18\n68.01\n76.12\n56.03\n80.36\n67.00\n65.01\n25.92\n69.03\n68.06\n74.73\n67.08\n76.92\n26.94\n39.69\n27.67\n8\nWikipedia + TBC\n83.65\n55.53\n92.78\n92.41\n89.22\n86.67\n86.27\n89.47\n92.29\n84.38\n83.45\n91.94\n76.90\n41.22\n19.28\n38.67\n82.08\n89.70\n73.24\n76.22\n95.40\n92.86\n69.00\n51.59\n50.93\n69.53\n68.51\n77.62\n66.93\n81.73\n26.77\n39.63\n27.57"}
{"doc_id": "1910.10683", "para_id": 327, "text": "9\n⋆Full data set\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n9\n229 (64 repeats)\n82.87\n53.82\n92.78\n91.79\n88.73\n87.56\n87.58\n88.73\n91.54\n84.07\n84.21\n90.59\n73.65\n41.18\n19.19\n38.67\n80.97\n88.90\n72.03\n76.76\n92.96\n92.86\n66.00\n65.11\n26.76\n69.35\n68.49\n75.81\n67.24\n82.69\n26.83\n39.74\n27.63\n9\n227 (256 repeats)\n82.62\n50.60\n92.32\n92.07\n88.73\n87.83\n87.60\n88.65\n91.54\n83.43\n84.37\n90.12\n75.81\n41.24\n19.20\n38.70\n79.78\n87.63\n69.97\n75.29\n93.42\n91.07\n63.00\n61.82\n23.61\n66.27\n65.39\n73.65\n66.30\n80.77\n27.02\n39.71\n27.33\n9\n225 (1,024 repeats)\n79.55\n43.84\n91.28\n89.32\n85.05\n85.92\n85.74\n88.05\n91.09\n81.29\n81.72\n87.90\n69.31\n40.66\n18.57\n38.13\n76.27\n84.58\n64.76\n72.63\n83.97\n82.14\n64.00\n59.39\n17.94\n56.94\n56.04\n64.98\n65.20\n73.08\n26.38\n39.56\n26.80\n9\n223 (4,096 repeats)\n76.34\n32.68\n89.45\n89.84\n86.03\n83.49\n83.42\n87.18\n90.61\n77.80\n78.69\n85.47\n64.62\n40.16\n18.33\n37.66\n70.92\n80.20\n59.29\n69.85\n73.48\n73.21\n56.00\n57.66\n14.38\n46.69\n45.79\n59.57\n65.05\n68.27\n26.37\n38.84\n25.81"}
{"doc_id": "1910.10683", "para_id": 328, "text": "10\n⋆All parameters\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n10\nAdapter layers, d = 32\n80.52\n45.33\n91.63\n90.59\n86.76\n88.38\n88.06\n86.99\n90.26\n83.63\n83.94\n90.72\n67.15\n34.50\n15.08\n32.15\n79.32\n87.70\n60.40\n65.32\n50.87\n73.21\n52.00\n58.61\n19.41\n65.50\n64.58\n62.09\n64.58\n73.08\n13.84\n17.88\n15.54\n10\nAdapter layers, d = 128\n81.51\n45.35\n92.89\n91.49\n88.24\n87.73\n87.65\n87.73\n90.93\n83.64\n84.09\n90.52\n72.56\n36.71\n16.62\n34.37\n79.47\n87.61\n63.03\n69.20\n52.21\n75.00\n56.00\n61.08\n18.05\n67.94\n66.97\n68.59\n66.77\n73.08\n19.83\n27.50\n22.63\n10\nAdapter layers, d = 512\n81.54\n44.25\n93.35\n91.00\n87.25\n88.74\n88.44\n88.02\n91.15\n83.08\n83.80\n89.62\n74.37\n38.63\n17.78\n36.25\n79.18\n87.32\n64.30\n73.18\n59.86\n71.43\n56.00\n62.94\n18.57\n66.56\n65.74\n70.76\n67.87\n74.04\n23.45\n33.98\n25.81\n10\nAdapter layers, d = 2048\n82.62\n49.86\n92.55\n91.30\n87.99\n88.46\n88.35\n88.36\n91.40\n83.63\n83.18\n90.66\n76.53\n39.44\n18.30\n37.06\n79.40\n87.36\n68.61\n74.53\n88.00\n91.07\n58.00\n61.10\n18.89\n66.73\n66.06\n73.29\n71.16\n75.96\n25.64\n36.92\n26.93\n10\nGradual Unfreezing\n82.50\n51.74\n91.97\n92.61\n89.71\n87.27\n86.90\n88.26\n91.35\n83.42\n83.49\n89.71\n75.09\n40.88\n18.95\n38.40\n79.17\n87.30\n70.79\n75.51\n93.09\n94.64\n70.00\n62.03\n21.51\n65.69\n64.79\n72.92\n69.12\n77.89\n26.71\n39.02\n26.93"}
{"doc_id": "1910.10683", "para_id": 329, "text": "11\n⋆Baseline (pre-train/fine-tune)\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n11\nEqual\n76.13\n39.47\n90.94\n82.90\n75.74\n78.83\n78.44\n86.45\n89.71\n82.08\n82.92\n90.13\n59.93\n40.95\n19.02\n38.39\n76.51\n85.61\n63.37\n73.06\n82.37\n83.93\n65.00\n60.89\n17.52\n60.51\n59.70\n61.01\n60.03\n65.38\n23.89\n34.31\n26.78\n11\nExamples-proportional, K = 216\n80.45\n42.07\n91.97\n90.97\n87.50\n85.41\n85.04\n86.89\n90.10\n83.01\n83.66\n90.74\n72.56\n41.16\n19.04\n38.59\n77.25\n85.72\n69.95\n76.67\n86.38\n89.29\n70.00\n65.93\n27.91\n62.78\n61.95\n76.90\n65.83\n73.08\n24.35\n34.99\n27.10\n11\nExamples-proportional, K = 217\n81.56\n47.35\n91.40\n91.55\n88.24\n86.15\n85.93\n86.94\n90.06\n82.76\n84.12\n90.79\n75.09\n41.06\n19.12\n38.47\n77.00\n85.87\n67.91\n77.89\n77.54\n85.71\n57.00\n67.78\n27.07\n61.51\n60.54\n79.06\n65.20\n74.04\n24.36\n35.00\n27.25\n11\nExamples-proportional, K = 218\n81.67\n46.85\n91.63\n91.99\n88.73\n87.68\n87.20\n86.93\n90.35\n83.30\n84.01\n91.47\n73.29\n40.96\n19.07\n38.43\n78.17\n86.74\n67.94\n76.57\n78.88\n87.50\n62.00\n67.70\n30.85\n63.43\n62.54\n76.53\n65.67\n67.31\n24.57\n35.19\n27.39\n11\nExamples-proportional, K = 219\n81.42\n45.94\n91.63\n92.20\n89.22\n88.44\n88.32\n86.84\n90.10\n83.73\n84.29\n91.84\n70.40\n41.26\n19.24\n38.71\n79.78\n88.15\n67.30\n75.66\n75.59\n87.50\n59.00\n68.22\n30.64\n65.32\n64.29\n73.65\n65.05\n69.23\n25.21\n36.30\n27.76\n11\nExamples-proportional, K = 220\n80.80\n42.55\n92.78\n91.27\n87.99\n88.36\n88.10\n86.10\n89.62\n84.15\n84.26\n92.20\n68.95\n41.05\n19.24\n38.46\n80.36\n88.27\n67.38\n73.21\n76.18\n83.93\n62.00\n67.57\n26.86\n66.12\n65.22\n76.90\n64.73\n69.23\n25.66\n36.93\n27.68\n11\nExamples-proportional, K = 221\n79.83\n44.45\n91.28\n89.00\n84.31\n87.54\n87.40\n84.93\n88.53\n82.54\n84.16\n90.85\n67.87\n40.51\n18.79\n37.92\n79.50\n87.48\n65.10\n71.16\n68.88\n85.71\n57.00\n62.75\n23.40\n64.50\n63.65\n72.92\n64.11\n71.15\n25.82\n37.22\n27.13\n11\nTemperature-scaled, T = 2\n81.90\n54.00\n91.74\n90.56\n86.76\n85.11\n84.60\n86.40\n89.74\n83.47\n84.15\n91.51\n72.56\n41.09\n19.28\n38.54\n79.42\n87.77\n69.92\n76.73\n92.37\n92.86\n57.00\n69.80\n31.90\n66.65\n65.74\n72.92\n67.08\n75.96\n25.42\n36.72\n27.20\n11\nTemperature-scaled, T = 4\n80.56\n45.38\n91.97\n89.68\n85.78\n83.13\n82.76\n86.39\n90.00\n82.78\n84.19\n91.16\n73.65\n41.09\n19.22\n38.51\n77.99\n86.81\n69.54\n76.76\n97.36\n96.43\n59.00\n68.10\n31.48\n64.26\n63.27\n74.73\n64.26\n71.15\n25.04\n35.82\n27.45\n11\nTemperature-scaled, T = 8\n77.21\n40.07\n91.06\n88.11\n83.33\n79.20\n79.06\n86.60\n89.90\n83.05\n83.56\n90.21\n59.93\n41.01\n19.10\n38.40\n77.14\n85.99\n66.07\n73.94\n93.70\n94.64\n60.00\n66.36\n26.86\n63.46\n62.60\n62.09\n63.32\n65.38\n24.55\n35.35\n27.17"}
{"doc_id": "1910.10683", "para_id": 330, "text": "12\n⋆Unsupervised pre-training + fine-tuning\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n12\nMulti-task training\n81.42\n45.94\n91.63\n92.20\n89.22\n88.44\n88.32\n86.84\n90.10\n83.73\n84.29\n91.84\n70.40\n41.26\n19.24\n38.71\n79.78\n88.15\n67.30\n75.66\n75.59\n87.50\n59.00\n68.22\n30.64\n65.32\n64.29\n73.65\n65.05\n69.23\n25.21\n36.30\n27.76\n12\nMulti-task pre-training + fine-tuning\n83.11\n51.42\n92.66\n91.73\n88.73\n88.06\n87.70\n88.61\n91.61\n84.09\n84.31\n91.85\n76.53\n41.15\n19.12\n38.59\n80.26\n88.50\n71.03\n79.54\n81.69\n87.50\n65.00\n70.72\n31.48\n65.94\n65.03\n81.23\n68.18\n73.08\n27.08\n39.80\n28.07\n12\nLeave-one-out multi-task training\n81.98\n48.00\n93.23\n91.72\n88.24\n87.76\n87.32\n88.61\n91.44\n84.00\n84.11\n90.79\n72.20\n41.34\n19.05\n38.77\n79.97\n88.10\n71.68\n78.35\n86.76\n89.29\n66.00\n68.09\n29.49\n66.23\n65.27\n79.06\n68.65\n78.85\n26.93\n39.79\n27.87\n12\nSupervised multi-task pre-training\n79.93\n36.60\n92.43\n91.58\n88.24\n87.03\n86.78\n88.15\n91.20\n82.87\n83.16\n90.13\n70.76\n41.12\n18.96\n38.49\n77.38\n85.65\n65.36\n75.66\n68.87\n83.93\n58.00\n64.81\n21.93\n55.37\n54.61\n71.12\n67.40\n75.96\n26.81\n40.13\n28.04"}
{"doc_id": "1910.10683", "para_id": 331, "text": "13\n⋆Baseline\n83.28\n53.84\n92.68\n92.07\n88.92\n88.02\n87.94\n88.67\n91.56\n84.24\n84.57\n90.48\n76.28\n41.33\n19.24\n38.77\n80.88\n88.81\n71.36\n76.62\n91.22\n91.96\n66.20\n66.13\n25.78\n69.05\n68.16\n75.34\n68.04\n78.56\n26.98\n39.82\n27.65\n13\n1× size, 4× training steps\n85.33\n60.29\n93.81\n94.06\n91.67\n89.42\n89.25\n89.15\n91.87\n86.01\n85.70\n91.63\n78.34\n41.52\n19.33\n38.96\n82.45\n90.19\n74.72\n79.17\n94.75\n92.86\n71.00\n67.34\n29.70\n72.63\n71.59\n78.34\n72.10\n82.69\n27.08\n40.66\n27.93\n13\n1× size, 4× batch size\n84.60\n56.08\n93.12\n92.31\n89.22\n88.85\n88.84\n89.35\n92.07\n85.98\n86.13\n91.07\n80.14\n41.70\n19.42\n39.08\n82.52\n90.21\n74.64\n78.78\n93.69\n94.64\n72.00\n68.09\n30.95\n74.73\n73.90\n76.53\n70.06\n81.73\n27.07\n40.60\n27.84\n13\n2× size, 2× training steps\n86.18\n62.04\n93.69\n93.36\n90.69\n89.18\n89.23\n89.35\n92.05\n87.23\n87.05\n92.68\n81.95\n41.74\n19.66\n39.14\n84.18\n91.29\n77.18\n80.98\n97.36\n96.43\n74.00\n71.34\n35.68\n77.11\n76.34\n80.51\n69.28\n85.58\n27.52\n41.03\n28.19\n13\n4× size, 1× training steps\n85.91\n57.58\n94.38\n92.67\n89.95\n89.60\n89.60\n89.44\n92.14\n87.05\n87.12\n93.12\n83.39\n41.60\n19.73\n39.08\n83.86\n91.32\n78.04\n81.38\n89.09\n94.64\n73.00\n73.74\n40.40\n78.25\n77.40\n81.59\n70.22\n91.35\n27.47\n40.71\n28.10\n13\n4× ensembled\n84.77\n56.14\n93.46\n93.31\n90.67\n89.71\n89.60\n89.62\n92.24\n86.22\n86.53\n91.60\n77.98\n42.10\n20.10\n39.56\n83.09\n90.40\n71.74\n77.58\n89.85\n91.07\n66.00\n69.32\n29.49\n72.67\n71.94\n76.90\n69.12\n72.12\n28.05\n40.53\n28.09\n13\n4× ensembled, fine-tune only\n84.05\n54.78\n92.78\n93.15\n90.44\n88.34\n88.12\n89.27\n91.97\n85.33\n85.88\n90.98\n77.62\n41.66\n19.57\n39.12\n82.36\n89.86\n71.56\n77.43\n90.07\n92.86\n69.00\n67.31\n26.34\n70.47\n69.64\n75.45\n68.18\n74.04\n27.55\n40.22\n28.09"}
{"doc_id": "1910.10683", "para_id": 332, "text": "Table 16: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with ⋆denotes our baseline model (described in Section 3.1)."}
{"doc_id": "1910.10683", "para_id": 333, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 334, "text": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. In Proceedings of the AAAI Conference on\nArtificial Intelligence, 2019."}
{"doc_id": "1910.10683", "para_id": 335, "text": "Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efficient adaptive\noptimization for large-scale learning. arXiv preprint arXiv:1901.11150, 2019."}
{"doc_id": "1910.10683", "para_id": 336, "text": "Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multi-\nlingual neural machine translation in the wild: Findings and challenges. arXiv preprint\narXiv:1907.05019, 2019."}
{"doc_id": "1910.10683", "para_id": 337, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016."}
{"doc_id": "1910.10683", "para_id": 338, "text": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-\ndriven pretraining of self-attention networks. arXiv preprint arXiv:1903.07785, 2019."}
{"doc_id": "1910.10683", "para_id": 339, "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In Third International Conference on Learning\nRepresentations, 2015."}
{"doc_id": "1910.10683", "para_id": 340, "text": "Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural\nmachine translation. arXiv preprint arXiv:1909.08478, 2019."}
{"doc_id": "1910.10683", "para_id": 341, "text": "Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific\ntext. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019."}
{"doc_id": "1910.10683", "para_id": 342, "text": "Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo-\nhannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al.\nFindings of the 2014 workshop on statistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation, 2014."}
{"doc_id": "1910.10683", "para_id": 343, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck,\nChris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al.\nFindings of the 2015 workshop on statistical machine translation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation, 2015."}
{"doc_id": "1910.10683", "para_id": 344, "text": "Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz,\net al. Findings of the 2016 conference on machine translation. In Proceedings of the First\nConference on Machine Translation, 2016."}
{"doc_id": "1910.10683", "para_id": 345, "text": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy\nBengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349,\n2015."}
{"doc_id": "1910.10683", "para_id": 346, "text": "Christian Buck, Kenneth Heafield, and Bas Van Ooyen. N-gram counts and language models\nfrom the common crawl. In LREC, 2014."}
{"doc_id": "1910.10683", "para_id": 347, "text": "Rich Caruana. Multitask learning. Machine learning, 28(1), 1997."}
{"doc_id": "1910.10683", "para_id": 348, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.\narXiv preprint arXiv:1708.00055, 2017."}
{"doc_id": "1910.10683", "para_id": 349, "text": "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\nmachine reading. arXiv preprint arXiv:1601.06733, 2016."}
{"doc_id": "1910.10683", "para_id": 350, "text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019."}
{"doc_id": "1910.10683", "para_id": 351, "text": "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning.\nElectra:\nPre-training text encoders as discriminators rather than generators.\narXiv preprint\narXiv:2003.10555, 2020."}
{"doc_id": "1910.10683", "para_id": 352, "text": "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence\nrepresentations. arXiv preprint arXiv:1803.05449, 2018."}
{"doc_id": "1910.10683", "para_id": 353, "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Super-\nvised learning of universal sentence representations from natural language inference data.\narXiv preprint arXiv:1705.02364, 2017."}
{"doc_id": "1910.10683", "para_id": 354, "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual\nentailment challenge. In Machine Learning Challenges Workshop, 2005."}
{"doc_id": "1910.10683", "para_id": 355, "text": "Andrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. In Advances in neural\ninformation processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 356, "text": "Marie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\nInvestigating projection in naturally occurring discourse. In Sinn und Bedeutung 23, 2019."}
{"doc_id": "1910.10683", "para_id": 357, "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision and\npattern recognition, 2009."}
{"doc_id": "1910.10683", "para_id": 358, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018."}
{"doc_id": "1910.10683", "para_id": 359, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),\n2005."}
{"doc_id": "1910.10683", "para_id": 360, "text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language\nunderstanding and generation. arXiv preprint arXiv:1905.03197, 2019."}
{"doc_id": "1910.10683", "para_id": 361, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 362, "text": "Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation\nat scale. arXiv preprint arXiv:1808.09381, 2018."}
{"doc_id": "1910.10683", "para_id": 363, "text": "Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\nLearning word vectors for 157 languages. arXiv preprint arXiv:1802.06893, 2018."}
{"doc_id": "1910.10683", "para_id": 364, "text": "Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013."}
{"doc_id": "1910.10683", "para_id": 365, "text": "Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\nwith free license. In Proceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 914–922, 2016."}
{"doc_id": "1910.10683", "para_id": 366, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016."}
{"doc_id": "1910.10683", "para_id": 367, "text": "Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training. arXiv\npreprint arXiv:1811.08883, 2018."}
{"doc_id": "1910.10683", "para_id": 368, "text": "Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\nmodel for commonsense reasoning. arXiv preprint arXiv:1907.11983, 2019."}
{"doc_id": "1910.10683", "para_id": 369, "text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\nAdvances in neural information processing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 370, "text": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically. arXiv preprint arXiv:1712.00409, 2017."}
{"doc_id": "1910.10683", "para_id": 371, "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016."}
{"doc_id": "1910.10683", "para_id": 372, "text": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015."}
{"doc_id": "1910.10683", "para_id": 373, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for NLP. arXiv preprint arXiv:1902.00751, 2019."}
{"doc_id": "1910.10683", "para_id": 374, "text": "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifi-\ncation. arXiv preprint arXiv:1801.06146, 2018."}
{"doc_id": "1910.10683", "para_id": 375, "text": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Dou-\nglas Eck. Music transformer: Generating music with long-term structure. In Seventh\nInternational Conference on Learning Representations, 2018a."}
{"doc_id": "1910.10683", "para_id": 376, "text": "Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V\nLe, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline\nparallelism. arXiv preprint arXiv:1811.06965, 2018b."}
{"doc_id": "1910.10683", "para_id": 377, "text": "Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet good for\ntransfer learning? arXiv preprint arXiv:1608.08614, 2016."}
{"doc_id": "1910.10683", "para_id": 378, "text": "Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First Quora dataset release: Question\npairs.\nhttps://data.quora.com/First-Quora-Dataset-Release-Question-Pairs,\n2017."}
{"doc_id": "1910.10683", "para_id": 379, "text": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\nSergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\nembedding. In Proceedings of the 22nd ACM international conference on Multimedia,\n2014."}
{"doc_id": "1910.10683", "para_id": 380, "text": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv preprint\narXiv:1909.10351, 2019."}
{"doc_id": "1910.10683", "para_id": 381, "text": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551, 2017."}
{"doc_id": "1910.10683", "para_id": 382, "text": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint\narXiv:1907.10529, 2019."}
{"doc_id": "1910.10683", "para_id": 383, "text": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."}
{"doc_id": "1910.10683", "para_id": 384, "text": "Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\nfor modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics, 2014."}
{"doc_id": "1910.10683", "para_id": 385, "text": "Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard\nSocher. CTRL: A conditional transformer language model for controllable generation.\narXiv preprint arXiv:1909.05858, 2019a."}
{"doc_id": "1910.10683", "para_id": 386, "text": "Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question\nanswering and text classification via span extraction. arXiv preprint arXiv:1904.09286,\n2019b."}
{"doc_id": "1910.10683", "para_id": 387, "text": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLooking beyond the surface: A challenge set for reading comprehension over multiple\nsentences. In Proceedings of North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018."}
{"doc_id": "1910.10683", "para_id": 388, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 389, "text": "Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information\nprocessing systems, 2015."}
{"doc_id": "1910.10683", "para_id": 390, "text": "Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. A surprisingly robust trick for Winograd schema challenge. arXiv preprint\narXiv:1905.06290, 2019."}
{"doc_id": "1910.10683", "para_id": 391, "text": "Jakub Konečn`y, Brendan McMahan, and Daniel Ramage. Federated optimization: Dis-\ntributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015."}
{"doc_id": "1910.10683", "para_id": 392, "text": "Jakub Konečn`y, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha\nSuresh, and Dave Bacon. Federated learning: Strategies for improving communication\nefficiency. arXiv preprint arXiv:1610.05492, 2016."}
{"doc_id": "1910.10683", "para_id": 393, "text": "Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better ImageNet models transfer\nbetter? arXiv preprint arXiv:1805.08974, 2018."}
{"doc_id": "1910.10683", "para_id": 394, "text": "Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv\npreprint arXiv:1404.5997, 2014."}
{"doc_id": "1910.10683", "para_id": 395, "text": "Taku Kudo. Subword regularization: Improving neural network translation models with\nmultiple subword candidates. arXiv preprint arXiv:1804.10959, 2018."}
{"doc_id": "1910.10683", "para_id": 396, "text": "Taku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226,\n2018."}
{"doc_id": "1910.10683", "para_id": 397, "text": "Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv\npreprint arXiv:1901.07291, 2019."}
{"doc_id": "1910.10683", "para_id": 398, "text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. ALBERT: A lite BERT for self-supervised learning of language representa-\ntions. arXiv preprint arXiv:1909.11942, 2019."}
{"doc_id": "1910.10683", "para_id": 399, "text": "Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge.\nIn Thirteenth International Conference on the Principles of Knowledge Representation\nand Reasoning, 2012."}
{"doc_id": "1910.10683", "para_id": 400, "text": "Qi Li. Literature survey: domain adaptation algorithms for natural language processing.\n2012."}
{"doc_id": "1910.10683", "para_id": 401, "text": "Chin-Yew Lin.\nROUGE: A package for automatic evaluation of summaries.\nIn Text\nsummarization branches out, 2004."}
{"doc_id": "1910.10683", "para_id": 402, "text": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018."}
{"doc_id": "1910.10683", "para_id": 403, "text": "Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization\nusing length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998, 2019a."}
{"doc_id": "1910.10683", "para_id": 404, "text": "Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Rep-\nresentation learning using multi-task deep neural networks for semantic classification\nand information retrieval. In Proceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\n2015."}
{"doc_id": "1910.10683", "para_id": 405, "text": "Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\nnetworks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019b."}
{"doc_id": "1910.10683", "para_id": 406, "text": "Yang Liu. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318,\n2019."}
{"doc_id": "1910.10683", "para_id": 407, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized\nBERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019c."}
{"doc_id": "1910.10683", "para_id": 408, "text": "Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence\nrepresentations. arXiv preprint arXiv:1803.02893, 2018."}
{"doc_id": "1910.10683", "para_id": 409, "text": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In Proceedings of the European Conference on Computer Vision\n(ECCV), 2018."}
{"doc_id": "1910.10683", "para_id": 410, "text": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The nat-\nural language decathlon: Multitask learning as question answering.\narXiv preprint\narXiv:1806.08730, 2018."}
{"doc_id": "1910.10683", "para_id": 411, "text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013a."}
{"doc_id": "1910.10683", "para_id": 412, "text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in neural\ninformation processing systems, 2013b."}
{"doc_id": "1910.10683", "para_id": 413, "text": "Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing\nXiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond.\narXiv preprint arXiv:1602.06023, 2016."}
{"doc_id": "1910.10683", "para_id": 414, "text": "Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring\nmid-level image representations using convolutional neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014."}
{"doc_id": "1910.10683", "para_id": 415, "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on\nassociation for computational linguistics. Association for Computational Linguistics, 2002."}
{"doc_id": "1910.10683", "para_id": 416, "text": "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017."}
{"doc_id": "1910.10683", "para_id": 417, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 418, "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors\nfor word representation. In Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), 2014."}
{"doc_id": "1910.10683", "para_id": 419, "text": "Matthew Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting\npretrained representations to diverse tasks. arXiv preprint arXiv:1903.05987, 2019."}
{"doc_id": "1910.10683", "para_id": 420, "text": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018."}
{"doc_id": "1910.10683", "para_id": 421, "text": "Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Sup-\nplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088,\n2018."}
{"doc_id": "1910.10683", "para_id": 422, "text": "Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for\nevaluating context-sensitive representations. arXiv preprint arXiv:1808.09121, 2018."}
{"doc_id": "1910.10683", "para_id": 423, "text": "Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771,\n2018."}
{"doc_id": "1910.10683", "para_id": 424, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018."}
{"doc_id": "1910.10683", "para_id": 425, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019."}
{"doc_id": "1910.10683", "para_id": 426, "text": "Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the Winograd\nschema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning. Association\nfor Computational Linguistics, 2012."}
{"doc_id": "1910.10683", "para_id": 427, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016."}
{"doc_id": "1910.10683", "para_id": 428, "text": "Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence\nto sequence learning. arXiv preprint arXiv:1611.02683, 2016."}
{"doc_id": "1910.10683", "para_id": 429, "text": "Alex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré.\nSnorkel MeTaL: Weak supervision for multi-task learning. In Proceedings of the Second\nWorkshop on Data Management for End-To-End Machine Learning, 2018."}
{"doc_id": "1910.10683", "para_id": 430, "text": "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring\nSymposium Series, 2011."}
{"doc_id": "1910.10683", "para_id": 431, "text": "Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017."}
{"doc_id": "1910.10683", "para_id": 432, "text": "Sebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI\nGalway, 2019."}
{"doc_id": "1910.10683", "para_id": 433, "text": "Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer\nlearning in natural language processing. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Tutorials,\npages 15–18, 2019."}
{"doc_id": "1910.10683", "para_id": 434, "text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale\nvisual recognition challenge. International journal of computer vision, 2015."}
{"doc_id": "1910.10683", "para_id": 435, "text": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108,\n2019."}
{"doc_id": "1910.10683", "para_id": 436, "text": "Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017."}
{"doc_id": "1910.10683", "para_id": 437, "text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. arXiv preprint arXiv:1508.07909, 2015."}
{"doc_id": "1910.10683", "para_id": 438, "text": "Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training.\narXiv preprint arXiv:1811.03600, 2018."}
{"doc_id": "1910.10683", "para_id": 439, "text": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\nrepresentations. arXiv preprint arXiv:1803.02155, 2018."}
{"doc_id": "1910.10683", "para_id": 440, "text": "Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. arXiv preprint arXiv:1804.04235, 2018."}
{"doc_id": "1910.10683", "para_id": 441, "text": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017."}
{"doc_id": "1910.10683", "para_id": 442, "text": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan\nSepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In\nAdvances in Neural Information Processing Systems, 2018."}
{"doc_id": "1910.10683", "para_id": 443, "text": "Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-\nBurch, and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In\nProceedings of the 51st Annual Meeting of the Association for Computational Linguistics,\n2013."}
{"doc_id": "1910.10683", "para_id": 444, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing, 2013."}
{"doc_id": "1910.10683", "para_id": 445, "text": "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019."}
{"doc_id": "1910.10683", "para_id": 446, "text": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"}
{"doc_id": "1910.10683", "para_id": 447, "text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of\nMachine Learning Research, 2014."}
{"doc_id": "1910.10683", "para_id": 448, "text": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning\ngeneral purpose distributed sentence representations via large scale multi-task learning.\narXiv preprint arXiv:1804.00079, 2018."}
{"doc_id": "1910.10683", "para_id": 449, "text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 450, "text": "Richard S. Sutton.\nThe bitter lesson.\nhttp://www.incompleteideas.net/IncIdeas/\nBitterLesson.html, 2019."}
{"doc_id": "1910.10683", "para_id": 451, "text": "Wilson L. Taylor. “Cloze procedure”: A new tool for measuring readability. Journalism\nBulletin, 1953."}
{"doc_id": "1910.10683", "para_id": 452, "text": "Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847, 2018."}
{"doc_id": "1910.10683", "para_id": 453, "text": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip\nBachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. arXiv\npreprint arXiv:1611.09830, 2016."}
{"doc_id": "1910.10683", "para_id": 454, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, 2017."}
{"doc_id": "1910.10683", "para_id": 455, "text": "Elena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations\nin the transformer: A study with machine translation and language modeling objectives.\narXiv preprint arXiv:1909.01380, 2019."}
{"doc_id": "1910.10683", "para_id": 456, "text": "Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018."}
{"doc_id": "1910.10683", "para_id": 457, "text": "Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma\nPatel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Can you tell me\nhow to get past Sesame Street? Sentence-level pretraining beyond language modeling. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n2019a."}
{"doc_id": "1910.10683", "para_id": 458, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-\npurpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019b."}
{"doc_id": "1910.10683", "para_id": 459, "text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT:\nIncorporating language structures into pre-training for deep language understanding.\narXiv preprint arXiv:1908.04577, 2019c."}
{"doc_id": "1910.10683", "para_id": 460, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability\njudgments. arXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "1910.10683", "para_id": 461, "text": "Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017."}
{"doc_id": "1910.10683", "para_id": 462, "text": "Ronald J. Williams and David Zipser. A learning algorithm for continually running fully\nrecurrent neural networks. Neural computation, 1989."}
{"doc_id": "1910.10683", "para_id": 463, "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144, 2016."}
{"doc_id": "1910.10683", "para_id": 464, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv\npreprint arXiv:1906.08237, 2019."}
{"doc_id": "1910.10683", "para_id": 465, "text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In Advances in neural information processing systems, 2014."}
{"doc_id": "1910.10683", "para_id": 466, "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad\nNorouzi, and Quoc V. Le. QAnet: Combining local convolution with global self-attention\nfor reading comprehension. arXiv preprint arXiv:1804.09541, 2018."}
{"doc_id": "1910.10683", "para_id": 467, "text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-\nner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616,\n2019."}
{"doc_id": "1910.10683", "para_id": 468, "text": "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. ReCoRD: Bridging the gap between human and machine commonsense\nreading comprehension. arXiv preprint arXiv:1810.12885, 2018."}
{"doc_id": "1910.10683", "para_id": 469, "text": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: En-\nhanced adversarial training for language understanding. arXiv preprint arXiv:1909.11764,\n2019."}
{"doc_id": "1910.10683", "para_id": 470, "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In Proceedings of the IEEE international\nconference on computer vision, 2015."}
{"doc_id": "2106.09685", "para_id": 0, "text": "LORA: LOW-RANK ADAPTATION OF LARGE LAN-\nGUAGE MODELS"}
{"doc_id": "2106.09685", "para_id": 1, "text": "Edward Hu∗\nYelong Shen∗\nPhillip Wallis\nZeyuan Allen-Zhu\nYuanzhi Li\nShean Wang\nLu Wang\nWeizhu Chen\nMicrosoft Corporation\n{edwardhu, yeshe, phwallis, zeyuana,\nyuanzhil, swang, luw, wzchen}@microsoft.com\nyuanzhil@andrew.cmu.edu\n(Version 2)"}
{"doc_id": "2106.09685", "para_id": 2, "text": "An important paradigm of natural language processing consists of large-scale pre-\ntraining on general domain data and adaptation to particular tasks or domains. As\nwe pre-train larger models, full ﬁne-tuning, which retrains all model parameters,\nbecomes less feasible. Using GPT-3 175B as an example – deploying indepen-\ndent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively\nexpensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-\ntrained model weights and injects trainable rank decomposition matrices into each\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\nrameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam,\nLoRA can reduce the number of trainable parameters by 10,000 times and the\nGPU memory requirement by 3 times. LoRA performs on-par or better than ﬁne-\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\nno additional inference latency. We also provide an empirical investigation into\nrank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\nmodels and provide our implementations and model checkpoints for RoBERTa,\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."}
{"doc_id": "2106.09685", "para_id": 3, "text": "Many applications in natural language processing rely on adapt-\ning one large-scale, pre-trained language model to multiple down-\nstream applications. Such adaptation is usually done via ﬁne-tuning,\nwhich updates all the parameters of the pre-trained model. The ma-\njor downside of ﬁne-tuning is that the new model contains as many\nparameters as in the original model. As larger models are trained\nevery few months, this changes from a mere “inconvenience” for\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\n175 billion trainable parameters.1"}
{"doc_id": "2106.09685", "para_id": 4, "text": "Many sought to mitigate this by adapting only some parameters or\nlearning external modules for new tasks. This way, we only need\nto store and load a small number of task-speciﬁc parameters in ad-\ndition to the pre-trained model for each task, greatly boosting the\noperational efﬁciency when deployed. However, existing techniques"}
{"doc_id": "2106.09685", "para_id": 5, "text": "Figure 1: Our reparametriza-\ntion. We only train A and B."}
{"doc_id": "2106.09685", "para_id": 6, "text": "∗Equal contribution.\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, ﬁne-tuning boosts its perfor-\nmance signiﬁcantly as shown in Appendix A."}
{"doc_id": "2106.09685", "para_id": 7, "text": "often introduce inference latency (Houlsby et al., 2019; Rebufﬁet al., 2017) by extending model\ndepth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\nmatch the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality."}
{"doc_id": "2106.09685", "para_id": 8, "text": "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\nchange in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\nLow-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers’ change during\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n175B as an example, we show that a very low rank (i.e., r in Figure 1 can be one or two) sufﬁces even\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient."}
{"doc_id": "2106.09685", "para_id": 9, "text": "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\nferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\nmatrices A and B in Figure 1, reducing the storage requirement and task-switching over-\nhead signiﬁcantly."}
{"doc_id": "2106.09685", "para_id": 10, "text": "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\nmuch smaller low-rank matrices."}
{"doc_id": "2106.09685", "para_id": 11, "text": "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\nwhen deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\nconstruction."}
{"doc_id": "2106.09685", "para_id": 12, "text": "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\nas preﬁx-tuning. We provide an example in Appendix E."}
{"doc_id": "2106.09685", "para_id": 13, "text": "Terminologies and Conventions\nWe make frequent references to the Transformer architecture\nand use the conventional terminologies for its dimensions.\nWe call the input and output di-\nmension size of a Transformer layer dmodel.\nWe use Wq, Wk, Wv, and Wo to refer to the\nquery/key/value/output projection matrices in the self-attention module. W or W0 refers to a pre-\ntrained weight matrix and ∆W its accumulated gradient update during adaptation. We use r to\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\noptimization and use a Transformer MLP feedforward dimension dffn = 4 × dmodel."}
{"doc_id": "2106.09685", "para_id": 14, "text": "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\ning use case. Below is a brief description of the language modeling problem and, in particular, the\nmaximization of conditional probabilities given a task-speciﬁc prompt."}
{"doc_id": "2106.09685", "para_id": 15, "text": "Suppose we are given a pre-trained autoregressive language model PΦ(y|x) parametrized by Φ.\nFor instance, PΦ(y|x) can be a generic multi-task learner such as GPT (Radford et al., b; Brown\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\nrepresented by a training dataset of context-target pairs: Z = {(xi, yi)}i=1,..,N, where both xi and\nyi are sequences of tokens. For example, in NL2SQL, xi is a natural language query and yi its\ncorresponding SQL command; for summarization, xi is the content of an article and yi its summary."}
{"doc_id": "2106.09685", "para_id": 16, "text": "During full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\nby repeatedly following the gradient to maximize the conditional language modeling objective:"}
{"doc_id": "2106.09685", "para_id": 17, "text": "One of the main drawbacks for full ﬁne-tuning is that for each downstream task, we learn a different\nset of parameters ∆Φ whose dimension |∆Φ| equals |Φ0|. Thus, if the pre-trained model is large\n(such as GPT-3 with |Φ0| ≈175 Billion), storing and deploying many independent instances of\nﬁne-tuned models can be challenging, if at all feasible."}
{"doc_id": "2106.09685", "para_id": 18, "text": "In this paper, we adopt a more parameter-efﬁcient approach, where the task-speciﬁc parameter\nincrement ∆Φ = ∆Φ(Θ) is further encoded by a much smaller-sized set of parameters Θ with\n|Θ| ≪|Φ0|. The task of ﬁnding ∆Φ thus becomes optimizing over Θ:"}
{"doc_id": "2106.09685", "para_id": 19, "text": "In the subsequent sections, we propose to use a low-rank representation to encode ∆Φ that is both\ncompute- and memory-efﬁcient. When the pre-trained model is GPT-3 175B, the number of train-\nable parameters |Θ| can be as small as 0.01% of |Φ0|."}
{"doc_id": "2106.09685", "para_id": 20, "text": "The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\nof works have sought to make model adaptation more parameter- and compute-efﬁcient. See Sec-\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\nare two prominent strategies when it comes to efﬁcient adaptations: adding adapter layers (Houlsby\net al., 2019; Rebufﬁet al., 2017; Pfeiffer et al., 2021; R¨uckl´e et al., 2020) or optimizing some forms\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\nlatency-sensitive production scenario."}
{"doc_id": "2106.09685", "para_id": 21, "text": "Adapter Layers Introduce Inference Latency\nThere are many variants of adapters. We focus\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\ning multi-task settings (R¨uckl´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\na difference in the online inference setting where the batch size is typically as small as one. In a\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\nvery small bottleneck dimension (Table 1)."}
{"doc_id": "2106.09685", "para_id": 22, "text": "This problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\nAllReduce and Broadcast, unless we store the adapter parameters redundantly many times."}
{"doc_id": "2106.09685", "para_id": 23, "text": "Directly Optimizing the Prompt is Hard\nThe other direction, as exempliﬁed by preﬁx tuning (Li\n& Liang, 2021), faces a different challenge. We observe that preﬁx tuning is difﬁcult to optimize\nand that its performance changes non-monotonically in trainable parameters, conﬁrming similar\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\nadaptation necessarily reduces the sequence length available to process a downstream task, which\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\non task performance to Section 5."}
{"doc_id": "2106.09685", "para_id": 24, "text": "Batch Size\n32\n16\n1\nSequence Length\n512\n256\n128\n|Θ|\n0.5M\n11M\n11M"}
{"doc_id": "2106.09685", "para_id": 25, "text": "AdapterL\n1482.0±1.0 (+2.2%)\n354.8±0.5 (+5.0%)\n23.9±2.1 (+20.7%)\nAdapterH\n1492.2±1.0 (+3.0%)\n366.3±0.5 (+8.4%)\n25.8±2.2 (+30.3%)"}
{"doc_id": "2106.09685", "para_id": 26, "text": "Table 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. “|Θ|” denotes the number of trainable\nparameters in adapter layers. AdapterL and AdapterH are two variants of adapter tuning, which we\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiﬁcant in an\nonline, short-sequence-length scenario. See the full study in Appendix B."}
{"doc_id": "2106.09685", "para_id": 27, "text": "We describe the simple design of LoRA and its practical beneﬁts. The principles outlined here apply\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\nlanguage models in our experiments as the motivating use case."}
{"doc_id": "2106.09685", "para_id": 28, "text": "A neural network contains many dense layers which perform matrix multiplication. The weight\nmatrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\n(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\nlearn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\nsize the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\nweight matrix W0 ∈Rd×k, we constrain its update by representing the latter with a low-rank de-\ncomposition W0 + ∆W = W0 + BA, where B ∈Rd×r, A ∈Rr×k, and the rank r ≪min(d, k).\nDuring training, W0 is frozen and does not receive gradient updates, while A and B contain trainable\nparameters. Note both W0 and ∆W = BA are multiplied with the same input, and their respective\noutput vectors are summed coordinate-wise. For h = W0x, our modiﬁed forward pass yields:"}
{"doc_id": "2106.09685", "para_id": 29, "text": "We illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for A and\nzero for B, so ∆W = BA is zero at the beginning of training. We then scale ∆Wx by α"}
{"doc_id": "2106.09685", "para_id": 30, "text": "r , where α\nis a constant in r. When optimizing with Adam, tuning α is roughly the same as tuning the learning\nrate if we scale the initialization appropriately. As a result, we simply set α to the ﬁrst r we try\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\nr (Yang & Hu, 2021)."}
{"doc_id": "2106.09685", "para_id": 31, "text": "A Generalization of Full Fine-tuning.\nA more general form of ﬁne-tuning allows the training of\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\nness of full ﬁne-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In\nother words, as we increase the number of trainable parameters 3, training LoRA roughly converges\nto training the original model, while adapter-based methods converges to an MLP and preﬁx-based\nmethods to a model that cannot take long input sequences."}
{"doc_id": "2106.09685", "para_id": 32, "text": "No Additional Inference Latency.\nWhen deployed in production, we can explicitly compute and\nstore W = W0 + BA and perform inference as usual. Note that both W0 and BA are in Rd×k.\nWhen we need to switch to another downstream task, we can recover W0 by subtracting BA and\nthen adding a different B′A′, a quick operation with very little memory overhead. Critically, this"}
{"doc_id": "2106.09685", "para_id": 33, "text": "2They represent a negligible number of parameters compared to weights.\n3An inevitability when adapting to hard tasks."}
{"doc_id": "2106.09685", "para_id": 34, "text": "guarantees that we do not introduce any additional latency during inference compared to a ﬁne-tuned\nmodel by construction."}
{"doc_id": "2106.09685", "para_id": 35, "text": "In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\nthe self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv)\nas a single matrix of dimension dmodel ×dmodel, even though the output dimension is usually sliced\ninto attention heads. We limit our study to only adapting the attention weights for downstream\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\nand parameter-efﬁciency.We further study the effect on adapting different types of attention weight\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\nlayers, LayerNorm layers, and biases to a future work."}
{"doc_id": "2106.09685", "para_id": 36, "text": "Practical Beneﬁts and Limitations.\nThe most signiﬁcant beneﬁt comes from the reduction in\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\nusage by up to 2/3 if r ≪dmodel as we do not need to store the optimizer states for the frozen\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\n350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint\nsize is reduced by roughly 10,000× (from 350GB to 35MB)4. This allows us to train with signiﬁ-\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneﬁt is that we can switch between tasks\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\nparameters. This allows for the creation of many customized models that can be swapped in and out\non the ﬂy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\nduring training on GPT-3 175B compared to full ﬁne-tuning5 as we do not need to calculate the\ngradient for the vast majority of the parameters."}
{"doc_id": "2106.09685", "para_id": 37, "text": "LoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\nwith different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical."}
{"doc_id": "2106.09685", "para_id": 38, "text": "We evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\n(NLU) to generation (NLG). Speciﬁcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments."}
{"doc_id": "2106.09685", "para_id": 39, "text": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their\nreported numbers whenever possible. This, however, means that some baselines might only appear\nin certain experiments."}
{"doc_id": "2106.09685", "para_id": 40, "text": "Fine-Tuning (FT) is a common approach for adaptation. During ﬁne-tuning, the model is initialized\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\nvariant is to update only some layers while freezing others. We include one such baseline reported\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers (FTTop2)."}
{"doc_id": "2106.09685", "para_id": 41, "text": "4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\n350GB + 35MB * 100 ≈354GB as opposed to 100 * 350GB ≈35TB.\n5For GPT-3 175B, the training throughput for full ﬁne-tuning is 32.5 tokens/s per V100 GPU; with the same\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA."}
{"doc_id": "2106.09685", "para_id": 42, "text": "Parameters MNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B Avg."}
{"doc_id": "2106.09685", "para_id": 43, "text": "RoBbase (FT)*\n125.0M\n87.6\n94.8\n90.2\n63.6\n92.8\n91.9\n78.7\n91.2\n86.4\nRoBbase (BitFit)*\n0.1M\n84.7\n93.7\n92.7\n62.0\n91.8\n84.0\n81.5\n90.8\n85.2\nRoBbase (AdptD)*\n0.3M 87.1±.0 94.2±.1 88.5±1.1 60.8±.4 93.1±.1 90.2±.0 71.5±2.7 89.7±.3 84.4\nRoBbase (AdptD)*\n0.9M 87.3±.1 94.7±.3 88.4±.1\n62.6±.9 93.0±.2 90.6±.0 75.9±2.2 90.3±.1 85.4\nRoBbase (LoRA)\n0.3M 87.5±.3 95.1±.2 89.7±.7 63.4±1.2 93.3±.3 90.8±.1 86.6±.7\n91.5±.2 87.2"}
{"doc_id": "2106.09685", "para_id": 44, "text": "RoBlarge (FT)*\n355.0M\n90.2\n96.4\n90.9\n68.0\n94.7\n92.2\n86.6\n92.4\n88.9\nRoBlarge (LoRA)\n0.8M 90.6±.2 96.2±.5 90.9±1.2 68.2±1.9 94.9±.3 91.6±.1 87.4±2.5 92.6±.2 89.0"}
{"doc_id": "2106.09685", "para_id": 45, "text": "RoBlarge (AdptP)†\n3.0M 90.2±.3 96.1±.3 90.2±.7 68.3±1.0 94.8±.2 91.9±.1 83.8±2.9 92.1±.7 88.4\nRoBlarge (AdptP)†\n0.8M 90.5±.3 96.6±.2 89.7±1.2 67.8±2.5 94.8±.3 91.7±.2 80.1±2.9 91.9±.4 87.9\nRoBlarge (AdptH)†\n6.0M 89.9±.5 96.2±.3 88.7±2.9 66.5±4.4 94.7±.2 92.1±.1 83.4±1.1 91.0±1.7 87.8\nRoBlarge (AdptH)†\n0.8M 90.3±.3 96.3±.5 87.7±1.7 66.3±2.0 94.7±.2 91.5±.1 72.9±2.9 91.5±.5 86.4\nRoBlarge (LoRA)†\n0.8M 90.6±.2 96.2±.5 90.2±1.0 68.2±1.9 94.8±.3 91.6±.2 85.2±1.1 92.3±.5 88.6"}
{"doc_id": "2106.09685", "para_id": 46, "text": "DeBXXL (FT)*\n1500.0M\n91.8\n97.2\n92.0\n72.0\n96.0\n92.7\n93.9\n92.9\n91.1\nDeBXXL (LoRA)\n4.7M 91.9±.2 96.9±.2 92.6±.6 72.4±1.1 96.0±.1 92.9±.1 94.9±.4\n93.0±.2 91.3"}
{"doc_id": "2106.09685", "para_id": 47, "text": "Table 2: RoBERTabase, RoBERTalarge, and DeBERTaXXL with different adaptation methods on the\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew’s\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\nfor all metrics. * indicates numbers published in prior works. † indicates runs conﬁgured in a setup\nsimilar to Houlsby et al. (2019) for a fair comparison."}
{"doc_id": "2106.09685", "para_id": 48, "text": "Bias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021)."}
{"doc_id": "2106.09685", "para_id": 49, "text": "Preﬁx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\ncial tokens have trainable word embeddings and are generally not in the model’s vocabulary. Where\nto place such tokens can have an impact on performance. We focus on “preﬁxing”, which prepends\nsuch tokens to the prompt, and “inﬁxing”, which appends to the prompt; both are discussed in Li &\nLiang (2021). We use lp (resp. li) denote the number of preﬁx (resp. inﬁx) tokens. The number of\ntrainable parameters is |Θ| = dmodel × (lp + li)."}
{"doc_id": "2106.09685", "para_id": 50, "text": "Preﬁx-layer tuning (PreLayer) is an extension to preﬁx-embedding tuning. Instead of just learning\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\n|Θ| = L × dmodel × (lp + li), where L is the number of Transformer layers."}
{"doc_id": "2106.09685", "para_id": 51, "text": "Adapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\nattention module (and the MLP module) and the subsequent residual connection. There are two\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efﬁcient design with the\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\ninclude another baseline call AdapterDrop (R¨uckl´e et al., 2020) which drops some adapter layers for\ngreater efﬁciency (AdapterD). We cite numbers from prior works whenever possible to maximize\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the ﬁrst column.\nIn all cases, we have |Θ| = ˆLAdpt ×(2×dmodel ×r+r+dmodel)+2× ˆLLN ×dmodel where ˆLAdpt\nis the number of adapter layers and ˆLLN the number of trainable LayerNorms (e.g., in AdapterL)."}
{"doc_id": "2106.09685", "para_id": 52, "text": "LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in Section 4.2, we only apply LoRA to Wq and Wv in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank r and the shape of the original weights:\n|Θ| = 2 × ˆLLoRA × dmodel × r, where ˆLLoRA is the number of weight matrices we apply LoRA to."}
{"doc_id": "2106.09685", "para_id": 53, "text": "Model & Method\n# Trainable\nE2E NLG Challenge\nParameters\nBLEU\nNIST\nMET\nROUGE-L\nCIDEr"}
{"doc_id": "2106.09685", "para_id": 54, "text": "GPT-2 M (FT)*\n354.92M\n68.2\n8.62\n46.2\n71.0\n2.47\nGPT-2 M (AdapterL)*\n0.37M\n66.3\n8.41\n45.0\n69.8\n2.40\nGPT-2 M (AdapterL)*\n11.09M\n68.9\n8.71\n46.1\n71.3\n2.47\nGPT-2 M (AdapterH)\n11.09M\n67.3±.6\n8.50±.07\n46.0±.2\n70.7±.2\n2.44±.01\nGPT-2 M (FTTop2)*\n25.19M\n68.1\n8.59\n46.0\n70.8\n2.41\nGPT-2 M (PreLayer)*\n0.35M\n69.7\n8.81\n46.1\n71.4\n2.49\nGPT-2 M (LoRA)\n0.35M\n70.4±.1\n8.85±.02\n46.8±.2\n71.8±.1\n2.53±.02\nGPT-2 L (FT)*\n774.03M\n68.5\n8.78\n46.0\n69.9\n2.45\nGPT-2 L (AdapterL)\n0.88M\n69.1±.1\n8.68±.03\n46.3±.0\n71.4±.2\n2.49±.0\nGPT-2 L (AdapterL)\n23.00M\n68.9±.3\n8.70±.04\n46.1±.1\n71.3±.2\n2.45±.02\nGPT-2 L (PreLayer)*\n0.77M\n70.3\n8.85\n46.2\n71.7\n2.47\nGPT-2 L (LoRA)\n0.77M\n70.4±.1\n8.89±.02\n46.8±.2\n72.0±.2\n2.47±.02"}
{"doc_id": "2106.09685", "para_id": 55, "text": "Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\nor fewer trainable parameters. Conﬁdence intervals are shown for experiments we ran. * indicates\nnumbers published in prior works."}
{"doc_id": "2106.09685", "para_id": 56, "text": "RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\net al., 2019a) and boosted the latter’s task performance without introducing many more trainable\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\nand evaluate the performance of different efﬁcient adaptation approaches on tasks from the GLUE\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the ﬁne-tuning baseline. Runs\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 57, "text": "DeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\nﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\nSee Section D.2 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 58, "text": "Having shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\ninclude a list of the hyperparameters used in Section D.3."}
{"doc_id": "2106.09685", "para_id": 59, "text": "Model&Method\n# Trainable\nWikiSQL\nMNLI-m\nSAMSum\nParameters\nAcc. (%)\nAcc. (%)\nR1/R2/RL"}
{"doc_id": "2106.09685", "para_id": 60, "text": "GPT-3 (FT)\n175,255.8M\n73.8\n89.5\n52.0/28.0/44.5\nGPT-3 (BitFit)\n14.2M\n71.3\n91.0\n51.3/27.4/43.5\nGPT-3 (PreEmbed)\n3.2M\n63.1\n88.6\n48.3/24.2/40.5\nGPT-3 (PreLayer)\n20.2M\n70.1\n89.5\n50.8/27.3/43.5\nGPT-3 (AdapterH)\n7.1M\n71.9\n89.8\n53.0/28.9/44.8\nGPT-3 (AdapterH)\n40.1M\n73.2\n91.5\n53.2/29.0/45.1"}
{"doc_id": "2106.09685", "para_id": 61, "text": "GPT-3 (LoRA)\n4.7M\n73.4\n91.7\n53.8/29.8/45.9\nGPT-3 (LoRA)\n37.7M\n74.0\n91.6\n53.4/29.2/45.1"}
{"doc_id": "2106.09685", "para_id": 62, "text": "Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\nSAMSum. LoRA performs better than prior approaches, including full ﬁne-tuning. The results\non WikiSQL have a ﬂuctuation around ±0.5%, MNLI-m around ±0.1%, and SAMSum around\n±0.2/±0.2/±0.1 for the three metrics."}
{"doc_id": "2106.09685", "para_id": 63, "text": "As a ﬁnal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used."}
{"doc_id": "2106.09685", "para_id": 64, "text": "As shown in Table 4, LoRA matches or exceeds the ﬁne-tuning baseline on all three datasets. Note\nthat not all methods beneﬁt monotonically from having more trainable parameters, as shown in Fig-\nure 2. We observe a signiﬁcant performance drop when we use more than 256 special tokens for\npreﬁx-embedding tuning or more than 32 special tokens for preﬁx-layer tuning. This corroborates\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\nperformance of different adaptation approaches in the low-data regime in Section F.3."}
{"doc_id": "2106.09685", "para_id": 65, "text": "Figure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\nSee Section F.2 for more details on the plotted data points."}
{"doc_id": "2106.09685", "para_id": 66, "text": "Transformer Language Models.\nTransformer (Vaswani et al., 2017) is a sequence-to-sequence\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) – both are large Transformer lan-"}
{"doc_id": "2106.09685", "para_id": 67, "text": "guage models trained on a large amount of text – where ﬁne-tuning on task-speciﬁc data after pre-\ntraining on general domain data provides a signiﬁcant performance gain compared to training on\ntask-speciﬁc data directly. Training larger Transformers generally results in better performance and\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\nlanguage model trained to-date with 175B parameters."}
{"doc_id": "2106.09685", "para_id": 68, "text": "Prompt Engineering and Fine-Tuning.\nWhile GPT-3 175B can adapt its behavior with just a\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\nmodel’s performance on a desired task, which is known as prompt engineering or prompt hacking.\nFine-tuning retrains a model pre-trained on general domains to a speciﬁc task Devlin et al. (2019b);\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform ﬁne-tuning in\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\nhas the same memory footprint as pre-training."}
{"doc_id": "2106.09685", "para_id": 69, "text": "Parameter-Efﬁcient Adaptation.\nMany have proposed inserting adapter layers between existing\nlayers in a neural network (Houlsby et al., 2019; Rebufﬁet al., 2017; Lin et al., 2020). Our method\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\nkey functional difference is that our learned weights can be merged with the main weights during\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\nimprove its parameter efﬁciency, which we leave to future work. More recently, many proposed\noptimizing the input word embeddings in lieu of ﬁne-tuning, akin to a continuous and differentiable\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\ntake up available sequence length for task tokens when positional embeddings are learned."}
{"doc_id": "2106.09685", "para_id": 70, "text": "Low-Rank Structures in Deep Learning.\nLow-rank structure is very common in machine learn-\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\nconsiders low-rank update to a frozen model for adaptation to downstream tasks. In theory liter-\nature, it is known that neural networks outperform other classical learning methods, including the\ncorresponding (ﬁnite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\nlow-rank adaptation update is well-motivated by the literature."}
{"doc_id": "2106.09685", "para_id": 71, "text": "Given the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\n(up to 10,000×) without adversely affecting task performances."}
{"doc_id": "2106.09685", "para_id": 72, "text": "We perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt"}
{"doc_id": "2106.09685", "para_id": 73, "text": "to maximize downstream performance? 2) Is the “optimal” adaptation matrix ∆W really rank-\ndeﬁcient? If so, what is a good rank to use in practice? 3) What is the connection between ∆W and\nW? Does ∆W highly correlate with W? How large is ∆W comparing to W?"}
{"doc_id": "2106.09685", "para_id": 74, "text": "We believe that our answers to question (2) and (3) shed light on the fundamental principles of using\npre-trained language models for downstream tasks, which is a critical topic in NLP."}
{"doc_id": "2106.09685", "para_id": 75, "text": "7.1\nWHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?"}
{"doc_id": "2106.09685", "para_id": 76, "text": "Given a limited parameter budget, which types of weights should we adapt with LoRA to obtain\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\nin FP16) on GPT-3 175B, which corresponds to r = 8 if we adapt one type of attention weights or\nr = 4 if we adapt two types, for all 96 layers. The result is presented in Table 5."}
{"doc_id": "2106.09685", "para_id": 77, "text": "Weight Type\nWq\nWk\nWv\nWo\nWq, Wk\nWq, Wv\nWq, Wk, Wv, Wo\nRank r\n8\n8\n8\n8\n4\n4\n2"}
{"doc_id": "2106.09685", "para_id": 78, "text": "WikiSQL (±0.5%)\n70.4\n70.0\n73.0\n73.2\n71.4\n73.7\n73.7\nMultiNLI (±0.1%)\n91.0\n90.8\n91.0\n91.3\n91.3\n91.3\n91.7"}
{"doc_id": "2106.09685", "para_id": 79, "text": "Table 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wq and\nWv gives the best performance overall. We ﬁnd the standard deviation across random seeds to be\nconsistent for a given dataset, which we report in the ﬁrst column."}
{"doc_id": "2106.09685", "para_id": 80, "text": "Note that putting all the parameters in ∆Wq or ∆Wk results in signiﬁcantly lower performance,\nwhile adapting both Wq and Wv yields the best result. This suggests that even a rank of four\ncaptures enough information in ∆W such that it is preferable to adapt more weight matrices than\nadapting a single type of weights with a larger rank."}
{"doc_id": "2106.09685", "para_id": 81, "text": "We turn our attention to the effect of rank r on model performance.\nWe adapt {Wq, Wv},\n{Wq, Wk, Wv, Wc}, and just Wq for a comparison."}
{"doc_id": "2106.09685", "para_id": 82, "text": "WikiSQL(±0.5%)\nWq\n68.8\n69.6\n70.5\n70.4\n70.0\nWq, Wv\n73.4\n73.3\n73.7\n73.8\n73.5\nWq, Wk, Wv, Wo\n74.1\n73.7\n74.0\n74.0\n73.9"}
{"doc_id": "2106.09685", "para_id": 83, "text": "Wq\n90.7\n90.9\n91.1\n90.7\n90.7\nWq, Wv\n91.3\n91.4\n91.3\n91.6\n91.4\nWq, Wk, Wv, Wo\n91.2\n91.7\n91.7\n91.5\n91.4"}
{"doc_id": "2106.09685", "para_id": 84, "text": "Table 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\nrank as small as one sufﬁces for adapting both Wq and Wv on these datasets while training Wq alone\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2."}
{"doc_id": "2106.09685", "para_id": 85, "text": "Table 6 shows that, surprisingly, LoRA already performs competitively with a very small r (more\nso for {Wq, Wv} than just Wq). This suggests the update matrix ∆W could have a very small\n“intrinsic rank”.6 To further support this ﬁnding, we check the overlap of the subspaces learned by\ndifferent choices of r and by different random seeds. We argue that increasing r does not cover a\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufﬁcient."}
{"doc_id": "2106.09685", "para_id": 86, "text": "6However, we do not expect a small r to work for every task or dataset. Consider the following thought\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\nthe entire model (similar to LoRA with r = dmodel) could certainly outperform LoRA with a small r."}
{"doc_id": "2106.09685", "para_id": 87, "text": "Subspace similarity between different r.\nGiven Ar=8 and Ar=64 which are the learned adapta-\ntion matrices with rank r = 8 and 64 using the same pre-trained model, we perform singular value\ndecomposition and obtain the right-singular unitary matrices UAr=8 and UAr=64.7 We hope to an-\nswer: how much of the subspace spanned by the top i singular vectors in UAr=8 (for 1 ≤i ≤8) is\ncontained in the subspace spanned by top j singular vectors of UAr=64 (for 1 ≤j ≤64)? We mea-\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\npendix G for a more formal discussion)"}
{"doc_id": "2106.09685", "para_id": 88, "text": "φ(Ar=8, Ar=64, i, j) = ||U i⊤\nAr=8U j\nAr=64||2\nF\nmin(i, j)\n∈[0, 1]\n(4)"}
{"doc_id": "2106.09685", "para_id": 89, "text": "where U i\nAr=8 represents the columns of UAr=8 corresponding to the top-i singular vectors."}
{"doc_id": "2106.09685", "para_id": 90, "text": "φ(·) has a range of [0, 1], where 1 represents a complete overlap of subspaces and 0 a complete\nseparation. See Figure 3 for how φ changes as we vary i and j. We only look at the 48th layer\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\nin Section H.1."}
{"doc_id": "2106.09685", "para_id": 91, "text": "Figure 3: Subspace similarity between column vectors of Ar=8 and Ar=64 for both ∆Wq and ∆Wv.\nThe third and the fourth ﬁgures zoom in on the lower-left triangle in the ﬁrst two ﬁgures. The top\ndirections in r = 8 are included in r = 64, and vice versa."}
{"doc_id": "2106.09685", "para_id": 92, "text": "Directions corresponding to the top singular vector overlap signiﬁcantly between\nAr=8 and Ar=64, while others do not. Speciﬁcally, ∆Wv (resp. ∆Wq) of Ar=8\nand ∆Wv (resp. ∆Wq) of Ar=64 share a subspace of dimension 1 with normalized\nsimilarity > 0.5, providing an explanation of why r = 1 performs quite well in our\ndownstream tasks for GPT-3."}
{"doc_id": "2106.09685", "para_id": 93, "text": "Since both Ar=8 and Ar=64 are learned using the same pre-trained model, Figure 3 indicates that\nthe top singular-vector directions of Ar=8 and Ar=64 are the most useful, while other directions\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\ncan indeed have a very low rank."}
{"doc_id": "2106.09685", "para_id": 94, "text": "Subspace similarity between different random seeds.\nWe further conﬁrm this by plotting the\nnormalized subspace similarity between two randomly seeded runs with r = 64, shown in Figure 4.\n∆Wq appears to have a higher “intrinsic rank” than ∆Wv, since more common singular value direc-\ntions are learned by both runs for ∆Wq, which is in line with our empirical observation in Table 6.\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\nsingular value directions with each other."}
{"doc_id": "2106.09685", "para_id": 95, "text": "7.3\nHOW DOES THE ADAPTATION MATRIX ∆W COMPARE TO W ?"}
{"doc_id": "2106.09685", "para_id": 96, "text": "We further investigate the relationship between ∆W and W. In particular, does ∆W highly correlate\nwith W? (Or mathematically, is ∆W mostly contained in the top singular directions of W?) Also,"}
{"doc_id": "2106.09685", "para_id": 97, "text": "7Note that a similar analysis can be carried out with B and the left-singular unitary matrices – we stick with\nA for our experiments."}
{"doc_id": "2106.09685", "para_id": 98, "text": "Figure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\nfrom two random seeds, for both ∆Wq and ∆Wv in the 48-th layer. Right: the same heat-map\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers."}
{"doc_id": "2106.09685", "para_id": 99, "text": "how “large” is ∆W comparing to its corresponding directions in W? This can shed light on the\nunderlying mechanism for adapting pre-trained language models."}
{"doc_id": "2106.09685", "para_id": 100, "text": "To answer these questions, we project W onto the r-dimensional subspace of ∆W by comput-\ning U ⊤WV ⊤, with U/V being the left/right singular-vector matrix of ∆W.\nThen, we com-\npare the Frobenius norm between ∥U ⊤WV ⊤∥F and ∥W∥F . As a comparison, we also compute\n∥U ⊤WV ⊤∥F by replacing U, V with the top r singular vectors of W or a random matrix."}
{"doc_id": "2106.09685", "para_id": 101, "text": "Table 7: The Frobenius norm of U ⊤WqV ⊤where U and V are the left/right top r singular vector\ndirections of either (1) ∆Wq, (2) Wq, or (3) a random matrix. The weight matrices are taken from\nthe 48th layer of GPT-3."}
{"doc_id": "2106.09685", "para_id": 102, "text": "We draw several conclusions from Table 7. First, ∆W has a stronger correlation with W compared\nto a random matrix, indicating that ∆W ampliﬁes some features that are already in W. Second,\ninstead of repeating the top singular directions of W, ∆W only ampliﬁes directions that are not\nemphasized in W. Third, the ampliﬁcation factor is rather huge: 21.5 ≈6.91/0.32 for r = 4.\nSee Section H.4 for why r = 64 has a smaller ampliﬁcation factor. We also provide a visualization\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\nThis suggests that the low-rank adaptation matrix potentially ampliﬁes the important features for\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model."}
{"doc_id": "2106.09685", "para_id": 103, "text": "Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required\nand the storage/switching cost for hosting independent instances for different tasks. We propose\nLoRA, an efﬁcient adaptation strategy that neither introduces inference latency nor reduces input\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\non Transformer language models, the proposed principles are generally applicable to any neural\nnetworks with dense layers."}
{"doc_id": "2106.09685", "para_id": 104, "text": "There are many directions for future works. 1) LoRA can be combined with other efﬁcient adapta-\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ﬁne-tuning\nor LoRA is far from clear – how are features learned during pre-training transformed to do well\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ﬁne-"}
{"doc_id": "2106.09685", "para_id": 105, "text": "tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\nthere more principled ways to do it? 4) Finally, the rank-deﬁciency of ∆W suggests that W could\nbe rank-deﬁcient as well, which can also be a source of inspiration for future works."}
{"doc_id": "2106.09685", "para_id": 106, "text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL\nhttp://arxiv.org/abs/2012.13255."}
{"doc_id": "2106.09685", "para_id": 107, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efﬁciently, Going Beyond Kernels? In\nNeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337."}
{"doc_id": "2106.09685", "para_id": 108, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\nlearning. arXiv preprint arXiv:2001.04413, 2020a."}
{"doc_id": "2106.09685", "para_id": 109, "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust\ndeep learning. arXiv preprint arXiv:2005.10190, 2020b."}
{"doc_id": "2106.09685", "para_id": 110, "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.\n03962."}
{"doc_id": "2106.09685", "para_id": 111, "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."}
{"doc_id": "2106.09685", "para_id": 112, "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165."}
{"doc_id": "2106.09685", "para_id": 113, "text": "Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for\nmatrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010."}
{"doc_id": "2106.09685", "para_id": 114, "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001."}
{"doc_id": "2106.09685", "para_id": 115, "text": "Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep\nneural networks with multitask learning. In Proceedings of the 25th international conference\non Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\nhttps://doi.org/10.1145/1390156.1390177."}
{"doc_id": "2106.09685", "para_id": 116, "text": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning, 2014."}
{"doc_id": "2106.09685", "para_id": 117, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019a."}
{"doc_id": "2106.09685", "para_id": 118, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b.\nURL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805."}
{"doc_id": "2106.09685", "para_id": 119, "text": "William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\nhttps://aclanthology.org/I05-5002."}
{"doc_id": "2106.09685", "para_id": 120, "text": "Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\nNatural Language Generation, pp. 124–133, 2017."}
{"doc_id": "2106.09685", "para_id": 121, "text": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020."}
{"doc_id": "2106.09685", "para_id": 122, "text": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL\nhttp://arxiv.org/abs/1911.12237."}
{"doc_id": "2106.09685", "para_id": 123, "text": "Lars Grasedyck, Daniel Kressner, and Christine Tobler.\nA literature survey of low-rank tensor\napproximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013."}
{"doc_id": "2106.09685", "para_id": 124, "text": "Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\nlearning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.\n1390204."}
{"doc_id": "2106.09685", "para_id": 125, "text": "Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\nReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/\n2101.00121. arXiv: 2101.00121."}
{"doc_id": "2106.09685", "para_id": 126, "text": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention, 2021."}
{"doc_id": "2106.09685", "para_id": 127, "text": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efﬁcient Transfer Learning\nfor NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.\n00751."}
{"doc_id": "2106.09685", "para_id": 128, "text": "Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions. arXiv preprint arXiv:1405.3866, 2014."}
{"doc_id": "2106.09685", "para_id": 129, "text": "Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol`o Fusi. Initialization and regularization\nof factorized neural layers, 2021."}
{"doc_id": "2106.09685", "para_id": 130, "text": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."}
{"doc_id": "2106.09685", "para_id": 131, "text": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding, 2020."}
{"doc_id": "2106.09685", "para_id": 132, "text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efﬁcient Prompt\nTuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691.\narXiv: 2104.08691."}
{"doc_id": "2106.09685", "para_id": 133, "text": "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\nmension of Objective Landscapes.\narXiv:1804.08838 [cs, stat], April 2018a.\nURL http:\n//arxiv.org/abs/1804.08838. arXiv: 1804.08838."}
{"doc_id": "2106.09685", "para_id": 134, "text": "Xiang Lisa Li and Percy Liang. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation.\narXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190."}
{"doc_id": "2106.09685", "para_id": 135, "text": "Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\ndescent on structured data. In Advances in Neural Information Processing Systems, 2018."}
{"doc_id": "2106.09685", "para_id": 136, "text": "Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\nproximation via alternating minimization. In International Conference on Machine Learning, pp.\n2358–2367. PMLR, 2016."}
{"doc_id": "2106.09685", "para_id": 137, "text": "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\nory, pp. 2–47. PMLR, 2018b."}
{"doc_id": "2106.09685", "para_id": 138, "text": "Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\nvia parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2020.ﬁndings-emnlp.41.\nURL https://aclanthology.\norg/2020.findings-emnlp.41."}
{"doc_id": "2106.09685", "para_id": 139, "text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nUnderstands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/\n2103.10385. arXiv: 2103.10385."}
{"doc_id": "2106.09685", "para_id": 140, "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019."}
{"doc_id": "2106.09685", "para_id": 141, "text": "Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017."}
{"doc_id": "2106.09685", "para_id": 142, "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019."}
{"doc_id": "2106.09685", "para_id": 143, "text": "Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank\nhypercomplex adapter layers, 2021."}
{"doc_id": "2106.09685", "para_id": 144, "text": "Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\ndata record to text generation. arXiv preprint arXiv:2007.02871, 2020."}
{"doc_id": "2106.09685", "para_id": 145, "text": "Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end-\nto-end generation. arXiv preprint arXiv:1706.09254, 2017."}
{"doc_id": "2106.09685", "para_id": 146, "text": "Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.\nGeneralization guaran-\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\narXiv:1906.05392, 2019."}
{"doc_id": "2106.09685", "para_id": 147, "text": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning, 2021."}
{"doc_id": "2106.09685", "para_id": 148, "text": "Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\nInterspeech, pp. 3743–3747, 2018."}
{"doc_id": "2106.09685", "para_id": 149, "text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\nstanding by Generative Pre-Training. pp. 12, a."}
{"doc_id": "2106.09685", "para_id": 150, "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. pp. 24, b."}
{"doc_id": "2106.09685", "para_id": 151, "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822."}
{"doc_id": "2106.09685", "para_id": 152, "text": "Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\nresidual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/\nabs/1705.08045. arXiv: 1705.08045."}
{"doc_id": "2106.09685", "para_id": 153, "text": "Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers, 2020."}
{"doc_id": "2106.09685", "para_id": 154, "text": "Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-dimensional output targets.\nIn 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655–\n6659. IEEE, 2013."}
{"doc_id": "2106.09685", "para_id": 155, "text": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism, 2020."}
{"doc_id": "2106.09685", "para_id": 156, "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/D13-1170."}
{"doc_id": "2106.09685", "para_id": 157, "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\nternational Conference on Neural Information Processing Systems, pp. 6000–6010, 2017."}
{"doc_id": "2106.09685", "para_id": 158, "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019."}
{"doc_id": "2106.09685", "para_id": 159, "text": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems, 2020."}
{"doc_id": "2106.09685", "para_id": 160, "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018."}
{"doc_id": "2106.09685", "para_id": 161, "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\norg/anthology/N18-1101."}
{"doc_id": "2106.09685", "para_id": 162, "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6."}
{"doc_id": "2106.09685", "para_id": 163, "text": "Greg Yang and Edward J. Hu.\nFeature Learning in Inﬁnite-Width Neural Networks.\narXiv:2011.14522 [cond-mat], May 2021. URL http://arxiv.org/abs/2011.14522.\narXiv: 2011.14522."}
{"doc_id": "2106.09685", "para_id": 164, "text": "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models, 2021."}
{"doc_id": "2106.09685", "para_id": 165, "text": "Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck\nfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pp. 185–189. IEEE, 2014."}
{"doc_id": "2106.09685", "para_id": 166, "text": "Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks.\nIn 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npp. 5005–5009. IEEE, 2016."}
{"doc_id": "2106.09685", "para_id": 167, "text": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL http://\narxiv.org/abs/1709.00103."}
{"doc_id": "2106.09685", "para_id": 168, "text": "A\nLARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES"}
{"doc_id": "2106.09685", "para_id": 169, "text": "Few-shot learning, or prompt engineering, is very advantageous when we only have a handful of\ntraining samples. However, in practice, we can often afford to curate a few thousand or more training\nexamples for performance-sensitive applications. As shown in Table 8, ﬁne-tuning improves the\nmodel performance drastically compared to few-shot learning on datasets large and small. We take\nthe GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched,\nwe use two demonstrations per class and six in-context examples in total."}
{"doc_id": "2106.09685", "para_id": 170, "text": "GPT-3 Few-Shot\n40.6\n69.0\nGPT-3 Fine-Tuned\n89.5\n85.4"}
{"doc_id": "2106.09685", "para_id": 171, "text": "Table 8: Fine-tuning signiﬁcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020)."}
{"doc_id": "2106.09685", "para_id": 172, "text": "Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas\nour proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\nadapter layers must be computed in addition to the base model, inevitably introducing additional\nlatency. While as pointed out in R¨uckl´e et al. (2020), the latency introduced by adapter layers can\nbe mitigated when the model batch size and/or sequence length is large enough to full utilize the\nhardware parallelism. We conﬁrm their observation with a similar latency study on GPT-2 medium\nand point out that there are scenarios, notably online inference where the batch size is small, where\nthe added latency can be signiﬁcant."}
{"doc_id": "2106.09685", "para_id": 173, "text": "We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\nover 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\nr. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\nand a recent, more efﬁcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\nfor more details on the designs. We plot the slow-down in percentage compared to the no-adapter\nbaseline in Figure 5."}
{"doc_id": "2106.09685", "para_id": 174, "text": "Figure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline.\nThe top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and\nsequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\nonline, short-sequence-length scenario. We tweak the colormap for better visibility."}
{"doc_id": "2106.09685", "para_id": 175, "text": "GLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\nMNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\n(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\n(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8 (question-answering), RTE (inference),"}
{"doc_id": "2106.09685", "para_id": 176, "text": "8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs"}
{"doc_id": "2106.09685", "para_id": 177, "text": "and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\nstandard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\nare released under different permissive licenses."}
{"doc_id": "2106.09685", "para_id": 178, "text": "WikiSQL is introduced in Zhong et al. (2017) and contains 56, 355/8, 421 training/validation ex-\namples. The task is to generate SQL queries from natural language questions and table schemata.\nWe encode context as x = {table schema, query} and target as y = {SQL}. The dataset is release\nunder the BSD 3-Clause License."}
{"doc_id": "2106.09685", "para_id": 179, "text": "SAMSum is introduced in Gliwa et al. (2019) and contains 14, 732/819 training/test examples. It\nconsists of staged chat conversations between two people and corresponding abstractive summaries\nwritten by linguists. We encode context as ”\\n” concatenated utterances followed by a ”\\n\\n”,\nand target as y = {summary}. The dataset is released under the non-commercial licence: Creative\nCommons BY-NC-ND 4.0."}
{"doc_id": "2106.09685", "para_id": 180, "text": "E2E NLG Challenge was ﬁrst introduced in Novikova et al. (2017) as a dataset for training end-to-\nend, data-driven natural language generation systems and is commonly used for data-to-text evalua-\ntion. The E2E dataset consists of roughly 42, 000 training, 4, 600 validation, and 4, 600 test exam-\nples from the restaurant domain. Each source table used as input can have multiple references. Each\nsample input (x, y) consists of a sequence of slot-value pairs, along with a corresponding natural\nlanguage reference text. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 181, "text": "DART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\nstructured as sequences of ENTITY — RELATION — ENTITY triples. With 82K examples in\ntotal, DART is a signiﬁcantly larger and more complex data-to-text task compared to E2E. The\ndataset is released under the MIT license."}
{"doc_id": "2106.09685", "para_id": 182, "text": "WebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With\n22K examples in total WebNLG comprises 14 distinct categories, nine of which are seen during\ntraining. Since ﬁve of the 14 total categories are not seen during training, but are represented in\nthe test set, evaluation is typically broken out by “seen” categories (S), “unseen” categories (U)\nand “all” (A). Each input example is represented by a sequence of SUBJECT — PROPERTY —\nOBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0."}
{"doc_id": "2106.09685", "para_id": 183, "text": "We train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\nof training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\nmodules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\nusual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\nrandom seeds; the result for each run is taken from the best epoch. For a fair comparison with the\nsetup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\nand used a ﬁxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\nmodel when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\nThe runs with this restricted setup are marked with †. See the hyperparameters used in our runs\nin Table 9."}
{"doc_id": "2106.09685", "para_id": 184, "text": "We again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\nwe tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\nsequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\nwe initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\nSTS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\nthe median over 5 random seeds; the result for each run is taken from the best epoch. See the\nhyperparameters used in our runs in Table 10."}
{"doc_id": "2106.09685", "para_id": 185, "text": "Method\nDataset\nMNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B"}
{"doc_id": "2106.09685", "para_id": 186, "text": "Optimizer\nAdamW\nWarmup Ratio\n0.06\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 187, "text": "Batch Size\n16\n16\n16\n32\n32\n16\n32\n16\n# Epochs\n30\n60\n30\n80\n25\n25\n80\n40\nLearning Rate\n5E-04\n5E-04\n4E-04\n4E-04\n4E-04\n5E-04\n5E-04\n4E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n8\nMax Seq. Len.\n512"}
{"doc_id": "2106.09685", "para_id": 188, "text": "Batch Size\n4\n4\n4\n4\n4\n4\n8\n8\n# Epochs\n10\n10\n20\n20\n10\n20\n20\n30\nLearning Rate\n3E-04\n4E-04\n3E-04\n2E-04\n2E-04\n3E-04\n4E-04\n2E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n16\nMax Seq. Len.\n128\n128\n512\n128\n512\n512\n512\n512"}
{"doc_id": "2106.09685", "para_id": 189, "text": "Batch Size\n4\n# Epochs\n10\n10\n20\n20\n10\n20\n20\n10\nLearning Rate\n3E-04\n4E-04\n3E-04\n2E-04\n2E-04\n3E-04\n4E-04\n2E-04\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n16\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 190, "text": "Batch Size\n32\n# Epochs\n10\n20\n20\n20\n10\n20\n20\n20\nLearning Rate\n3E-05\n3E-05\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n64\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 191, "text": "Batch Size\n32\n# Epochs\n5\n20\n20\n20\n10\n20\n20\n20\nLearning Rate\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n16\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 192, "text": "Batch Size\n32\n# Epochs\n10\n5\n10\n10\n5\n20\n20\n10\nLearning Rate\n3E-05\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n64\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 193, "text": "Batch Size\n32\n# Epochs\n10\n5\n10\n10\n5\n20\n20\n10\nLearning Rate\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\n3E-04\nBottleneck r\n8\nMax Seq. Len.\n128"}
{"doc_id": "2106.09685", "para_id": 194, "text": "Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 195, "text": "We train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\nrate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\nin Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\nmean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\nused for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\n(2021)."}
{"doc_id": "2106.09685", "para_id": 196, "text": "For all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\na batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for"}
{"doc_id": "2106.09685", "para_id": 197, "text": "Method\nDataset\nMNLI\nSST-2\nMRPC\nCoLA\nQNLI\nQQP\nRTE\nSTS-B"}
{"doc_id": "2106.09685", "para_id": 198, "text": "Optimizer\nAdamW\nWarmup Ratio\n0.1\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 199, "text": "Batch Size\n8\n8\n32\n4\n6\n8\n4\n4\n# Epochs\n5\n16\n30\n10\n8\n11\n11\n10\nLearning Rate\n1E-04\n6E-05\n2E-04\n1E-04\n1E-04\n1E-04\n2E-04\n2E-04\nWeight Decay\n0\n0.01\n0.01\n0\n0.01\n0.01\n0.01\n0.1\nCLS Dropout\n0.15\n0\n0\n0.1\n0.1\n0.2\n0.2\n0.2\nLoRA Conﬁg.\nrq = rv = 8\nLoRA α\n8\nMax Seq. Len.\n256\n128\n128\n64\n512\n320\n320\n128"}
{"doc_id": "2106.09685", "para_id": 200, "text": "Table 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark."}
{"doc_id": "2106.09685", "para_id": 201, "text": "Optimizer\nAdamW\nWeight Decay\n0.01\n0.01\n0.0\nDropout Prob\n0.1\n0.1\n0.0\nBatch Size\n8\n# Epoch\n5\nWarmup Steps\n500\nLearning Rate Schedule\nLinear\nLabel Smooth\n0.1\n0.1\n0.0\nLearning Rate\n0.0002\nAdaptation\nrq = rv = 4\nLoRA α\n32"}
{"doc_id": "2106.09685", "para_id": 202, "text": "Beam Size\n10\nLength Penalty\n0.9\n0.8\n0.8\nno repeat ngram size\n4"}
{"doc_id": "2106.09685", "para_id": 203, "text": "Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART."}
{"doc_id": "2106.09685", "para_id": 204, "text": "WikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa\net al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\ndetails on the hyperparameters used. For preﬁx-embedding tuning, we ﬁnd the optimal lp and li\nto be 256 and 8, respectively, totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for\npreﬁx-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We\npresent two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8\nor rq = rk = rv = ro = 2). We report the best validation performance from each run. The training\nhyperparameters used in our GPT-3 experiments are listed in Table 12."}
{"doc_id": "2106.09685", "para_id": 205, "text": "LoRA can be naturally combined with existing preﬁx-based approaches. In this section, we evaluate\ntwo combinations of LoRA and variants of preﬁx-tuning on WikiSQL and MNLI."}
{"doc_id": "2106.09685", "para_id": 206, "text": "LoRA+PreﬁxEmbed (LoRA+PE) combines LoRA with preﬁx-embedding tuning, where we insert\nlp + li special tokens whose embeddings are treated as trainable parameters. For more on preﬁx-\nembedding tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 207, "text": "LoRA+PreﬁxLayer (LoRA+PL) combines LoRA with preﬁx-layer tuning. We also insert lp + li\nspecial tokens; however, instead of letting the hidden representations of these tokens evolve natu-"}
{"doc_id": "2106.09685", "para_id": 208, "text": "Hyperparameters\nFine-Tune\nPreEmbed\nPreLayer\nBitFit\nAdapterH\nLoRA"}
{"doc_id": "2106.09685", "para_id": 209, "text": "Optimizer\nAdamW\nBatch Size\n128\n# Epoch\n2\nWarmup Tokens\n250,000\nLR Schedule\nLinear"}
{"doc_id": "2106.09685", "para_id": 210, "text": "Learning Rate\n5.00E-06\n5.00E-04\n1.00E-04\n1.6E-03\n1.00E-04\n2.00E-04"}
{"doc_id": "2106.09685", "para_id": 211, "text": "Table 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\nsame hyperparameters for all datasets after tuning learning rate."}
{"doc_id": "2106.09685", "para_id": 212, "text": "rally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\nmore on preﬁx-layer tuning, see Section 5.1."}
{"doc_id": "2106.09685", "para_id": 213, "text": "In Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\nFirst of all, LoRA+PE signiﬁcantly outperforms both LoRA and preﬁx-embedding tuning on\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preﬁx-embedding tuning. On\nMultiNLI, the combination of LoRA+PE doesn’t perform better than LoRA, possibly because LoRA\non its own already achieves performance comparable to the human baseline. Secondly, we notice\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\ntribute this to the fact that preﬁx-layer tuning is very sensitive to the choice of learning rate and thus\nmakes the optimization of LoRA weights more difﬁcult in LoRA+PL."}
{"doc_id": "2106.09685", "para_id": 214, "text": "We also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\npreﬁx-based approaches given the same number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 215, "text": "Method\n# Trainable\nDART\nParameters\nBLEU↑\nMET↑\nTER↓"}
{"doc_id": "2106.09685", "para_id": 216, "text": "GPT-2 Medium\nFine-Tune\n354M\n46.2\n0.39\n0.46\nAdapterL\n0.37M\n42.4\n0.36\n0.48\nAdapterL\n11M\n45.2\n0.38\n0.46\nFTTop2\n24M\n41.0\n0.34\n0.56\nPrefLayer\n0.35M\n46.4\n0.38\n0.46\nLoRA\n0.35M\n47.1±.2\n0.39\n0.46"}
{"doc_id": "2106.09685", "para_id": 217, "text": "GPT-2 Large\nFine-Tune\n774M\n47.0\n0.39\n0.46\nAdapterL\n0.88M\n45.7±.1\n0.38\n0.46\nAdapterL\n23M\n47.1±.1\n0.39\n0.45\nPrefLayer\n0.77M\n46.7\n0.38\n0.45\nLoRA\n0.77M\n47.5±.1\n0.39\n0.45"}
{"doc_id": "2106.09685", "para_id": 218, "text": "Table 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\nless than 0.01 for all adaption approaches."}
{"doc_id": "2106.09685", "para_id": 219, "text": "GPT-2 Medium\nFine-Tune (354M)\n27.7\n64.2\n46.5\n.30\n.45\n.38\n.76\n.33\n.53\nAdapterL (0.37M)\n45.1\n54.5\n50.2\n.36\n.39\n.38\n.46\n.40\n.43\nAdapterL (11M)\n48.3\n60.4\n54.9\n.38\n.43\n.41\n.45\n.35\n.39\nFTTop2 (24M)\n18.9\n53.6\n36.0\n.23\n.38\n.31\n.99\n.49\n.72\nPreﬁx (0.35M)\n45.6\n62.9\n55.1\n.38\n.44\n.41\n.49\n.35\n.40\nLoRA (0.35M)\n46.7±.4\n62.1±.2\n55.3±.2\n.38\n.44\n.41\n.46\n.33\n.39"}
{"doc_id": "2106.09685", "para_id": 220, "text": "GPT-2 Large\nFine-Tune (774M)\n43.1\n65.3\n55.5\n.38\n.46\n.42\n.53\n.33\n.42\nAdapterL (0.88M)\n49.8±.0\n61.1±.0\n56.0±.0\n.38\n.43\n.41\n.44\n.35\n.39\nAdapterL (23M)\n49.2±.1\n64.7±.2\n57.7±.1\n.39\n.46\n.43\n.46\n.33\n.39\nPreﬁx (0.77M)\n47.7\n63.4\n56.3\n.39\n.45\n.42\n.48\n.34\n.40\nLoRA (0.77M)\n48.4±.3\n64.0±.3\n57.0±.1\n.39\n.45\n.42\n.45\n.32\n.38"}
{"doc_id": "2106.09685", "para_id": 221, "text": "Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\nare less than 0.01 for all the experiments we ran. “U” indicates unseen categories, “S” indicates seen\ncategories, and “A” indicates all categories in the test set of WebNLG."}
{"doc_id": "2106.09685", "para_id": 222, "text": "We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\nidentifying the trade-off between performance and the number of trainable parameters."}
{"doc_id": "2106.09685", "para_id": 223, "text": "To evaluate the performance of different adaptation approaches in the low-data regime. we randomly\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\nMNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\nn. To our surprise, PreﬁxEmbed and PreﬁxLayer performs very poorly on MNLI-100 dataset, with\nPreﬁxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreﬁxLayer\nperforms better than PreﬁxEmbed but is still signiﬁcantly worse than Fine-Tune or LoRA on MNLI-\n100. The gap between preﬁx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\ncrease the number of training examples, which might suggest that preﬁx-based approaches are not\nsuitable for low-data tasks in GPT-3. LoRA achieves better performance than ﬁne-tuning on both\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\n(±0.3) variance due to random seeds."}
{"doc_id": "2106.09685", "para_id": 224, "text": "The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\nble 17. We use a smaller learning rate for PreﬁxLayer on the MNLI-100 set, as the training loss does\nnot decrease with a larger learning rate."}
{"doc_id": "2106.09685", "para_id": 225, "text": "In this paper we use the measure φ(A, B, i, j) = ψ(U i\nA, U j\nB) = ∥U i⊤\nA UB∥2\nF\nmin{i,j}\nto measure the subspace"}
{"doc_id": "2106.09685", "para_id": 226, "text": "similarity between two column orthonormal matrices U i\nA ∈Rd×i and U j\nB ∈Rd×j, obtained by\ntaking columns of the left singular matrices of A and B. We point out that this similarity is simply\na reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\n(2008)."}
{"doc_id": "2106.09685", "para_id": 227, "text": "Method\nHyperparameters\n# Trainable Parameters\nWikiSQL\nMNLI-m"}
{"doc_id": "2106.09685", "para_id": 228, "text": "lp = 32, li = 8\n0.4 M\n55.9\n84.9\nlp = 64, li = 8\n0.9 M\n58.7\n88.1\nlp = 128, li = 8\n1.7 M\n60.6\n88.0\nlp = 256, li = 8\n3.2 M\n63.1\n88.6\nlp = 512, li = 8\n6.4 M\n55.9\n85.8"}
{"doc_id": "2106.09685", "para_id": 229, "text": "lp = 2, li = 2\n5.1 M\n68.5\n89.2\nlp = 8, li = 0\n10.1 M\n69.8\n88.2\nlp = 8, li = 8\n20.2 M\n70.1\n89.5\nlp = 32, li = 4\n44.1 M\n66.4\n89.6\nlp = 64, li = 0\n76.1 M\n64.9\n87.9"}
{"doc_id": "2106.09685", "para_id": 230, "text": "r = 1\n7.1 M\n71.9\n89.8\nr = 4\n21.2 M\n73.2\n91.0\nr = 8\n40.1 M\n73.2\n91.5\nr = 16\n77.9 M\n73.2\n91.5\nr = 64\n304.4 M\n72.6\n91.5"}
{"doc_id": "2106.09685", "para_id": 231, "text": "rv = 2\n4.7 M\n73.4\n91.7\nrq = rv = 1\n4.7 M\n73.4\n91.3\nrq = rv = 2\n9.4 M\n73.3\n91.4\nrq = rk = rv = ro = 1\n9.4 M\n74.1\n91.2\nrq = rv = 4\n18.8 M\n73.7\n91.3\nrq = rk = rv = ro = 2\n18.8 M\n73.7\n91.7\nrq = rv = 8\n37.7 M\n73.8\n91.6\nrq = rk = rv = ro = 4\n37.7 M\n74.0\n91.7\nrq = rv = 64\n301.9 M\n73.6\n91.4\nrq = rk = rv = ro = 64\n603.8 M\n73.9\n91.4"}
{"doc_id": "2106.09685", "para_id": 232, "text": "rq = rv = 8, lp = 8, li = 4\n37.8 M\n75.0\n91.4\nrq = rv = 32, lp = 8, li = 4\n151.1 M\n75.9\n91.1\nrq = rv = 64, lp = 8, li = 4\n302.1 M\n76.2\n91.3"}
{"doc_id": "2106.09685", "para_id": 233, "text": "LoRA+PL\nrq = rv = 8, lp = 8, li = 4\n52.8 M\n72.9\n90.2"}
{"doc_id": "2106.09685", "para_id": 234, "text": "Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\npreﬁx-embedding tuning (PreﬁxEmbed) and preﬁx-layer tuning (PreﬁxLayer) perform worse as we\nincrease the number of trainable parameters, while LoRA’s performance stabilizes. Performance is\nmeasured in validation accuracy."}
{"doc_id": "2106.09685", "para_id": 235, "text": "Method\nMNLI(m)-100\nMNLI(m)-1k\nMNLI(m)-10k\nMNLI(m)-392K"}
{"doc_id": "2106.09685", "para_id": 236, "text": "GPT-3 (Fine-Tune)\n60.2\n85.8\n88.9\n89.5\nGPT-3 (PreﬁxEmbed)\n37.6\n75.2\n79.5\n88.6\nGPT-3 (PreﬁxLayer)\n48.3\n82.5\n85.9\n89.6\nGPT-3 (LoRA)\n63.8\n85.6\n89.2\n91.7"}
{"doc_id": "2106.09685", "para_id": 237, "text": "Table 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\nn describes a subset with n training examples. We evaluate with the full validation set. LoRA\nperforms exhibits favorable sample-efﬁciency compared to other methods, including ﬁne-tuning."}
{"doc_id": "2106.09685", "para_id": 238, "text": "To be concrete, let the singular values of U i⊤\nA U j\nB to be σ1, σ2, · · · , σp where p = min{i, j}. We\nknow that the Projection Metric Ham & Lee (2008) is deﬁned as:"}
{"doc_id": "2106.09685", "para_id": 239, "text": "Hyperparameters\nAdaptation\nMNLI-100\nMNLI-1k\nMNLI-10K\nMNLI-392K"}
{"doc_id": "2106.09685", "para_id": 240, "text": "Optimizer\n-\nAdamW\nWarmup Tokens\n-\n250,000\nLR Schedule\n-\nLinear\nBatch Size\n-\n20\n20\n100\n128\n# Epoch\n-\n40\n40\n4\n2"}
{"doc_id": "2106.09685", "para_id": 241, "text": "FineTune\n5.00E-6\nPreﬁxEmbed\n2.00E-04\n2.00E-04\n4.00E-04\n5.00E-04\nPreﬁxLayer\n5.00E-05\n5.00E-05\n5.00E-05\n1.00E-04\nLoRA\n2.00E-4"}
{"doc_id": "2106.09685", "para_id": 242, "text": "PreﬁxEmbed lp\n16\n32\n64\n256\nAdaptation-\nPreﬁxEmbed li\n8\nSpeciﬁc\nPreﬁxTune\nlp = li = 8\nLoRA\nrq = rv = 8"}
{"doc_id": "2106.09685", "para_id": 243, "text": "Table 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n."}
{"doc_id": "2106.09685", "para_id": 244, "text": "φ(A, B, i, j) = ψ(U i\nA, U j\nB) =\nPp\ni=1 σ2\ni\np\n= 1"}
{"doc_id": "2106.09685", "para_id": 245, "text": "This similarity satisﬁes that if U i\nA and U j\nB share the same column span, then φ(A, B, i, j) = 1. If\nthey are completely orthogonal, then φ(A, B, i, j) = 0. Otherwise, φ(A, B, i, j) ∈(0, 1)."}
{"doc_id": "2106.09685", "para_id": 246, "text": "We present additional results from our investigation into the low-rank update matrices."}
{"doc_id": "2106.09685", "para_id": 247, "text": "See Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\nlayers."}
{"doc_id": "2106.09685", "para_id": 248, "text": "We repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge\ndataset as an example, we report the validation loss and test metrics achieved by different choices\nof r after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\nNote that the relationship between model size and the optimal rank for adaptation is still an open\nquestion."}
{"doc_id": "2106.09685", "para_id": 249, "text": "See Figure 8 for the normalized subspace similarity between W and ∆W with varying r."}
{"doc_id": "2106.09685", "para_id": 250, "text": "Note again that ∆W does not contain the top singular directions of W, since the similarity between\nthe top 4 directions in ∆W and the top-10% of those in W barely exceeds 0.2. This gives evidence\nthat ∆W contains those “task-speciﬁc” directions that are otherwise not emphasized in W."}
{"doc_id": "2106.09685", "para_id": 251, "text": "An interesting next question to answer, is how “strong” do we need to amplify those task-speciﬁc\ndirections, in order for the model adaptation to work well?"}
{"doc_id": "2106.09685", "para_id": 252, "text": "Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both\n∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer."}
{"doc_id": "2106.09685", "para_id": 253, "text": "One can naturally consider a feature ampliﬁcation factor as the ratio\n∥∆W ∥F\n∥U ⊤W V ⊤∥F , where U and V\nare the left- and right-singular matrices of the SVD decomposition of ∆W. (Recall UU ⊤WV ⊤V\ngives the “projection” of W onto the subspace spanned by ∆W.)"}
{"doc_id": "2106.09685", "para_id": 254, "text": "Intuitively, when ∆W mostly contains task-speciﬁc directions, this quantity measures how much of\nthem are ampliﬁed by ∆W. As shown in Section 7.3, for r = 4, this ampliﬁcation factor is as large\nas 20. In other words, there are (generally speaking) four feature directions in each layer (out of the\nentire feature space from the pre-trained model W), that need to be ampliﬁed by a very large factor\n20, in order to achieve our reported accuracy for the downstream speciﬁc task. And, one should\nexpect a very different set of feature directions to be ampliﬁed for each different downstream task."}
{"doc_id": "2106.09685", "para_id": 255, "text": "One may notice, however, for r = 64, this ampliﬁcation factor is only around 2, meaning that\nmost directions learned in ∆W with r = 64 are not being ampliﬁed by much. This should not\nbe surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\nthe “task-speciﬁc directions” (thus for model adaptation) is low. In contrast, those directions in the\nrank-4 version of ∆W (corresponding to r = 4) are ampliﬁed by a much larger factor 20."}
{"doc_id": "2106.09685", "para_id": 256, "text": "Figure 7: Normalized subspace similarity between the column vectors of Ar=64 from two randomly\nseeded runs, for both ∆Wq and ∆Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\nformer."}
{"doc_id": "2106.09685", "para_id": 257, "text": "1\n1.23\n68.72\n8.7215\n0.4565\n0.7052\n2.4329\n2\n1.21\n69.17\n8.7413\n0.4590\n0.7052\n2.4639\n4\n1.18\n70.38\n8.8439\n0.4689\n0.7186\n2.5349\n8\n1.17\n69.57\n8.7457\n0.4636\n0.7196\n2.5196\n16\n1.16\n69.61\n8.7483\n0.4629\n0.7177\n2.4985\n32\n1.16\n69.33\n8.7736\n0.4642\n0.7105\n2.5255\n64\n1.16\n69.24\n8.7174\n0.4651\n0.7180\n2.5070\n128\n1.16\n68.73\n8.6718\n0.4628\n0.7127\n2.5030\n256\n1.16\n68.92\n8.6982\n0.4629\n0.7128\n2.5012\n512\n1.16\n68.78\n8.6857\n0.4637\n0.7128\n2.5025\n1024\n1.17\n69.37\n8.7495\n0.4659\n0.7149\n2.5090"}
{"doc_id": "2106.09685", "para_id": 258, "text": "Table 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\ndifferent rank r using GPT-2 Medium. Unlike on GPT-3 where r = 1 sufﬁces for many tasks, here\nthe performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2\nMedium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\nhyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and\nthus might not be optimal for other choices of r."}
{"doc_id": "2106.09685", "para_id": 259, "text": "Figure 8: Normalized subspace similarity between the singular directions of Wq and those of ∆Wq\nwith varying r and a random baseline. ∆Wq ampliﬁes directions that are important but not empha-\nsized in W. ∆W with a larger r tends to pick up more directions that are already emphasized in\nW."}
{"doc_id": "gutenberg_1342", "para_id": 0, "text": "_Walt Whitman has somewhere a fine and just distinction between “loving"}
{"doc_id": "gutenberg_1342", "para_id": 1, "text": "by allowance” and “loving with personal love.” This distinction applies"}
{"doc_id": "gutenberg_1342", "para_id": 2, "text": "to books as well as to men and women; and in the case of the not very"}
{"doc_id": "gutenberg_1342", "para_id": 3, "text": "numerous authors who are the objects of the personal affection, it"}
{"doc_id": "gutenberg_1342", "para_id": 4, "text": "brings a curious consequence with it. There is much more difference as"}
{"doc_id": "gutenberg_1342", "para_id": 5, "text": "to their best work than in the case of those others who are loved “by"}
{"doc_id": "gutenberg_1342", "para_id": 6, "text": "allowance” by convention, and because it is felt to be the right and"}
{"doc_id": "gutenberg_1342", "para_id": 7, "text": "proper thing to love them. And in the sect--fairly large and yet"}
{"doc_id": "gutenberg_1342", "para_id": 8, "text": "unusually choice--of Austenians or Janites, there would probably be"}
{"doc_id": "gutenberg_1342", "para_id": 9, "text": "found partisans of the claim to primacy of almost every one of the"}
{"doc_id": "gutenberg_1342", "para_id": 10, "text": "novels. To some the delightful freshness and humour of_ Northanger"}
{"doc_id": "gutenberg_1342", "para_id": 11, "text": "Abbey, _its completeness, finish, and_ entrain, _obscure the undoubted"}
{"doc_id": "gutenberg_1342", "para_id": 12, "text": "critical facts that its scale is small, and its scheme, after all, that"}
{"doc_id": "gutenberg_1342", "para_id": 13, "text": "of burlesque or parody, a kind in which the first rank is reached with"}
{"doc_id": "gutenberg_1342", "para_id": 14, "text": "difficulty._ Persuasion, _relatively faint in tone, and not enthralling"}
{"doc_id": "gutenberg_1342", "para_id": 15, "text": "in interest, has devotees who exalt above all the others its exquisite"}
{"doc_id": "gutenberg_1342", "para_id": 16, "text": "delicacy and keeping. The catastrophe of_ Mansfield Park _is admittedly"}
{"doc_id": "gutenberg_1342", "para_id": 17, "text": "theatrical, the hero and heroine are insipid, and the author has almost"}
{"doc_id": "gutenberg_1342", "para_id": 18, "text": "wickedly destroyed all romantic interest by expressly admitting that"}
{"doc_id": "gutenberg_1342", "para_id": 19, "text": "Edmund only took Fanny because Mary shocked him, and that Fanny might"}
{"doc_id": "gutenberg_1342", "para_id": 20, "text": "very likely have taken Crawford if he had been a little more assiduous;"}
{"doc_id": "gutenberg_1342", "para_id": 21, "text": "yet the matchless rehearsal-scenes and the characters of Mrs. Norris and"}
{"doc_id": "gutenberg_1342", "para_id": 22, "text": "others have secured, I believe, a considerable party for it._ Sense and"}
{"doc_id": "gutenberg_1342", "para_id": 23, "text": "Sensibility _has perhaps the fewest out-and-out admirers; but it does"}
{"doc_id": "gutenberg_1342", "para_id": 24, "text": "_I suppose, however, that the majority of at least competent votes"}
{"doc_id": "gutenberg_1342", "para_id": 25, "text": "would, all things considered, be divided between_ Emma _and the present"}
{"doc_id": "gutenberg_1342", "para_id": 26, "text": "book; and perhaps the vulgar verdict (if indeed a fondness for Miss"}
{"doc_id": "gutenberg_1342", "para_id": 27, "text": "Austen be not of itself a patent of exemption from any possible charge"}
{"doc_id": "gutenberg_1342", "para_id": 28, "text": "of vulgarity) would go for_ Emma. _It is the larger, the more varied, the"}
{"doc_id": "gutenberg_1342", "para_id": 29, "text": "more popular; the author had by the time of its composition seen rather"}
{"doc_id": "gutenberg_1342", "para_id": 30, "text": "more of the world, and had improved her general, though not her most"}
{"doc_id": "gutenberg_1342", "para_id": 31, "text": "peculiar and characteristic dialogue; such figures as Miss Bates, as the"}
{"doc_id": "gutenberg_1342", "para_id": 32, "text": "Eltons, cannot but unite the suffrages of everybody. On the other hand,"}
{"doc_id": "gutenberg_1342", "para_id": 33, "text": "I, for my part, declare for_ Pride and Prejudice _unhesitatingly. It"}
{"doc_id": "gutenberg_1342", "para_id": 34, "text": "seems to me the most perfect, the most characteristic, the most"}
{"doc_id": "gutenberg_1342", "para_id": 35, "text": "eminently quintessential of its author’s works; and for this contention"}
{"doc_id": "gutenberg_1342", "para_id": 36, "text": "in such narrow space as is permitted to me, I propose here to show"}
{"doc_id": "gutenberg_1342", "para_id": 37, "text": "_In the first place, the book (it may be barely necessary to remind the"}
{"doc_id": "gutenberg_1342", "para_id": 38, "text": "reader) was in its first shape written very early, somewhere about 1796,"}
{"doc_id": "gutenberg_1342", "para_id": 39, "text": "when Miss Austen was barely twenty-one; though it was revised and"}
{"doc_id": "gutenberg_1342", "para_id": 40, "text": "finished at Chawton some fifteen years later, and was not published till"}
{"doc_id": "gutenberg_1342", "para_id": 41, "text": "1813, only four years before her death. I do not know whether, in this"}
{"doc_id": "gutenberg_1342", "para_id": 42, "text": "combination of the fresh and vigorous projection of youth, and the"}
{"doc_id": "gutenberg_1342", "para_id": 43, "text": "critical revision of middle life, there may be traced the distinct"}
{"doc_id": "gutenberg_1342", "para_id": 44, "text": "superiority in point of construction, which, as it seems to me, it"}
{"doc_id": "gutenberg_1342", "para_id": 45, "text": "possesses over all the others. The plot, though not elaborate, is almost"}
{"doc_id": "gutenberg_1342", "para_id": 46, "text": "regular enough for Fielding; hardly a character, hardly an incident"}
{"doc_id": "gutenberg_1342", "para_id": 47, "text": "could be retrenched without loss to the story. The elopement of Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 48, "text": "and Wickham is not, like that of Crawford and Mrs. Rushworth, a_ coup de"}
{"doc_id": "gutenberg_1342", "para_id": 49, "text": "théâtre; _it connects itself in the strictest way with the course of the"}
{"doc_id": "gutenberg_1342", "para_id": 50, "text": "story earlier, and brings about the denouement with complete propriety."}
{"doc_id": "gutenberg_1342", "para_id": 51, "text": "All the minor passages--the loves of Jane and Bingley, the advent of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 52, "text": "Collins, the visit to Hunsford, the Derbyshire tour--fit in after the"}
{"doc_id": "gutenberg_1342", "para_id": 53, "text": "same unostentatious, but masterly fashion. There is no attempt at the"}
{"doc_id": "gutenberg_1342", "para_id": 54, "text": "hide-and-seek, in-and-out business, which in the transactions between"}
{"doc_id": "gutenberg_1342", "para_id": 55, "text": "Frank Churchill and Jane Fairfax contributes no doubt a good deal to the"}
{"doc_id": "gutenberg_1342", "para_id": 56, "text": "intrigue of_ Emma, _but contributes it in a fashion which I do not think"}
{"doc_id": "gutenberg_1342", "para_id": 57, "text": "the best feature of that otherwise admirable book. Although Miss Austen"}
{"doc_id": "gutenberg_1342", "para_id": 58, "text": "always liked something of the misunderstanding kind, which afforded her"}
{"doc_id": "gutenberg_1342", "para_id": 59, "text": "opportunities for the display of the peculiar and incomparable talent to"}
{"doc_id": "gutenberg_1342", "para_id": 60, "text": "be noticed presently, she has been satisfied here with the perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 61, "text": "natural occasions provided by the false account of Darcy’s conduct given"}
{"doc_id": "gutenberg_1342", "para_id": 62, "text": "by Wickham, and by the awkwardness (arising with equal naturalness) from"}
{"doc_id": "gutenberg_1342", "para_id": 63, "text": "the gradual transformation of Elizabeth’s own feelings from positive"}
{"doc_id": "gutenberg_1342", "para_id": 64, "text": "aversion to actual love. I do not know whether the all-grasping hand of"}
{"doc_id": "gutenberg_1342", "para_id": 65, "text": "the playwright has ever been laid upon_ Pride and Prejudice; _and I dare"}
{"doc_id": "gutenberg_1342", "para_id": 66, "text": "say that, if it were, the situations would prove not startling or"}
{"doc_id": "gutenberg_1342", "para_id": 67, "text": "garish enough for the footlights, the character-scheme too subtle and"}
{"doc_id": "gutenberg_1342", "para_id": 68, "text": "delicate for pit and gallery. But if the attempt were made, it would"}
{"doc_id": "gutenberg_1342", "para_id": 69, "text": "certainly not be hampered by any of those loosenesses of construction,"}
{"doc_id": "gutenberg_1342", "para_id": 70, "text": "which, sometimes disguised by the conveniences of which the novelist can"}
{"doc_id": "gutenberg_1342", "para_id": 71, "text": "_I think, however, though the thought will doubtless seem heretical to"}
{"doc_id": "gutenberg_1342", "para_id": 72, "text": "more than one school of critics, that construction is not the highest"}
{"doc_id": "gutenberg_1342", "para_id": 73, "text": "merit, the choicest gift, of the novelist. It sets off his other gifts"}
{"doc_id": "gutenberg_1342", "para_id": 74, "text": "and graces most advantageously to the critical eye; and the want of it"}
{"doc_id": "gutenberg_1342", "para_id": 75, "text": "will sometimes mar those graces--appreciably, though not quite"}
{"doc_id": "gutenberg_1342", "para_id": 76, "text": "consciously--to eyes by no means ultra-critical. But a very badly-built"}
{"doc_id": "gutenberg_1342", "para_id": 77, "text": "novel which excelled in pathetic or humorous character, or which"}
{"doc_id": "gutenberg_1342", "para_id": 78, "text": "displayed consummate command of dialogue--perhaps the rarest of all"}
{"doc_id": "gutenberg_1342", "para_id": 79, "text": "faculties--would be an infinitely better thing than a faultless plot"}
{"doc_id": "gutenberg_1342", "para_id": 80, "text": "acted and told by puppets with pebbles in their mouths. And despite the"}
{"doc_id": "gutenberg_1342", "para_id": 81, "text": "ability which Miss Austen has shown in working out the story, I for one"}
{"doc_id": "gutenberg_1342", "para_id": 82, "text": "should put_ Pride and Prejudice _far lower if it did not contain what"}
{"doc_id": "gutenberg_1342", "para_id": 83, "text": "seem to me the very masterpieces of Miss Austen’s humour and of her"}
{"doc_id": "gutenberg_1342", "para_id": 84, "text": "faculty of character-creation--masterpieces who may indeed admit John"}
{"doc_id": "gutenberg_1342", "para_id": 85, "text": "Thorpe, the Eltons, Mrs. Norris, and one or two others to their company,"}
{"doc_id": "gutenberg_1342", "para_id": 86, "text": "but who, in one instance certainly, and perhaps in others, are still"}
{"doc_id": "gutenberg_1342", "para_id": 87, "text": "_The characteristics of Miss Austen’s humour are so subtle and delicate"}
{"doc_id": "gutenberg_1342", "para_id": 88, "text": "that they are, perhaps, at all times easier to apprehend than to"}
{"doc_id": "gutenberg_1342", "para_id": 89, "text": "express, and at any particular time likely to be differently"}
{"doc_id": "gutenberg_1342", "para_id": 90, "text": "apprehended by different persons. To me this humour seems to possess a"}
{"doc_id": "gutenberg_1342", "para_id": 91, "text": "greater affinity, on the whole, to that of Addison than to any other of"}
{"doc_id": "gutenberg_1342", "para_id": 92, "text": "the numerous species of this great British genus. The differences of"}
{"doc_id": "gutenberg_1342", "para_id": 93, "text": "scheme, of time, of subject, of literary convention, are, of course,"}
{"doc_id": "gutenberg_1342", "para_id": 94, "text": "obvious enough; the difference of sex does not, perhaps, count for much,"}
{"doc_id": "gutenberg_1342", "para_id": 95, "text": "for there was a distinctly feminine element in “Mr. Spectator,” and in"}
{"doc_id": "gutenberg_1342", "para_id": 96, "text": "Jane Austen’s genius there was, though nothing mannish, much that was"}
{"doc_id": "gutenberg_1342", "para_id": 97, "text": "masculine. But the likeness of quality consists in a great number of"}
{"doc_id": "gutenberg_1342", "para_id": 98, "text": "common subdivisions of quality--demureness, extreme minuteness of touch,"}
{"doc_id": "gutenberg_1342", "para_id": 99, "text": "avoidance of loud tones and glaring effects. Also there is in both a"}
{"doc_id": "gutenberg_1342", "para_id": 100, "text": "certain not inhuman or unamiable cruelty. It is the custom with those"}
{"doc_id": "gutenberg_1342", "para_id": 101, "text": "who judge grossly to contrast the good nature of Addison with the"}
{"doc_id": "gutenberg_1342", "para_id": 102, "text": "savagery of Swift, the mildness of Miss Austen with the boisterousness"}
{"doc_id": "gutenberg_1342", "para_id": 103, "text": "of Fielding and Smollett, even with the ferocious practical jokes that"}
{"doc_id": "gutenberg_1342", "para_id": 104, "text": "her immediate predecessor, Miss Burney, allowed without very much"}
{"doc_id": "gutenberg_1342", "para_id": 105, "text": "protest. Yet, both in Mr. Addison and in Miss Austen there is, though a"}
{"doc_id": "gutenberg_1342", "para_id": 106, "text": "restrained and well-mannered, an insatiable and ruthless delight in"}
{"doc_id": "gutenberg_1342", "para_id": 107, "text": "roasting and cutting up a fool. A man in the early eighteenth century,"}
{"doc_id": "gutenberg_1342", "para_id": 108, "text": "of course, could push this taste further than a lady in the early"}
{"doc_id": "gutenberg_1342", "para_id": 109, "text": "nineteenth; and no doubt Miss Austen’s principles, as well as her heart,"}
{"doc_id": "gutenberg_1342", "para_id": 110, "text": "would have shrunk from such things as the letter from the unfortunate"}
{"doc_id": "gutenberg_1342", "para_id": 111, "text": "husband in the_ Spectator, _who describes, with all the gusto and all the"}
{"doc_id": "gutenberg_1342", "para_id": 112, "text": "innocence in the world, how his wife and his friend induce him to play"}
{"doc_id": "gutenberg_1342", "para_id": 113, "text": "at blind-man’s-buff. But another_ Spectator _letter--that of the damsel"}
{"doc_id": "gutenberg_1342", "para_id": 114, "text": "of fourteen who wishes to marry Mr. Shapely, and assures her selected"}
{"doc_id": "gutenberg_1342", "para_id": 115, "text": "Mentor that “he admires your_ Spectators _mightily”--might have been"}
{"doc_id": "gutenberg_1342", "para_id": 116, "text": "written by a rather more ladylike and intelligent Lydia Bennet in the"}
{"doc_id": "gutenberg_1342", "para_id": 117, "text": "days of Lydia’s great-grandmother; while, on the other hand, some (I"}
{"doc_id": "gutenberg_1342", "para_id": 118, "text": "think unreasonably) have found “cynicism” in touches of Miss Austen’s"}
{"doc_id": "gutenberg_1342", "para_id": 119, "text": "own, such as her satire of Mrs. Musgrove’s self-deceiving regrets over"}
{"doc_id": "gutenberg_1342", "para_id": 120, "text": "her son. But this word “cynical” is one of the most misused in the"}
{"doc_id": "gutenberg_1342", "para_id": 121, "text": "English language, especially when, by a glaring and gratuitous"}
{"doc_id": "gutenberg_1342", "para_id": 122, "text": "falsification of its original sense, it is applied, not to rough and"}
{"doc_id": "gutenberg_1342", "para_id": 123, "text": "snarling invective, but to gentle and oblique satire. If cynicism means"}
{"doc_id": "gutenberg_1342", "para_id": 124, "text": "the perception of “the other side,” the sense of “the accepted hells"}
{"doc_id": "gutenberg_1342", "para_id": 125, "text": "beneath,” the consciousness that motives are nearly always mixed, and"}
{"doc_id": "gutenberg_1342", "para_id": 126, "text": "that to seem is not identical with to be--if this be cynicism, then"}
{"doc_id": "gutenberg_1342", "para_id": 127, "text": "every man and woman who is not a fool, who does not care to live in a"}
{"doc_id": "gutenberg_1342", "para_id": 128, "text": "fool’s paradise, who has knowledge of nature and the world and life, is"}
{"doc_id": "gutenberg_1342", "para_id": 129, "text": "a cynic. And in that sense Miss Austen certainly was one. She may even"}
{"doc_id": "gutenberg_1342", "para_id": 130, "text": "have been one in the further sense that, like her own Mr. Bennet, she"}
{"doc_id": "gutenberg_1342", "para_id": 131, "text": "took an epicurean delight in dissecting, in displaying, in setting at"}
{"doc_id": "gutenberg_1342", "para_id": 132, "text": "work her fools and her mean persons. I think she did take this delight,"}
{"doc_id": "gutenberg_1342", "para_id": 133, "text": "and I do not think at all the worse of her for it as a woman, while she"}
{"doc_id": "gutenberg_1342", "para_id": 134, "text": "_In respect of her art generally, Mr. Goldwin Smith has truly observed"}
{"doc_id": "gutenberg_1342", "para_id": 135, "text": "that “metaphor has been exhausted in depicting the perfection of it,"}
{"doc_id": "gutenberg_1342", "para_id": 136, "text": "combined with the narrowness of her field;” and he has justly added that"}
{"doc_id": "gutenberg_1342", "para_id": 137, "text": "we need not go beyond her own comparison to the art of a miniature"}
{"doc_id": "gutenberg_1342", "para_id": 138, "text": "painter. To make this latter observation quite exact we must not use the"}
{"doc_id": "gutenberg_1342", "para_id": 139, "text": "term miniature in its restricted sense, and must think rather of Memling"}
{"doc_id": "gutenberg_1342", "para_id": 140, "text": "at one end of the history of painting and Meissonier at the other, than"}
{"doc_id": "gutenberg_1342", "para_id": 141, "text": "of Cosway or any of his kind. And I am not so certain that I should"}
{"doc_id": "gutenberg_1342", "para_id": 142, "text": "myself use the word “narrow” in connection with her. If her world is a"}
{"doc_id": "gutenberg_1342", "para_id": 143, "text": "microcosm, the cosmic quality of it is at least as eminent as the"}
{"doc_id": "gutenberg_1342", "para_id": 144, "text": "littleness. She does not touch what she did not feel herself called to"}
{"doc_id": "gutenberg_1342", "para_id": 145, "text": "paint; I am not so sure that she could not have painted what she did not"}
{"doc_id": "gutenberg_1342", "para_id": 146, "text": "feel herself called to touch. It is at least remarkable that in two very"}
{"doc_id": "gutenberg_1342", "para_id": 147, "text": "short periods of writing--one of about three years, and another of not"}
{"doc_id": "gutenberg_1342", "para_id": 148, "text": "much more than five--she executed six capital works, and has not left a"}
{"doc_id": "gutenberg_1342", "para_id": 149, "text": "single failure. It is possible that the romantic paste in her"}
{"doc_id": "gutenberg_1342", "para_id": 150, "text": "composition was defective: we must always remember that hardly"}
{"doc_id": "gutenberg_1342", "para_id": 151, "text": "anybody born in her decade--that of the eighteenth-century"}
{"doc_id": "gutenberg_1342", "para_id": 152, "text": "seventies--independently exhibited the full romantic quality. Even Scott"}
{"doc_id": "gutenberg_1342", "para_id": 153, "text": "required hill and mountain and ballad, even Coleridge metaphysics and"}
{"doc_id": "gutenberg_1342", "para_id": 154, "text": "German to enable them to chip the classical shell. Miss Austen was an"}
{"doc_id": "gutenberg_1342", "para_id": 155, "text": "English girl, brought up in a country retirement, at the time when"}
{"doc_id": "gutenberg_1342", "para_id": 156, "text": "ladies went back into the house if there was a white frost which might"}
{"doc_id": "gutenberg_1342", "para_id": 157, "text": "pierce their kid shoes, when a sudden cold was the subject of the"}
{"doc_id": "gutenberg_1342", "para_id": 158, "text": "gravest fears, when their studies, their ways, their conduct were"}
{"doc_id": "gutenberg_1342", "para_id": 159, "text": "subject to all those fantastic limits and restrictions against which"}
{"doc_id": "gutenberg_1342", "para_id": 160, "text": "Mary Wollstonecraft protested with better general sense than particular"}
{"doc_id": "gutenberg_1342", "para_id": 161, "text": "taste or judgment. Miss Austen, too, drew back when the white frost"}
{"doc_id": "gutenberg_1342", "para_id": 162, "text": "touched her shoes; but I think she would have made a pretty good journey"}
{"doc_id": "gutenberg_1342", "para_id": 163, "text": "_For if her knowledge was not very extended, she knew two things which"}
{"doc_id": "gutenberg_1342", "para_id": 164, "text": "only genius knows. The one was humanity, and the other was art. On the"}
{"doc_id": "gutenberg_1342", "para_id": 165, "text": "first head she could not make a mistake; her men, though limited, are"}
{"doc_id": "gutenberg_1342", "para_id": 166, "text": "true, and her women are, in the old sense, “absolute.” As to art, if she"}
{"doc_id": "gutenberg_1342", "para_id": 167, "text": "has never tried idealism, her realism is real to a degree which makes"}
{"doc_id": "gutenberg_1342", "para_id": 168, "text": "the false realism of our own day look merely dead-alive. Take almost any"}
{"doc_id": "gutenberg_1342", "para_id": 169, "text": "Frenchman, except the late M. de Maupassant, and watch him laboriously"}
{"doc_id": "gutenberg_1342", "para_id": 170, "text": "piling up strokes in the hope of giving a complete impression. You get"}
{"doc_id": "gutenberg_1342", "para_id": 171, "text": "none; you are lucky if, discarding two-thirds of what he gives, you can"}
{"doc_id": "gutenberg_1342", "para_id": 172, "text": "shape a real impression out of the rest. But with Miss Austen the"}
{"doc_id": "gutenberg_1342", "para_id": 173, "text": "myriad, trivial, unforced strokes build up the picture like magic."}
{"doc_id": "gutenberg_1342", "para_id": 174, "text": "Nothing is false; nothing is superfluous. When (to take the present book"}
{"doc_id": "gutenberg_1342", "para_id": 175, "text": "only) Mr. Collins changed his mind from Jane to Elizabeth “while Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 176, "text": "Bennet was stirring the fire” (and we know_ how _Mrs. Bennet would have"}
{"doc_id": "gutenberg_1342", "para_id": 177, "text": "stirred the fire), when Mr. Darcy “brought his coffee-cup back_"}
{"doc_id": "gutenberg_1342", "para_id": 178, "text": "himself,” _the touch in each case is like that of Swift--“taller by the"}
{"doc_id": "gutenberg_1342", "para_id": 179, "text": "breadth of my nail”--which impressed the half-reluctant Thackeray with"}
{"doc_id": "gutenberg_1342", "para_id": 180, "text": "just and outspoken admiration. Indeed, fantastic as it may seem, I"}
{"doc_id": "gutenberg_1342", "para_id": 181, "text": "should put Miss Austen as near to Swift in some ways, as I have put her"}
{"doc_id": "gutenberg_1342", "para_id": 182, "text": "_This Swiftian quality appears in the present novel as it appears"}
{"doc_id": "gutenberg_1342", "para_id": 183, "text": "nowhere else in the character of the immortal, the ineffable Mr."}
{"doc_id": "gutenberg_1342", "para_id": 184, "text": "Collins. Mr. Collins is really_ great; _far greater than anything Addison"}
{"doc_id": "gutenberg_1342", "para_id": 185, "text": "ever did, almost great enough for Fielding or for Swift himself. It has"}
{"doc_id": "gutenberg_1342", "para_id": 186, "text": "been said that no one ever was like him. But in the first place,_ he"}
{"doc_id": "gutenberg_1342", "para_id": 187, "text": "_was like him; he is there--alive, imperishable, more real than hundreds"}
{"doc_id": "gutenberg_1342", "para_id": 188, "text": "of prime ministers and archbishops, of “metals, semi-metals, and"}
{"doc_id": "gutenberg_1342", "para_id": 189, "text": "distinguished philosophers.” In the second place, it is rash, I think,"}
{"doc_id": "gutenberg_1342", "para_id": 190, "text": "to conclude that an actual Mr. Collins was impossible or non-existent at"}
{"doc_id": "gutenberg_1342", "para_id": 191, "text": "the end of the eighteenth century. It is very interesting that we"}
{"doc_id": "gutenberg_1342", "para_id": 192, "text": "possess, in this same gallery, what may be called a spoiled first"}
{"doc_id": "gutenberg_1342", "para_id": 193, "text": "draught, or an unsuccessful study of him, in John Dashwood. The"}
{"doc_id": "gutenberg_1342", "para_id": 194, "text": "formality, the under-breeding, the meanness, are there; but the portrait"}
{"doc_id": "gutenberg_1342", "para_id": 195, "text": "is only half alive, and is felt to be even a little unnatural. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 196, "text": "Collins is perfectly natural, and perfectly alive. In fact, for all the"}
{"doc_id": "gutenberg_1342", "para_id": 197, "text": "“miniature,” there is something gigantic in the way in which a certain"}
{"doc_id": "gutenberg_1342", "para_id": 198, "text": "side, and more than one, of humanity, and especially eighteenth-century"}
{"doc_id": "gutenberg_1342", "para_id": 199, "text": "humanity, its Philistinism, its well-meaning but hide-bound morality,"}
{"doc_id": "gutenberg_1342", "para_id": 200, "text": "its formal pettiness, its grovelling respect for rank, its materialism,"}
{"doc_id": "gutenberg_1342", "para_id": 201, "text": "its selfishness, receives exhibition. I will not admit that one speech"}
{"doc_id": "gutenberg_1342", "para_id": 202, "text": "or one action of this inestimable man is incapable of being reconciled"}
{"doc_id": "gutenberg_1342", "para_id": 203, "text": "with reality, and I should not wonder if many of these words and actions"}
{"doc_id": "gutenberg_1342", "para_id": 204, "text": "_But the greatness of Mr. Collins could not have been so satisfactorily"}
{"doc_id": "gutenberg_1342", "para_id": 205, "text": "exhibited if his creatress had not adjusted so artfully to him the"}
{"doc_id": "gutenberg_1342", "para_id": 206, "text": "figures of Mr. Bennet and of Lady Catherine de Bourgh. The latter, like"}
{"doc_id": "gutenberg_1342", "para_id": 207, "text": "Mr. Collins himself, has been charged with exaggeration. There is,"}
{"doc_id": "gutenberg_1342", "para_id": 208, "text": "perhaps, a very faint shade of colour for the charge; but it seems to me"}
{"doc_id": "gutenberg_1342", "para_id": 209, "text": "very faint indeed. Even now I do not think that it would be impossible"}
{"doc_id": "gutenberg_1342", "para_id": 210, "text": "to find persons, especially female persons, not necessarily of noble"}
{"doc_id": "gutenberg_1342", "para_id": 211, "text": "birth, as overbearing, as self-centred, as neglectful of good manners,"}
{"doc_id": "gutenberg_1342", "para_id": 212, "text": "as Lady Catherine. A hundred years ago, an earl’s daughter, the Lady"}
{"doc_id": "gutenberg_1342", "para_id": 213, "text": "Powerful (if not exactly Bountiful) of an out-of-the-way country parish,"}
{"doc_id": "gutenberg_1342", "para_id": 214, "text": "rich, long out of marital authority, and so forth, had opportunities of"}
{"doc_id": "gutenberg_1342", "para_id": 215, "text": "developing these agreeable characteristics which seldom present"}
{"doc_id": "gutenberg_1342", "para_id": 216, "text": "themselves now. As for Mr. Bennet, Miss Austen, and Mr. Darcy, and even"}
{"doc_id": "gutenberg_1342", "para_id": 217, "text": "Miss Elizabeth herself, were, I am inclined to think, rather hard on him"}
{"doc_id": "gutenberg_1342", "para_id": 218, "text": "for the “impropriety” of his conduct. His wife was evidently, and must"}
{"doc_id": "gutenberg_1342", "para_id": 219, "text": "always have been, a quite irreclaimable fool; and unless he had shot her"}
{"doc_id": "gutenberg_1342", "para_id": 220, "text": "or himself there was no way out of it for a man of sense and spirit but"}
{"doc_id": "gutenberg_1342", "para_id": 221, "text": "the ironic. From no other point of view is he open to any reproach,"}
{"doc_id": "gutenberg_1342", "para_id": 222, "text": "except for an excusable and not unnatural helplessness at the crisis of"}
{"doc_id": "gutenberg_1342", "para_id": 223, "text": "the elopement, and his utterances are the most acutely delightful in the"}
{"doc_id": "gutenberg_1342", "para_id": 224, "text": "consciously humorous kind--in the kind that we laugh with, not at--that"}
{"doc_id": "gutenberg_1342", "para_id": 225, "text": "even Miss Austen has put into the mouth of any of her characters. It is"}
{"doc_id": "gutenberg_1342", "para_id": 226, "text": "difficult to know whether he is most agreeable when talking to his wife,"}
{"doc_id": "gutenberg_1342", "para_id": 227, "text": "or when putting Mr. Collins through his paces; but the general sense of"}
{"doc_id": "gutenberg_1342", "para_id": 228, "text": "the world has probably been right in preferring to the first rank his"}
{"doc_id": "gutenberg_1342", "para_id": 229, "text": "consolation to the former when she maunders over the entail, “My dear,"}
{"doc_id": "gutenberg_1342", "para_id": 230, "text": "do not give way to such gloomy thoughts. Let us hope for better things."}
{"doc_id": "gutenberg_1342", "para_id": 231, "text": "Let us flatter ourselves that_ I _may be the survivor;” and his inquiry"}
{"doc_id": "gutenberg_1342", "para_id": 232, "text": "to his colossal cousin as to the compliments which Mr. Collins has just"}
{"doc_id": "gutenberg_1342", "para_id": 233, "text": "related as made by himself to Lady Catherine, “May I ask whether these"}
{"doc_id": "gutenberg_1342", "para_id": 234, "text": "pleasing attentions proceed from the impulse of the moment, or are the"}
{"doc_id": "gutenberg_1342", "para_id": 235, "text": "result of previous study?” These are the things which give Miss Austen’s"}
{"doc_id": "gutenberg_1342", "para_id": 236, "text": "readers the pleasant shocks, the delightful thrills, which are felt by"}
{"doc_id": "gutenberg_1342", "para_id": 237, "text": "the readers of Swift, of Fielding, and we may here add, of Thackeray, as"}
{"doc_id": "gutenberg_1342", "para_id": 238, "text": "they are felt by the readers of no other English author of fiction"}
{"doc_id": "gutenberg_1342", "para_id": 239, "text": "_The goodness of the minor characters in_ Pride and Prejudice _has been"}
{"doc_id": "gutenberg_1342", "para_id": 240, "text": "already alluded to, and it makes a detailed dwelling on their beauties"}
{"doc_id": "gutenberg_1342", "para_id": 241, "text": "difficult in any space, and impossible in this. Mrs. Bennet we have"}
{"doc_id": "gutenberg_1342", "para_id": 242, "text": "glanced at, and it is not easy to say whether she is more exquisitely"}
{"doc_id": "gutenberg_1342", "para_id": 243, "text": "amusing or more horribly true. Much the same may be said of Kitty and"}
{"doc_id": "gutenberg_1342", "para_id": 244, "text": "Lydia; but it is not every author, even of genius, who would have"}
{"doc_id": "gutenberg_1342", "para_id": 245, "text": "differentiated with such unerring skill the effects of folly and"}
{"doc_id": "gutenberg_1342", "para_id": 246, "text": "vulgarity of intellect and disposition working upon the common"}
{"doc_id": "gutenberg_1342", "para_id": 247, "text": "weaknesses of woman at such different ages. With Mary, Miss Austen has"}
{"doc_id": "gutenberg_1342", "para_id": 248, "text": "taken rather less pains, though she has been even more unkind to her;"}
{"doc_id": "gutenberg_1342", "para_id": 249, "text": "not merely in the text, but, as we learn from those interesting"}
{"doc_id": "gutenberg_1342", "para_id": 250, "text": "traditional appendices which Mr. Austen Leigh has given us, in dooming"}
{"doc_id": "gutenberg_1342", "para_id": 251, "text": "her privately to marry “one of Mr. Philips’s clerks.” The habits of"}
{"doc_id": "gutenberg_1342", "para_id": 252, "text": "first copying and then retailing moral sentiments, of playing and"}
{"doc_id": "gutenberg_1342", "para_id": 253, "text": "singing too long in public, are, no doubt, grievous and criminal; but"}
{"doc_id": "gutenberg_1342", "para_id": 254, "text": "perhaps poor Mary was rather the scapegoat of the sins of blue stockings"}
{"doc_id": "gutenberg_1342", "para_id": 255, "text": "in that Fordyce-belectured generation. It is at any rate difficult not"}
{"doc_id": "gutenberg_1342", "para_id": 256, "text": "to extend to her a share of the respect and affection (affection and"}
{"doc_id": "gutenberg_1342", "para_id": 257, "text": "respect of a peculiar kind; doubtless), with which one regards Mr."}
{"doc_id": "gutenberg_1342", "para_id": 258, "text": "Collins, when she draws the moral of Lydia’s fall. I sometimes wish"}
{"doc_id": "gutenberg_1342", "para_id": 259, "text": "that the exigencies of the story had permitted Miss Austen to unite"}
{"doc_id": "gutenberg_1342", "para_id": 260, "text": "these personages, and thus at once achieve a notable mating and soothe"}
{"doc_id": "gutenberg_1342", "para_id": 261, "text": "_The Bingleys and the Gardiners and the Lucases, Miss Darcy and Miss de"}
{"doc_id": "gutenberg_1342", "para_id": 262, "text": "Bourgh, Jane, Wickham, and the rest, must pass without special comment,"}
{"doc_id": "gutenberg_1342", "para_id": 263, "text": "further than the remark that Charlotte Lucas (her egregious papa, though"}
{"doc_id": "gutenberg_1342", "para_id": 264, "text": "delightful, is just a little on the thither side of the line between"}
{"doc_id": "gutenberg_1342", "para_id": 265, "text": "comedy and farce) is a wonderfully clever study in drab of one kind, and"}
{"doc_id": "gutenberg_1342", "para_id": 266, "text": "that Wickham (though something of Miss Austen’s hesitation of touch in"}
{"doc_id": "gutenberg_1342", "para_id": 267, "text": "dealing with young men appears) is a not much less notable sketch in"}
{"doc_id": "gutenberg_1342", "para_id": 268, "text": "drab of another. Only genius could have made Charlotte what she is, yet"}
{"doc_id": "gutenberg_1342", "para_id": 269, "text": "not disagreeable; Wickham what he is, without investing him either with"}
{"doc_id": "gutenberg_1342", "para_id": 270, "text": "a cheap Don Juanish attractiveness or a disgusting rascality. But the"}
{"doc_id": "gutenberg_1342", "para_id": 271, "text": "hero and the heroine are not tints to be dismissed._"}
{"doc_id": "gutenberg_1342", "para_id": 272, "text": "_Darcy has always seemed to me by far the best and most interesting of"}
{"doc_id": "gutenberg_1342", "para_id": 273, "text": "Miss Austen’s heroes; the only possible competitor being Henry Tilney,"}
{"doc_id": "gutenberg_1342", "para_id": 274, "text": "whose part is so slight and simple that it hardly enters into"}
{"doc_id": "gutenberg_1342", "para_id": 275, "text": "comparison. It has sometimes, I believe, been urged that his pride is"}
{"doc_id": "gutenberg_1342", "para_id": 276, "text": "unnatural at first in its expression and later in its yielding, while"}
{"doc_id": "gutenberg_1342", "para_id": 277, "text": "his falling in love at all is not extremely probable. Here again I"}
{"doc_id": "gutenberg_1342", "para_id": 278, "text": "cannot go with the objectors. Darcy’s own account of the way in which"}
{"doc_id": "gutenberg_1342", "para_id": 279, "text": "his pride had been pampered, is perfectly rational and sufficient; and"}
{"doc_id": "gutenberg_1342", "para_id": 280, "text": "nothing could be, psychologically speaking, a_ causa verior _for its"}
{"doc_id": "gutenberg_1342", "para_id": 281, "text": "sudden restoration to healthy conditions than the shock of Elizabeth’s"}
{"doc_id": "gutenberg_1342", "para_id": 282, "text": "scornful refusal acting on a nature_ ex hypothesi _generous. Nothing in"}
{"doc_id": "gutenberg_1342", "para_id": 283, "text": "even our author is finer and more delicately touched than the change of"}
{"doc_id": "gutenberg_1342", "para_id": 284, "text": "his demeanour at the sudden meeting in the grounds of Pemberley. Had he"}
{"doc_id": "gutenberg_1342", "para_id": 285, "text": "been a bad prig or a bad coxcomb, he might have been still smarting"}
{"doc_id": "gutenberg_1342", "para_id": 286, "text": "under his rejection, or suspicious that the girl had come"}
{"doc_id": "gutenberg_1342", "para_id": 287, "text": "husband-hunting. His being neither is exactly consistent with the"}
{"doc_id": "gutenberg_1342", "para_id": 288, "text": "probable feelings of a man spoilt in the common sense, but not really"}
{"doc_id": "gutenberg_1342", "para_id": 289, "text": "injured in disposition, and thoroughly in love. As for his being in"}
{"doc_id": "gutenberg_1342", "para_id": 290, "text": "love, Elizabeth has given as just an exposition of the causes of that"}
{"doc_id": "gutenberg_1342", "para_id": 291, "text": "phenomenon as Darcy has of the conditions of his unregenerate state,"}
{"doc_id": "gutenberg_1342", "para_id": 292, "text": "only she has of course not counted in what was due to her own personal"}
{"doc_id": "gutenberg_1342", "para_id": 293, "text": "_The secret of that charm many men and not a few women, from Miss Austen"}
{"doc_id": "gutenberg_1342", "para_id": 294, "text": "herself downwards, have felt, and like most charms it is a thing rather"}
{"doc_id": "gutenberg_1342", "para_id": 295, "text": "to be felt than to be explained. Elizabeth of course belongs to the_"}
{"doc_id": "gutenberg_1342", "para_id": 296, "text": "allegro _or_ allegra _division of the army of Venus. Miss Austen was"}
{"doc_id": "gutenberg_1342", "para_id": 297, "text": "always provokingly chary of description in regard to her beauties; and"}
{"doc_id": "gutenberg_1342", "para_id": 298, "text": "except the fine eyes, and a hint or two that she had at any rate"}
{"doc_id": "gutenberg_1342", "para_id": 299, "text": "sometimes a bright complexion, and was not very tall, we hear nothing"}
{"doc_id": "gutenberg_1342", "para_id": 300, "text": "about her looks. But her chief difference from other heroines of the"}
{"doc_id": "gutenberg_1342", "para_id": 301, "text": "lively type seems to lie first in her being distinctly clever--almost"}
{"doc_id": "gutenberg_1342", "para_id": 302, "text": "strong-minded, in the better sense of that objectionable word--and"}
{"doc_id": "gutenberg_1342", "para_id": 303, "text": "secondly in her being entirely destitute of ill-nature for all her"}
{"doc_id": "gutenberg_1342", "para_id": 304, "text": "propensity to tease and the sharpness of her tongue. Elizabeth can give"}
{"doc_id": "gutenberg_1342", "para_id": 305, "text": "at least as good as she gets when she is attacked; but she never"}
{"doc_id": "gutenberg_1342", "para_id": 306, "text": "“scratches,” and she never attacks first. Some of the merest"}
{"doc_id": "gutenberg_1342", "para_id": 307, "text": "obsoletenesses of phrase and manner give one or two of her early"}
{"doc_id": "gutenberg_1342", "para_id": 308, "text": "speeches a slight pertness, but that is nothing, and when she comes to"}
{"doc_id": "gutenberg_1342", "para_id": 309, "text": "serious business, as in the great proposal scene with Darcy (which is,"}
{"doc_id": "gutenberg_1342", "para_id": 310, "text": "as it should be, the climax of the interest of the book), and in the"}
{"doc_id": "gutenberg_1342", "para_id": 311, "text": "final ladies’ battle with Lady Catherine, she is unexceptionable. Then"}
{"doc_id": "gutenberg_1342", "para_id": 312, "text": "too she is a perfectly natural girl. She does not disguise from herself"}
{"doc_id": "gutenberg_1342", "para_id": 313, "text": "or anybody that she resents Darcy’s first ill-mannered personality with"}
{"doc_id": "gutenberg_1342", "para_id": 314, "text": "as personal a feeling. (By the way, the reproach that the ill-manners of"}
{"doc_id": "gutenberg_1342", "para_id": 315, "text": "this speech are overdone is certainly unjust; for things of the same"}
{"doc_id": "gutenberg_1342", "para_id": 316, "text": "kind, expressed no doubt less stiltedly but more coarsely, might have"}
{"doc_id": "gutenberg_1342", "para_id": 317, "text": "been heard in more than one ball-room during this very year from persons"}
{"doc_id": "gutenberg_1342", "para_id": 318, "text": "who ought to have been no worse bred than Darcy.) And she lets the"}
{"doc_id": "gutenberg_1342", "para_id": 319, "text": "injury done to Jane and the contempt shown to the rest of her family"}
{"doc_id": "gutenberg_1342", "para_id": 320, "text": "aggravate this resentment in the healthiest way in the world._"}
{"doc_id": "gutenberg_1342", "para_id": 321, "text": "_Still, all this does not explain her charm, which, taking beauty as a"}
{"doc_id": "gutenberg_1342", "para_id": 322, "text": "common form of all heroines, may perhaps consist in the addition to her"}
{"doc_id": "gutenberg_1342", "para_id": 323, "text": "playfulness, her wit, her affectionate and natural disposition, of a"}
{"doc_id": "gutenberg_1342", "para_id": 324, "text": "certain fearlessness very uncommon in heroines of her type and age."}
{"doc_id": "gutenberg_1342", "para_id": 325, "text": "Nearly all of them would have been in speechless awe of the magnificent"}
{"doc_id": "gutenberg_1342", "para_id": 326, "text": "Darcy; nearly all of them would have palpitated and fluttered at the"}
{"doc_id": "gutenberg_1342", "para_id": 327, "text": "idea of proposals, even naughty ones, from the fascinating Wickham."}
{"doc_id": "gutenberg_1342", "para_id": 328, "text": "Elizabeth, with nothing offensive, nothing_ viraginous, _nothing of the"}
{"doc_id": "gutenberg_1342", "para_id": 329, "text": "“New Woman” about her, has by nature what the best modern (not “new”)"}
{"doc_id": "gutenberg_1342", "para_id": 330, "text": "women have by education and experience, a perfect freedom from the idea"}
{"doc_id": "gutenberg_1342", "para_id": 331, "text": "that all men may bully her if they choose, and that most will away with"}
{"doc_id": "gutenberg_1342", "para_id": 332, "text": "her if they can. Though not in the least “impudent and mannish grown,”"}
{"doc_id": "gutenberg_1342", "para_id": 333, "text": "she has no mere sensibility, no nasty niceness about her. The form of"}
{"doc_id": "gutenberg_1342", "para_id": 334, "text": "passion common and likely to seem natural in Miss Austen’s day was so"}
{"doc_id": "gutenberg_1342", "para_id": 335, "text": "invariably connected with the display of one or the other, or both of"}
{"doc_id": "gutenberg_1342", "para_id": 336, "text": "these qualities, that she has not made Elizabeth outwardly passionate."}
{"doc_id": "gutenberg_1342", "para_id": 337, "text": "But I, at least, have not the slightest doubt that she would have"}
{"doc_id": "gutenberg_1342", "para_id": 338, "text": "married Darcy just as willingly without Pemberley as with it, and"}
{"doc_id": "gutenberg_1342", "para_id": 339, "text": "anybody who can read between lines will not find the lovers’"}
{"doc_id": "gutenberg_1342", "para_id": 340, "text": "conversations in the final chapters so frigid as they might have looked"}
{"doc_id": "gutenberg_1342", "para_id": 341, "text": "to the Della Cruscans of their own day, and perhaps do look to the Della"}
{"doc_id": "gutenberg_1342", "para_id": 342, "text": "_And, after all, what is the good of seeking for the reason of"}
{"doc_id": "gutenberg_1342", "para_id": 343, "text": "charm?--it is there. There were better sense in the sad mechanic"}
{"doc_id": "gutenberg_1342", "para_id": 344, "text": "exercise of determining the reason of its absence where it is not. In"}
{"doc_id": "gutenberg_1342", "para_id": 345, "text": "the novels of the last hundred years there are vast numbers of young"}
{"doc_id": "gutenberg_1342", "para_id": 346, "text": "ladies with whom it might be a pleasure to fall in love; there are at"}
{"doc_id": "gutenberg_1342", "para_id": 347, "text": "least five with whom, as it seems to me, no man of taste and spirit can"}
{"doc_id": "gutenberg_1342", "para_id": 348, "text": "help doing so. Their names are, in chronological order, Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 349, "text": "Bennet, Diana Vernon, Argemone Lavington, Beatrix Esmond, and Barbara"}
{"doc_id": "gutenberg_1342", "para_id": 350, "text": "Grant. I should have been most in love with Beatrix and Argemone; I"}
{"doc_id": "gutenberg_1342", "para_id": 351, "text": "should, I think, for mere occasional companionship, have preferred Diana"}
{"doc_id": "gutenberg_1342", "para_id": 352, "text": "and Barbara. But to live with and to marry, I do not know that any one"}
{"doc_id": "gutenberg_1342", "para_id": 353, "text": "of the four can come into competition with Elizabeth._"}
{"doc_id": "gutenberg_1342", "para_id": 354, "text": "Frontispiece                                                          iv"}
{"doc_id": "gutenberg_1342", "para_id": 355, "text": "Title-page                                                             v"}
{"doc_id": "gutenberg_1342", "para_id": 356, "text": "Dedication                                                           vii"}
{"doc_id": "gutenberg_1342", "para_id": 357, "text": "Heading to Preface                                                    ix"}
{"doc_id": "gutenberg_1342", "para_id": 358, "text": "Heading to List of Illustrations                                     xxv"}
{"doc_id": "gutenberg_1342", "para_id": 359, "text": "Heading to Chapter I.                                                  1"}
{"doc_id": "gutenberg_1342", "para_id": 360, "text": "“He came down to see the place”                                        2"}
{"doc_id": "gutenberg_1342", "para_id": 361, "text": "Mr. and Mrs. Bennet                                                    5"}
{"doc_id": "gutenberg_1342", "para_id": 362, "text": "“I hope Mr. Bingley will like it”                                      6"}
{"doc_id": "gutenberg_1342", "para_id": 363, "text": "“I’m the tallest”                                                      9"}
{"doc_id": "gutenberg_1342", "para_id": 364, "text": "“He rode a black horse”                                               10"}
{"doc_id": "gutenberg_1342", "para_id": 365, "text": "“When the party entered”                                              12"}
{"doc_id": "gutenberg_1342", "para_id": 366, "text": "“She is tolerable”                                                    15"}
{"doc_id": "gutenberg_1342", "para_id": 367, "text": "Heading to Chapter IV.                                                18"}
{"doc_id": "gutenberg_1342", "para_id": 368, "text": "Heading to Chapter V.                                                 22"}
{"doc_id": "gutenberg_1342", "para_id": 369, "text": "“Without once opening his lips”                                       24"}
{"doc_id": "gutenberg_1342", "para_id": 370, "text": "Tailpiece to Chapter V.                                               26"}
{"doc_id": "gutenberg_1342", "para_id": 371, "text": "Heading to Chapter VI.                                                27"}
{"doc_id": "gutenberg_1342", "para_id": 372, "text": "“The entreaties of several”                                           31"}
{"doc_id": "gutenberg_1342", "para_id": 373, "text": "“A note for Miss Bennet”                                              36"}
{"doc_id": "gutenberg_1342", "para_id": 374, "text": "“Cheerful prognostics”                                                40"}
{"doc_id": "gutenberg_1342", "para_id": 375, "text": "“The apothecary came”                                                 43"}
{"doc_id": "gutenberg_1342", "para_id": 376, "text": "“Covering a screen”                                                   45"}
{"doc_id": "gutenberg_1342", "para_id": 377, "text": "“Mrs. Bennet and her two youngest girls”                              53"}
{"doc_id": "gutenberg_1342", "para_id": 378, "text": "Heading to Chapter X.                                                 60"}
{"doc_id": "gutenberg_1342", "para_id": 379, "text": "“No, no; stay where you are”                                          67"}
{"doc_id": "gutenberg_1342", "para_id": 380, "text": "“Piling up the fire”                                                  69"}
{"doc_id": "gutenberg_1342", "para_id": 381, "text": "Heading to Chapter XII.                                               75"}
{"doc_id": "gutenberg_1342", "para_id": 382, "text": "Heading to Chapter XIII.                                              78"}
{"doc_id": "gutenberg_1342", "para_id": 383, "text": "Heading to Chapter XIV.                                               84"}
{"doc_id": "gutenberg_1342", "para_id": 384, "text": "“Protested that he never read novels”                                 87"}
{"doc_id": "gutenberg_1342", "para_id": 385, "text": "Heading to Chapter XV.                                                89"}
{"doc_id": "gutenberg_1342", "para_id": 386, "text": "Heading to Chapter XVI.                                               95"}
{"doc_id": "gutenberg_1342", "para_id": 387, "text": "“The officers of the ----shire”                                       97"}
{"doc_id": "gutenberg_1342", "para_id": 388, "text": "“Delighted to see their dear friend again”                           108"}
{"doc_id": "gutenberg_1342", "para_id": 389, "text": "Heading to Chapter XVIII.                                            113"}
{"doc_id": "gutenberg_1342", "para_id": 390, "text": "“Such very superior dancing is not often seen”                       118"}
{"doc_id": "gutenberg_1342", "para_id": 391, "text": "“To assure you in the most animated language”                        132"}
{"doc_id": "gutenberg_1342", "para_id": 392, "text": "Heading to Chapter XX.                                               139"}
{"doc_id": "gutenberg_1342", "para_id": 393, "text": "“They entered the breakfast-room”                                    143"}
{"doc_id": "gutenberg_1342", "para_id": 394, "text": "Heading to Chapter XXI.                                              146"}
{"doc_id": "gutenberg_1342", "para_id": 395, "text": "“Walked back with them”                                              148"}
{"doc_id": "gutenberg_1342", "para_id": 396, "text": "Heading to Chapter XXII.                                             154"}
{"doc_id": "gutenberg_1342", "para_id": 397, "text": "“So much love and eloquence”                                         156"}
{"doc_id": "gutenberg_1342", "para_id": 398, "text": "“Protested he must be entirely mistaken”                             161"}
{"doc_id": "gutenberg_1342", "para_id": 399, "text": "“Whenever she spoke in a low voice”                                  166"}
{"doc_id": "gutenberg_1342", "para_id": 400, "text": "Heading to Chapter XXIV.                                             168"}
{"doc_id": "gutenberg_1342", "para_id": 401, "text": "Heading to Chapter XXV.                                              175"}
{"doc_id": "gutenberg_1342", "para_id": 402, "text": "“Offended two or three young ladies”                                 177"}
{"doc_id": "gutenberg_1342", "para_id": 403, "text": "“Will you come and see me?”                                          181"}
{"doc_id": "gutenberg_1342", "para_id": 404, "text": "“On the stairs”                                                      189"}
{"doc_id": "gutenberg_1342", "para_id": 405, "text": "“At the door”                                                        194"}
{"doc_id": "gutenberg_1342", "para_id": 406, "text": "“In conversation with the ladies”                                    198"}
{"doc_id": "gutenberg_1342", "para_id": 407, "text": "“Lady Catherine,” said she, “you have given me a treasure”           200"}
{"doc_id": "gutenberg_1342", "para_id": 408, "text": "Heading to Chapter XXX.                                              209"}
{"doc_id": "gutenberg_1342", "para_id": 409, "text": "“He never failed to inform them”                                     211"}
{"doc_id": "gutenberg_1342", "para_id": 410, "text": "“The gentlemen accompanied him”                                      213"}
{"doc_id": "gutenberg_1342", "para_id": 411, "text": "Heading to Chapter XXXI.                                             215"}
{"doc_id": "gutenberg_1342", "para_id": 412, "text": "Heading to Chapter XXXII.                                            221"}
{"doc_id": "gutenberg_1342", "para_id": 413, "text": "“Accompanied by their aunt”                                          225"}
{"doc_id": "gutenberg_1342", "para_id": 414, "text": "“On looking up”                                                      228"}
{"doc_id": "gutenberg_1342", "para_id": 415, "text": "Heading to Chapter XXXIV.                                            235"}
{"doc_id": "gutenberg_1342", "para_id": 416, "text": "“Hearing herself called”                                             243"}
{"doc_id": "gutenberg_1342", "para_id": 417, "text": "Heading to Chapter XXXVI.                                            253"}
{"doc_id": "gutenberg_1342", "para_id": 418, "text": "“Meeting accidentally in town”                                       256"}
{"doc_id": "gutenberg_1342", "para_id": 419, "text": "“His parting obeisance”                                              261"}
{"doc_id": "gutenberg_1342", "para_id": 420, "text": "“Dawson”                                                             263"}
{"doc_id": "gutenberg_1342", "para_id": 421, "text": "“The elevation of his feelings”                                      267"}
{"doc_id": "gutenberg_1342", "para_id": 422, "text": "“They had forgotten to leave any message”                            270"}
{"doc_id": "gutenberg_1342", "para_id": 423, "text": "“How nicely we are crammed in!”                                      272"}
{"doc_id": "gutenberg_1342", "para_id": 424, "text": "Heading to Chapter XL.                                               278"}
{"doc_id": "gutenberg_1342", "para_id": 425, "text": "“I am determined never to speak of it again”                         283"}
{"doc_id": "gutenberg_1342", "para_id": 426, "text": "“When Colonel Miller’s regiment went away”                           285"}
{"doc_id": "gutenberg_1342", "para_id": 427, "text": "“Tenderly flirting”                                                  290"}
{"doc_id": "gutenberg_1342", "para_id": 428, "text": "The arrival of the Gardiners                                         294"}
{"doc_id": "gutenberg_1342", "para_id": 429, "text": "“Conjecturing as to the date”                                        301"}
{"doc_id": "gutenberg_1342", "para_id": 430, "text": "Heading to Chapter XLIV.                                             318"}
{"doc_id": "gutenberg_1342", "para_id": 431, "text": "“To make herself agreeable to all”                                   321"}
{"doc_id": "gutenberg_1342", "para_id": 432, "text": "“Engaged by the river”                                               327"}
{"doc_id": "gutenberg_1342", "para_id": 433, "text": "Heading to Chapter XLVI.                                             334"}
{"doc_id": "gutenberg_1342", "para_id": 434, "text": "“I have not an instant to lose”                                      339"}
{"doc_id": "gutenberg_1342", "para_id": 435, "text": "“The first pleasing earnest of their welcome”                        345"}
{"doc_id": "gutenberg_1342", "para_id": 436, "text": "The Post                                                             359"}
{"doc_id": "gutenberg_1342", "para_id": 437, "text": "“To whom I have related the affair”                                  363"}
{"doc_id": "gutenberg_1342", "para_id": 438, "text": "Heading to Chapter XLIX.                                             368"}
{"doc_id": "gutenberg_1342", "para_id": 439, "text": "“But perhaps you would like to read it”                              370"}
{"doc_id": "gutenberg_1342", "para_id": 440, "text": "“The spiteful old ladies”                                            377"}
{"doc_id": "gutenberg_1342", "para_id": 441, "text": "“With an affectionate smile”                                         385"}
{"doc_id": "gutenberg_1342", "para_id": 442, "text": "“I am sure she did not listen”                                       393"}
{"doc_id": "gutenberg_1342", "para_id": 443, "text": "“Mr. Darcy with him”                                                 404"}
{"doc_id": "gutenberg_1342", "para_id": 444, "text": "“Jane happened to look round”                                        415"}
{"doc_id": "gutenberg_1342", "para_id": 445, "text": "“Mrs. Long and her nieces”                                           420"}
{"doc_id": "gutenberg_1342", "para_id": 446, "text": "“Lizzy, my dear, I want to speak to you”                             422"}
{"doc_id": "gutenberg_1342", "para_id": 447, "text": "Heading to Chapter LVI.                                              431"}
{"doc_id": "gutenberg_1342", "para_id": 448, "text": "“After a short survey”                                               434"}
{"doc_id": "gutenberg_1342", "para_id": 449, "text": "“But now it comes out”                                               442"}
{"doc_id": "gutenberg_1342", "para_id": 450, "text": "“The efforts of his aunt”                                            448"}
{"doc_id": "gutenberg_1342", "para_id": 451, "text": "“Unable to utter a syllable”                                         457"}
{"doc_id": "gutenberg_1342", "para_id": 452, "text": "“The obsequious civility”                                            466"}
{"doc_id": "gutenberg_1342", "para_id": 453, "text": "Heading to Chapter LXI.                                              472"}
{"doc_id": "gutenberg_1342", "para_id": 454, "text": "The End                                                              476"}
{"doc_id": "gutenberg_1342", "para_id": 455, "text": "It is a truth universally acknowledged, that a single man in possession"}
{"doc_id": "gutenberg_1342", "para_id": 456, "text": "However little known the feelings or views of such a man may be on his"}
{"doc_id": "gutenberg_1342", "para_id": 457, "text": "first entering a neighbourhood, this truth is so well fixed in the minds"}
{"doc_id": "gutenberg_1342", "para_id": 458, "text": "of the surrounding families, that he is considered as the rightful"}
{"doc_id": "gutenberg_1342", "para_id": 459, "text": "“My dear Mr. Bennet,” said his lady to him one day, “have you heard that"}
{"doc_id": "gutenberg_1342", "para_id": 460, "text": "“But it is,” returned she; “for Mrs. Long has just been here, and she"}
{"doc_id": "gutenberg_1342", "para_id": 461, "text": "“Do not you want to know who has taken it?” cried his wife, impatiently."}
{"doc_id": "gutenberg_1342", "para_id": 462, "text": "“_You_ want to tell me, and I have no objection to hearing it.”"}
{"doc_id": "gutenberg_1342", "para_id": 463, "text": "“Why, my dear, you must know, Mrs. Long says that Netherfield is taken"}
{"doc_id": "gutenberg_1342", "para_id": 464, "text": "by a young man of large fortune from the north of England; that he came"}
{"doc_id": "gutenberg_1342", "para_id": 465, "text": "down on Monday in a chaise and four to see the place, and was so much"}
{"doc_id": "gutenberg_1342", "para_id": 466, "text": "delighted with it that he agreed with Mr. Morris immediately; that he is"}
{"doc_id": "gutenberg_1342", "para_id": 467, "text": "to take possession before Michaelmas, and some of his servants are to be"}
{"doc_id": "gutenberg_1342", "para_id": 468, "text": "“Oh, single, my dear, to be sure! A single man of large fortune; four or"}
{"doc_id": "gutenberg_1342", "para_id": 469, "text": "five thousand a year. What a fine thing for our girls!”"}
{"doc_id": "gutenberg_1342", "para_id": 470, "text": "“My dear Mr. Bennet,” replied his wife, “how can you be so tiresome? You"}
{"doc_id": "gutenberg_1342", "para_id": 471, "text": "must know that I am thinking of his marrying one of them.”"}
{"doc_id": "gutenberg_1342", "para_id": 472, "text": "“Design? Nonsense, how can you talk so! But it is very likely that he"}
{"doc_id": "gutenberg_1342", "para_id": 473, "text": "_may_ fall in love with one of them, and therefore you must visit him as"}
{"doc_id": "gutenberg_1342", "para_id": 474, "text": "“I see no occasion for that. You and the girls may go--or you may send"}
{"doc_id": "gutenberg_1342", "para_id": 475, "text": "them by themselves, which perhaps will be still better; for as you are"}
{"doc_id": "gutenberg_1342", "para_id": 476, "text": "as handsome as any of them, Mr. Bingley might like you the best of the"}
{"doc_id": "gutenberg_1342", "para_id": 477, "text": "“My dear, you flatter me. I certainly _have_ had my share of beauty, but"}
{"doc_id": "gutenberg_1342", "para_id": 478, "text": "I do not pretend to be anything extraordinary now. When a woman has five"}
{"doc_id": "gutenberg_1342", "para_id": 479, "text": "grown-up daughters, she ought to give over thinking of her own beauty.”"}
{"doc_id": "gutenberg_1342", "para_id": 480, "text": "“In such cases, a woman has not often much beauty to think of.”"}
{"doc_id": "gutenberg_1342", "para_id": 481, "text": "“But, my dear, you must indeed go and see Mr. Bingley when he comes into"}
{"doc_id": "gutenberg_1342", "para_id": 482, "text": "“But consider your daughters. Only think what an establishment it would"}
{"doc_id": "gutenberg_1342", "para_id": 483, "text": "be for one of them. Sir William and Lady Lucas are determined to go,"}
{"doc_id": "gutenberg_1342", "para_id": 484, "text": "merely on that account; for in general, you know, they visit no new"}
{"doc_id": "gutenberg_1342", "para_id": 485, "text": "comers. Indeed you must go, for it will be impossible for _us_ to visit"}
{"doc_id": "gutenberg_1342", "para_id": 486, "text": "“You are over scrupulous, surely. I dare say Mr. Bingley will be very"}
{"doc_id": "gutenberg_1342", "para_id": 487, "text": "glad to see you; and I will send a few lines by you to assure him of my"}
{"doc_id": "gutenberg_1342", "para_id": 488, "text": "hearty consent to his marrying whichever he chooses of the girls--though"}
{"doc_id": "gutenberg_1342", "para_id": 489, "text": "“I desire you will do no such thing. Lizzy is not a bit better than the"}
{"doc_id": "gutenberg_1342", "para_id": 490, "text": "others: and I am sure she is not half so handsome as Jane, nor half so"}
{"doc_id": "gutenberg_1342", "para_id": 491, "text": "good-humoured as Lydia. But you are always giving _her_ the preference.”"}
{"doc_id": "gutenberg_1342", "para_id": 492, "text": "“They have none of them much to recommend them,” replied he: “they are"}
{"doc_id": "gutenberg_1342", "para_id": 493, "text": "all silly and ignorant like other girls; but Lizzy has something more of"}
{"doc_id": "gutenberg_1342", "para_id": 494, "text": "“Mr. Bennet, how can you abuse your own children in such a way? You take"}
{"doc_id": "gutenberg_1342", "para_id": 495, "text": "delight in vexing me. You have no compassion on my poor nerves.”"}
{"doc_id": "gutenberg_1342", "para_id": 496, "text": "“You mistake me, my dear. I have a high respect for your nerves. They"}
{"doc_id": "gutenberg_1342", "para_id": 497, "text": "are my old friends. I have heard you mention them with consideration"}
{"doc_id": "gutenberg_1342", "para_id": 498, "text": "“But I hope you will get over it, and live to see many young men of four"}
{"doc_id": "gutenberg_1342", "para_id": 499, "text": "“It will be no use to us, if twenty such should come, since you will not"}
{"doc_id": "gutenberg_1342", "para_id": 500, "text": "“Depend upon it, my dear, that when there are twenty, I will visit them"}
{"doc_id": "gutenberg_1342", "para_id": 501, "text": "Mr. Bennet was so odd a mixture of quick parts, sarcastic humour,"}
{"doc_id": "gutenberg_1342", "para_id": 502, "text": "reserve, and caprice, that the experience of three-and-twenty years had"}
{"doc_id": "gutenberg_1342", "para_id": 503, "text": "been insufficient to make his wife understand his character. _Her_ mind"}
{"doc_id": "gutenberg_1342", "para_id": 504, "text": "was less difficult to develope. She was a woman of mean understanding,"}
{"doc_id": "gutenberg_1342", "para_id": 505, "text": "little information, and uncertain temper. When she was discontented, she"}
{"doc_id": "gutenberg_1342", "para_id": 506, "text": "fancied herself nervous. The business of her life was to get her"}
{"doc_id": "gutenberg_1342", "para_id": 507, "text": "daughters married: its solace was visiting and news."}
{"doc_id": "gutenberg_1342", "para_id": 508, "text": "Mr. Bennet was among the earliest of those who waited on Mr. Bingley. He"}
{"doc_id": "gutenberg_1342", "para_id": 509, "text": "had always intended to visit him, though to the last always assuring his"}
{"doc_id": "gutenberg_1342", "para_id": 510, "text": "wife that he should not go; and till the evening after the visit was"}
{"doc_id": "gutenberg_1342", "para_id": 511, "text": "paid she had no knowledge of it. It was then disclosed in the following"}
{"doc_id": "gutenberg_1342", "para_id": 512, "text": "manner. Observing his second daughter employed in trimming a hat, he"}
{"doc_id": "gutenberg_1342", "para_id": 513, "text": "“We are not in a way to know _what_ Mr. Bingley likes,” said her mother,"}
{"doc_id": "gutenberg_1342", "para_id": 514, "text": "“But you forget, mamma,” said Elizabeth, “that we shall meet him at the"}
{"doc_id": "gutenberg_1342", "para_id": 515, "text": "assemblies, and that Mrs. Long has promised to introduce him.”"}
{"doc_id": "gutenberg_1342", "para_id": 516, "text": "“I do not believe Mrs. Long will do any such thing. She has two nieces"}
{"doc_id": "gutenberg_1342", "para_id": 517, "text": "of her own. She is a selfish, hypocritical woman, and I have no opinion"}
{"doc_id": "gutenberg_1342", "para_id": 518, "text": "“No more have I,” said Mr. Bennet; “and I am glad to find that you do"}
{"doc_id": "gutenberg_1342", "para_id": 519, "text": "Mrs. Bennet deigned not to make any reply; but, unable to contain"}
{"doc_id": "gutenberg_1342", "para_id": 520, "text": "“Don’t keep coughing so, Kitty, for heaven’s sake! Have a little"}
{"doc_id": "gutenberg_1342", "para_id": 521, "text": "“Kitty has no discretion in her coughs,” said her father; “she times"}
{"doc_id": "gutenberg_1342", "para_id": 522, "text": "“I do not cough for my own amusement,” replied Kitty, fretfully. “When"}
{"doc_id": "gutenberg_1342", "para_id": 523, "text": "“Ay, so it is,” cried her mother, “and Mrs. Long does not come back till"}
{"doc_id": "gutenberg_1342", "para_id": 524, "text": "the day before; so, it will be impossible for her to introduce him, for"}
{"doc_id": "gutenberg_1342", "para_id": 525, "text": "“Then, my dear, you may have the advantage of your friend, and introduce"}
{"doc_id": "gutenberg_1342", "para_id": 526, "text": "“Impossible, Mr. Bennet, impossible, when I am not acquainted with him"}
{"doc_id": "gutenberg_1342", "para_id": 527, "text": "“I honour your circumspection. A fortnight’s acquaintance is certainly"}
{"doc_id": "gutenberg_1342", "para_id": 528, "text": "very little. One cannot know what a man really is by the end of a"}
{"doc_id": "gutenberg_1342", "para_id": 529, "text": "fortnight. But if _we_ do not venture, somebody else will; and after"}
{"doc_id": "gutenberg_1342", "para_id": 530, "text": "all, Mrs. Long and her nieces must stand their chance; and, therefore,"}
{"doc_id": "gutenberg_1342", "para_id": 531, "text": "as she will think it an act of kindness, if you decline the office, I"}
{"doc_id": "gutenberg_1342", "para_id": 532, "text": "The girls stared at their father. Mrs. Bennet said only, “Nonsense,"}
{"doc_id": "gutenberg_1342", "para_id": 533, "text": "“What can be the meaning of that emphatic exclamation?” cried he. “Do"}
{"doc_id": "gutenberg_1342", "para_id": 534, "text": "you consider the forms of introduction, and the stress that is laid on"}
{"doc_id": "gutenberg_1342", "para_id": 535, "text": "them, as nonsense? I cannot quite agree with you _there_. What say you,"}
{"doc_id": "gutenberg_1342", "para_id": 536, "text": "Mary? For you are a young lady of deep reflection, I know, and read"}
{"doc_id": "gutenberg_1342", "para_id": 537, "text": "Mary wished to say something very sensible, but knew not how."}
{"doc_id": "gutenberg_1342", "para_id": 538, "text": "“While Mary is adjusting her ideas,” he continued, “let us return to Mr."}
{"doc_id": "gutenberg_1342", "para_id": 539, "text": "“I am sorry to hear _that_; but why did you not tell me so before? If I"}
{"doc_id": "gutenberg_1342", "para_id": 540, "text": "had known as much this morning, I certainly would not have called on"}
{"doc_id": "gutenberg_1342", "para_id": 541, "text": "him. It is very unlucky; but as I have actually paid the visit, we"}
{"doc_id": "gutenberg_1342", "para_id": 542, "text": "The astonishment of the ladies was just what he wished--that of Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 543, "text": "Bennet perhaps surpassing the rest; though when the first tumult of joy"}
{"doc_id": "gutenberg_1342", "para_id": 544, "text": "was over, she began to declare that it was what she had expected all the"}
{"doc_id": "gutenberg_1342", "para_id": 545, "text": "“How good it was in you, my dear Mr. Bennet! But I knew I should"}
{"doc_id": "gutenberg_1342", "para_id": 546, "text": "persuade you at last. I was sure you loved your girls too well to"}
{"doc_id": "gutenberg_1342", "para_id": 547, "text": "neglect such an acquaintance. Well, how pleased I am! And it is such a"}
{"doc_id": "gutenberg_1342", "para_id": 548, "text": "good joke, too, that you should have gone this morning, and never said a"}
{"doc_id": "gutenberg_1342", "para_id": 549, "text": "“Now, Kitty, you may cough as much as you choose,” said Mr. Bennet; and,"}
{"doc_id": "gutenberg_1342", "para_id": 550, "text": "as he spoke, he left the room, fatigued with the raptures of his wife."}
{"doc_id": "gutenberg_1342", "para_id": 551, "text": "“What an excellent father you have, girls,” said she, when the door was"}
{"doc_id": "gutenberg_1342", "para_id": 552, "text": "shut. “I do not know how you will ever make him amends for his kindness;"}
{"doc_id": "gutenberg_1342", "para_id": 553, "text": "or me either, for that matter. At our time of life, it is not so"}
{"doc_id": "gutenberg_1342", "para_id": 554, "text": "pleasant, I can tell you, to be making new acquaintances every day; but"}
{"doc_id": "gutenberg_1342", "para_id": 555, "text": "for your sakes we would do anything. Lydia, my love, though you _are_"}
{"doc_id": "gutenberg_1342", "para_id": 556, "text": "the youngest, I dare say Mr. Bingley will dance with you at the next"}
{"doc_id": "gutenberg_1342", "para_id": 557, "text": "“Oh,” said Lydia, stoutly, “I am not afraid; for though I _am_ the"}
{"doc_id": "gutenberg_1342", "para_id": 558, "text": "The rest of the evening was spent in conjecturing how soon he would"}
{"doc_id": "gutenberg_1342", "para_id": 559, "text": "return Mr. Bennet’s visit, and determining when they should ask him to"}
{"doc_id": "gutenberg_1342", "para_id": 560, "text": "Not all that Mrs. Bennet, however, with the assistance of her five"}
{"doc_id": "gutenberg_1342", "para_id": 561, "text": "daughters, could ask on the subject, was sufficient to draw from her"}
{"doc_id": "gutenberg_1342", "para_id": 562, "text": "husband any satisfactory description of Mr. Bingley. They attacked him"}
{"doc_id": "gutenberg_1342", "para_id": 563, "text": "in various ways, with barefaced questions, ingenious suppositions, and"}
{"doc_id": "gutenberg_1342", "para_id": 564, "text": "distant surmises; but he eluded the skill of them all; and they were at"}
{"doc_id": "gutenberg_1342", "para_id": 565, "text": "last obliged to accept the second-hand intelligence of their neighbour,"}
{"doc_id": "gutenberg_1342", "para_id": 566, "text": "Lady Lucas. Her report was highly favourable. Sir William had been"}
{"doc_id": "gutenberg_1342", "para_id": 567, "text": "delighted with him. He was quite young, wonderfully handsome, extremely"}
{"doc_id": "gutenberg_1342", "para_id": 568, "text": "agreeable, and, to crown the whole, he meant to be at the next assembly"}
{"doc_id": "gutenberg_1342", "para_id": 569, "text": "with a large party. Nothing could be more delightful! To be fond of"}
{"doc_id": "gutenberg_1342", "para_id": 570, "text": "dancing was a certain step towards falling in love; and very lively"}
{"doc_id": "gutenberg_1342", "para_id": 571, "text": "“If I can but see one of my daughters happily settled at Netherfield,”"}
{"doc_id": "gutenberg_1342", "para_id": 572, "text": "said Mrs. Bennet to her husband, “and all the others equally well"}
{"doc_id": "gutenberg_1342", "para_id": 573, "text": "In a few days Mr. Bingley returned Mr. Bennet’s visit, and sat about ten"}
{"doc_id": "gutenberg_1342", "para_id": 574, "text": "minutes with him in his library. He had entertained hopes of being"}
{"doc_id": "gutenberg_1342", "para_id": 575, "text": "admitted to a sight of the young ladies, of whose beauty he had heard"}
{"doc_id": "gutenberg_1342", "para_id": 576, "text": "much; but he saw only the father. The ladies were somewhat more"}
{"doc_id": "gutenberg_1342", "para_id": 577, "text": "fortunate, for they had the advantage of ascertaining, from an upper"}
{"doc_id": "gutenberg_1342", "para_id": 578, "text": "window, that he wore a blue coat and rode a black horse."}
{"doc_id": "gutenberg_1342", "para_id": 579, "text": "An invitation to dinner was soon afterwards despatched; and already had"}
{"doc_id": "gutenberg_1342", "para_id": 580, "text": "Mrs. Bennet planned the courses that were to do credit to her"}
{"doc_id": "gutenberg_1342", "para_id": 581, "text": "housekeeping, when an answer arrived which deferred it all. Mr. Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 582, "text": "was obliged to be in town the following day, and consequently unable to"}
{"doc_id": "gutenberg_1342", "para_id": 583, "text": "accept the honour of their invitation, etc. Mrs. Bennet was quite"}
{"doc_id": "gutenberg_1342", "para_id": 584, "text": "disconcerted. She could not imagine what business he could have in town"}
{"doc_id": "gutenberg_1342", "para_id": 585, "text": "so soon after his arrival in Hertfordshire; and she began to fear that"}
{"doc_id": "gutenberg_1342", "para_id": 586, "text": "he might always be flying about from one place to another, and never"}
{"doc_id": "gutenberg_1342", "para_id": 587, "text": "settled at Netherfield as he ought to be. Lady Lucas quieted her fears a"}
{"doc_id": "gutenberg_1342", "para_id": 588, "text": "being gone to London only to get a large party for the ball; and a"}
{"doc_id": "gutenberg_1342", "para_id": 589, "text": "report soon followed that Mr. Bingley was to bring twelve ladies and"}
{"doc_id": "gutenberg_1342", "para_id": 590, "text": "seven gentlemen with him to the assembly. The girls grieved over such a"}
{"doc_id": "gutenberg_1342", "para_id": 591, "text": "number of ladies; but were comforted the day before the ball by hearing"}
{"doc_id": "gutenberg_1342", "para_id": 592, "text": "that, instead of twelve, he had brought only six with him from London,"}
{"doc_id": "gutenberg_1342", "para_id": 593, "text": "his five sisters and a cousin. And when the party entered the"}
{"doc_id": "gutenberg_1342", "para_id": 594, "text": "assembly-room, it consisted of only five altogether: Mr. Bingley, his"}
{"doc_id": "gutenberg_1342", "para_id": 595, "text": "two sisters, the husband of the eldest, and another young man."}
{"doc_id": "gutenberg_1342", "para_id": 596, "text": "Mr. Bingley was good-looking and gentlemanlike: he had a pleasant"}
{"doc_id": "gutenberg_1342", "para_id": 597, "text": "countenance, and easy, unaffected manners. His sisters were fine women,"}
{"doc_id": "gutenberg_1342", "para_id": 598, "text": "with an air of decided fashion. His brother-in-law, Mr. Hurst, merely"}
{"doc_id": "gutenberg_1342", "para_id": 599, "text": "looked the gentleman; but his friend Mr. Darcy soon drew the attention"}
{"doc_id": "gutenberg_1342", "para_id": 600, "text": "of the room by his fine, tall person, handsome features, noble mien, and"}
{"doc_id": "gutenberg_1342", "para_id": 601, "text": "the report, which was in general circulation within five minutes after"}
{"doc_id": "gutenberg_1342", "para_id": 602, "text": "his entrance, of his having ten thousand a year. The gentlemen"}
{"doc_id": "gutenberg_1342", "para_id": 603, "text": "pronounced him to be a fine figure of a man, the ladies declared he was"}
{"doc_id": "gutenberg_1342", "para_id": 604, "text": "much handsomer than Mr. Bingley, and he was looked at with great"}
{"doc_id": "gutenberg_1342", "para_id": 605, "text": "admiration for about half the evening, till his manners gave a disgust"}
{"doc_id": "gutenberg_1342", "para_id": 606, "text": "which turned the tide of his popularity; for he was discovered to be"}
{"doc_id": "gutenberg_1342", "para_id": 607, "text": "proud, to be above his company, and above being pleased; and not all his"}
{"doc_id": "gutenberg_1342", "para_id": 608, "text": "large estate in Derbyshire could save him from having a most forbidding,"}
{"doc_id": "gutenberg_1342", "para_id": 609, "text": "disagreeable countenance, and being unworthy to be compared with his"}
{"doc_id": "gutenberg_1342", "para_id": 610, "text": "Mr. Bingley had soon made himself acquainted with all the principal"}
{"doc_id": "gutenberg_1342", "para_id": 611, "text": "people in the room: he was lively and unreserved, danced every dance,"}
{"doc_id": "gutenberg_1342", "para_id": 612, "text": "was angry that the ball closed so early, and talked of giving one"}
{"doc_id": "gutenberg_1342", "para_id": 613, "text": "himself at Netherfield. Such amiable qualities must speak for"}
{"doc_id": "gutenberg_1342", "para_id": 614, "text": "themselves. What a contrast between him and his friend! Mr. Darcy danced"}
{"doc_id": "gutenberg_1342", "para_id": 615, "text": "only once with Mrs. Hurst and once with Miss Bingley, declined being"}
{"doc_id": "gutenberg_1342", "para_id": 616, "text": "introduced to any other lady, and spent the rest of the evening in"}
{"doc_id": "gutenberg_1342", "para_id": 617, "text": "walking about the room, speaking occasionally to one of his own party."}
{"doc_id": "gutenberg_1342", "para_id": 618, "text": "His character was decided. He was the proudest, most disagreeable man in"}
{"doc_id": "gutenberg_1342", "para_id": 619, "text": "the world, and everybody hoped that he would never come there again."}
{"doc_id": "gutenberg_1342", "para_id": 620, "text": "Amongst the most violent against him was Mrs. Bennet, whose dislike of"}
{"doc_id": "gutenberg_1342", "para_id": 621, "text": "his general behaviour was sharpened into particular resentment by his"}
{"doc_id": "gutenberg_1342", "para_id": 622, "text": "Elizabeth Bennet had been obliged, by the scarcity of gentlemen, to sit"}
{"doc_id": "gutenberg_1342", "para_id": 623, "text": "down for two dances; and during part of that time, Mr. Darcy had been"}
{"doc_id": "gutenberg_1342", "para_id": 624, "text": "standing near enough for her to overhear a conversation between him and"}
{"doc_id": "gutenberg_1342", "para_id": 625, "text": "Mr. Bingley, who came from the dance for a few minutes to press his"}
{"doc_id": "gutenberg_1342", "para_id": 626, "text": "“Come, Darcy,” said he, “I must have you dance. I hate to see you"}
{"doc_id": "gutenberg_1342", "para_id": 627, "text": "standing about by yourself in this stupid manner. You had much better"}
{"doc_id": "gutenberg_1342", "para_id": 628, "text": "“I certainly shall not. You know how I detest it, unless I am"}
{"doc_id": "gutenberg_1342", "para_id": 629, "text": "particularly acquainted with my partner. At such an assembly as this, it"}
{"doc_id": "gutenberg_1342", "para_id": 630, "text": "would be insupportable. Your sisters are engaged, and there is not"}
{"doc_id": "gutenberg_1342", "para_id": 631, "text": "another woman in the room whom it would not be a punishment to me to"}
{"doc_id": "gutenberg_1342", "para_id": 632, "text": "“I would not be so fastidious as you are,” cried Bingley, “for a"}
{"doc_id": "gutenberg_1342", "para_id": 633, "text": "kingdom! Upon my honour, I never met with so many pleasant girls in my"}
{"doc_id": "gutenberg_1342", "para_id": 634, "text": "life as I have this evening; and there are several of them, you see,"}
{"doc_id": "gutenberg_1342", "para_id": 635, "text": "“_You_ are dancing with the only handsome girl in the room,” said Mr."}
{"doc_id": "gutenberg_1342", "para_id": 636, "text": "“Oh, she is the most beautiful creature I ever beheld! But there is one"}
{"doc_id": "gutenberg_1342", "para_id": 637, "text": "of her sisters sitting down just behind you, who is very pretty, and I"}
{"doc_id": "gutenberg_1342", "para_id": 638, "text": "dare say very agreeable. Do let me ask my partner to introduce you.”"}
{"doc_id": "gutenberg_1342", "para_id": 639, "text": "“Which do you mean?” and turning round, he looked for a moment at"}
{"doc_id": "gutenberg_1342", "para_id": 640, "text": "Elizabeth, till, catching her eye, he withdrew his own, and coldly said,"}
{"doc_id": "gutenberg_1342", "para_id": 641, "text": "“She is tolerable: but not handsome enough to tempt _me_; and I am in no"}
{"doc_id": "gutenberg_1342", "para_id": 642, "text": "humour at present to give consequence to young ladies who are slighted"}
{"doc_id": "gutenberg_1342", "para_id": 643, "text": "by other men. You had better return to your partner and enjoy her"}
{"doc_id": "gutenberg_1342", "para_id": 644, "text": "Mr. Bingley followed his advice. Mr. Darcy walked off; and Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 645, "text": "remained with no very cordial feelings towards him. She told the story,"}
{"doc_id": "gutenberg_1342", "para_id": 646, "text": "however, with great spirit among her friends; for she had a lively,"}
{"doc_id": "gutenberg_1342", "para_id": 647, "text": "playful disposition, which delighted in anything ridiculous."}
{"doc_id": "gutenberg_1342", "para_id": 648, "text": "The evening altogether passed off pleasantly to the whole family. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 649, "text": "Bennet had seen her eldest daughter much admired by the Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 650, "text": "party. Mr. Bingley had danced with her twice, and she had been"}
{"doc_id": "gutenberg_1342", "para_id": 651, "text": "distinguished by his sisters. Jane was as much gratified by this as her"}
{"doc_id": "gutenberg_1342", "para_id": 652, "text": "mother could be, though in a quieter way. Elizabeth felt Jane’s"}
{"doc_id": "gutenberg_1342", "para_id": 653, "text": "pleasure. Mary had heard herself mentioned to Miss Bingley as the most"}
{"doc_id": "gutenberg_1342", "para_id": 654, "text": "accomplished girl in the neighbourhood; and Catherine and Lydia had been"}
{"doc_id": "gutenberg_1342", "para_id": 655, "text": "fortunate enough to be never without partners, which was all that they"}
{"doc_id": "gutenberg_1342", "para_id": 656, "text": "had yet learnt to care for at a ball. They returned, therefore, in good"}
{"doc_id": "gutenberg_1342", "para_id": 657, "text": "spirits to Longbourn, the village where they lived, and of which they"}
{"doc_id": "gutenberg_1342", "para_id": 658, "text": "were the principal inhabitants. They found Mr. Bennet still up. With a"}
{"doc_id": "gutenberg_1342", "para_id": 659, "text": "book, he was regardless of time; and on the present occasion he had a"}
{"doc_id": "gutenberg_1342", "para_id": 660, "text": "good deal of curiosity as to the event of an evening which had raised"}
{"doc_id": "gutenberg_1342", "para_id": 661, "text": "such splendid expectations. He had rather hoped that all his wife’s"}
{"doc_id": "gutenberg_1342", "para_id": 662, "text": "views on the stranger would be disappointed; but he soon found that he"}
{"doc_id": "gutenberg_1342", "para_id": 663, "text": "“Oh, my dear Mr. Bennet,” as she entered the room, “we have had a most"}
{"doc_id": "gutenberg_1342", "para_id": 664, "text": "delightful evening, a most excellent ball. I wish you had been there."}
{"doc_id": "gutenberg_1342", "para_id": 665, "text": "Jane was so admired, nothing could be like it. Everybody said how well"}
{"doc_id": "gutenberg_1342", "para_id": 666, "text": "she looked; and Mr. Bingley thought her quite beautiful, and danced with"}
{"doc_id": "gutenberg_1342", "para_id": 667, "text": "her twice. Only think of _that_, my dear: he actually danced with her"}
{"doc_id": "gutenberg_1342", "para_id": 668, "text": "twice; and she was the only creature in the room that he asked a second"}
{"doc_id": "gutenberg_1342", "para_id": 669, "text": "time. First of all, he asked Miss Lucas. I was so vexed to see him stand"}
{"doc_id": "gutenberg_1342", "para_id": 670, "text": "up with her; but, however, he did not admire her at all; indeed, nobody"}
{"doc_id": "gutenberg_1342", "para_id": 671, "text": "can, you know; and he seemed quite struck with Jane as she was going"}
{"doc_id": "gutenberg_1342", "para_id": 672, "text": "down the dance. So he inquired who she was, and got introduced, and"}
{"doc_id": "gutenberg_1342", "para_id": 673, "text": "asked her for the two next. Then, the two third he danced with Miss"}
{"doc_id": "gutenberg_1342", "para_id": 674, "text": "King, and the two fourth with Maria Lucas, and the two fifth with Jane"}
{"doc_id": "gutenberg_1342", "para_id": 675, "text": "again, and the two sixth with Lizzy, and the _Boulanger_----”"}
{"doc_id": "gutenberg_1342", "para_id": 676, "text": "“If he had had any compassion for _me_,” cried her husband impatiently,"}
{"doc_id": "gutenberg_1342", "para_id": 677, "text": "“he would not have danced half so much! For God’s sake, say no more of"}
{"doc_id": "gutenberg_1342", "para_id": 678, "text": "his partners. O that he had sprained his ancle in the first dance!”"}
{"doc_id": "gutenberg_1342", "para_id": 679, "text": "“Oh, my dear,” continued Mrs. Bennet, “I am quite delighted with him. He"}
{"doc_id": "gutenberg_1342", "para_id": 680, "text": "is so excessively handsome! and his sisters are charming women. I never"}
{"doc_id": "gutenberg_1342", "para_id": 681, "text": "in my life saw anything more elegant than their dresses. I dare say the"}
{"doc_id": "gutenberg_1342", "para_id": 682, "text": "Here she was interrupted again. Mr. Bennet protested against any"}
{"doc_id": "gutenberg_1342", "para_id": 683, "text": "description of finery. She was therefore obliged to seek another branch"}
{"doc_id": "gutenberg_1342", "para_id": 684, "text": "of the subject, and related, with much bitterness of spirit, and some"}
{"doc_id": "gutenberg_1342", "para_id": 685, "text": "“But I can assure you,” she added, “that Lizzy does not lose much by not"}
{"doc_id": "gutenberg_1342", "para_id": 686, "text": "suiting _his_ fancy; for he is a most disagreeable, horrid man, not at"}
{"doc_id": "gutenberg_1342", "para_id": 687, "text": "all worth pleasing. So high and so conceited, that there was no enduring"}
{"doc_id": "gutenberg_1342", "para_id": 688, "text": "him! He walked here, and he walked there, fancying himself so very"}
{"doc_id": "gutenberg_1342", "para_id": 689, "text": "great! Not handsome enough to dance with! I wish you had been there, my"}
{"doc_id": "gutenberg_1342", "para_id": 690, "text": "dear, to have given him one of your set-downs. I quite detest the man.”"}
{"doc_id": "gutenberg_1342", "para_id": 691, "text": "When Jane and Elizabeth were alone, the former, who had been cautious in"}
{"doc_id": "gutenberg_1342", "para_id": 692, "text": "her praise of Mr. Bingley before, expressed to her sister how very much"}
{"doc_id": "gutenberg_1342", "para_id": 693, "text": "“He is just what a young-man ought to be,” said she, “sensible,"}
{"doc_id": "gutenberg_1342", "para_id": 694, "text": "good-humoured, lively; and I never saw such happy manners! so much ease,"}
{"doc_id": "gutenberg_1342", "para_id": 695, "text": "“He is also handsome,” replied Elizabeth, “which a young man ought"}
{"doc_id": "gutenberg_1342", "para_id": 696, "text": "likewise to be if he possibly can. His character is thereby complete.”"}
{"doc_id": "gutenberg_1342", "para_id": 697, "text": "“I was very much flattered by his asking me to dance a second time. I"}
{"doc_id": "gutenberg_1342", "para_id": 698, "text": "“Did not you? _I_ did for you. But that is one great difference between"}
{"doc_id": "gutenberg_1342", "para_id": 699, "text": "us. Compliments always take _you_ by surprise, and _me_ never. What"}
{"doc_id": "gutenberg_1342", "para_id": 700, "text": "could be more natural than his asking you again? He could not help"}
{"doc_id": "gutenberg_1342", "para_id": 701, "text": "seeing that you were about five times as pretty as every other woman in"}
{"doc_id": "gutenberg_1342", "para_id": 702, "text": "the room. No thanks to his gallantry for that. Well, he certainly is"}
{"doc_id": "gutenberg_1342", "para_id": 703, "text": "very agreeable, and I give you leave to like him. You have liked many a"}
{"doc_id": "gutenberg_1342", "para_id": 704, "text": "“Oh, you are a great deal too apt, you know, to like people in general."}
{"doc_id": "gutenberg_1342", "para_id": 705, "text": "You never see a fault in anybody. All the world are good and agreeable"}
{"doc_id": "gutenberg_1342", "para_id": 706, "text": "in your eyes. I never heard you speak ill of a human being in my life.”"}
{"doc_id": "gutenberg_1342", "para_id": 707, "text": "“I would wish not to be hasty in censuring anyone; but I always speak"}
{"doc_id": "gutenberg_1342", "para_id": 708, "text": "“I know you do: and it is _that_ which makes the wonder. With _your_"}
{"doc_id": "gutenberg_1342", "para_id": 709, "text": "good sense, to be so honestly blind to the follies and nonsense of"}
{"doc_id": "gutenberg_1342", "para_id": 710, "text": "others! Affectation of candour is common enough; one meets with it"}
{"doc_id": "gutenberg_1342", "para_id": 711, "text": "everywhere. But to be candid without ostentation or design,--to take the"}
{"doc_id": "gutenberg_1342", "para_id": 712, "text": "good of everybody’s character and make it still better, and say nothing"}
{"doc_id": "gutenberg_1342", "para_id": 713, "text": "of the bad,--belongs to you alone. And so, you like this man’s sisters,"}
{"doc_id": "gutenberg_1342", "para_id": 714, "text": "“Certainly not, at first; but they are very pleasing women when you"}
{"doc_id": "gutenberg_1342", "para_id": 715, "text": "converse with them. Miss Bingley is to live with her brother, and keep"}
{"doc_id": "gutenberg_1342", "para_id": 716, "text": "his house; and I am much mistaken if we shall not find a very charming"}
{"doc_id": "gutenberg_1342", "para_id": 717, "text": "Elizabeth listened in silence, but was not convinced: their behaviour at"}
{"doc_id": "gutenberg_1342", "para_id": 718, "text": "the assembly had not been calculated to please in general; and with more"}
{"doc_id": "gutenberg_1342", "para_id": 719, "text": "quickness of observation and less pliancy of temper than her sister, and"}
{"doc_id": "gutenberg_1342", "para_id": 720, "text": "with a judgment, too, unassailed by any attention to herself, she was"}
{"doc_id": "gutenberg_1342", "para_id": 721, "text": "very little disposed to approve them. They were, in fact, very fine"}
{"doc_id": "gutenberg_1342", "para_id": 722, "text": "ladies; not deficient in good-humour when they were pleased, nor in the"}
{"doc_id": "gutenberg_1342", "para_id": 723, "text": "power of being agreeable where they chose it; but proud and conceited."}
{"doc_id": "gutenberg_1342", "para_id": 724, "text": "They were rather handsome; had been educated in one of the first private"}
{"doc_id": "gutenberg_1342", "para_id": 725, "text": "seminaries in town; had a fortune of twenty thousand pounds; were in the"}
{"doc_id": "gutenberg_1342", "para_id": 726, "text": "habit of spending more than they ought, and of associating with people"}
{"doc_id": "gutenberg_1342", "para_id": 727, "text": "of rank; and were, therefore, in every respect entitled to think well of"}
{"doc_id": "gutenberg_1342", "para_id": 728, "text": "themselves and meanly of others. They were of a respectable family in"}
{"doc_id": "gutenberg_1342", "para_id": 729, "text": "the north of England; a circumstance more deeply impressed on their"}
{"doc_id": "gutenberg_1342", "para_id": 730, "text": "memories than that their brother’s fortune and their own had been"}
{"doc_id": "gutenberg_1342", "para_id": 731, "text": "Mr. Bingley inherited property to the amount of nearly a hundred"}
{"doc_id": "gutenberg_1342", "para_id": 732, "text": "thousand pounds from his father, who had intended to purchase an estate,"}
{"doc_id": "gutenberg_1342", "para_id": 733, "text": "but did not live to do it. Mr. Bingley intended it likewise, and"}
{"doc_id": "gutenberg_1342", "para_id": 734, "text": "sometimes made choice of his county; but, as he was now provided with a"}
{"doc_id": "gutenberg_1342", "para_id": 735, "text": "good house and the liberty of a manor, it was doubtful to many of those"}
{"doc_id": "gutenberg_1342", "para_id": 736, "text": "who best knew the easiness of his temper, whether he might not spend the"}
{"doc_id": "gutenberg_1342", "para_id": 737, "text": "remainder of his days at Netherfield, and leave the next generation to"}
{"doc_id": "gutenberg_1342", "para_id": 738, "text": "His sisters were very anxious for his having an estate of his own; but"}
{"doc_id": "gutenberg_1342", "para_id": 739, "text": "though he was now established only as a tenant, Miss Bingley was by no"}
{"doc_id": "gutenberg_1342", "para_id": 740, "text": "means unwilling to preside at his table; nor was Mrs. Hurst, who had"}
{"doc_id": "gutenberg_1342", "para_id": 741, "text": "married a man of more fashion than fortune, less disposed to consider"}
{"doc_id": "gutenberg_1342", "para_id": 742, "text": "his house as her home when it suited her. Mr. Bingley had not been of"}
{"doc_id": "gutenberg_1342", "para_id": 743, "text": "age two years when he was tempted, by an accidental recommendation, to"}
{"doc_id": "gutenberg_1342", "para_id": 744, "text": "look at Netherfield House. He did look at it, and into it, for half an"}
{"doc_id": "gutenberg_1342", "para_id": 745, "text": "hour; was pleased with the situation and the principal rooms, satisfied"}
{"doc_id": "gutenberg_1342", "para_id": 746, "text": "with what the owner said in its praise, and took it immediately."}
{"doc_id": "gutenberg_1342", "para_id": 747, "text": "Between him and Darcy there was a very steady friendship, in spite of a"}
{"doc_id": "gutenberg_1342", "para_id": 748, "text": "great opposition of character. Bingley was endeared to Darcy by the"}
{"doc_id": "gutenberg_1342", "para_id": 749, "text": "easiness, openness, and ductility of his temper, though no disposition"}
{"doc_id": "gutenberg_1342", "para_id": 750, "text": "could offer a greater contrast to his own, and though with his own he"}
{"doc_id": "gutenberg_1342", "para_id": 751, "text": "never appeared dissatisfied. On the strength of Darcy’s regard, Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 752, "text": "had the firmest reliance, and of his judgment the highest opinion. In"}
{"doc_id": "gutenberg_1342", "para_id": 753, "text": "understanding, Darcy was the superior. Bingley was by no means"}
{"doc_id": "gutenberg_1342", "para_id": 754, "text": "deficient; but Darcy was clever. He was at the same time haughty,"}
{"doc_id": "gutenberg_1342", "para_id": 755, "text": "reserved, and fastidious; and his manners, though well bred, were not"}
{"doc_id": "gutenberg_1342", "para_id": 756, "text": "inviting. In that respect his friend had greatly the advantage. Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 757, "text": "was sure of being liked wherever he appeared; Darcy was continually"}
{"doc_id": "gutenberg_1342", "para_id": 758, "text": "The manner in which they spoke of the Meryton assembly was sufficiently"}
{"doc_id": "gutenberg_1342", "para_id": 759, "text": "characteristic. Bingley had never met with pleasanter people or prettier"}
{"doc_id": "gutenberg_1342", "para_id": 760, "text": "girls in his life; everybody had been most kind and attentive to him;"}
{"doc_id": "gutenberg_1342", "para_id": 761, "text": "there had been no formality, no stiffness; he had soon felt acquainted"}
{"doc_id": "gutenberg_1342", "para_id": 762, "text": "with all the room; and as to Miss Bennet, he could not conceive an angel"}
{"doc_id": "gutenberg_1342", "para_id": 763, "text": "more beautiful. Darcy, on the contrary, had seen a collection of people"}
{"doc_id": "gutenberg_1342", "para_id": 764, "text": "in whom there was little beauty and no fashion, for none of whom he had"}
{"doc_id": "gutenberg_1342", "para_id": 765, "text": "felt the smallest interest, and from none received either attention or"}
{"doc_id": "gutenberg_1342", "para_id": 766, "text": "pleasure. Miss Bennet he acknowledged to be pretty; but she smiled too"}
{"doc_id": "gutenberg_1342", "para_id": 767, "text": "Mrs. Hurst and her sister allowed it to be so; but still they admired"}
{"doc_id": "gutenberg_1342", "para_id": 768, "text": "her and liked her, and pronounced her to be a sweet girl, and one whom"}
{"doc_id": "gutenberg_1342", "para_id": 769, "text": "they should not object to know more of. Miss Bennet was therefore"}
{"doc_id": "gutenberg_1342", "para_id": 770, "text": "established as a sweet girl; and their brother felt authorized by such"}
{"doc_id": "gutenberg_1342", "para_id": 771, "text": "[Illustration: [_Copyright 1894 by George Allen._]]"}
{"doc_id": "gutenberg_1342", "para_id": 772, "text": "Within a short walk of Longbourn lived a family with whom the Bennets"}
{"doc_id": "gutenberg_1342", "para_id": 773, "text": "were particularly intimate. Sir William Lucas had been formerly in trade"}
{"doc_id": "gutenberg_1342", "para_id": 774, "text": "in Meryton, where he had made a tolerable fortune, and risen to the"}
{"doc_id": "gutenberg_1342", "para_id": 775, "text": "honour of knighthood by an address to the king during his mayoralty. The"}
{"doc_id": "gutenberg_1342", "para_id": 776, "text": "distinction had, perhaps, been felt too strongly. It had given him a"}
{"doc_id": "gutenberg_1342", "para_id": 777, "text": "disgust to his business and to his residence in a small market town;"}
{"doc_id": "gutenberg_1342", "para_id": 778, "text": "and, quitting them both, he had removed with his family to a house about"}
{"doc_id": "gutenberg_1342", "para_id": 779, "text": "a mile from Meryton, denominated from that period Lucas Lodge; where he"}
{"doc_id": "gutenberg_1342", "para_id": 780, "text": "could think with pleasure of his own importance, and, unshackled by"}
{"doc_id": "gutenberg_1342", "para_id": 781, "text": "business, occupy himself solely in being civil to all the world. For,"}
{"doc_id": "gutenberg_1342", "para_id": 782, "text": "though elated by his rank, it did not render him supercilious; on the"}
{"doc_id": "gutenberg_1342", "para_id": 783, "text": "contrary, he was all attention to everybody. By nature inoffensive,"}
{"doc_id": "gutenberg_1342", "para_id": 784, "text": "friendly, and obliging, his presentation at St. James’s had made him"}
{"doc_id": "gutenberg_1342", "para_id": 785, "text": "Lady Lucas was a very good kind of woman, not too clever to be a"}
{"doc_id": "gutenberg_1342", "para_id": 786, "text": "valuable neighbour to Mrs. Bennet. They had several children. The eldest"}
{"doc_id": "gutenberg_1342", "para_id": 787, "text": "of them, a sensible, intelligent young woman, about twenty-seven, was"}
{"doc_id": "gutenberg_1342", "para_id": 788, "text": "That the Miss Lucases and the Miss Bennets should meet to talk over a"}
{"doc_id": "gutenberg_1342", "para_id": 789, "text": "ball was absolutely necessary; and the morning after the assembly"}
{"doc_id": "gutenberg_1342", "para_id": 790, "text": "brought the former to Longbourn to hear and to communicate."}
{"doc_id": "gutenberg_1342", "para_id": 791, "text": "“_You_ began the evening well, Charlotte,” said Mrs. Bennet, with civil"}
{"doc_id": "gutenberg_1342", "para_id": 792, "text": "self-command, to Miss Lucas. “_You_ were Mr. Bingley’s first choice.”"}
{"doc_id": "gutenberg_1342", "para_id": 793, "text": "“Oh, you mean Jane, I suppose, because he danced with her twice. To be"}
{"doc_id": "gutenberg_1342", "para_id": 794, "text": "sure that _did_ seem as if he admired her--indeed, I rather believe he"}
{"doc_id": "gutenberg_1342", "para_id": 795, "text": "_did_--I heard something about it--but I hardly know what--something"}
{"doc_id": "gutenberg_1342", "para_id": 796, "text": "“Perhaps you mean what I overheard between him and Mr. Robinson: did not"}
{"doc_id": "gutenberg_1342", "para_id": 797, "text": "I mention it to you? Mr. Robinson’s asking him how he liked our Meryton"}
{"doc_id": "gutenberg_1342", "para_id": 798, "text": "assemblies, and whether he did not think there were a great many pretty"}
{"doc_id": "gutenberg_1342", "para_id": 799, "text": "women in the room, and _which_ he thought the prettiest? and his"}
{"doc_id": "gutenberg_1342", "para_id": 800, "text": "answering immediately to the last question, ‘Oh, the eldest Miss Bennet,"}
{"doc_id": "gutenberg_1342", "para_id": 801, "text": "beyond a doubt: there cannot be two opinions on that point.’”"}
{"doc_id": "gutenberg_1342", "para_id": 802, "text": "“Upon my word! Well, that was very decided, indeed--that does seem as"}
{"doc_id": "gutenberg_1342", "para_id": 803, "text": "if--but, however, it may all come to nothing, you know.”"}
{"doc_id": "gutenberg_1342", "para_id": 804, "text": "“_My_ overhearings were more to the purpose than _yours_, Eliza,” said"}
{"doc_id": "gutenberg_1342", "para_id": 805, "text": "Charlotte. “Mr. Darcy is not so well worth listening to as his friend,"}
{"doc_id": "gutenberg_1342", "para_id": 806, "text": "“I beg you will not put it into Lizzy’s head to be vexed by his"}
{"doc_id": "gutenberg_1342", "para_id": 807, "text": "ill-treatment, for he is such a disagreeable man that it would be quite"}
{"doc_id": "gutenberg_1342", "para_id": 808, "text": "a misfortune to be liked by him. Mrs. Long told me last night that he"}
{"doc_id": "gutenberg_1342", "para_id": 809, "text": "sat close to her for half an hour without once opening his lips.”"}
{"doc_id": "gutenberg_1342", "para_id": 810, "text": "“Are you quite sure, ma’am? Is not there a little mistake?” said Jane."}
{"doc_id": "gutenberg_1342", "para_id": 811, "text": "“Ay, because she asked him at last how he liked Netherfield, and he"}
{"doc_id": "gutenberg_1342", "para_id": 812, "text": "could not help answering her; but she said he seemed very angry at being"}
{"doc_id": "gutenberg_1342", "para_id": 813, "text": "“Miss Bingley told me,” said Jane, “that he never speaks much unless"}
{"doc_id": "gutenberg_1342", "para_id": 814, "text": "among his intimate acquaintance. With _them_ he is remarkably"}
{"doc_id": "gutenberg_1342", "para_id": 815, "text": "“I do not believe a word of it, my dear. If he had been so very"}
{"doc_id": "gutenberg_1342", "para_id": 816, "text": "agreeable, he would have talked to Mrs. Long. But I can guess how it"}
{"doc_id": "gutenberg_1342", "para_id": 817, "text": "was; everybody says that he is eat up with pride, and I dare say he had"}
{"doc_id": "gutenberg_1342", "para_id": 818, "text": "heard somehow that Mrs. Long does not keep a carriage, and had to come"}
{"doc_id": "gutenberg_1342", "para_id": 819, "text": "“I do not mind his not talking to Mrs. Long,” said Miss Lucas, “but I"}
{"doc_id": "gutenberg_1342", "para_id": 820, "text": "“Another time, Lizzy,” said her mother, “I would not dance with _him_,"}
{"doc_id": "gutenberg_1342", "para_id": 821, "text": "“I believe, ma’am, I may safely promise you _never_ to dance with him.”"}
{"doc_id": "gutenberg_1342", "para_id": 822, "text": "“His pride,” said Miss Lucas, “does not offend _me_ so much as pride"}
{"doc_id": "gutenberg_1342", "para_id": 823, "text": "often does, because there is an excuse for it. One cannot wonder that so"}
{"doc_id": "gutenberg_1342", "para_id": 824, "text": "very fine a young man, with family, fortune, everything in his favour,"}
{"doc_id": "gutenberg_1342", "para_id": 825, "text": "should think highly of himself. If I may so express it, he has a _right_"}
{"doc_id": "gutenberg_1342", "para_id": 826, "text": "“That is very true,” replied Elizabeth, “and I could easily forgive"}
{"doc_id": "gutenberg_1342", "para_id": 827, "text": "“Pride,” observed Mary, who piqued herself upon the solidity of her"}
{"doc_id": "gutenberg_1342", "para_id": 828, "text": "reflections, “is a very common failing, I believe. By all that I have"}
{"doc_id": "gutenberg_1342", "para_id": 829, "text": "ever read, I am convinced that it is very common indeed; that human"}
{"doc_id": "gutenberg_1342", "para_id": 830, "text": "nature is particularly prone to it, and that there are very few of us"}
{"doc_id": "gutenberg_1342", "para_id": 831, "text": "who do not cherish a feeling of self-complacency on the score of some"}
{"doc_id": "gutenberg_1342", "para_id": 832, "text": "quality or other, real or imaginary. Vanity and pride are different"}
{"doc_id": "gutenberg_1342", "para_id": 833, "text": "things, though the words are often used synonymously. A person may be"}
{"doc_id": "gutenberg_1342", "para_id": 834, "text": "proud without being vain. Pride relates more to our opinion of"}
{"doc_id": "gutenberg_1342", "para_id": 835, "text": "ourselves; vanity to what we would have others think of us.”"}
{"doc_id": "gutenberg_1342", "para_id": 836, "text": "“If I were as rich as Mr. Darcy,” cried a young Lucas, who came with his"}
{"doc_id": "gutenberg_1342", "para_id": 837, "text": "sisters, “I should not care how proud I was. I would keep a pack of"}
{"doc_id": "gutenberg_1342", "para_id": 838, "text": "“Then you would drink a great deal more than you ought,” said Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 839, "text": "Bennet; “and if I were to see you at it, I should take away your bottle"}
{"doc_id": "gutenberg_1342", "para_id": 840, "text": "The boy protested that she should not; she continued to declare that she"}
{"doc_id": "gutenberg_1342", "para_id": 841, "text": "The ladies of Longbourn soon waited on those of Netherfield. The visit"}
{"doc_id": "gutenberg_1342", "para_id": 842, "text": "was returned in due form. Miss Bennet’s pleasing manners grew on the"}
{"doc_id": "gutenberg_1342", "para_id": 843, "text": "good-will of Mrs. Hurst and Miss Bingley; and though the mother was"}
{"doc_id": "gutenberg_1342", "para_id": 844, "text": "found to be intolerable, and the younger sisters not worth speaking to,"}
{"doc_id": "gutenberg_1342", "para_id": 845, "text": "a wish of being better acquainted with _them_ was expressed towards the"}
{"doc_id": "gutenberg_1342", "para_id": 846, "text": "two eldest. By Jane this attention was received with the greatest"}
{"doc_id": "gutenberg_1342", "para_id": 847, "text": "pleasure; but Elizabeth still saw superciliousness in their treatment of"}
{"doc_id": "gutenberg_1342", "para_id": 848, "text": "everybody, hardly excepting even her sister, and could not like them;"}
{"doc_id": "gutenberg_1342", "para_id": 849, "text": "though their kindness to Jane, such as it was, had a value, as arising,"}
{"doc_id": "gutenberg_1342", "para_id": 850, "text": "in all probability, from the influence of their brother’s admiration. It"}
{"doc_id": "gutenberg_1342", "para_id": 851, "text": "was generally evident, whenever they met, that he _did_ admire her; and"}
{"doc_id": "gutenberg_1342", "para_id": 852, "text": "to _her_ it was equally evident that Jane was yielding to the preference"}
{"doc_id": "gutenberg_1342", "para_id": 853, "text": "which she had begun to entertain for him from the first, and was in a"}
{"doc_id": "gutenberg_1342", "para_id": 854, "text": "way to be very much in love; but she considered with pleasure that it"}
{"doc_id": "gutenberg_1342", "para_id": 855, "text": "was not likely to be discovered by the world in general, since Jane"}
{"doc_id": "gutenberg_1342", "para_id": 856, "text": "united with great strength of feeling, a composure of temper and an"}
{"doc_id": "gutenberg_1342", "para_id": 857, "text": "uniform cheerfulness of manner, which would guard her from the"}
{"doc_id": "gutenberg_1342", "para_id": 858, "text": "suspicions of the impertinent. She mentioned this to her friend, Miss"}
{"doc_id": "gutenberg_1342", "para_id": 859, "text": "“It may, perhaps, be pleasant,” replied Charlotte, “to be able to impose"}
{"doc_id": "gutenberg_1342", "para_id": 860, "text": "on the public in such a case; but it is sometimes a disadvantage to be"}
{"doc_id": "gutenberg_1342", "para_id": 861, "text": "so very guarded. If a woman conceals her affection with the same skill"}
{"doc_id": "gutenberg_1342", "para_id": 862, "text": "from the object of it, she may lose the opportunity of fixing him; and"}
{"doc_id": "gutenberg_1342", "para_id": 863, "text": "it will then be but poor consolation to believe the world equally in the"}
{"doc_id": "gutenberg_1342", "para_id": 864, "text": "dark. There is so much of gratitude or vanity in almost every"}
{"doc_id": "gutenberg_1342", "para_id": 865, "text": "attachment, that it is not safe to leave any to itself. We can all"}
{"doc_id": "gutenberg_1342", "para_id": 866, "text": "_begin_ freely--a slight preference is natural enough; but there are"}
{"doc_id": "gutenberg_1342", "para_id": 867, "text": "very few of us who have heart enough to be really in love without"}
{"doc_id": "gutenberg_1342", "para_id": 868, "text": "encouragement. In nine cases out of ten, a woman had better show _more_"}
{"doc_id": "gutenberg_1342", "para_id": 869, "text": "affection than she feels. Bingley likes your sister undoubtedly; but he"}
{"doc_id": "gutenberg_1342", "para_id": 870, "text": "may never do more than like her, if she does not help him on.”"}
{"doc_id": "gutenberg_1342", "para_id": 871, "text": "“But she does help him on, as much as her nature will allow. If _I_ can"}
{"doc_id": "gutenberg_1342", "para_id": 872, "text": "perceive her regard for him, he must be a simpleton indeed not to"}
{"doc_id": "gutenberg_1342", "para_id": 873, "text": "“Remember, Eliza, that he does not know Jane’s disposition as you do.”"}
{"doc_id": "gutenberg_1342", "para_id": 874, "text": "“But if a woman is partial to a man, and does not endeavor to conceal"}
{"doc_id": "gutenberg_1342", "para_id": 875, "text": "“Perhaps he must, if he sees enough of her. But though Bingley and Jane"}
{"doc_id": "gutenberg_1342", "para_id": 876, "text": "meet tolerably often, it is never for many hours together; and as they"}
{"doc_id": "gutenberg_1342", "para_id": 877, "text": "always see each other in large mixed parties, it is impossible that"}
{"doc_id": "gutenberg_1342", "para_id": 878, "text": "every moment should be employed in conversing together. Jane should"}
{"doc_id": "gutenberg_1342", "para_id": 879, "text": "therefore make the most of every half hour in which she can command his"}
{"doc_id": "gutenberg_1342", "para_id": 880, "text": "attention. When she is secure of him, there will be leisure for falling"}
{"doc_id": "gutenberg_1342", "para_id": 881, "text": "“Your plan is a good one,” replied Elizabeth, “where nothing is in"}
{"doc_id": "gutenberg_1342", "para_id": 882, "text": "question but the desire of being well married; and if I were determined"}
{"doc_id": "gutenberg_1342", "para_id": 883, "text": "to get a rich husband, or any husband, I dare say I should adopt it. But"}
{"doc_id": "gutenberg_1342", "para_id": 884, "text": "these are not Jane’s feelings; she is not acting by design. As yet she"}
{"doc_id": "gutenberg_1342", "para_id": 885, "text": "cannot even be certain of the degree of her own regard, nor of its"}
{"doc_id": "gutenberg_1342", "para_id": 886, "text": "reasonableness. She has known him only a fortnight. She danced four"}
{"doc_id": "gutenberg_1342", "para_id": 887, "text": "dances with him at Meryton; she saw him one morning at his own house,"}
{"doc_id": "gutenberg_1342", "para_id": 888, "text": "and has since dined in company with him four times. This is not quite"}
{"doc_id": "gutenberg_1342", "para_id": 889, "text": "“Not as you represent it. Had she merely _dined_ with him, she might"}
{"doc_id": "gutenberg_1342", "para_id": 890, "text": "only have discovered whether he had a good appetite; but you must"}
{"doc_id": "gutenberg_1342", "para_id": 891, "text": "remember that four evenings have been also spent together--and four"}
{"doc_id": "gutenberg_1342", "para_id": 892, "text": "“Yes: these four evenings have enabled them to ascertain that they both"}
{"doc_id": "gutenberg_1342", "para_id": 893, "text": "like Vingt-un better than Commerce, but with respect to any other"}
{"doc_id": "gutenberg_1342", "para_id": 894, "text": "leading characteristic, I do not imagine that much has been unfolded.”"}
{"doc_id": "gutenberg_1342", "para_id": 895, "text": "“Well,” said Charlotte, “I wish Jane success with all my heart; and if"}
{"doc_id": "gutenberg_1342", "para_id": 896, "text": "she were married to him to-morrow, I should think she had as good a"}
{"doc_id": "gutenberg_1342", "para_id": 897, "text": "chance of happiness as if she were to be studying his character for a"}
{"doc_id": "gutenberg_1342", "para_id": 898, "text": "twelvemonth. Happiness in marriage is entirely a matter of chance. If"}
{"doc_id": "gutenberg_1342", "para_id": 899, "text": "the dispositions of the parties are ever so well known to each other, or"}
{"doc_id": "gutenberg_1342", "para_id": 900, "text": "ever so similar beforehand, it does not advance their felicity in the"}
{"doc_id": "gutenberg_1342", "para_id": 901, "text": "least. They always continue to grow sufficiently unlike afterwards to"}
{"doc_id": "gutenberg_1342", "para_id": 902, "text": "have their share of vexation; and it is better to know as little as"}
{"doc_id": "gutenberg_1342", "para_id": 903, "text": "possible of the defects of the person with whom you are to pass your"}
{"doc_id": "gutenberg_1342", "para_id": 904, "text": "“You make me laugh, Charlotte; but it is not sound. You know it is not"}
{"doc_id": "gutenberg_1342", "para_id": 905, "text": "sound, and that you would never act in this way yourself.”"}
{"doc_id": "gutenberg_1342", "para_id": 906, "text": "Occupied in observing Mr. Bingley’s attention to her sister, Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 907, "text": "was far from suspecting that she was herself becoming an object of some"}
{"doc_id": "gutenberg_1342", "para_id": 908, "text": "interest in the eyes of his friend. Mr. Darcy had at first scarcely"}
{"doc_id": "gutenberg_1342", "para_id": 909, "text": "allowed her to be pretty: he had looked at her without admiration at the"}
{"doc_id": "gutenberg_1342", "para_id": 910, "text": "ball; and when they next met, he looked at her only to criticise. But no"}
{"doc_id": "gutenberg_1342", "para_id": 911, "text": "sooner had he made it clear to himself and his friends that she had"}
{"doc_id": "gutenberg_1342", "para_id": 912, "text": "hardly a good feature in her face, than he began to find it was rendered"}
{"doc_id": "gutenberg_1342", "para_id": 913, "text": "uncommonly intelligent by the beautiful expression of her dark eyes. To"}
{"doc_id": "gutenberg_1342", "para_id": 914, "text": "this discovery succeeded some others equally mortifying. Though he had"}
{"doc_id": "gutenberg_1342", "para_id": 915, "text": "detected with a critical eye more than one failure of perfect symmetry"}
{"doc_id": "gutenberg_1342", "para_id": 916, "text": "in her form, he was forced to acknowledge her figure to be light and"}
{"doc_id": "gutenberg_1342", "para_id": 917, "text": "pleasing; and in spite of his asserting that her manners were not those"}
{"doc_id": "gutenberg_1342", "para_id": 918, "text": "of the fashionable world, he was caught by their easy playfulness. Of"}
{"doc_id": "gutenberg_1342", "para_id": 919, "text": "this she was perfectly unaware: to her he was only the man who made"}
{"doc_id": "gutenberg_1342", "para_id": 920, "text": "himself agreeable nowhere, and who had not thought her handsome enough"}
{"doc_id": "gutenberg_1342", "para_id": 921, "text": "He began to wish to know more of her; and, as a step towards conversing"}
{"doc_id": "gutenberg_1342", "para_id": 922, "text": "with her himself, attended to her conversation with others. His doing so"}
{"doc_id": "gutenberg_1342", "para_id": 923, "text": "drew her notice. It was at Sir William Lucas’s, where a large party were"}
{"doc_id": "gutenberg_1342", "para_id": 924, "text": "“What does Mr. Darcy mean,” said she to Charlotte, “by listening to my"}
{"doc_id": "gutenberg_1342", "para_id": 925, "text": "“That is a question which Mr. Darcy only can answer.”"}
{"doc_id": "gutenberg_1342", "para_id": 926, "text": "“But if he does it any more, I shall certainly let him know that I see"}
{"doc_id": "gutenberg_1342", "para_id": 927, "text": "what he is about. He has a very satirical eye, and if I do not begin by"}
{"doc_id": "gutenberg_1342", "para_id": 928, "text": "being impertinent myself, I shall soon grow afraid of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 929, "text": "[Illustration: “The entreaties of several” [_Copyright 1894 by George"}
{"doc_id": "gutenberg_1342", "para_id": 930, "text": "On his approaching them soon afterwards, though without seeming to have"}
{"doc_id": "gutenberg_1342", "para_id": 931, "text": "any intention of speaking, Miss Lucas defied her friend to mention such"}
{"doc_id": "gutenberg_1342", "para_id": 932, "text": "a subject to him, which immediately provoking Elizabeth to do it, she"}
{"doc_id": "gutenberg_1342", "para_id": 933, "text": "“Did not you think, Mr. Darcy, that I expressed myself uncommonly well"}
{"doc_id": "gutenberg_1342", "para_id": 934, "text": "just now, when I was teasing Colonel Forster to give us a ball at"}
{"doc_id": "gutenberg_1342", "para_id": 935, "text": "“With great energy; but it is a subject which always makes a lady"}
{"doc_id": "gutenberg_1342", "para_id": 936, "text": "“It will be _her_ turn soon to be teased,” said Miss Lucas. “I am going"}
{"doc_id": "gutenberg_1342", "para_id": 937, "text": "to open the instrument, Eliza, and you know what follows.”"}
{"doc_id": "gutenberg_1342", "para_id": 938, "text": "“You are a very strange creature by way of a friend!--always wanting me"}
{"doc_id": "gutenberg_1342", "para_id": 939, "text": "to play and sing before anybody and everybody! If my vanity had taken a"}
{"doc_id": "gutenberg_1342", "para_id": 940, "text": "musical turn, you would have been invaluable; but as it is, I would"}
{"doc_id": "gutenberg_1342", "para_id": 941, "text": "really rather not sit down before those who must be in the habit of"}
{"doc_id": "gutenberg_1342", "para_id": 942, "text": "hearing the very best performers.” On Miss Lucas’s persevering, however,"}
{"doc_id": "gutenberg_1342", "para_id": 943, "text": "she added, “Very well; if it must be so, it must.” And gravely glancing"}
{"doc_id": "gutenberg_1342", "para_id": 944, "text": "at Mr. Darcy, “There is a very fine old saying, which everybody here is"}
{"doc_id": "gutenberg_1342", "para_id": 945, "text": "of course familiar with--‘Keep your breath to cool your porridge,’--and"}
{"doc_id": "gutenberg_1342", "para_id": 946, "text": "Her performance was pleasing, though by no means capital. After a song"}
{"doc_id": "gutenberg_1342", "para_id": 947, "text": "or two, and before she could reply to the entreaties of several that she"}
{"doc_id": "gutenberg_1342", "para_id": 948, "text": "would sing again, she was eagerly succeeded at the instrument by her"}
{"doc_id": "gutenberg_1342", "para_id": 949, "text": "sister Mary, who having, in consequence of being the only plain one in"}
{"doc_id": "gutenberg_1342", "para_id": 950, "text": "the family, worked hard for knowledge and accomplishments, was always"}
{"doc_id": "gutenberg_1342", "para_id": 951, "text": "Mary had neither genius nor taste; and though vanity had given her"}
{"doc_id": "gutenberg_1342", "para_id": 952, "text": "application, it had given her likewise a pedantic air and conceited"}
{"doc_id": "gutenberg_1342", "para_id": 953, "text": "manner, which would have injured a higher degree of excellence than she"}
{"doc_id": "gutenberg_1342", "para_id": 954, "text": "had reached. Elizabeth, easy and unaffected, had been listened to with"}
{"doc_id": "gutenberg_1342", "para_id": 955, "text": "much more pleasure, though not playing half so well; and Mary, at the"}
{"doc_id": "gutenberg_1342", "para_id": 956, "text": "end of a long concerto, was glad to purchase praise and gratitude by"}
{"doc_id": "gutenberg_1342", "para_id": 957, "text": "Scotch and Irish airs, at the request of her younger sisters, who with"}
{"doc_id": "gutenberg_1342", "para_id": 958, "text": "some of the Lucases, and two or three officers, joined eagerly in"}
{"doc_id": "gutenberg_1342", "para_id": 959, "text": "Mr. Darcy stood near them in silent indignation at such a mode of"}
{"doc_id": "gutenberg_1342", "para_id": 960, "text": "passing the evening, to the exclusion of all conversation, and was too"}
{"doc_id": "gutenberg_1342", "para_id": 961, "text": "much engrossed by his own thoughts to perceive that Sir William Lucas"}
{"doc_id": "gutenberg_1342", "para_id": 962, "text": "“What a charming amusement for young people this is, Mr. Darcy! There is"}
{"doc_id": "gutenberg_1342", "para_id": 963, "text": "nothing like dancing, after all. I consider it as one of the first"}
{"doc_id": "gutenberg_1342", "para_id": 964, "text": "“Certainly, sir; and it has the advantage also of being in vogue amongst"}
{"doc_id": "gutenberg_1342", "para_id": 965, "text": "the less polished societies of the world: every savage can dance.”"}
{"doc_id": "gutenberg_1342", "para_id": 966, "text": "Sir William only smiled. “Your friend performs delightfully,” he"}
{"doc_id": "gutenberg_1342", "para_id": 967, "text": "continued, after a pause, on seeing Bingley join the group; “and I doubt"}
{"doc_id": "gutenberg_1342", "para_id": 968, "text": "not that you are an adept in the science yourself, Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 969, "text": "“Yes, indeed, and received no inconsiderable pleasure from the sight. Do"}
{"doc_id": "gutenberg_1342", "para_id": 970, "text": "“Do you not think it would be a proper compliment to the place?”"}
{"doc_id": "gutenberg_1342", "para_id": 971, "text": "“It is a compliment which I never pay to any place if I can avoid it.”"}
{"doc_id": "gutenberg_1342", "para_id": 972, "text": "“I had once some thoughts of fixing in town myself, for I am fond of"}
{"doc_id": "gutenberg_1342", "para_id": 973, "text": "superior society; but I did not feel quite certain that the air of"}
{"doc_id": "gutenberg_1342", "para_id": 974, "text": "He paused in hopes of an answer: but his companion was not disposed to"}
{"doc_id": "gutenberg_1342", "para_id": 975, "text": "make any; and Elizabeth at that instant moving towards them, he was"}
{"doc_id": "gutenberg_1342", "para_id": 976, "text": "struck with the notion of doing a very gallant thing, and called out to"}
{"doc_id": "gutenberg_1342", "para_id": 977, "text": "“My dear Miss Eliza, why are not you dancing? Mr. Darcy, you must allow"}
{"doc_id": "gutenberg_1342", "para_id": 978, "text": "me to present this young lady to you as a very desirable partner. You"}
{"doc_id": "gutenberg_1342", "para_id": 979, "text": "cannot refuse to dance, I am sure, when so much beauty is before you.”"}
{"doc_id": "gutenberg_1342", "para_id": 980, "text": "And, taking her hand, he would have given it to Mr. Darcy, who, though"}
{"doc_id": "gutenberg_1342", "para_id": 981, "text": "extremely surprised, was not unwilling to receive it, when she instantly"}
{"doc_id": "gutenberg_1342", "para_id": 982, "text": "drew back, and said with some discomposure to Sir William,--"}
{"doc_id": "gutenberg_1342", "para_id": 983, "text": "“Indeed, sir, I have not the least intention of dancing. I entreat you"}
{"doc_id": "gutenberg_1342", "para_id": 984, "text": "not to suppose that I moved this way in order to beg for a partner.”"}
{"doc_id": "gutenberg_1342", "para_id": 985, "text": "Mr. Darcy, with grave propriety, requested to be allowed the honour of"}
{"doc_id": "gutenberg_1342", "para_id": 986, "text": "her hand, but in vain. Elizabeth was determined; nor did Sir William at"}
{"doc_id": "gutenberg_1342", "para_id": 987, "text": "all shake her purpose by his attempt at persuasion."}
{"doc_id": "gutenberg_1342", "para_id": 988, "text": "“You excel so much in the dance, Miss Eliza, that it is cruel to deny me"}
{"doc_id": "gutenberg_1342", "para_id": 989, "text": "the happiness of seeing you; and though this gentleman dislikes the"}
{"doc_id": "gutenberg_1342", "para_id": 990, "text": "amusement in general, he can have no objection, I am sure, to oblige us"}
{"doc_id": "gutenberg_1342", "para_id": 991, "text": "“Mr. Darcy is all politeness,” said Elizabeth, smiling."}
{"doc_id": "gutenberg_1342", "para_id": 992, "text": "“He is, indeed: but considering the inducement, my dear Miss Eliza, we"}
{"doc_id": "gutenberg_1342", "para_id": 993, "text": "cannot wonder at his complaisance; for who would object to such a"}
{"doc_id": "gutenberg_1342", "para_id": 994, "text": "Elizabeth looked archly, and turned away. Her resistance had not injured"}
{"doc_id": "gutenberg_1342", "para_id": 995, "text": "her with the gentleman, and he was thinking of her with some"}
{"doc_id": "gutenberg_1342", "para_id": 996, "text": "“You are considering how insupportable it would be to pass many"}
{"doc_id": "gutenberg_1342", "para_id": 997, "text": "evenings in this manner,--in such society; and, indeed, I am quite of"}
{"doc_id": "gutenberg_1342", "para_id": 998, "text": "your opinion. I was never more annoyed! The insipidity, and yet the"}
{"doc_id": "gutenberg_1342", "para_id": 999, "text": "noise--the nothingness, and yet the self-importance, of all these"}
{"doc_id": "gutenberg_1342", "para_id": 1000, "text": "people! What would I give to hear your strictures on them!”"}
{"doc_id": "gutenberg_1342", "para_id": 1001, "text": "“Your conjecture is totally wrong, I assure you. My mind was more"}
{"doc_id": "gutenberg_1342", "para_id": 1002, "text": "agreeably engaged. I have been meditating on the very great pleasure"}
{"doc_id": "gutenberg_1342", "para_id": 1003, "text": "which a pair of fine eyes in the face of a pretty woman can bestow.”"}
{"doc_id": "gutenberg_1342", "para_id": 1004, "text": "Miss Bingley immediately fixed her eyes on his face, and desired he"}
{"doc_id": "gutenberg_1342", "para_id": 1005, "text": "would tell her what lady had the credit of inspiring such reflections."}
{"doc_id": "gutenberg_1342", "para_id": 1006, "text": "“Miss Elizabeth Bennet!” repeated Miss Bingley. “I am all astonishment."}
{"doc_id": "gutenberg_1342", "para_id": 1007, "text": "How long has she been such a favourite? and pray when am I to wish you"}
{"doc_id": "gutenberg_1342", "para_id": 1008, "text": "“That is exactly the question which I expected you to ask. A lady’s"}
{"doc_id": "gutenberg_1342", "para_id": 1009, "text": "imagination is very rapid; it jumps from admiration to love, from love"}
{"doc_id": "gutenberg_1342", "para_id": 1010, "text": "to matrimony, in a moment. I knew you would be wishing me joy.”"}
{"doc_id": "gutenberg_1342", "para_id": 1011, "text": "“Nay, if you are so serious about it, I shall consider the matter as"}
{"doc_id": "gutenberg_1342", "para_id": 1012, "text": "absolutely settled. You will have a charming mother-in-law, indeed, and"}
{"doc_id": "gutenberg_1342", "para_id": 1013, "text": "of course she will be always at Pemberley with you.”"}
{"doc_id": "gutenberg_1342", "para_id": 1014, "text": "He listened to her with perfect indifference, while she chose to"}
{"doc_id": "gutenberg_1342", "para_id": 1015, "text": "entertain herself in this manner; and as his composure convinced her"}
{"doc_id": "gutenberg_1342", "para_id": 1016, "text": "Mr. Bennet’s property consisted almost entirely in an estate of two"}
{"doc_id": "gutenberg_1342", "para_id": 1017, "text": "thousand a year, which, unfortunately for his daughters, was entailed,"}
{"doc_id": "gutenberg_1342", "para_id": 1018, "text": "in default of heirs male, on a distant relation; and their mother’s"}
{"doc_id": "gutenberg_1342", "para_id": 1019, "text": "fortune, though ample for her situation in life, could but ill supply"}
{"doc_id": "gutenberg_1342", "para_id": 1020, "text": "the deficiency of his. Her father had been an attorney in Meryton, and"}
{"doc_id": "gutenberg_1342", "para_id": 1021, "text": "She had a sister married to a Mr. Philips, who had been a clerk to their"}
{"doc_id": "gutenberg_1342", "para_id": 1022, "text": "father and succeeded him in the business, and a brother settled in"}
{"doc_id": "gutenberg_1342", "para_id": 1023, "text": "The village of Longbourn was only one mile from Meryton; a most"}
{"doc_id": "gutenberg_1342", "para_id": 1024, "text": "convenient distance for the young ladies, who were usually tempted"}
{"doc_id": "gutenberg_1342", "para_id": 1025, "text": "thither three or four times a week, to pay their duty to their aunt, and"}
{"doc_id": "gutenberg_1342", "para_id": 1026, "text": "to a milliner’s shop just over the way. The two youngest of the family,"}
{"doc_id": "gutenberg_1342", "para_id": 1027, "text": "Catherine and Lydia, were particularly frequent in these attentions:"}
{"doc_id": "gutenberg_1342", "para_id": 1028, "text": "their minds were more vacant than their sisters’, and when nothing"}
{"doc_id": "gutenberg_1342", "para_id": 1029, "text": "better offered, a walk to Meryton was necessary to amuse their morning"}
{"doc_id": "gutenberg_1342", "para_id": 1030, "text": "hours and furnish conversation for the evening; and, however bare of"}
{"doc_id": "gutenberg_1342", "para_id": 1031, "text": "news the country in general might be, they always contrived to learn"}
{"doc_id": "gutenberg_1342", "para_id": 1032, "text": "some from their aunt. At present, indeed, they were well supplied both"}
{"doc_id": "gutenberg_1342", "para_id": 1033, "text": "with news and happiness by the recent arrival of a militia regiment in"}
{"doc_id": "gutenberg_1342", "para_id": 1034, "text": "the neighbourhood; it was to remain the whole winter, and Meryton was"}
{"doc_id": "gutenberg_1342", "para_id": 1035, "text": "Their visits to Mrs. Philips were now productive of the most interesting"}
{"doc_id": "gutenberg_1342", "para_id": 1036, "text": "intelligence. Every day added something to their knowledge of the"}
{"doc_id": "gutenberg_1342", "para_id": 1037, "text": "officers’ names and connections. Their lodgings were not long a secret,"}
{"doc_id": "gutenberg_1342", "para_id": 1038, "text": "and at length they began to know the officers themselves. Mr. Philips"}
{"doc_id": "gutenberg_1342", "para_id": 1039, "text": "visited them all, and this opened to his nieces a source of felicity"}
{"doc_id": "gutenberg_1342", "para_id": 1040, "text": "unknown before. They could talk of nothing but officers; and Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1041, "text": "Bingley’s large fortune, the mention of which gave animation to their"}
{"doc_id": "gutenberg_1342", "para_id": 1042, "text": "mother, was worthless in their eyes when opposed to the regimentals of"}
{"doc_id": "gutenberg_1342", "para_id": 1043, "text": "After listening one morning to their effusions on this subject, Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1044, "text": "“From all that I can collect by your manner of talking, you must be two"}
{"doc_id": "gutenberg_1342", "para_id": 1045, "text": "of the silliest girls in the country. I have suspected it some time, but"}
{"doc_id": "gutenberg_1342", "para_id": 1046, "text": "Catherine was disconcerted, and made no answer; but Lydia, with perfect"}
{"doc_id": "gutenberg_1342", "para_id": 1047, "text": "indifference, continued to express her admiration of Captain Carter, and"}
{"doc_id": "gutenberg_1342", "para_id": 1048, "text": "her hope of seeing him in the course of the day, as he was going the"}
{"doc_id": "gutenberg_1342", "para_id": 1049, "text": "“I am astonished, my dear,” said Mrs. Bennet, “that you should be so"}
{"doc_id": "gutenberg_1342", "para_id": 1050, "text": "ready to think your own children silly. If I wished to think slightingly"}
{"doc_id": "gutenberg_1342", "para_id": 1051, "text": "of anybody’s children, it should not be of my own, however.”"}
{"doc_id": "gutenberg_1342", "para_id": 1052, "text": "“If my children are silly, I must hope to be always sensible of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1053, "text": "“Yes; but as it happens, they are all of them very clever.”"}
{"doc_id": "gutenberg_1342", "para_id": 1054, "text": "“This is the only point, I flatter myself, on which we do not agree. I"}
{"doc_id": "gutenberg_1342", "para_id": 1055, "text": "had hoped that our sentiments coincided in every particular, but I must"}
{"doc_id": "gutenberg_1342", "para_id": 1056, "text": "so far differ from you as to think our two youngest daughters uncommonly"}
{"doc_id": "gutenberg_1342", "para_id": 1057, "text": "“My dear Mr. Bennet, you must not expect such girls to have the sense of"}
{"doc_id": "gutenberg_1342", "para_id": 1058, "text": "their father and mother. When they get to our age, I dare say they will"}
{"doc_id": "gutenberg_1342", "para_id": 1059, "text": "not think about officers any more than we do. I remember the time when I"}
{"doc_id": "gutenberg_1342", "para_id": 1060, "text": "liked a red coat myself very well--and, indeed, so I do still at my"}
{"doc_id": "gutenberg_1342", "para_id": 1061, "text": "heart; and if a smart young colonel, with five or six thousand a year,"}
{"doc_id": "gutenberg_1342", "para_id": 1062, "text": "should want one of my girls, I shall not say nay to him; and I thought"}
{"doc_id": "gutenberg_1342", "para_id": 1063, "text": "Colonel Forster looked very becoming the other night at Sir William’s in"}
{"doc_id": "gutenberg_1342", "para_id": 1064, "text": "“Mamma,” cried Lydia, “my aunt says that Colonel Forster and Captain"}
{"doc_id": "gutenberg_1342", "para_id": 1065, "text": "Carter do not go so often to Miss Watson’s as they did when they first"}
{"doc_id": "gutenberg_1342", "para_id": 1066, "text": "came; she sees them now very often standing in Clarke’s library.”"}
{"doc_id": "gutenberg_1342", "para_id": 1067, "text": "Mrs. Bennet was prevented replying by the entrance of the footman with a"}
{"doc_id": "gutenberg_1342", "para_id": 1068, "text": "note for Miss Bennet; it came from Netherfield, and the servant waited"}
{"doc_id": "gutenberg_1342", "para_id": 1069, "text": "for an answer. Mrs. Bennet’s eyes sparkled with pleasure, and she was"}
{"doc_id": "gutenberg_1342", "para_id": 1070, "text": "“Well, Jane, who is it from? What is it about? What does he say? Well,"}
{"doc_id": "gutenberg_1342", "para_id": 1071, "text": "Jane, make haste and tell us; make haste, my love.”"}
{"doc_id": "gutenberg_1342", "para_id": 1072, "text": "“It is from Miss Bingley,” said Jane, and then read it aloud."}
{"doc_id": "gutenberg_1342", "para_id": 1073, "text": "“If you are not so compassionate as to dine to-day with Louisa and"}
{"doc_id": "gutenberg_1342", "para_id": 1074, "text": "me, we shall be in danger of hating each other for the rest of our"}
{"doc_id": "gutenberg_1342", "para_id": 1075, "text": "lives; for a whole day’s _tête-à-tête_ between two women can never"}
{"doc_id": "gutenberg_1342", "para_id": 1076, "text": "end without a quarrel. Come as soon as you can on the receipt of"}
{"doc_id": "gutenberg_1342", "para_id": 1077, "text": "this. My brother and the gentlemen are to dine with the officers."}
{"doc_id": "gutenberg_1342", "para_id": 1078, "text": "“With the officers!” cried Lydia: “I wonder my aunt did not tell us of"}
{"doc_id": "gutenberg_1342", "para_id": 1079, "text": "“Dining out,” said Mrs. Bennet; “that is very unlucky.”"}
{"doc_id": "gutenberg_1342", "para_id": 1080, "text": "“No, my dear, you had better go on horseback, because it seems likely to"}
{"doc_id": "gutenberg_1342", "para_id": 1081, "text": "“That would be a good scheme,” said Elizabeth, “if you were sure that"}
{"doc_id": "gutenberg_1342", "para_id": 1082, "text": "“Oh, but the gentlemen will have Mr. Bingley’s chaise to go to Meryton;"}
{"doc_id": "gutenberg_1342", "para_id": 1083, "text": "“But, my dear, your father cannot spare the horses, I am sure. They are"}
{"doc_id": "gutenberg_1342", "para_id": 1084, "text": "“They are wanted in the farm much oftener than I can get them.”"}
{"doc_id": "gutenberg_1342", "para_id": 1085, "text": "“But if you have got them to-day,” said Elizabeth, “my mother’s purpose"}
{"doc_id": "gutenberg_1342", "para_id": 1086, "text": "She did at last extort from her father an acknowledgment that the horses"}
{"doc_id": "gutenberg_1342", "para_id": 1087, "text": "were engaged; Jane was therefore obliged to go on horseback, and her"}
{"doc_id": "gutenberg_1342", "para_id": 1088, "text": "mother attended her to the door with many cheerful prognostics of a bad"}
{"doc_id": "gutenberg_1342", "para_id": 1089, "text": "day. Her hopes were answered; Jane had not been gone long before it"}
{"doc_id": "gutenberg_1342", "para_id": 1090, "text": "rained hard. Her sisters were uneasy for her, but her mother was"}
{"doc_id": "gutenberg_1342", "para_id": 1091, "text": "delighted. The rain continued the whole evening without intermission;"}
{"doc_id": "gutenberg_1342", "para_id": 1092, "text": "“This was a lucky idea of mine, indeed!” said Mrs. Bennet, more than"}
{"doc_id": "gutenberg_1342", "para_id": 1093, "text": "once, as if the credit of making it rain were all her own. Till the next"}
{"doc_id": "gutenberg_1342", "para_id": 1094, "text": "morning, however, she was not aware of all the felicity of her"}
{"doc_id": "gutenberg_1342", "para_id": 1095, "text": "contrivance. Breakfast was scarcely over when a servant from Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 1096, "text": "“I find myself very unwell this morning, which, I suppose, is to be"}
{"doc_id": "gutenberg_1342", "para_id": 1097, "text": "imputed to my getting wet through yesterday. My kind friends will"}
{"doc_id": "gutenberg_1342", "para_id": 1098, "text": "not hear of my returning home till I am better. They insist also on"}
{"doc_id": "gutenberg_1342", "para_id": 1099, "text": "my seeing Mr. Jones--therefore do not be alarmed if you should hear"}
{"doc_id": "gutenberg_1342", "para_id": 1100, "text": "of his having been to me--and, excepting a sore throat and a"}
{"doc_id": "gutenberg_1342", "para_id": 1101, "text": "“Well, my dear,” said Mr. Bennet, when Elizabeth had read the note"}
{"doc_id": "gutenberg_1342", "para_id": 1102, "text": "aloud, “if your daughter should have a dangerous fit of illness--if she"}
{"doc_id": "gutenberg_1342", "para_id": 1103, "text": "should die--it would be a comfort to know that it was all in pursuit of"}
{"doc_id": "gutenberg_1342", "para_id": 1104, "text": "“Oh, I am not at all afraid of her dying. People do not die of little"}
{"doc_id": "gutenberg_1342", "para_id": 1105, "text": "trifling colds. She will be taken good care of. As long as she stays"}
{"doc_id": "gutenberg_1342", "para_id": 1106, "text": "there, it is all very well. I would go and see her if I could have the"}
{"doc_id": "gutenberg_1342", "para_id": 1107, "text": "Elizabeth, feeling really anxious, determined to go to her, though the"}
{"doc_id": "gutenberg_1342", "para_id": 1108, "text": "carriage was not to be had: and as she was no horsewoman, walking was"}
{"doc_id": "gutenberg_1342", "para_id": 1109, "text": "“How can you be so silly,” cried her mother, “as to think of such a"}
{"doc_id": "gutenberg_1342", "para_id": 1110, "text": "thing, in all this dirt! You will not be fit to be seen when you get"}
{"doc_id": "gutenberg_1342", "para_id": 1111, "text": "“I shall be very fit to see Jane--which is all I want.”"}
{"doc_id": "gutenberg_1342", "para_id": 1112, "text": "“Is this a hint to me, Lizzy,” said her father, “to send for the"}
{"doc_id": "gutenberg_1342", "para_id": 1113, "text": "“No, indeed. I do not wish to avoid the walk. The distance is nothing,"}
{"doc_id": "gutenberg_1342", "para_id": 1114, "text": "when one has a motive; only three miles. I shall be back by dinner.”"}
{"doc_id": "gutenberg_1342", "para_id": 1115, "text": "“I admire the activity of your benevolence,” observed Mary, “but every"}
{"doc_id": "gutenberg_1342", "para_id": 1116, "text": "impulse of feeling should be guided by reason; and, in my opinion,"}
{"doc_id": "gutenberg_1342", "para_id": 1117, "text": "exertion should always be in proportion to what is required.”"}
{"doc_id": "gutenberg_1342", "para_id": 1118, "text": "“We will go as far as Meryton with you,” said Catherine and Lydia."}
{"doc_id": "gutenberg_1342", "para_id": 1119, "text": "Elizabeth accepted their company, and the three young ladies set off"}
{"doc_id": "gutenberg_1342", "para_id": 1120, "text": "“If we make haste,” said Lydia, as they walked along, “perhaps we may"}
{"doc_id": "gutenberg_1342", "para_id": 1121, "text": "In Meryton they parted: the two youngest repaired to the lodgings of one"}
{"doc_id": "gutenberg_1342", "para_id": 1122, "text": "of the officers’ wives, and Elizabeth continued her walk alone, crossing"}
{"doc_id": "gutenberg_1342", "para_id": 1123, "text": "field after field at a quick pace, jumping over stiles and springing"}
{"doc_id": "gutenberg_1342", "para_id": 1124, "text": "over puddles, with impatient activity, and finding herself at last"}
{"doc_id": "gutenberg_1342", "para_id": 1125, "text": "within view of the house, with weary ancles, dirty stockings, and a face"}
{"doc_id": "gutenberg_1342", "para_id": 1126, "text": "She was shown into the breakfast parlour, where all but Jane were"}
{"doc_id": "gutenberg_1342", "para_id": 1127, "text": "assembled, and where her appearance created a great deal of surprise."}
{"doc_id": "gutenberg_1342", "para_id": 1128, "text": "That she should have walked three miles so early in the day in such"}
{"doc_id": "gutenberg_1342", "para_id": 1129, "text": "dirty weather, and by herself, was almost incredible to Mrs. Hurst and"}
{"doc_id": "gutenberg_1342", "para_id": 1130, "text": "Miss Bingley; and Elizabeth was convinced that they held her in contempt"}
{"doc_id": "gutenberg_1342", "para_id": 1131, "text": "for it. She was received, however, very politely by them; and in their"}
{"doc_id": "gutenberg_1342", "para_id": 1132, "text": "brother’s manners there was something better than politeness--there was"}
{"doc_id": "gutenberg_1342", "para_id": 1133, "text": "good-humour and kindness. Mr. Darcy said very little, and Mr. Hurst"}
{"doc_id": "gutenberg_1342", "para_id": 1134, "text": "nothing at all. The former was divided between admiration of the"}
{"doc_id": "gutenberg_1342", "para_id": 1135, "text": "brilliancy which exercise had given to her complexion and doubt as to"}
{"doc_id": "gutenberg_1342", "para_id": 1136, "text": "the occasion’s justifying her coming so far alone. The latter was"}
{"doc_id": "gutenberg_1342", "para_id": 1137, "text": "Her inquiries after her sister were not very favourably answered. Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1138, "text": "Bennet had slept ill, and though up, was very feverish, and not well"}
{"doc_id": "gutenberg_1342", "para_id": 1139, "text": "enough to leave her room. Elizabeth was glad to be taken to her"}
{"doc_id": "gutenberg_1342", "para_id": 1140, "text": "immediately; and Jane, who had only been withheld by the fear of giving"}
{"doc_id": "gutenberg_1342", "para_id": 1141, "text": "alarm or inconvenience, from expressing in her note how much she longed"}
{"doc_id": "gutenberg_1342", "para_id": 1142, "text": "for such a visit, was delighted at her entrance. She was not equal,"}
{"doc_id": "gutenberg_1342", "para_id": 1143, "text": "however, to much conversation; and when Miss Bingley left them together,"}
{"doc_id": "gutenberg_1342", "para_id": 1144, "text": "could attempt little beside expressions of gratitude for the"}
{"doc_id": "gutenberg_1342", "para_id": 1145, "text": "extraordinary kindness she was treated with. Elizabeth silently attended"}
{"doc_id": "gutenberg_1342", "para_id": 1146, "text": "When breakfast was over, they were joined by the sisters; and Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 1147, "text": "began to like them herself, when she saw how much affection and"}
{"doc_id": "gutenberg_1342", "para_id": 1148, "text": "solicitude they showed for Jane. The apothecary came; and having"}
{"doc_id": "gutenberg_1342", "para_id": 1149, "text": "examined his patient, said, as might be supposed, that she had caught a"}
{"doc_id": "gutenberg_1342", "para_id": 1150, "text": "violent cold, and that they must endeavour to get the better of it;"}
{"doc_id": "gutenberg_1342", "para_id": 1151, "text": "advised her to return to bed, and promised her some draughts. The advice"}
{"doc_id": "gutenberg_1342", "para_id": 1152, "text": "was followed readily, for the feverish symptoms increased, and her head"}
{"doc_id": "gutenberg_1342", "para_id": 1153, "text": "ached acutely. Elizabeth did not quit her room for a moment, nor were"}
{"doc_id": "gutenberg_1342", "para_id": 1154, "text": "the other ladies often absent; the gentlemen being out, they had in fact"}
{"doc_id": "gutenberg_1342", "para_id": 1155, "text": "When the clock struck three, Elizabeth felt that she must go, and very"}
{"doc_id": "gutenberg_1342", "para_id": 1156, "text": "unwillingly said so. Miss Bingley offered her the carriage, and she only"}
{"doc_id": "gutenberg_1342", "para_id": 1157, "text": "wanted a little pressing to accept it, when Jane testified such concern"}
{"doc_id": "gutenberg_1342", "para_id": 1158, "text": "at parting with her that Miss Bingley was obliged to convert the offer"}
{"doc_id": "gutenberg_1342", "para_id": 1159, "text": "of the chaise into an invitation to remain at Netherfield for the"}
{"doc_id": "gutenberg_1342", "para_id": 1160, "text": "present. Elizabeth most thankfully consented, and a servant was"}
{"doc_id": "gutenberg_1342", "para_id": 1161, "text": "despatched to Longbourn, to acquaint the family with her stay, and bring"}
{"doc_id": "gutenberg_1342", "para_id": 1162, "text": "At five o’clock the two ladies retired to dress, and at half-past six"}
{"doc_id": "gutenberg_1342", "para_id": 1163, "text": "Elizabeth was summoned to dinner. To the civil inquiries which then"}
{"doc_id": "gutenberg_1342", "para_id": 1164, "text": "poured in, and amongst which she had the pleasure of distinguishing the"}
{"doc_id": "gutenberg_1342", "para_id": 1165, "text": "much superior solicitude of Mr. Bingley, she could not make a very"}
{"doc_id": "gutenberg_1342", "para_id": 1166, "text": "favourable answer. Jane was by no means better. The sisters, on hearing"}
{"doc_id": "gutenberg_1342", "para_id": 1167, "text": "this, repeated three or four times how much they were grieved, how"}
{"doc_id": "gutenberg_1342", "para_id": 1168, "text": "shocking it was to have a bad cold, and how excessively they disliked"}
{"doc_id": "gutenberg_1342", "para_id": 1169, "text": "being ill themselves; and then thought no more of the matter: and their"}
{"doc_id": "gutenberg_1342", "para_id": 1170, "text": "indifference towards Jane, when not immediately before them, restored"}
{"doc_id": "gutenberg_1342", "para_id": 1171, "text": "Elizabeth to the enjoyment of all her original dislike."}
{"doc_id": "gutenberg_1342", "para_id": 1172, "text": "Their brother, indeed, was the only one of the party whom she could"}
{"doc_id": "gutenberg_1342", "para_id": 1173, "text": "regard with any complacency. His anxiety for Jane was evident, and his"}
{"doc_id": "gutenberg_1342", "para_id": 1174, "text": "attentions to herself most pleasing; and they prevented her feeling"}
{"doc_id": "gutenberg_1342", "para_id": 1175, "text": "herself so much an intruder as she believed she was considered by the"}
{"doc_id": "gutenberg_1342", "para_id": 1176, "text": "others. She had very little notice from any but him. Miss Bingley was"}
{"doc_id": "gutenberg_1342", "para_id": 1177, "text": "engrossed by Mr. Darcy, her sister scarcely less so; and as for Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1178, "text": "Hurst, by whom Elizabeth sat, he was an indolent man, who lived only to"}
{"doc_id": "gutenberg_1342", "para_id": 1179, "text": "eat, drink, and play at cards, who, when he found her prefer a plain"}
{"doc_id": "gutenberg_1342", "para_id": 1180, "text": "When dinner was over, she returned directly to Jane, and Miss Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 1181, "text": "began abusing her as soon as she was out of the room. Her manners were"}
{"doc_id": "gutenberg_1342", "para_id": 1182, "text": "pronounced to be very bad indeed,--a mixture of pride and impertinence:"}
{"doc_id": "gutenberg_1342", "para_id": 1183, "text": "she had no conversation, no style, no taste, no beauty. Mrs. Hurst"}
{"doc_id": "gutenberg_1342", "para_id": 1184, "text": "“She has nothing, in short, to recommend her, but being an excellent"}
{"doc_id": "gutenberg_1342", "para_id": 1185, "text": "walker. I shall never forget her appearance this morning. She really"}
{"doc_id": "gutenberg_1342", "para_id": 1186, "text": "“She did indeed, Louisa. I could hardly keep my countenance. Very"}
{"doc_id": "gutenberg_1342", "para_id": 1187, "text": "nonsensical to come at all! Why must _she_ be scampering about the"}
{"doc_id": "gutenberg_1342", "para_id": 1188, "text": "country, because her sister had a cold? Her hair so untidy, so blowzy!”"}
{"doc_id": "gutenberg_1342", "para_id": 1189, "text": "“Yes, and her petticoat; I hope you saw her petticoat, six inches deep"}
{"doc_id": "gutenberg_1342", "para_id": 1190, "text": "in mud, I am absolutely certain, and the gown which had been let down to"}
{"doc_id": "gutenberg_1342", "para_id": 1191, "text": "“Your picture may be very exact, Louisa,” said Bingley; “but this was"}
{"doc_id": "gutenberg_1342", "para_id": 1192, "text": "all lost upon me. I thought Miss Elizabeth Bennet looked remarkably well"}
{"doc_id": "gutenberg_1342", "para_id": 1193, "text": "when she came into the room this morning. Her dirty petticoat quite"}
{"doc_id": "gutenberg_1342", "para_id": 1194, "text": "“_You_ observed it, Mr. Darcy, I am sure,” said Miss Bingley; “and I am"}
{"doc_id": "gutenberg_1342", "para_id": 1195, "text": "inclined to think that you would not wish to see _your sister_ make such"}
{"doc_id": "gutenberg_1342", "para_id": 1196, "text": "“To walk three miles, or four miles, or five miles, or whatever it is,"}
{"doc_id": "gutenberg_1342", "para_id": 1197, "text": "above her ancles in dirt, and alone, quite alone! what could she mean by"}
{"doc_id": "gutenberg_1342", "para_id": 1198, "text": "it? It seems to me to show an abominable sort of conceited independence,"}
{"doc_id": "gutenberg_1342", "para_id": 1199, "text": "“It shows an affection for her sister that is very pleasing,” said"}
{"doc_id": "gutenberg_1342", "para_id": 1200, "text": "“I am afraid, Mr. Darcy,” observed Miss Bingley, in a half whisper,"}
{"doc_id": "gutenberg_1342", "para_id": 1201, "text": "“that this adventure has rather affected your admiration of her fine"}
{"doc_id": "gutenberg_1342", "para_id": 1202, "text": "“Not at all,” he replied: “they were brightened by the exercise.” A"}
{"doc_id": "gutenberg_1342", "para_id": 1203, "text": "short pause followed this speech, and Mrs. Hurst began again,--"}
{"doc_id": "gutenberg_1342", "para_id": 1204, "text": "“I have an excessive regard for Jane Bennet,--she is really a very sweet"}
{"doc_id": "gutenberg_1342", "para_id": 1205, "text": "girl,--and I wish with all my heart she were well settled. But with such"}
{"doc_id": "gutenberg_1342", "para_id": 1206, "text": "a father and mother, and such low connections, I am afraid there is no"}
{"doc_id": "gutenberg_1342", "para_id": 1207, "text": "“I think I have heard you say that their uncle is an attorney in"}
{"doc_id": "gutenberg_1342", "para_id": 1208, "text": "“Yes; and they have another, who lives somewhere near Cheapside.”"}
{"doc_id": "gutenberg_1342", "para_id": 1209, "text": "“That is capital,” added her sister; and they both laughed heartily."}
{"doc_id": "gutenberg_1342", "para_id": 1210, "text": "“If they had uncles enough to fill _all_ Cheapside,” cried Bingley, “it"}
{"doc_id": "gutenberg_1342", "para_id": 1211, "text": "“But it must very materially lessen their chance of marrying men of any"}
{"doc_id": "gutenberg_1342", "para_id": 1212, "text": "To this speech Bingley made no answer; but his sisters gave it their"}
{"doc_id": "gutenberg_1342", "para_id": 1213, "text": "hearty assent, and indulged their mirth for some time at the expense of"}
{"doc_id": "gutenberg_1342", "para_id": 1214, "text": "With a renewal of tenderness, however, they repaired to her room on"}
{"doc_id": "gutenberg_1342", "para_id": 1215, "text": "leaving the dining-parlour, and sat with her till summoned to coffee."}
{"doc_id": "gutenberg_1342", "para_id": 1216, "text": "She was still very poorly, and Elizabeth would not quit her at all, till"}
{"doc_id": "gutenberg_1342", "para_id": 1217, "text": "late in the evening, when she had the comfort of seeing her asleep, and"}
{"doc_id": "gutenberg_1342", "para_id": 1218, "text": "when it appeared to her rather right than pleasant that she should go"}
{"doc_id": "gutenberg_1342", "para_id": 1219, "text": "down stairs herself. On entering the drawing-room, she found the whole"}
{"doc_id": "gutenberg_1342", "para_id": 1220, "text": "party at loo, and was immediately invited to join them; but suspecting"}
{"doc_id": "gutenberg_1342", "para_id": 1221, "text": "them to be playing high, she declined it, and making her sister the"}
{"doc_id": "gutenberg_1342", "para_id": 1222, "text": "excuse, said she would amuse herself, for the short time she could stay"}
{"doc_id": "gutenberg_1342", "para_id": 1223, "text": "below, with a book. Mr. Hurst looked at her with astonishment."}
{"doc_id": "gutenberg_1342", "para_id": 1224, "text": "“Do you prefer reading to cards?” said he; “that is rather singular.”"}
{"doc_id": "gutenberg_1342", "para_id": 1225, "text": "“Miss Eliza Bennet,” said Miss Bingley, “despises cards. She is a great"}
{"doc_id": "gutenberg_1342", "para_id": 1226, "text": "“I deserve neither such praise nor such censure,” cried Elizabeth; “I"}
{"doc_id": "gutenberg_1342", "para_id": 1227, "text": "am _not_ a great reader, and I have pleasure in many things.”"}
{"doc_id": "gutenberg_1342", "para_id": 1228, "text": "“In nursing your sister I am sure you have pleasure,” said Bingley; “and"}
{"doc_id": "gutenberg_1342", "para_id": 1229, "text": "I hope it will soon be increased by seeing her quite well.”"}
{"doc_id": "gutenberg_1342", "para_id": 1230, "text": "Elizabeth thanked him from her heart, and then walked towards a table"}
{"doc_id": "gutenberg_1342", "para_id": 1231, "text": "where a few books were lying. He immediately offered to fetch her"}
{"doc_id": "gutenberg_1342", "para_id": 1232, "text": "“And I wish my collection were larger for your benefit and my own"}
{"doc_id": "gutenberg_1342", "para_id": 1233, "text": "credit; but I am an idle fellow; and though I have not many, I have more"}
{"doc_id": "gutenberg_1342", "para_id": 1234, "text": "Elizabeth assured him that she could suit herself perfectly with those"}
{"doc_id": "gutenberg_1342", "para_id": 1235, "text": "“I am astonished,” said Miss Bingley, “that my father should have left"}
{"doc_id": "gutenberg_1342", "para_id": 1236, "text": "so small a collection of books. What a delightful library you have at"}
{"doc_id": "gutenberg_1342", "para_id": 1237, "text": "“It ought to be good,” he replied: “it has been the work of many"}
{"doc_id": "gutenberg_1342", "para_id": 1238, "text": "“And then you have added so much to it yourself--you are always buying"}
{"doc_id": "gutenberg_1342", "para_id": 1239, "text": "“I cannot comprehend the neglect of a family library in such days as"}
{"doc_id": "gutenberg_1342", "para_id": 1240, "text": "“Neglect! I am sure you neglect nothing that can add to the beauties of"}
{"doc_id": "gutenberg_1342", "para_id": 1241, "text": "that noble place. Charles, when you build _your_ house, I wish it may be"}
{"doc_id": "gutenberg_1342", "para_id": 1242, "text": "“But I would really advise you to make your purchase in that"}
{"doc_id": "gutenberg_1342", "para_id": 1243, "text": "neighbourhood, and take Pemberley for a kind of model. There is not a"}
{"doc_id": "gutenberg_1342", "para_id": 1244, "text": "“With all my heart: I will buy Pemberley itself, if Darcy will sell it.”"}
{"doc_id": "gutenberg_1342", "para_id": 1245, "text": "“Upon my word, Caroline, I should think it more possible to get"}
{"doc_id": "gutenberg_1342", "para_id": 1246, "text": "Elizabeth was so much caught by what passed, as to leave her very little"}
{"doc_id": "gutenberg_1342", "para_id": 1247, "text": "attention for her book; and, soon laying it wholly aside, she drew near"}
{"doc_id": "gutenberg_1342", "para_id": 1248, "text": "the card-table, and stationed herself between Mr. Bingley and his eldest"}
{"doc_id": "gutenberg_1342", "para_id": 1249, "text": "“Is Miss Darcy much grown since the spring?” said Miss Bingley: “will"}
{"doc_id": "gutenberg_1342", "para_id": 1250, "text": "“I think she will. She is now about Miss Elizabeth Bennet’s height, or"}
{"doc_id": "gutenberg_1342", "para_id": 1251, "text": "“How I long to see her again! I never met with anybody who delighted me"}
{"doc_id": "gutenberg_1342", "para_id": 1252, "text": "so much. Such a countenance, such manners, and so extremely accomplished"}
{"doc_id": "gutenberg_1342", "para_id": 1253, "text": "for her age! Her performance on the pianoforte is exquisite.”"}
{"doc_id": "gutenberg_1342", "para_id": 1254, "text": "“It is amazing to me,” said Bingley, “how young ladies can have patience"}
{"doc_id": "gutenberg_1342", "para_id": 1255, "text": "“All young ladies accomplished! My dear Charles, what do you mean?”"}
{"doc_id": "gutenberg_1342", "para_id": 1256, "text": "“Yes, all of them, I think. They all paint tables, cover screens, and"}
{"doc_id": "gutenberg_1342", "para_id": 1257, "text": "net purses. I scarcely know any one who cannot do all this; and I am"}
{"doc_id": "gutenberg_1342", "para_id": 1258, "text": "sure I never heard a young lady spoken of for the first time, without"}
{"doc_id": "gutenberg_1342", "para_id": 1259, "text": "“Your list of the common extent of accomplishments,” said Darcy, “has"}
{"doc_id": "gutenberg_1342", "para_id": 1260, "text": "too much truth. The word is applied to many a woman who deserves it no"}
{"doc_id": "gutenberg_1342", "para_id": 1261, "text": "otherwise than by netting a purse or covering a screen; but I am very"}
{"doc_id": "gutenberg_1342", "para_id": 1262, "text": "far from agreeing with you in your estimation of ladies in general. I"}
{"doc_id": "gutenberg_1342", "para_id": 1263, "text": "cannot boast of knowing more than half-a-dozen in the whole range of my"}
{"doc_id": "gutenberg_1342", "para_id": 1264, "text": "“Then,” observed Elizabeth, “you must comprehend a great deal in your"}
{"doc_id": "gutenberg_1342", "para_id": 1265, "text": "“Oh, certainly,” cried his faithful assistant, “no one can be really"}
{"doc_id": "gutenberg_1342", "para_id": 1266, "text": "esteemed accomplished who does not greatly surpass what is usually met"}
{"doc_id": "gutenberg_1342", "para_id": 1267, "text": "with. A woman must have a thorough knowledge of music, singing, drawing,"}
{"doc_id": "gutenberg_1342", "para_id": 1268, "text": "dancing, and the modern languages, to deserve the word; and, besides all"}
{"doc_id": "gutenberg_1342", "para_id": 1269, "text": "this, she must possess a certain something in her air and manner of"}
{"doc_id": "gutenberg_1342", "para_id": 1270, "text": "walking, the tone of her voice, her address and expressions, or the word"}
{"doc_id": "gutenberg_1342", "para_id": 1271, "text": "“All this she must possess,” added Darcy; “and to all she must yet add"}
{"doc_id": "gutenberg_1342", "para_id": 1272, "text": "something more substantial in the improvement of her mind by extensive"}
{"doc_id": "gutenberg_1342", "para_id": 1273, "text": "“I am no longer surprised at your knowing _only_ six accomplished women."}
{"doc_id": "gutenberg_1342", "para_id": 1274, "text": "“Are you so severe upon your own sex as to doubt the possibility of all"}
{"doc_id": "gutenberg_1342", "para_id": 1275, "text": "“_I_ never saw such a woman. _I_ never saw such capacity, and taste, and"}
{"doc_id": "gutenberg_1342", "para_id": 1276, "text": "application, and elegance, as you describe, united.”"}
{"doc_id": "gutenberg_1342", "para_id": 1277, "text": "Mrs. Hurst and Miss Bingley both cried out against the injustice of her"}
{"doc_id": "gutenberg_1342", "para_id": 1278, "text": "implied doubt, and were both protesting that they knew many women who"}
{"doc_id": "gutenberg_1342", "para_id": 1279, "text": "answered this description, when Mr. Hurst called them to order, with"}
{"doc_id": "gutenberg_1342", "para_id": 1280, "text": "bitter complaints of their inattention to what was going forward. As all"}
{"doc_id": "gutenberg_1342", "para_id": 1281, "text": "conversation was thereby at an end, Elizabeth soon afterwards left the"}
{"doc_id": "gutenberg_1342", "para_id": 1282, "text": "“Eliza Bennet,” said Miss Bingley, when the door was closed on her, “is"}
{"doc_id": "gutenberg_1342", "para_id": 1283, "text": "one of those young ladies who seek to recommend themselves to the other"}
{"doc_id": "gutenberg_1342", "para_id": 1284, "text": "sex by undervaluing their own; and with many men, I daresay, it"}
{"doc_id": "gutenberg_1342", "para_id": 1285, "text": "succeeds; but, in my opinion, it is a paltry device, a very mean art.”"}
{"doc_id": "gutenberg_1342", "para_id": 1286, "text": "“Undoubtedly,” replied Darcy, to whom this remark was chiefly addressed,"}
{"doc_id": "gutenberg_1342", "para_id": 1287, "text": "“there is meanness in _all_ the arts which ladies sometimes condescend"}
{"doc_id": "gutenberg_1342", "para_id": 1288, "text": "to employ for captivation. Whatever bears affinity to cunning is"}
{"doc_id": "gutenberg_1342", "para_id": 1289, "text": "Miss Bingley was not so entirely satisfied with this reply as to"}
{"doc_id": "gutenberg_1342", "para_id": 1290, "text": "Elizabeth joined them again only to say that her sister was worse, and"}
{"doc_id": "gutenberg_1342", "para_id": 1291, "text": "that she could not leave her. Bingley urged Mr. Jones’s being sent for"}
{"doc_id": "gutenberg_1342", "para_id": 1292, "text": "immediately; while his sisters, convinced that no country advice could"}
{"doc_id": "gutenberg_1342", "para_id": 1293, "text": "be of any service, recommended an express to town for one of the most"}
{"doc_id": "gutenberg_1342", "para_id": 1294, "text": "eminent physicians. This she would not hear of; but she was not so"}
{"doc_id": "gutenberg_1342", "para_id": 1295, "text": "unwilling to comply with their brother’s proposal; and it was settled"}
{"doc_id": "gutenberg_1342", "para_id": 1296, "text": "that Mr. Jones should be sent for early in the morning, if Miss Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 1297, "text": "were not decidedly better. Bingley was quite uncomfortable; his sisters"}
{"doc_id": "gutenberg_1342", "para_id": 1298, "text": "declared that they were miserable. They solaced their wretchedness,"}
{"doc_id": "gutenberg_1342", "para_id": 1299, "text": "however, by duets after supper; while he could find no better relief to"}
{"doc_id": "gutenberg_1342", "para_id": 1300, "text": "his feelings than by giving his housekeeper directions that every"}
{"doc_id": "gutenberg_1342", "para_id": 1301, "text": "possible attention might be paid to the sick lady and her sister."}
{"doc_id": "gutenberg_1342", "para_id": 1302, "text": "Elizabeth passed the chief of the night in her sister’s room, and in the"}
{"doc_id": "gutenberg_1342", "para_id": 1303, "text": "morning had the pleasure of being able to send a tolerable answer to the"}
{"doc_id": "gutenberg_1342", "para_id": 1304, "text": "inquiries which she very early received from Mr. Bingley by a housemaid,"}
{"doc_id": "gutenberg_1342", "para_id": 1305, "text": "and some time afterwards from the two elegant ladies who waited on his"}
{"doc_id": "gutenberg_1342", "para_id": 1306, "text": "sisters. In spite of this amendment, however, she requested to have a"}
{"doc_id": "gutenberg_1342", "para_id": 1307, "text": "note sent to Longbourn, desiring her mother to visit Jane, and form her"}
{"doc_id": "gutenberg_1342", "para_id": 1308, "text": "own judgment of her situation. The note was immediately despatched, and"}
{"doc_id": "gutenberg_1342", "para_id": 1309, "text": "its contents as quickly complied with. Mrs. Bennet, accompanied by her"}
{"doc_id": "gutenberg_1342", "para_id": 1310, "text": "two youngest girls, reached Netherfield soon after the family breakfast."}
{"doc_id": "gutenberg_1342", "para_id": 1311, "text": "Had she found Jane in any apparent danger, Mrs. Bennet would have been"}
{"doc_id": "gutenberg_1342", "para_id": 1312, "text": "very miserable; but being satisfied on seeing her that her illness was"}
{"doc_id": "gutenberg_1342", "para_id": 1313, "text": "not alarming, she had no wish of her recovering immediately, as her"}
{"doc_id": "gutenberg_1342", "para_id": 1314, "text": "restoration to health would probably remove her from Netherfield. She"}
{"doc_id": "gutenberg_1342", "para_id": 1315, "text": "would not listen, therefore, to her daughter’s proposal of being carried"}
{"doc_id": "gutenberg_1342", "para_id": 1316, "text": "home; neither did the apothecary, who arrived about the same time, think"}
{"doc_id": "gutenberg_1342", "para_id": 1317, "text": "it at all advisable. After sitting a little while with Jane, on Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1318, "text": "Bingley’s appearance and invitation, the mother and three daughters all"}
{"doc_id": "gutenberg_1342", "para_id": 1319, "text": "attended her into the breakfast parlour. Bingley met them with hopes"}
{"doc_id": "gutenberg_1342", "para_id": 1320, "text": "that Mrs. Bennet had not found Miss Bennet worse than she expected."}
{"doc_id": "gutenberg_1342", "para_id": 1321, "text": "“Indeed I have, sir,” was her answer. “She is a great deal too ill to be"}
{"doc_id": "gutenberg_1342", "para_id": 1322, "text": "moved. Mr. Jones says we must not think of moving her. We must trespass"}
{"doc_id": "gutenberg_1342", "para_id": 1323, "text": "“Removed!” cried Bingley. “It must not be thought of. My sister, I am"}
{"doc_id": "gutenberg_1342", "para_id": 1324, "text": "“You may depend upon it, madam,” said Miss Bingley, with cold civility,"}
{"doc_id": "gutenberg_1342", "para_id": 1325, "text": "“that Miss Bennet shall receive every possible attention while she"}
{"doc_id": "gutenberg_1342", "para_id": 1326, "text": "“I am sure,” she added, “if it was not for such good friends, I do not"}
{"doc_id": "gutenberg_1342", "para_id": 1327, "text": "know what would become of her, for she is very ill indeed, and suffers a"}
{"doc_id": "gutenberg_1342", "para_id": 1328, "text": "vast deal, though with the greatest patience in the world, which is"}
{"doc_id": "gutenberg_1342", "para_id": 1329, "text": "always the way with her, for she has, without exception, the sweetest"}
{"doc_id": "gutenberg_1342", "para_id": 1330, "text": "temper I ever met with. I often tell my other girls they are nothing to"}
{"doc_id": "gutenberg_1342", "para_id": 1331, "text": "_her_. You have a sweet room here, Mr. Bingley, and a charming prospect"}
{"doc_id": "gutenberg_1342", "para_id": 1332, "text": "over that gravel walk. I do not know a place in the country that is"}
{"doc_id": "gutenberg_1342", "para_id": 1333, "text": "equal to Netherfield. You will not think of quitting it in a hurry, I"}
{"doc_id": "gutenberg_1342", "para_id": 1334, "text": "“Whatever I do is done in a hurry,” replied he; “and therefore if I"}
{"doc_id": "gutenberg_1342", "para_id": 1335, "text": "should resolve to quit Netherfield, I should probably be off in five"}
{"doc_id": "gutenberg_1342", "para_id": 1336, "text": "minutes. At present, however, I consider myself as quite fixed here.”"}
{"doc_id": "gutenberg_1342", "para_id": 1337, "text": "“That is exactly what I should have supposed of you,” said Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 1338, "text": "“You begin to comprehend me, do you?” cried he, turning towards her."}
{"doc_id": "gutenberg_1342", "para_id": 1339, "text": "“I wish I might take this for a compliment; but to be so easily seen"}
{"doc_id": "gutenberg_1342", "para_id": 1340, "text": "“That is as it happens. It does not necessarily follow that a deep,"}
{"doc_id": "gutenberg_1342", "para_id": 1341, "text": "intricate character is more or less estimable than such a one as yours.”"}
{"doc_id": "gutenberg_1342", "para_id": 1342, "text": "“Lizzy,” cried her mother, “remember where you are, and do not run on in"}
{"doc_id": "gutenberg_1342", "para_id": 1343, "text": "the wild manner that you are suffered to do at home.”"}
{"doc_id": "gutenberg_1342", "para_id": 1344, "text": "“I did not know before,” continued Bingley, immediately, “that you were"}
{"doc_id": "gutenberg_1342", "para_id": 1345, "text": "a studier of character. It must be an amusing study.”"}
{"doc_id": "gutenberg_1342", "para_id": 1346, "text": "“Yes; but intricate characters are the _most_ amusing. They have at"}
{"doc_id": "gutenberg_1342", "para_id": 1347, "text": "“The country,” said Darcy, “can in general supply but few subjects for"}
{"doc_id": "gutenberg_1342", "para_id": 1348, "text": "such a study. In a country neighbourhood you move in a very confined and"}
{"doc_id": "gutenberg_1342", "para_id": 1349, "text": "“But people themselves alter so much, that there is something new to be"}
{"doc_id": "gutenberg_1342", "para_id": 1350, "text": "“Yes, indeed,” cried Mrs. Bennet, offended by his manner of mentioning a"}
{"doc_id": "gutenberg_1342", "para_id": 1351, "text": "country neighbourhood. “I assure you there is quite as much of _that_"}
{"doc_id": "gutenberg_1342", "para_id": 1352, "text": "Everybody was surprised; and Darcy, after looking at her for a moment,"}
{"doc_id": "gutenberg_1342", "para_id": 1353, "text": "turned silently away. Mrs. Bennet, who fancied she had gained a complete"}
{"doc_id": "gutenberg_1342", "para_id": 1354, "text": "“I cannot see that London has any great advantage over the country, for"}
{"doc_id": "gutenberg_1342", "para_id": 1355, "text": "my part, except the shops and public places. The country is a vast deal"}
{"doc_id": "gutenberg_1342", "para_id": 1356, "text": "“When I am in the country,” he replied, “I never wish to leave it; and"}
{"doc_id": "gutenberg_1342", "para_id": 1357, "text": "when I am in town, it is pretty much the same. They have each their"}
{"doc_id": "gutenberg_1342", "para_id": 1358, "text": "“Ay, that is because you have the right disposition. But that"}
{"doc_id": "gutenberg_1342", "para_id": 1359, "text": "gentleman,” looking at Darcy, “seemed to think the country was nothing"}
{"doc_id": "gutenberg_1342", "para_id": 1360, "text": "“Indeed, mamma, you are mistaken,” said Elizabeth, blushing for her"}
{"doc_id": "gutenberg_1342", "para_id": 1361, "text": "mother. “You quite mistook Mr. Darcy. He only meant that there was not"}
{"doc_id": "gutenberg_1342", "para_id": 1362, "text": "such a variety of people to be met with in the country as in town, which"}
{"doc_id": "gutenberg_1342", "para_id": 1363, "text": "“Certainly, my dear, nobody said there were; but as to not meeting with"}
{"doc_id": "gutenberg_1342", "para_id": 1364, "text": "many people in this neighbourhood, I believe there are few"}
{"doc_id": "gutenberg_1342", "para_id": 1365, "text": "neighbourhoods larger. I know we dine with four-and-twenty families.”"}
{"doc_id": "gutenberg_1342", "para_id": 1366, "text": "Nothing but concern for Elizabeth could enable Bingley to keep his"}
{"doc_id": "gutenberg_1342", "para_id": 1367, "text": "countenance. His sister was less delicate, and directed her eye towards"}
{"doc_id": "gutenberg_1342", "para_id": 1368, "text": "Mr. Darcy with a very expressive smile. Elizabeth, for the sake of"}
{"doc_id": "gutenberg_1342", "para_id": 1369, "text": "saying something that might turn her mother’s thoughts, now asked her if"}
{"doc_id": "gutenberg_1342", "para_id": 1370, "text": "Charlotte Lucas had been at Longbourn since _her_ coming away."}
{"doc_id": "gutenberg_1342", "para_id": 1371, "text": "“Yes, she called yesterday with her father. What an agreeable man Sir"}
{"doc_id": "gutenberg_1342", "para_id": 1372, "text": "William is, Mr. Bingley--is not he? so much the man of fashion! so"}
{"doc_id": "gutenberg_1342", "para_id": 1373, "text": "genteel and so easy! He has always something to say to everybody. _That_"}
{"doc_id": "gutenberg_1342", "para_id": 1374, "text": "is my idea of good breeding; and those persons who fancy themselves very"}
{"doc_id": "gutenberg_1342", "para_id": 1375, "text": "important and never open their mouths quite mistake the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 1376, "text": "“No, she would go home. I fancy she was wanted about the mince-pies. For"}
{"doc_id": "gutenberg_1342", "para_id": 1377, "text": "my part, Mr. Bingley, _I_ always keep servants that can do their own"}
{"doc_id": "gutenberg_1342", "para_id": 1378, "text": "work; _my_ daughters are brought up differently. But everybody is to"}
{"doc_id": "gutenberg_1342", "para_id": 1379, "text": "judge for themselves, and the Lucases are a very good sort of girls, I"}
{"doc_id": "gutenberg_1342", "para_id": 1380, "text": "assure you. It is a pity they are not handsome! Not that _I_ think"}
{"doc_id": "gutenberg_1342", "para_id": 1381, "text": "Charlotte so _very_ plain; but then she is our particular friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 1382, "text": "“She seems a very pleasant young woman,” said Bingley."}
{"doc_id": "gutenberg_1342", "para_id": 1383, "text": "“Oh dear, yes; but you must own she is very plain. Lady Lucas herself"}
{"doc_id": "gutenberg_1342", "para_id": 1384, "text": "has often said so, and envied me Jane’s beauty. I do not like to boast"}
{"doc_id": "gutenberg_1342", "para_id": 1385, "text": "of my own child; but to be sure, Jane--one does not often see anybody"}
{"doc_id": "gutenberg_1342", "para_id": 1386, "text": "better looking. It is what everybody says. I do not trust my own"}
{"doc_id": "gutenberg_1342", "para_id": 1387, "text": "partiality. When she was only fifteen there was a gentleman at my"}
{"doc_id": "gutenberg_1342", "para_id": 1388, "text": "brother Gardiner’s in town so much in love with her, that my"}
{"doc_id": "gutenberg_1342", "para_id": 1389, "text": "sister-in-law was sure he would make her an offer before we came away."}
{"doc_id": "gutenberg_1342", "para_id": 1390, "text": "But, however, he did not. Perhaps he thought her too young. However, he"}
{"doc_id": "gutenberg_1342", "para_id": 1391, "text": "wrote some verses on her, and very pretty they were.”"}
{"doc_id": "gutenberg_1342", "para_id": 1392, "text": "“And so ended his affection,” said Elizabeth, impatiently. “There has"}
{"doc_id": "gutenberg_1342", "para_id": 1393, "text": "been many a one, I fancy, overcome in the same way. I wonder who first"}
{"doc_id": "gutenberg_1342", "para_id": 1394, "text": "discovered the efficacy of poetry in driving away love!”"}
{"doc_id": "gutenberg_1342", "para_id": 1395, "text": "“I have been used to consider poetry as the _food_ of love,” said Darcy."}
{"doc_id": "gutenberg_1342", "para_id": 1396, "text": "“Of a fine, stout, healthy love it may. Everything nourishes what is"}
{"doc_id": "gutenberg_1342", "para_id": 1397, "text": "strong already. But if it be only a slight, thin sort of inclination, I"}
{"doc_id": "gutenberg_1342", "para_id": 1398, "text": "am convinced that one good sonnet will starve it entirely away.”"}
{"doc_id": "gutenberg_1342", "para_id": 1399, "text": "Darcy only smiled; and the general pause which ensued made Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 1400, "text": "tremble lest her mother should be exposing herself again. She longed to"}
{"doc_id": "gutenberg_1342", "para_id": 1401, "text": "speak, but could think of nothing to say; and after a short silence Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1402, "text": "Bennet began repeating her thanks to Mr. Bingley for his kindness to"}
{"doc_id": "gutenberg_1342", "para_id": 1403, "text": "Jane, with an apology for troubling him also with Lizzy. Mr. Bingley was"}
{"doc_id": "gutenberg_1342", "para_id": 1404, "text": "unaffectedly civil in his answer, and forced his younger sister to be"}
{"doc_id": "gutenberg_1342", "para_id": 1405, "text": "civil also, and say what the occasion required. She performed her part,"}
{"doc_id": "gutenberg_1342", "para_id": 1406, "text": "indeed, without much graciousness, but Mrs. Bennet was satisfied, and"}
{"doc_id": "gutenberg_1342", "para_id": 1407, "text": "soon afterwards ordered her carriage. Upon this signal, the youngest of"}
{"doc_id": "gutenberg_1342", "para_id": 1408, "text": "her daughters put herself forward. The two girls had been whispering to"}
{"doc_id": "gutenberg_1342", "para_id": 1409, "text": "each other during the whole visit; and the result of it was, that the"}
{"doc_id": "gutenberg_1342", "para_id": 1410, "text": "youngest should tax Mr. Bingley with having promised on his first coming"}
{"doc_id": "gutenberg_1342", "para_id": 1411, "text": "Lydia was a stout, well-grown girl of fifteen, with a fine complexion"}
{"doc_id": "gutenberg_1342", "para_id": 1412, "text": "and good-humoured countenance; a favourite with her mother, whose"}
{"doc_id": "gutenberg_1342", "para_id": 1413, "text": "affection had brought her into public at an early age. She had high"}
{"doc_id": "gutenberg_1342", "para_id": 1414, "text": "animal spirits, and a sort of natural self-consequence, which the"}
{"doc_id": "gutenberg_1342", "para_id": 1415, "text": "attentions of the officers, to whom her uncle’s good dinners and her"}
{"doc_id": "gutenberg_1342", "para_id": 1416, "text": "own easy manners recommended her, had increased into assurance. She was"}
{"doc_id": "gutenberg_1342", "para_id": 1417, "text": "very equal, therefore, to address Mr. Bingley on the subject of the"}
{"doc_id": "gutenberg_1342", "para_id": 1418, "text": "ball, and abruptly reminded him of his promise; adding, that it would be"}
{"doc_id": "gutenberg_1342", "para_id": 1419, "text": "the most shameful thing in the world if he did not keep it. His answer"}
{"doc_id": "gutenberg_1342", "para_id": 1420, "text": "to this sudden attack was delightful to her mother’s ear."}
{"doc_id": "gutenberg_1342", "para_id": 1421, "text": "“I am perfectly ready, I assure you, to keep my engagement; and, when"}
{"doc_id": "gutenberg_1342", "para_id": 1422, "text": "your sister is recovered, you shall, if you please, name the very day of"}
{"doc_id": "gutenberg_1342", "para_id": 1423, "text": "the ball. But you would not wish to be dancing while she is ill?”"}
{"doc_id": "gutenberg_1342", "para_id": 1424, "text": "Lydia declared herself satisfied. “Oh yes--it would be much better to"}
{"doc_id": "gutenberg_1342", "para_id": 1425, "text": "wait till Jane was well; and by that time, most likely, Captain Carter"}
{"doc_id": "gutenberg_1342", "para_id": 1426, "text": "would be at Meryton again. And when you have given _your_ ball,” she"}
{"doc_id": "gutenberg_1342", "para_id": 1427, "text": "added, “I shall insist on their giving one also. I shall tell Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 1428, "text": "Mrs. Bennet and her daughters then departed, and Elizabeth returned"}
{"doc_id": "gutenberg_1342", "para_id": 1429, "text": "instantly to Jane, leaving her own and her relations’ behaviour to the"}
{"doc_id": "gutenberg_1342", "para_id": 1430, "text": "remarks of the two ladies and Mr. Darcy; the latter of whom, however,"}
{"doc_id": "gutenberg_1342", "para_id": 1431, "text": "could not be prevailed on to join in their censure of _her_, in spite of"}
{"doc_id": "gutenberg_1342", "para_id": 1432, "text": "The day passed much as the day before had done. Mrs. Hurst and Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1433, "text": "Bingley had spent some hours of the morning with the invalid, who"}
{"doc_id": "gutenberg_1342", "para_id": 1434, "text": "continued, though slowly, to mend; and, in the evening, Elizabeth joined"}
{"doc_id": "gutenberg_1342", "para_id": 1435, "text": "their party in the drawing-room. The loo table, however, did not appear."}
{"doc_id": "gutenberg_1342", "para_id": 1436, "text": "Mr. Darcy was writing, and Miss Bingley, seated near him, was watching"}
{"doc_id": "gutenberg_1342", "para_id": 1437, "text": "the progress of his letter, and repeatedly calling off his attention by"}
{"doc_id": "gutenberg_1342", "para_id": 1438, "text": "messages to his sister. Mr. Hurst and Mr. Bingley were at piquet, and"}
{"doc_id": "gutenberg_1342", "para_id": 1439, "text": "Elizabeth took up some needlework, and was sufficiently amused in"}
{"doc_id": "gutenberg_1342", "para_id": 1440, "text": "attending to what passed between Darcy and his companion. The perpetual"}
{"doc_id": "gutenberg_1342", "para_id": 1441, "text": "commendations of the lady either on his hand-writing, or on the evenness"}
{"doc_id": "gutenberg_1342", "para_id": 1442, "text": "of his lines, or on the length of his letter, with the perfect unconcern"}
{"doc_id": "gutenberg_1342", "para_id": 1443, "text": "with which her praises were received, formed a curious dialogue, and was"}
{"doc_id": "gutenberg_1342", "para_id": 1444, "text": "“How delighted Miss Darcy will be to receive such a letter!”"}
{"doc_id": "gutenberg_1342", "para_id": 1445, "text": "“How many letters you must have occasion to write in the course of a"}
{"doc_id": "gutenberg_1342", "para_id": 1446, "text": "year! Letters of business, too! How odious I should think them!”"}
{"doc_id": "gutenberg_1342", "para_id": 1447, "text": "“It is fortunate, then, that they fall to my lot instead of to yours.”"}
{"doc_id": "gutenberg_1342", "para_id": 1448, "text": "“I am afraid you do not like your pen. Let me mend it for you. I mend"}
{"doc_id": "gutenberg_1342", "para_id": 1449, "text": "“Tell your sister I am delighted to hear of her improvement on the harp,"}
{"doc_id": "gutenberg_1342", "para_id": 1450, "text": "and pray let her know that I am quite in raptures with her beautiful"}
{"doc_id": "gutenberg_1342", "para_id": 1451, "text": "little design for a table, and I think it infinitely superior to Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1452, "text": "“Will you give me leave to defer your raptures till I write again? At"}
{"doc_id": "gutenberg_1342", "para_id": 1453, "text": "“Oh, it is of no consequence. I shall see her in January. But do you"}
{"doc_id": "gutenberg_1342", "para_id": 1454, "text": "always write such charming long letters to her, Mr. Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 1455, "text": "“They are generally long; but whether always charming, it is not for me"}
{"doc_id": "gutenberg_1342", "para_id": 1456, "text": "“It is a rule with me, that a person who can write a long letter with"}
{"doc_id": "gutenberg_1342", "para_id": 1457, "text": "“That will not do for a compliment to Darcy, Caroline,” cried her"}
{"doc_id": "gutenberg_1342", "para_id": 1458, "text": "brother, “because he does _not_ write with ease. He studies too much"}
{"doc_id": "gutenberg_1342", "para_id": 1459, "text": "“My style of writing is very different from yours.”"}
{"doc_id": "gutenberg_1342", "para_id": 1460, "text": "“Oh,” cried Miss Bingley, “Charles writes in the most careless way"}
{"doc_id": "gutenberg_1342", "para_id": 1461, "text": "imaginable. He leaves out half his words, and blots the rest.”"}
{"doc_id": "gutenberg_1342", "para_id": 1462, "text": "“My ideas flow so rapidly that I have not time to express them; by which"}
{"doc_id": "gutenberg_1342", "para_id": 1463, "text": "means my letters sometimes convey no ideas at all to my correspondents.”"}
{"doc_id": "gutenberg_1342", "para_id": 1464, "text": "“Your humility, Mr. Bingley,” said Elizabeth, “must disarm reproof.”"}
{"doc_id": "gutenberg_1342", "para_id": 1465, "text": "“Nothing is more deceitful,” said Darcy, “than the appearance of"}
{"doc_id": "gutenberg_1342", "para_id": 1466, "text": "humility. It is often only carelessness of opinion, and sometimes an"}
{"doc_id": "gutenberg_1342", "para_id": 1467, "text": "“And which of the two do you call _my_ little recent piece of modesty?”"}
{"doc_id": "gutenberg_1342", "para_id": 1468, "text": "“The indirect boast; for you are really proud of your defects in"}
{"doc_id": "gutenberg_1342", "para_id": 1469, "text": "writing, because you consider them as proceeding from a rapidity of"}
{"doc_id": "gutenberg_1342", "para_id": 1470, "text": "thought and carelessness of execution, which, if not estimable, you"}
{"doc_id": "gutenberg_1342", "para_id": 1471, "text": "think at least highly interesting. The power of doing anything with"}
{"doc_id": "gutenberg_1342", "para_id": 1472, "text": "quickness is always much prized by the possessor, and often without any"}
{"doc_id": "gutenberg_1342", "para_id": 1473, "text": "attention to the imperfection of the performance. When you told Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1474, "text": "Bennet this morning, that if you ever resolved on quitting Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 1475, "text": "you should be gone in five minutes, you meant it to be a sort of"}
{"doc_id": "gutenberg_1342", "para_id": 1476, "text": "panegyric, of compliment to yourself; and yet what is there so very"}
{"doc_id": "gutenberg_1342", "para_id": 1477, "text": "laudable in a precipitance which must leave very necessary business"}
{"doc_id": "gutenberg_1342", "para_id": 1478, "text": "undone, and can be of no real advantage to yourself or anyone else?”"}
{"doc_id": "gutenberg_1342", "para_id": 1479, "text": "“Nay,” cried Bingley, “this is too much, to remember at night all the"}
{"doc_id": "gutenberg_1342", "para_id": 1480, "text": "foolish things that were said in the morning. And yet, upon my honour, I"}
{"doc_id": "gutenberg_1342", "para_id": 1481, "text": "believed what I said of myself to be true, and I believe it at this"}
{"doc_id": "gutenberg_1342", "para_id": 1482, "text": "moment. At least, therefore, I did not assume the character of needless"}
{"doc_id": "gutenberg_1342", "para_id": 1483, "text": "precipitance merely to show off before the ladies.”"}
{"doc_id": "gutenberg_1342", "para_id": 1484, "text": "“I daresay you believed it; but I am by no means convinced that you"}
{"doc_id": "gutenberg_1342", "para_id": 1485, "text": "would be gone with such celerity. Your conduct would be quite as"}
{"doc_id": "gutenberg_1342", "para_id": 1486, "text": "dependent on chance as that of any man I know; and if, as you were"}
{"doc_id": "gutenberg_1342", "para_id": 1487, "text": "mounting your horse, a friend were to say, ‘Bingley, you had better stay"}
{"doc_id": "gutenberg_1342", "para_id": 1488, "text": "till next week,’ you would probably do it--you would probably not"}
{"doc_id": "gutenberg_1342", "para_id": 1489, "text": "“You have only proved by this,” cried Elizabeth, “that Mr. Bingley did"}
{"doc_id": "gutenberg_1342", "para_id": 1490, "text": "not do justice to his own disposition. You have shown him off now much"}
{"doc_id": "gutenberg_1342", "para_id": 1491, "text": "“I am exceedingly gratified,” said Bingley, “by your converting what my"}
{"doc_id": "gutenberg_1342", "para_id": 1492, "text": "friend says into a compliment on the sweetness of my temper. But I am"}
{"doc_id": "gutenberg_1342", "para_id": 1493, "text": "afraid you are giving it a turn which that gentleman did by no means"}
{"doc_id": "gutenberg_1342", "para_id": 1494, "text": "intend; for he would certainly think the better of me if, under such a"}
{"doc_id": "gutenberg_1342", "para_id": 1495, "text": "circumstance, I were to give a flat denial, and ride off as fast as I"}
{"doc_id": "gutenberg_1342", "para_id": 1496, "text": "“Would Mr. Darcy then consider the rashness of your original intention"}
{"doc_id": "gutenberg_1342", "para_id": 1497, "text": "as atoned for by your obstinacy in adhering to it?”"}
{"doc_id": "gutenberg_1342", "para_id": 1498, "text": "“Upon my word, I cannot exactly explain the matter--Darcy must speak for"}
{"doc_id": "gutenberg_1342", "para_id": 1499, "text": "“You expect me to account for opinions which you choose to call mine,"}
{"doc_id": "gutenberg_1342", "para_id": 1500, "text": "but which I have never acknowledged. Allowing the case, however, to"}
{"doc_id": "gutenberg_1342", "para_id": 1501, "text": "stand according to your representation, you must remember, Miss Bennet,"}
{"doc_id": "gutenberg_1342", "para_id": 1502, "text": "that the friend who is supposed to desire his return to the house, and"}
{"doc_id": "gutenberg_1342", "para_id": 1503, "text": "the delay of his plan, has merely desired it, asked it without offering"}
{"doc_id": "gutenberg_1342", "para_id": 1504, "text": "“To yield readily--easily--to the _persuasion_ of a friend is no merit"}
{"doc_id": "gutenberg_1342", "para_id": 1505, "text": "“To yield without conviction is no compliment to the understanding of"}
{"doc_id": "gutenberg_1342", "para_id": 1506, "text": "“You appear to me, Mr. Darcy, to allow nothing for the influence of"}
{"doc_id": "gutenberg_1342", "para_id": 1507, "text": "friendship and affection. A regard for the requester would often make"}
{"doc_id": "gutenberg_1342", "para_id": 1508, "text": "one readily yield to a request, without waiting for arguments to reason"}
{"doc_id": "gutenberg_1342", "para_id": 1509, "text": "one into it. I am not particularly speaking of such a case as you have"}
{"doc_id": "gutenberg_1342", "para_id": 1510, "text": "supposed about Mr. Bingley. We may as well wait, perhaps, till the"}
{"doc_id": "gutenberg_1342", "para_id": 1511, "text": "circumstance occurs, before we discuss the discretion of his behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 1512, "text": "thereupon. But in general and ordinary cases, between friend and friend,"}
{"doc_id": "gutenberg_1342", "para_id": 1513, "text": "where one of them is desired by the other to change a resolution of no"}
{"doc_id": "gutenberg_1342", "para_id": 1514, "text": "very great moment, should you think ill of that person for complying"}
{"doc_id": "gutenberg_1342", "para_id": 1515, "text": "with the desire, without waiting to be argued into it?”"}
{"doc_id": "gutenberg_1342", "para_id": 1516, "text": "“Will it not be advisable, before we proceed on this subject, to arrange"}
{"doc_id": "gutenberg_1342", "para_id": 1517, "text": "with rather more precision the degree of importance which is to"}
{"doc_id": "gutenberg_1342", "para_id": 1518, "text": "appertain to this request, as well as the degree of intimacy subsisting"}
{"doc_id": "gutenberg_1342", "para_id": 1519, "text": "“By all means,” cried Bingley; “let us hear all the particulars, not"}
{"doc_id": "gutenberg_1342", "para_id": 1520, "text": "forgetting their comparative height and size, for that will have more"}
{"doc_id": "gutenberg_1342", "para_id": 1521, "text": "weight in the argument, Miss Bennet, than you may be aware of. I assure"}
{"doc_id": "gutenberg_1342", "para_id": 1522, "text": "you that if Darcy were not such a great tall fellow, in comparison with"}
{"doc_id": "gutenberg_1342", "para_id": 1523, "text": "myself, I should not pay him half so much deference. I declare I do not"}
{"doc_id": "gutenberg_1342", "para_id": 1524, "text": "know a more awful object than Darcy on particular occasions, and in"}
{"doc_id": "gutenberg_1342", "para_id": 1525, "text": "particular places; at his own house especially, and of a Sunday evening,"}
{"doc_id": "gutenberg_1342", "para_id": 1526, "text": "Mr. Darcy smiled; but Elizabeth thought she could perceive that he was"}
{"doc_id": "gutenberg_1342", "para_id": 1527, "text": "rather offended, and therefore checked her laugh. Miss Bingley warmly"}
{"doc_id": "gutenberg_1342", "para_id": 1528, "text": "resented the indignity he had received, in an expostulation with her"}
{"doc_id": "gutenberg_1342", "para_id": 1529, "text": "“I see your design, Bingley,” said his friend. “You dislike an argument,"}
{"doc_id": "gutenberg_1342", "para_id": 1530, "text": "“Perhaps I do. Arguments are too much like disputes. If you and Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1531, "text": "Bennet will defer yours till I am out of the room, I shall be very"}
{"doc_id": "gutenberg_1342", "para_id": 1532, "text": "thankful; and then you may say whatever you like of me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1533, "text": "“What you ask,” said Elizabeth, “is no sacrifice on my side; and Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1534, "text": "Mr. Darcy took her advice, and did finish his letter."}
{"doc_id": "gutenberg_1342", "para_id": 1535, "text": "When that business was over, he applied to Miss Bingley and Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 1536, "text": "for the indulgence of some music. Miss Bingley moved with alacrity to"}
{"doc_id": "gutenberg_1342", "para_id": 1537, "text": "the pianoforte, and after a polite request that Elizabeth would lead the"}
{"doc_id": "gutenberg_1342", "para_id": 1538, "text": "way, which the other as politely and more earnestly negatived, she"}
{"doc_id": "gutenberg_1342", "para_id": 1539, "text": "Mrs. Hurst sang with her sister; and while they were thus employed,"}
{"doc_id": "gutenberg_1342", "para_id": 1540, "text": "Elizabeth could not help observing, as she turned over some music-books"}
{"doc_id": "gutenberg_1342", "para_id": 1541, "text": "that lay on the instrument, how frequently Mr. Darcy’s eyes were fixed"}
{"doc_id": "gutenberg_1342", "para_id": 1542, "text": "on her. She hardly knew how to suppose that she could be an object of"}
{"doc_id": "gutenberg_1342", "para_id": 1543, "text": "admiration to so great a man, and yet that he should look at her because"}
{"doc_id": "gutenberg_1342", "para_id": 1544, "text": "he disliked her was still more strange. She could only imagine, however,"}
{"doc_id": "gutenberg_1342", "para_id": 1545, "text": "at last, that she drew his notice because there was something about her"}
{"doc_id": "gutenberg_1342", "para_id": 1546, "text": "more wrong and reprehensible, according to his ideas of right, than in"}
{"doc_id": "gutenberg_1342", "para_id": 1547, "text": "any other person present. The supposition did not pain her. She liked"}
{"doc_id": "gutenberg_1342", "para_id": 1548, "text": "After playing some Italian songs, Miss Bingley varied the charm by a"}
{"doc_id": "gutenberg_1342", "para_id": 1549, "text": "lively Scotch air; and soon afterwards Mr. Darcy, drawing near"}
{"doc_id": "gutenberg_1342", "para_id": 1550, "text": "“Do you not feel a great inclination, Miss Bennet, to seize such an"}
{"doc_id": "gutenberg_1342", "para_id": 1551, "text": "She smiled, but made no answer. He repeated the question, with some"}
{"doc_id": "gutenberg_1342", "para_id": 1552, "text": "“Oh,” said she, “I heard you before; but I could not immediately"}
{"doc_id": "gutenberg_1342", "para_id": 1553, "text": "determine what to say in reply. You wanted me, I know, to say ‘Yes,’"}
{"doc_id": "gutenberg_1342", "para_id": 1554, "text": "that you might have the pleasure of despising my taste; but I always"}
{"doc_id": "gutenberg_1342", "para_id": 1555, "text": "delight in overthrowing those kind of schemes, and cheating a person of"}
{"doc_id": "gutenberg_1342", "para_id": 1556, "text": "their premeditated contempt. I have, therefore, made up my mind to tell"}
{"doc_id": "gutenberg_1342", "para_id": 1557, "text": "you that I do not want to dance a reel at all; and now despise me if you"}
{"doc_id": "gutenberg_1342", "para_id": 1558, "text": "Elizabeth, having rather expected to affront him, was amazed at his"}
{"doc_id": "gutenberg_1342", "para_id": 1559, "text": "gallantry; but there was a mixture of sweetness and archness in her"}
{"doc_id": "gutenberg_1342", "para_id": 1560, "text": "manner which made it difficult for her to affront anybody, and Darcy had"}
{"doc_id": "gutenberg_1342", "para_id": 1561, "text": "never been so bewitched by any woman as he was by her. He really"}
{"doc_id": "gutenberg_1342", "para_id": 1562, "text": "believed that, were it not for the inferiority of her connections, he"}
{"doc_id": "gutenberg_1342", "para_id": 1563, "text": "Miss Bingley saw, or suspected, enough to be jealous; and her great"}
{"doc_id": "gutenberg_1342", "para_id": 1564, "text": "anxiety for the recovery of her dear friend Jane received some"}
{"doc_id": "gutenberg_1342", "para_id": 1565, "text": "assistance from her desire of getting rid of Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 1566, "text": "She often tried to provoke Darcy into disliking her guest, by talking of"}
{"doc_id": "gutenberg_1342", "para_id": 1567, "text": "their supposed marriage, and planning his happiness in such an alliance."}
{"doc_id": "gutenberg_1342", "para_id": 1568, "text": "“I hope,” said she, as they were walking together in the shrubbery the"}
{"doc_id": "gutenberg_1342", "para_id": 1569, "text": "next day, “you will give your mother-in-law a few hints, when this"}
{"doc_id": "gutenberg_1342", "para_id": 1570, "text": "desirable event takes place, as to the advantage of holding her tongue;"}
{"doc_id": "gutenberg_1342", "para_id": 1571, "text": "and if you can compass it, to cure the younger girls of running after"}
{"doc_id": "gutenberg_1342", "para_id": 1572, "text": "the officers. And, if I may mention so delicate a subject, endeavour to"}
{"doc_id": "gutenberg_1342", "para_id": 1573, "text": "check that little something, bordering on conceit and impertinence,"}
{"doc_id": "gutenberg_1342", "para_id": 1574, "text": "“Have you anything else to propose for my domestic felicity?”"}
{"doc_id": "gutenberg_1342", "para_id": 1575, "text": "“Oh yes. Do let the portraits of your uncle and aunt Philips be placed"}
{"doc_id": "gutenberg_1342", "para_id": 1576, "text": "in the gallery at Pemberley. Put them next to your great-uncle the"}
{"doc_id": "gutenberg_1342", "para_id": 1577, "text": "judge. They are in the same profession, you know, only in different"}
{"doc_id": "gutenberg_1342", "para_id": 1578, "text": "lines. As for your Elizabeth’s picture, you must not attempt to have it"}
{"doc_id": "gutenberg_1342", "para_id": 1579, "text": "taken, for what painter could do justice to those beautiful eyes?”"}
{"doc_id": "gutenberg_1342", "para_id": 1580, "text": "“It would not be easy, indeed, to catch their expression; but their"}
{"doc_id": "gutenberg_1342", "para_id": 1581, "text": "colour and shape, and the eyelashes, so remarkably fine, might be"}
{"doc_id": "gutenberg_1342", "para_id": 1582, "text": "At that moment they were met from another walk by Mrs. Hurst and"}
{"doc_id": "gutenberg_1342", "para_id": 1583, "text": "“I did not know that you intended to walk,” said Miss Bingley, in some"}
{"doc_id": "gutenberg_1342", "para_id": 1584, "text": "“You used us abominably ill,” answered Mrs. Hurst, “running away without"}
{"doc_id": "gutenberg_1342", "para_id": 1585, "text": "Then taking the disengaged arm of Mr. Darcy, she left Elizabeth to walk"}
{"doc_id": "gutenberg_1342", "para_id": 1586, "text": "by herself. The path just admitted three. Mr. Darcy felt their rudeness,"}
{"doc_id": "gutenberg_1342", "para_id": 1587, "text": "“This walk is not wide enough for our party. We had better go into the"}
{"doc_id": "gutenberg_1342", "para_id": 1588, "text": "But Elizabeth, who had not the least inclination to remain with them,"}
{"doc_id": "gutenberg_1342", "para_id": 1589, "text": "“No, no; stay where you are. You are charmingly grouped, and appear to"}
{"doc_id": "gutenberg_1342", "para_id": 1590, "text": "uncommon advantage. The picturesque would be spoilt by admitting a"}
{"doc_id": "gutenberg_1342", "para_id": 1591, "text": "She then ran gaily off, rejoicing, as she rambled about, in the hope of"}
{"doc_id": "gutenberg_1342", "para_id": 1592, "text": "being at home again in a day or two. Jane was already so much recovered"}
{"doc_id": "gutenberg_1342", "para_id": 1593, "text": "as to intend leaving her room for a couple of hours that evening."}
{"doc_id": "gutenberg_1342", "para_id": 1594, "text": "When the ladies removed after dinner Elizabeth ran up to her sister, and"}
{"doc_id": "gutenberg_1342", "para_id": 1595, "text": "seeing her well guarded from cold, attended her into the drawing-room,"}
{"doc_id": "gutenberg_1342", "para_id": 1596, "text": "where she was welcomed by her two friends with many professions of"}
{"doc_id": "gutenberg_1342", "para_id": 1597, "text": "pleasure; and Elizabeth had never seen them so agreeable as they were"}
{"doc_id": "gutenberg_1342", "para_id": 1598, "text": "during the hour which passed before the gentlemen appeared. Their powers"}
{"doc_id": "gutenberg_1342", "para_id": 1599, "text": "of conversation were considerable. They could describe an entertainment"}
{"doc_id": "gutenberg_1342", "para_id": 1600, "text": "with accuracy, relate an anecdote with humour, and laugh at their"}
{"doc_id": "gutenberg_1342", "para_id": 1601, "text": "But when the gentlemen entered, Jane was no longer the first object;"}
{"doc_id": "gutenberg_1342", "para_id": 1602, "text": "Miss Bingley’s eyes were instantly turned towards Darcy, and she had"}
{"doc_id": "gutenberg_1342", "para_id": 1603, "text": "something to say to him before he had advanced many steps. He addressed"}
{"doc_id": "gutenberg_1342", "para_id": 1604, "text": "himself directly to Miss Bennet with a polite congratulation; Mr. Hurst"}
{"doc_id": "gutenberg_1342", "para_id": 1605, "text": "also made her a slight bow, and said he was “very glad;” but diffuseness"}
{"doc_id": "gutenberg_1342", "para_id": 1606, "text": "and warmth remained for Bingley’s salutation. He was full of joy and"}
{"doc_id": "gutenberg_1342", "para_id": 1607, "text": "attention. The first half hour was spent in piling up the fire, lest she"}
{"doc_id": "gutenberg_1342", "para_id": 1608, "text": "should suffer from the change of room; and she removed, at his desire,"}
{"doc_id": "gutenberg_1342", "para_id": 1609, "text": "to the other side of the fireplace, that she might be farther from the"}
{"doc_id": "gutenberg_1342", "para_id": 1610, "text": "door. He then sat down by her, and talked scarcely to anyone else."}
{"doc_id": "gutenberg_1342", "para_id": 1611, "text": "Elizabeth, at work in the opposite corner, saw it all with great"}
{"doc_id": "gutenberg_1342", "para_id": 1612, "text": "When tea was over Mr. Hurst reminded his sister-in-law of the"}
{"doc_id": "gutenberg_1342", "para_id": 1613, "text": "card-table--but in vain. She had obtained private intelligence that Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1614, "text": "Darcy did not wish for cards, and Mr. Hurst soon found even his open"}
{"doc_id": "gutenberg_1342", "para_id": 1615, "text": "petition rejected. She assured him that no one intended to play, and the"}
{"doc_id": "gutenberg_1342", "para_id": 1616, "text": "silence of the whole party on the subject seemed to justify her. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1617, "text": "Hurst had, therefore, nothing to do but to stretch himself on one of the"}
{"doc_id": "gutenberg_1342", "para_id": 1618, "text": "sofas and go to sleep. Darcy took up a book. Miss Bingley did the same;"}
{"doc_id": "gutenberg_1342", "para_id": 1619, "text": "and Mrs. Hurst, principally occupied in playing with her bracelets and"}
{"doc_id": "gutenberg_1342", "para_id": 1620, "text": "rings, joined now and then in her brother’s conversation with Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1621, "text": "Miss Bingley’s attention was quite as much engaged in watching Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1622, "text": "Darcy’s progress through _his_ book, as in reading her own; and she was"}
{"doc_id": "gutenberg_1342", "para_id": 1623, "text": "perpetually either making some inquiry, or looking at his page. She"}
{"doc_id": "gutenberg_1342", "para_id": 1624, "text": "could not win him, however, to any conversation; he merely answered her"}
{"doc_id": "gutenberg_1342", "para_id": 1625, "text": "question and read on. At length, quite exhausted by the attempt to be"}
{"doc_id": "gutenberg_1342", "para_id": 1626, "text": "amused with her own book, which she had only chosen because it was the"}
{"doc_id": "gutenberg_1342", "para_id": 1627, "text": "second volume of his, she gave a great yawn and said, “How pleasant it"}
{"doc_id": "gutenberg_1342", "para_id": 1628, "text": "is to spend an evening in this way! I declare, after all, there is no"}
{"doc_id": "gutenberg_1342", "para_id": 1629, "text": "enjoyment like reading! How much sooner one tires of anything than of a"}
{"doc_id": "gutenberg_1342", "para_id": 1630, "text": "book! When I have a house of my own, I shall be miserable if I have not"}
{"doc_id": "gutenberg_1342", "para_id": 1631, "text": "No one made any reply. She then yawned again, threw aside her book, and"}
{"doc_id": "gutenberg_1342", "para_id": 1632, "text": "cast her eyes round the room in quest of some amusement; when, hearing"}
{"doc_id": "gutenberg_1342", "para_id": 1633, "text": "her brother mentioning a ball to Miss Bennet, she turned suddenly"}
{"doc_id": "gutenberg_1342", "para_id": 1634, "text": "“By the bye Charles, are you really serious in meditating a dance at"}
{"doc_id": "gutenberg_1342", "para_id": 1635, "text": "Netherfield? I would advise you, before you determine on it, to consult"}
{"doc_id": "gutenberg_1342", "para_id": 1636, "text": "the wishes of the present party; I am much mistaken if there are not"}
{"doc_id": "gutenberg_1342", "para_id": 1637, "text": "some among us to whom a ball would be rather a punishment than a"}
{"doc_id": "gutenberg_1342", "para_id": 1638, "text": "“If you mean Darcy,” cried her brother, “he may go to bed, if he"}
{"doc_id": "gutenberg_1342", "para_id": 1639, "text": "chooses, before it begins; but as for the ball, it is quite a settled"}
{"doc_id": "gutenberg_1342", "para_id": 1640, "text": "thing, and as soon as Nicholls has made white soup enough I shall send"}
{"doc_id": "gutenberg_1342", "para_id": 1641, "text": "“I should like balls infinitely better,” she replied, “if they were"}
{"doc_id": "gutenberg_1342", "para_id": 1642, "text": "carried on in a different manner; but there is something insufferably"}
{"doc_id": "gutenberg_1342", "para_id": 1643, "text": "tedious in the usual process of such a meeting. It would surely be much"}
{"doc_id": "gutenberg_1342", "para_id": 1644, "text": "more rational if conversation instead of dancing made the order of the"}
{"doc_id": "gutenberg_1342", "para_id": 1645, "text": "“Much more rational, my dear Caroline, I dare say; but it would not be"}
{"doc_id": "gutenberg_1342", "para_id": 1646, "text": "Miss Bingley made no answer, and soon afterwards got up and walked about"}
{"doc_id": "gutenberg_1342", "para_id": 1647, "text": "the room. Her figure was elegant, and she walked well; but Darcy, at"}
{"doc_id": "gutenberg_1342", "para_id": 1648, "text": "whom it was all aimed, was still inflexibly studious. In the"}
{"doc_id": "gutenberg_1342", "para_id": 1649, "text": "desperation of her feelings, she resolved on one effort more; and,"}
{"doc_id": "gutenberg_1342", "para_id": 1650, "text": "“Miss Eliza Bennet, let me persuade you to follow my example, and take a"}
{"doc_id": "gutenberg_1342", "para_id": 1651, "text": "turn about the room. I assure you it is very refreshing after sitting so"}
{"doc_id": "gutenberg_1342", "para_id": 1652, "text": "Elizabeth was surprised, but agreed to it immediately. Miss Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 1653, "text": "succeeded no less in the real object of her civility: Mr. Darcy looked"}
{"doc_id": "gutenberg_1342", "para_id": 1654, "text": "up. He was as much awake to the novelty of attention in that quarter as"}
{"doc_id": "gutenberg_1342", "para_id": 1655, "text": "Elizabeth herself could be, and unconsciously closed his book. He was"}
{"doc_id": "gutenberg_1342", "para_id": 1656, "text": "directly invited to join their party, but he declined it, observing that"}
{"doc_id": "gutenberg_1342", "para_id": 1657, "text": "he could imagine but two motives for their choosing to walk up and down"}
{"doc_id": "gutenberg_1342", "para_id": 1658, "text": "the room together, with either of which motives his joining them would"}
{"doc_id": "gutenberg_1342", "para_id": 1659, "text": "interfere. What could he mean? She was dying to know what could be his"}
{"doc_id": "gutenberg_1342", "para_id": 1660, "text": "meaning--and asked Elizabeth whether she could at all understand him."}
{"doc_id": "gutenberg_1342", "para_id": 1661, "text": "“Not at all,” was her answer; “but, depend upon it, he means to be"}
{"doc_id": "gutenberg_1342", "para_id": 1662, "text": "severe on us, and our surest way of disappointing him will be to ask"}
{"doc_id": "gutenberg_1342", "para_id": 1663, "text": "Miss Bingley, however, was incapable of disappointing Mr. Darcy in"}
{"doc_id": "gutenberg_1342", "para_id": 1664, "text": "anything, and persevered, therefore, in requiring an explanation of his"}
{"doc_id": "gutenberg_1342", "para_id": 1665, "text": "“I have not the smallest objection to explaining them,” said he, as soon"}
{"doc_id": "gutenberg_1342", "para_id": 1666, "text": "as she allowed him to speak. “You either choose this method of passing"}
{"doc_id": "gutenberg_1342", "para_id": 1667, "text": "the evening because you are in each other’s confidence, and have secret"}
{"doc_id": "gutenberg_1342", "para_id": 1668, "text": "affairs to discuss, or because you are conscious that your figures"}
{"doc_id": "gutenberg_1342", "para_id": 1669, "text": "appear to the greatest advantage in walking: if the first, I should be"}
{"doc_id": "gutenberg_1342", "para_id": 1670, "text": "completely in your way; and if the second, I can admire you much better"}
{"doc_id": "gutenberg_1342", "para_id": 1671, "text": "“Oh, shocking!” cried Miss Bingley. “I never heard anything so"}
{"doc_id": "gutenberg_1342", "para_id": 1672, "text": "abominable. How shall we punish him for such a speech?”"}
{"doc_id": "gutenberg_1342", "para_id": 1673, "text": "“Nothing so easy, if you have but the inclination,” said Elizabeth. “We"}
{"doc_id": "gutenberg_1342", "para_id": 1674, "text": "can all plague and punish one another. Tease him--laugh at him. Intimate"}
{"doc_id": "gutenberg_1342", "para_id": 1675, "text": "“But upon my honour I do _not_. I do assure you that my intimacy has not"}
{"doc_id": "gutenberg_1342", "para_id": 1676, "text": "yet taught me _that_. Tease calmness of temper and presence of mind! No,"}
{"doc_id": "gutenberg_1342", "para_id": 1677, "text": "no; I feel he may defy us there. And as to laughter, we will not expose"}
{"doc_id": "gutenberg_1342", "para_id": 1678, "text": "ourselves, if you please, by attempting to laugh without a subject. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1679, "text": "“Mr. Darcy is not to be laughed at!” cried Elizabeth. “That is an"}
{"doc_id": "gutenberg_1342", "para_id": 1680, "text": "uncommon advantage, and uncommon I hope it will continue, for it would"}
{"doc_id": "gutenberg_1342", "para_id": 1681, "text": "be a great loss to _me_ to have many such acquaintance. I dearly love a"}
{"doc_id": "gutenberg_1342", "para_id": 1682, "text": "“Miss Bingley,” said he, “has given me credit for more than can be. The"}
{"doc_id": "gutenberg_1342", "para_id": 1683, "text": "wisest and best of men,--nay, the wisest and best of their actions,--may"}
{"doc_id": "gutenberg_1342", "para_id": 1684, "text": "be rendered ridiculous by a person whose first object in life is a"}
{"doc_id": "gutenberg_1342", "para_id": 1685, "text": "“Certainly,” replied Elizabeth, “there are such people, but I hope I am"}
{"doc_id": "gutenberg_1342", "para_id": 1686, "text": "not one of _them_. I hope I never ridicule what is wise or good. Follies"}
{"doc_id": "gutenberg_1342", "para_id": 1687, "text": "and nonsense, whims and inconsistencies, _do_ divert me, I own, and I"}
{"doc_id": "gutenberg_1342", "para_id": 1688, "text": "laugh at them whenever I can. But these, I suppose, are precisely what"}
{"doc_id": "gutenberg_1342", "para_id": 1689, "text": "“Perhaps that is not possible for anyone. But it has been the study of"}
{"doc_id": "gutenberg_1342", "para_id": 1690, "text": "my life to avoid those weaknesses which often expose a strong"}
{"doc_id": "gutenberg_1342", "para_id": 1691, "text": "“Yes, vanity is a weakness indeed. But pride--where there is a real"}
{"doc_id": "gutenberg_1342", "para_id": 1692, "text": "superiority of mind--pride will be always under good regulation.”"}
{"doc_id": "gutenberg_1342", "para_id": 1693, "text": "“Your examination of Mr. Darcy is over, I presume,” said Miss Bingley;"}
{"doc_id": "gutenberg_1342", "para_id": 1694, "text": "“I am perfectly convinced by it that Mr. Darcy has no defect. He owns it"}
{"doc_id": "gutenberg_1342", "para_id": 1695, "text": "“No,” said Darcy, “I have made no such pretension. I have faults enough,"}
{"doc_id": "gutenberg_1342", "para_id": 1696, "text": "but they are not, I hope, of understanding. My temper I dare not vouch"}
{"doc_id": "gutenberg_1342", "para_id": 1697, "text": "for. It is, I believe, too little yielding; certainly too little for the"}
{"doc_id": "gutenberg_1342", "para_id": 1698, "text": "convenience of the world. I cannot forget the follies and vices of"}
{"doc_id": "gutenberg_1342", "para_id": 1699, "text": "others so soon as I ought, nor their offences against myself. My"}
{"doc_id": "gutenberg_1342", "para_id": 1700, "text": "feelings are not puffed about with every attempt to move them. My temper"}
{"doc_id": "gutenberg_1342", "para_id": 1701, "text": "would perhaps be called resentful. My good opinion once lost is lost for"}
{"doc_id": "gutenberg_1342", "para_id": 1702, "text": "“_That_ is a failing, indeed!” cried Elizabeth. “Implacable resentment"}
{"doc_id": "gutenberg_1342", "para_id": 1703, "text": "_is_ a shade in a character. But you have chosen your fault well. I"}
{"doc_id": "gutenberg_1342", "para_id": 1704, "text": "really cannot _laugh_ at it. You are safe from me.”"}
{"doc_id": "gutenberg_1342", "para_id": 1705, "text": "“There is, I believe, in every disposition a tendency to some particular"}
{"doc_id": "gutenberg_1342", "para_id": 1706, "text": "evil, a natural defect, which not even the best education can overcome.”"}
{"doc_id": "gutenberg_1342", "para_id": 1707, "text": "“And _your_ defect is a propensity to hate everybody.”"}
{"doc_id": "gutenberg_1342", "para_id": 1708, "text": "“And yours,” he replied, with a smile, “is wilfully to misunderstand"}
{"doc_id": "gutenberg_1342", "para_id": 1709, "text": "“Do let us have a little music,” cried Miss Bingley, tired of a"}
{"doc_id": "gutenberg_1342", "para_id": 1710, "text": "conversation in which she had no share. “Louisa, you will not mind my"}
{"doc_id": "gutenberg_1342", "para_id": 1711, "text": "Her sister made not the smallest objection, and the pianoforte was"}
{"doc_id": "gutenberg_1342", "para_id": 1712, "text": "opened; and Darcy, after a few moments’ recollection, was not sorry for"}
{"doc_id": "gutenberg_1342", "para_id": 1713, "text": "it. He began to feel the danger of paying Elizabeth too much attention."}
{"doc_id": "gutenberg_1342", "para_id": 1714, "text": "In consequence of an agreement between the sisters, Elizabeth wrote the"}
{"doc_id": "gutenberg_1342", "para_id": 1715, "text": "next morning to her mother, to beg that the carriage might be sent for"}
{"doc_id": "gutenberg_1342", "para_id": 1716, "text": "them in the course of the day. But Mrs. Bennet, who had calculated on"}
{"doc_id": "gutenberg_1342", "para_id": 1717, "text": "her daughters remaining at Netherfield till the following Tuesday, which"}
{"doc_id": "gutenberg_1342", "para_id": 1718, "text": "would exactly finish Jane’s week, could not bring herself to receive"}
{"doc_id": "gutenberg_1342", "para_id": 1719, "text": "them with pleasure before. Her answer, therefore, was not propitious, at"}
{"doc_id": "gutenberg_1342", "para_id": 1720, "text": "least not to Elizabeth’s wishes, for she was impatient to get home. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1721, "text": "Bennet sent them word that they could not possibly have the carriage"}
{"doc_id": "gutenberg_1342", "para_id": 1722, "text": "before Tuesday; and in her postscript it was added, that if Mr. Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 1723, "text": "and his sister pressed them to stay longer, she could spare them very"}
{"doc_id": "gutenberg_1342", "para_id": 1724, "text": "well. Against staying longer, however, Elizabeth was positively"}
{"doc_id": "gutenberg_1342", "para_id": 1725, "text": "resolved--nor did she much expect it would be asked; and fearful, on the"}
{"doc_id": "gutenberg_1342", "para_id": 1726, "text": "contrary, of being considered as intruding themselves needlessly long,"}
{"doc_id": "gutenberg_1342", "para_id": 1727, "text": "she urged Jane to borrow Mr. Bingley’s carriage immediately, and at"}
{"doc_id": "gutenberg_1342", "para_id": 1728, "text": "length it was settled that their original design of leaving Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 1729, "text": "that morning should be mentioned, and the request made."}
{"doc_id": "gutenberg_1342", "para_id": 1730, "text": "The communication excited many professions of concern; and enough was"}
{"doc_id": "gutenberg_1342", "para_id": 1731, "text": "said of wishing them to stay at least till the following day to work on"}
{"doc_id": "gutenberg_1342", "para_id": 1732, "text": "Jane; and till the morrow their going was deferred. Miss Bingley was"}
{"doc_id": "gutenberg_1342", "para_id": 1733, "text": "then sorry that she had proposed the delay; for her jealousy and dislike"}
{"doc_id": "gutenberg_1342", "para_id": 1734, "text": "of one sister much exceeded her affection for the other."}
{"doc_id": "gutenberg_1342", "para_id": 1735, "text": "The master of the house heard with real sorrow that they were to go so"}
{"doc_id": "gutenberg_1342", "para_id": 1736, "text": "soon, and repeatedly tried to persuade Miss Bennet that it would not be"}
{"doc_id": "gutenberg_1342", "para_id": 1737, "text": "safe for her--that she was not enough recovered; but Jane was firm where"}
{"doc_id": "gutenberg_1342", "para_id": 1738, "text": "To Mr. Darcy it was welcome intelligence: Elizabeth had been at"}
{"doc_id": "gutenberg_1342", "para_id": 1739, "text": "Netherfield long enough. She attracted him more than he liked; and Miss"}
{"doc_id": "gutenberg_1342", "para_id": 1740, "text": "Bingley was uncivil to _her_ and more teasing than usual to himself. He"}
{"doc_id": "gutenberg_1342", "para_id": 1741, "text": "wisely resolved to be particularly careful that no sign of admiration"}
{"doc_id": "gutenberg_1342", "para_id": 1742, "text": "should _now_ escape him--nothing that could elevate her with the hope of"}
{"doc_id": "gutenberg_1342", "para_id": 1743, "text": "influencing his felicity; sensible that, if such an idea had been"}
{"doc_id": "gutenberg_1342", "para_id": 1744, "text": "suggested, his behaviour during the last day must have material weight"}
{"doc_id": "gutenberg_1342", "para_id": 1745, "text": "in confirming or crushing it. Steady to his purpose, he scarcely spoke"}
{"doc_id": "gutenberg_1342", "para_id": 1746, "text": "ten words to her through the whole of Saturday: and though they were at"}
{"doc_id": "gutenberg_1342", "para_id": 1747, "text": "one time left by themselves for half an hour, he adhered most"}
{"doc_id": "gutenberg_1342", "para_id": 1748, "text": "conscientiously to his book, and would not even look at her."}
{"doc_id": "gutenberg_1342", "para_id": 1749, "text": "On Sunday, after morning service, the separation, so agreeable to almost"}
{"doc_id": "gutenberg_1342", "para_id": 1750, "text": "all, took place. Miss Bingley’s civility to Elizabeth increased at last"}
{"doc_id": "gutenberg_1342", "para_id": 1751, "text": "very rapidly, as well as her affection for Jane; and when they parted,"}
{"doc_id": "gutenberg_1342", "para_id": 1752, "text": "after assuring the latter of the pleasure it would always give her to"}
{"doc_id": "gutenberg_1342", "para_id": 1753, "text": "see her either at Longbourn or Netherfield, and embracing her most"}
{"doc_id": "gutenberg_1342", "para_id": 1754, "text": "tenderly, she even shook hands with the former. Elizabeth took leave of"}
{"doc_id": "gutenberg_1342", "para_id": 1755, "text": "They were not welcomed home very cordially by their mother. Mrs. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 1756, "text": "wondered at their coming, and thought them very wrong to give so much"}
{"doc_id": "gutenberg_1342", "para_id": 1757, "text": "trouble, and was sure Jane would have caught cold again. But their"}
{"doc_id": "gutenberg_1342", "para_id": 1758, "text": "father, though very laconic in his expressions of pleasure, was really"}
{"doc_id": "gutenberg_1342", "para_id": 1759, "text": "glad to see them; he had felt their importance in the family circle. The"}
{"doc_id": "gutenberg_1342", "para_id": 1760, "text": "evening conversation, when they were all assembled, had lost much of its"}
{"doc_id": "gutenberg_1342", "para_id": 1761, "text": "animation, and almost all its sense, by the absence of Jane and"}
{"doc_id": "gutenberg_1342", "para_id": 1762, "text": "They found Mary, as usual, deep in the study of thorough bass and human"}
{"doc_id": "gutenberg_1342", "para_id": 1763, "text": "nature; and had some new extracts to admire and some new observations of"}
{"doc_id": "gutenberg_1342", "para_id": 1764, "text": "threadbare morality to listen to. Catherine and Lydia had information"}
{"doc_id": "gutenberg_1342", "para_id": 1765, "text": "for them of a different sort. Much had been done, and much had been said"}
{"doc_id": "gutenberg_1342", "para_id": 1766, "text": "in the regiment since the preceding Wednesday; several of the officers"}
{"doc_id": "gutenberg_1342", "para_id": 1767, "text": "had dined lately with their uncle; a private had been flogged; and it"}
{"doc_id": "gutenberg_1342", "para_id": 1768, "text": "had actually been hinted that Colonel Forster was going to be married."}
{"doc_id": "gutenberg_1342", "para_id": 1769, "text": "“I hope, my dear,” said Mr. Bennet to his wife, as they were at"}
{"doc_id": "gutenberg_1342", "para_id": 1770, "text": "breakfast the next morning, “that you have ordered a good dinner to-day,"}
{"doc_id": "gutenberg_1342", "para_id": 1771, "text": "because I have reason to expect an addition to our family party.”"}
{"doc_id": "gutenberg_1342", "para_id": 1772, "text": "“Who do you mean, my dear? I know of nobody that is coming, I am sure,"}
{"doc_id": "gutenberg_1342", "para_id": 1773, "text": "unless Charlotte Lucas should happen to call in; and I hope _my_ dinners"}
{"doc_id": "gutenberg_1342", "para_id": 1774, "text": "are good enough for her. I do not believe she often sees such at home.”"}
{"doc_id": "gutenberg_1342", "para_id": 1775, "text": "“The person of whom I speak is a gentleman and a stranger.”"}
{"doc_id": "gutenberg_1342", "para_id": 1776, "text": "Mrs. Bennet’s eyes sparkled. “A gentleman and a stranger! It is Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1777, "text": "Bingley, I am sure. Why, Jane--you never dropped a word of this--you sly"}
{"doc_id": "gutenberg_1342", "para_id": 1778, "text": "thing! Well, I am sure I shall be extremely glad to see Mr. Bingley."}
{"doc_id": "gutenberg_1342", "para_id": 1779, "text": "But--good Lord! how unlucky! there is not a bit of fish to be got"}
{"doc_id": "gutenberg_1342", "para_id": 1780, "text": "to-day. Lydia, my love, ring the bell. I must speak to Hill this"}
{"doc_id": "gutenberg_1342", "para_id": 1781, "text": "“It is _not_ Mr. Bingley,” said her husband; “it is a person whom I"}
{"doc_id": "gutenberg_1342", "para_id": 1782, "text": "This roused a general astonishment; and he had the pleasure of being"}
{"doc_id": "gutenberg_1342", "para_id": 1783, "text": "eagerly questioned by his wife and five daughters at once."}
{"doc_id": "gutenberg_1342", "para_id": 1784, "text": "After amusing himself some time with their curiosity, he thus"}
{"doc_id": "gutenberg_1342", "para_id": 1785, "text": "explained:--“About a month ago I received this letter, and about a"}
{"doc_id": "gutenberg_1342", "para_id": 1786, "text": "fortnight ago I answered it; for I thought it a case of some delicacy,"}
{"doc_id": "gutenberg_1342", "para_id": 1787, "text": "and requiring early attention. It is from my cousin, Mr. Collins, who,"}
{"doc_id": "gutenberg_1342", "para_id": 1788, "text": "when I am dead, may turn you all out of this house as soon as he"}
{"doc_id": "gutenberg_1342", "para_id": 1789, "text": "“Oh, my dear,” cried his wife, “I cannot bear to hear that mentioned."}
{"doc_id": "gutenberg_1342", "para_id": 1790, "text": "Pray do not talk of that odious man. I do think it is the hardest thing"}
{"doc_id": "gutenberg_1342", "para_id": 1791, "text": "in the world, that your estate should be entailed away from your own"}
{"doc_id": "gutenberg_1342", "para_id": 1792, "text": "children; and I am sure, if I had been you, I should have tried long ago"}
{"doc_id": "gutenberg_1342", "para_id": 1793, "text": "Jane and Elizabeth attempted to explain to her the nature of an entail."}
{"doc_id": "gutenberg_1342", "para_id": 1794, "text": "They had often attempted it before: but it was a subject on which Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1795, "text": "Bennet was beyond the reach of reason; and she continued to rail"}
{"doc_id": "gutenberg_1342", "para_id": 1796, "text": "bitterly against the cruelty of settling an estate away from a family of"}
{"doc_id": "gutenberg_1342", "para_id": 1797, "text": "five daughters, in favour of a man whom nobody cared anything about."}
{"doc_id": "gutenberg_1342", "para_id": 1798, "text": "“It certainly is a most iniquitous affair,” said Mr. Bennet; “and"}
{"doc_id": "gutenberg_1342", "para_id": 1799, "text": "nothing can clear Mr. Collins from the guilt of inheriting Longbourn."}
{"doc_id": "gutenberg_1342", "para_id": 1800, "text": "But if you will listen to his letter, you may, perhaps, be a little"}
{"doc_id": "gutenberg_1342", "para_id": 1801, "text": "“No, that I am sure I shall not: and I think it was very impertinent of"}
{"doc_id": "gutenberg_1342", "para_id": 1802, "text": "him to write to you at all, and very hypocritical. I hate such false"}
{"doc_id": "gutenberg_1342", "para_id": 1803, "text": "friends. Why could not he keep on quarrelling with you, as his father"}
{"doc_id": "gutenberg_1342", "para_id": 1804, "text": "“Why, indeed, he does seem to have had some filial scruples on that"}
{"doc_id": "gutenberg_1342", "para_id": 1805, "text": "/* RIGHT “Hunsford, near Westerham, Kent, _15th October_. */"}
{"doc_id": "gutenberg_1342", "para_id": 1806, "text": "“The disagreement subsisting between yourself and my late honoured"}
{"doc_id": "gutenberg_1342", "para_id": 1807, "text": "father always gave me much uneasiness; and, since I have had the"}
{"doc_id": "gutenberg_1342", "para_id": 1808, "text": "misfortune to lose him, I have frequently wished to heal the"}
{"doc_id": "gutenberg_1342", "para_id": 1809, "text": "breach: but, for some time, I was kept back by my own doubts,"}
{"doc_id": "gutenberg_1342", "para_id": 1810, "text": "fearing lest it might seem disrespectful to his memory for me to be"}
{"doc_id": "gutenberg_1342", "para_id": 1811, "text": "on good terms with anyone with whom it had always pleased him to be"}
{"doc_id": "gutenberg_1342", "para_id": 1812, "text": "at variance.”--‘There, Mrs. Bennet.’--“My mind, however, is now"}
{"doc_id": "gutenberg_1342", "para_id": 1813, "text": "made up on the subject; for, having received ordination at Easter,"}
{"doc_id": "gutenberg_1342", "para_id": 1814, "text": "I have been so fortunate as to be distinguished by the patronage of"}
{"doc_id": "gutenberg_1342", "para_id": 1815, "text": "the Right Honourable Lady Catherine de Bourgh, widow of Sir Lewis"}
{"doc_id": "gutenberg_1342", "para_id": 1816, "text": "de Bourgh, whose bounty and beneficence has preferred me to the"}
{"doc_id": "gutenberg_1342", "para_id": 1817, "text": "valuable rectory of this parish, where it shall be my earnest"}
{"doc_id": "gutenberg_1342", "para_id": 1818, "text": "endeavour to demean myself with grateful respect towards her"}
{"doc_id": "gutenberg_1342", "para_id": 1819, "text": "Ladyship, and be ever ready to perform those rites and ceremonies"}
{"doc_id": "gutenberg_1342", "para_id": 1820, "text": "which are instituted by the Church of England. As a clergyman,"}
{"doc_id": "gutenberg_1342", "para_id": 1821, "text": "moreover, I feel it my duty to promote and establish the blessing"}
{"doc_id": "gutenberg_1342", "para_id": 1822, "text": "of peace in all families within the reach of my influence; and on"}
{"doc_id": "gutenberg_1342", "para_id": 1823, "text": "these grounds I flatter myself that my present overtures of"}
{"doc_id": "gutenberg_1342", "para_id": 1824, "text": "good-will are highly commendable, and that the circumstance of my"}
{"doc_id": "gutenberg_1342", "para_id": 1825, "text": "being next in the entail of Longbourn estate will be kindly"}
{"doc_id": "gutenberg_1342", "para_id": 1826, "text": "overlooked on your side, and not lead you to reject the offered"}
{"doc_id": "gutenberg_1342", "para_id": 1827, "text": "olive branch. I cannot be otherwise than concerned at being the"}
{"doc_id": "gutenberg_1342", "para_id": 1828, "text": "means of injuring your amiable daughters, and beg leave to"}
{"doc_id": "gutenberg_1342", "para_id": 1829, "text": "apologize for it, as well as to assure you of my readiness to make"}
{"doc_id": "gutenberg_1342", "para_id": 1830, "text": "them every possible amends; but of this hereafter. If you should"}
{"doc_id": "gutenberg_1342", "para_id": 1831, "text": "have no objection to receive me into your house, I propose myself"}
{"doc_id": "gutenberg_1342", "para_id": 1832, "text": "the satisfaction of waiting on you and your family, Monday,"}
{"doc_id": "gutenberg_1342", "para_id": 1833, "text": "November 18th, by four o’clock, and shall probably trespass on your"}
{"doc_id": "gutenberg_1342", "para_id": 1834, "text": "hospitality till the Saturday se’nnight following, which I can do"}
{"doc_id": "gutenberg_1342", "para_id": 1835, "text": "without any inconvenience, as Lady Catherine is far from objecting"}
{"doc_id": "gutenberg_1342", "para_id": 1836, "text": "to my occasional absence on a Sunday, provided that some other"}
{"doc_id": "gutenberg_1342", "para_id": 1837, "text": "clergyman is engaged to do the duty of the day. I remain, dear sir,"}
{"doc_id": "gutenberg_1342", "para_id": 1838, "text": "with respectful compliments to your lady and daughters, your"}
{"doc_id": "gutenberg_1342", "para_id": 1839, "text": "“At four o’clock, therefore, we may expect this peace-making gentleman,”"}
{"doc_id": "gutenberg_1342", "para_id": 1840, "text": "said Mr. Bennet, as he folded up the letter. “He seems to be a most"}
{"doc_id": "gutenberg_1342", "para_id": 1841, "text": "conscientious and polite young man, upon my word; and, I doubt not, will"}
{"doc_id": "gutenberg_1342", "para_id": 1842, "text": "prove a valuable acquaintance, especially if Lady Catherine should be so"}
{"doc_id": "gutenberg_1342", "para_id": 1843, "text": "“There is some sense in what he says about the girls, however; and, if"}
{"doc_id": "gutenberg_1342", "para_id": 1844, "text": "he is disposed to make them any amends, I shall not be the person to"}
{"doc_id": "gutenberg_1342", "para_id": 1845, "text": "“Though it is difficult,” said Jane, “to guess in what way he can mean"}
{"doc_id": "gutenberg_1342", "para_id": 1846, "text": "to make us the atonement he thinks our due, the wish is certainly to his"}
{"doc_id": "gutenberg_1342", "para_id": 1847, "text": "Elizabeth was chiefly struck with his extraordinary deference for Lady"}
{"doc_id": "gutenberg_1342", "para_id": 1848, "text": "Catherine, and his kind intention of christening, marrying, and burying"}
{"doc_id": "gutenberg_1342", "para_id": 1849, "text": "“He must be an oddity, I think,” said she. “I cannot make him out. There"}
{"doc_id": "gutenberg_1342", "para_id": 1850, "text": "is something very pompous in his style. And what can he mean by"}
{"doc_id": "gutenberg_1342", "para_id": 1851, "text": "apologizing for being next in the entail? We cannot suppose he would"}
{"doc_id": "gutenberg_1342", "para_id": 1852, "text": "help it, if he could. Can he be a sensible man, sir?”"}
{"doc_id": "gutenberg_1342", "para_id": 1853, "text": "“No, my dear; I think not. I have great hopes of finding him quite the"}
{"doc_id": "gutenberg_1342", "para_id": 1854, "text": "reverse. There is a mixture of servility and self-importance in his"}
{"doc_id": "gutenberg_1342", "para_id": 1855, "text": "letter which promises well. I am impatient to see him.”"}
{"doc_id": "gutenberg_1342", "para_id": 1856, "text": "“In point of composition,” said Mary, “his letter does not seem"}
{"doc_id": "gutenberg_1342", "para_id": 1857, "text": "defective. The idea of the olive branch perhaps is not wholly new, yet I"}
{"doc_id": "gutenberg_1342", "para_id": 1858, "text": "To Catherine and Lydia neither the letter nor its writer were in any"}
{"doc_id": "gutenberg_1342", "para_id": 1859, "text": "degree interesting. It was next to impossible that their cousin should"}
{"doc_id": "gutenberg_1342", "para_id": 1860, "text": "come in a scarlet coat, and it was now some weeks since they had"}
{"doc_id": "gutenberg_1342", "para_id": 1861, "text": "received pleasure from the society of a man in any other colour. As for"}
{"doc_id": "gutenberg_1342", "para_id": 1862, "text": "their mother, Mr. Collins’s letter had done away much of her ill-will,"}
{"doc_id": "gutenberg_1342", "para_id": 1863, "text": "and she was preparing to see him with a degree of composure which"}
{"doc_id": "gutenberg_1342", "para_id": 1864, "text": "Mr. Collins was punctual to his time, and was received with great"}
{"doc_id": "gutenberg_1342", "para_id": 1865, "text": "politeness by the whole family. Mr. Bennet indeed said little; but the"}
{"doc_id": "gutenberg_1342", "para_id": 1866, "text": "ladies were ready enough to talk, and Mr. Collins seemed neither in need"}
{"doc_id": "gutenberg_1342", "para_id": 1867, "text": "of encouragement, nor inclined to be silent himself. He was a tall,"}
{"doc_id": "gutenberg_1342", "para_id": 1868, "text": "heavy-looking young man of five-and-twenty. His air was grave and"}
{"doc_id": "gutenberg_1342", "para_id": 1869, "text": "stately, and his manners were very formal. He had not been long seated"}
{"doc_id": "gutenberg_1342", "para_id": 1870, "text": "before he complimented Mrs. Bennet on having so fine a family of"}
{"doc_id": "gutenberg_1342", "para_id": 1871, "text": "daughters, said he had heard much of their beauty, but that, in this"}
{"doc_id": "gutenberg_1342", "para_id": 1872, "text": "instance, fame had fallen short of the truth; and added, that he did not"}
{"doc_id": "gutenberg_1342", "para_id": 1873, "text": "doubt her seeing them all in due time well disposed of in marriage. This"}
{"doc_id": "gutenberg_1342", "para_id": 1874, "text": "gallantry was not much to the taste of some of his hearers; but Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1875, "text": "Bennet, who quarrelled with no compliments, answered most readily,--"}
{"doc_id": "gutenberg_1342", "para_id": 1876, "text": "“You are very kind, sir, I am sure; and I wish with all my heart it may"}
{"doc_id": "gutenberg_1342", "para_id": 1877, "text": "prove so; for else they will be destitute enough. Things are settled so"}
{"doc_id": "gutenberg_1342", "para_id": 1878, "text": "“You allude, perhaps, to the entail of this estate.”"}
{"doc_id": "gutenberg_1342", "para_id": 1879, "text": "“Ah, sir, I do indeed. It is a grievous affair to my poor girls, you"}
{"doc_id": "gutenberg_1342", "para_id": 1880, "text": "must confess. Not that I mean to find fault with _you_, for such things,"}
{"doc_id": "gutenberg_1342", "para_id": 1881, "text": "I know, are all chance in this world. There is no knowing how estates"}
{"doc_id": "gutenberg_1342", "para_id": 1882, "text": "“I am very sensible, madam, of the hardship to my fair cousins, and"}
{"doc_id": "gutenberg_1342", "para_id": 1883, "text": "could say much on the subject, but that I am cautious of appearing"}
{"doc_id": "gutenberg_1342", "para_id": 1884, "text": "forward and precipitate. But I can assure the young ladies that I come"}
{"doc_id": "gutenberg_1342", "para_id": 1885, "text": "prepared to admire them. At present I will not say more, but, perhaps,"}
{"doc_id": "gutenberg_1342", "para_id": 1886, "text": "He was interrupted by a summons to dinner; and the girls smiled on each"}
{"doc_id": "gutenberg_1342", "para_id": 1887, "text": "other. They were not the only objects of Mr. Collins’s admiration. The"}
{"doc_id": "gutenberg_1342", "para_id": 1888, "text": "hall, the dining-room, and all its furniture, were examined and praised;"}
{"doc_id": "gutenberg_1342", "para_id": 1889, "text": "and his commendation of everything would have touched Mrs. Bennet’s"}
{"doc_id": "gutenberg_1342", "para_id": 1890, "text": "heart, but for the mortifying supposition of his viewing it all as his"}
{"doc_id": "gutenberg_1342", "para_id": 1891, "text": "own future property. The dinner, too, in its turn, was highly admired;"}
{"doc_id": "gutenberg_1342", "para_id": 1892, "text": "and he begged to know to which of his fair cousins the excellence of its"}
{"doc_id": "gutenberg_1342", "para_id": 1893, "text": "cookery was owing. But here he was set right by Mrs. Bennet, who assured"}
{"doc_id": "gutenberg_1342", "para_id": 1894, "text": "him, with some asperity, that they were very well able to keep a good"}
{"doc_id": "gutenberg_1342", "para_id": 1895, "text": "cook, and that her daughters had nothing to do in the kitchen. He begged"}
{"doc_id": "gutenberg_1342", "para_id": 1896, "text": "pardon for having displeased her. In a softened tone she declared"}
{"doc_id": "gutenberg_1342", "para_id": 1897, "text": "herself not at all offended; but he continued to apologize for about a"}
{"doc_id": "gutenberg_1342", "para_id": 1898, "text": "During dinner, Mr. Bennet scarcely spoke at all; but when the servants"}
{"doc_id": "gutenberg_1342", "para_id": 1899, "text": "were withdrawn, he thought it time to have some conversation with his"}
{"doc_id": "gutenberg_1342", "para_id": 1900, "text": "guest, and therefore started a subject in which he expected him to"}
{"doc_id": "gutenberg_1342", "para_id": 1901, "text": "shine, by observing that he seemed very fortunate in his patroness. Lady"}
{"doc_id": "gutenberg_1342", "para_id": 1902, "text": "Catherine de Bourgh’s attention to his wishes, and consideration for his"}
{"doc_id": "gutenberg_1342", "para_id": 1903, "text": "comfort, appeared very remarkable. Mr. Bennet could not have chosen"}
{"doc_id": "gutenberg_1342", "para_id": 1904, "text": "better. Mr. Collins was eloquent in her praise. The subject elevated him"}
{"doc_id": "gutenberg_1342", "para_id": 1905, "text": "to more than usual solemnity of manner; and with a most important aspect"}
{"doc_id": "gutenberg_1342", "para_id": 1906, "text": "he protested that he had never in his life witnessed such behaviour in a"}
{"doc_id": "gutenberg_1342", "para_id": 1907, "text": "person of rank--such affability and condescension, as he had himself"}
{"doc_id": "gutenberg_1342", "para_id": 1908, "text": "experienced from Lady Catherine. She had been graciously pleased to"}
{"doc_id": "gutenberg_1342", "para_id": 1909, "text": "approve of both the discourses which he had already had the honour of"}
{"doc_id": "gutenberg_1342", "para_id": 1910, "text": "preaching before her. She had also asked him twice to dine at Rosings,"}
{"doc_id": "gutenberg_1342", "para_id": 1911, "text": "and had sent for him only the Saturday before, to make up her pool of"}
{"doc_id": "gutenberg_1342", "para_id": 1912, "text": "quadrille in the evening. Lady Catherine was reckoned proud by many"}
{"doc_id": "gutenberg_1342", "para_id": 1913, "text": "people, he knew, but _he_ had never seen anything but affability in her."}
{"doc_id": "gutenberg_1342", "para_id": 1914, "text": "She had always spoken to him as she would to any other gentleman; she"}
{"doc_id": "gutenberg_1342", "para_id": 1915, "text": "made not the smallest objection to his joining in the society of the"}
{"doc_id": "gutenberg_1342", "para_id": 1916, "text": "neighbourhood, nor to his leaving his parish occasionally for a week or"}
{"doc_id": "gutenberg_1342", "para_id": 1917, "text": "two to visit his relations. She had even condescended to advise him to"}
{"doc_id": "gutenberg_1342", "para_id": 1918, "text": "marry as soon as he could, provided he chose with discretion; and had"}
{"doc_id": "gutenberg_1342", "para_id": 1919, "text": "once paid him a visit in his humble parsonage, where she had perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 1920, "text": "approved all the alterations he had been making, and had even vouchsafed"}
{"doc_id": "gutenberg_1342", "para_id": 1921, "text": "to suggest some herself,--some shelves in the closets upstairs."}
{"doc_id": "gutenberg_1342", "para_id": 1922, "text": "“That is all very proper and civil, I am sure,” said Mrs. Bennet, “and I"}
{"doc_id": "gutenberg_1342", "para_id": 1923, "text": "dare say she is a very agreeable woman. It is a pity that great ladies"}
{"doc_id": "gutenberg_1342", "para_id": 1924, "text": "in general are not more like her. Does she live near you, sir?”"}
{"doc_id": "gutenberg_1342", "para_id": 1925, "text": "“The garden in which stands my humble abode is separated only by a lane"}
{"doc_id": "gutenberg_1342", "para_id": 1926, "text": "“I think you said she was a widow, sir? has she any family?”"}
{"doc_id": "gutenberg_1342", "para_id": 1927, "text": "“She has one only daughter, the heiress of Rosings, and of very"}
{"doc_id": "gutenberg_1342", "para_id": 1928, "text": "“Ah,” cried Mrs. Bennet, shaking her head, “then she is better off than"}
{"doc_id": "gutenberg_1342", "para_id": 1929, "text": "many girls. And what sort of young lady is she? Is she handsome?”"}
{"doc_id": "gutenberg_1342", "para_id": 1930, "text": "“She is a most charming young lady, indeed. Lady Catherine herself says"}
{"doc_id": "gutenberg_1342", "para_id": 1931, "text": "that, in point of true beauty, Miss de Bourgh is far superior to the"}
{"doc_id": "gutenberg_1342", "para_id": 1932, "text": "handsomest of her sex; because there is that in her features which marks"}
{"doc_id": "gutenberg_1342", "para_id": 1933, "text": "the young woman of distinguished birth. She is unfortunately of a sickly"}
{"doc_id": "gutenberg_1342", "para_id": 1934, "text": "constitution, which has prevented her making that progress in many"}
{"doc_id": "gutenberg_1342", "para_id": 1935, "text": "accomplishments which she could not otherwise have failed of, as I am"}
{"doc_id": "gutenberg_1342", "para_id": 1936, "text": "informed by the lady who superintended her education, and who still"}
{"doc_id": "gutenberg_1342", "para_id": 1937, "text": "resides with them. But she is perfectly amiable, and often condescends"}
{"doc_id": "gutenberg_1342", "para_id": 1938, "text": "to drive by my humble abode in her little phaeton and ponies.”"}
{"doc_id": "gutenberg_1342", "para_id": 1939, "text": "“Has she been presented? I do not remember her name among the ladies at"}
{"doc_id": "gutenberg_1342", "para_id": 1940, "text": "“Her indifferent state of health unhappily prevents her being in town;"}
{"doc_id": "gutenberg_1342", "para_id": 1941, "text": "and by that means, as I told Lady Catherine myself one day, has deprived"}
{"doc_id": "gutenberg_1342", "para_id": 1942, "text": "the British Court of its brightest ornament. Her Ladyship seemed pleased"}
{"doc_id": "gutenberg_1342", "para_id": 1943, "text": "with the idea; and you may imagine that I am happy on every occasion to"}
{"doc_id": "gutenberg_1342", "para_id": 1944, "text": "offer those little delicate compliments which are always acceptable to"}
{"doc_id": "gutenberg_1342", "para_id": 1945, "text": "ladies. I have more than once observed to Lady Catherine, that her"}
{"doc_id": "gutenberg_1342", "para_id": 1946, "text": "charming daughter seemed born to be a duchess; and that the most"}
{"doc_id": "gutenberg_1342", "para_id": 1947, "text": "elevated rank, instead of giving her consequence, would be adorned by"}
{"doc_id": "gutenberg_1342", "para_id": 1948, "text": "her. These are the kind of little things which please her Ladyship, and"}
{"doc_id": "gutenberg_1342", "para_id": 1949, "text": "it is a sort of attention which I conceive myself peculiarly bound to"}
{"doc_id": "gutenberg_1342", "para_id": 1950, "text": "“You judge very properly,” said Mr. Bennet; “and it is happy for you"}
{"doc_id": "gutenberg_1342", "para_id": 1951, "text": "that you possess the talent of flattering with delicacy. May I ask"}
{"doc_id": "gutenberg_1342", "para_id": 1952, "text": "whether these pleasing attentions proceed from the impulse of the"}
{"doc_id": "gutenberg_1342", "para_id": 1953, "text": "“They arise chiefly from what is passing at the time; and though I"}
{"doc_id": "gutenberg_1342", "para_id": 1954, "text": "sometimes amuse myself with suggesting and arranging such little elegant"}
{"doc_id": "gutenberg_1342", "para_id": 1955, "text": "compliments as may be adapted to ordinary occasions, I always wish to"}
{"doc_id": "gutenberg_1342", "para_id": 1956, "text": "Mr. Bennet’s expectations were fully answered. His cousin was as absurd"}
{"doc_id": "gutenberg_1342", "para_id": 1957, "text": "as he had hoped; and he listened to him with the keenest enjoyment,"}
{"doc_id": "gutenberg_1342", "para_id": 1958, "text": "maintaining at the same time the most resolute composure of countenance,"}
{"doc_id": "gutenberg_1342", "para_id": 1959, "text": "and, except in an occasional glance at Elizabeth, requiring no partner"}
{"doc_id": "gutenberg_1342", "para_id": 1960, "text": "By tea-time, however, the dose had been enough, and Mr. Bennet was glad"}
{"doc_id": "gutenberg_1342", "para_id": 1961, "text": "to take his guest into the drawing-room again, and when tea was over,"}
{"doc_id": "gutenberg_1342", "para_id": 1962, "text": "to read aloud to the ladies. Mr. Collins readily assented, and a book"}
{"doc_id": "gutenberg_1342", "para_id": 1963, "text": "was produced; but on beholding it (for everything announced it to be"}
{"doc_id": "gutenberg_1342", "para_id": 1964, "text": "from a circulating library) he started back, and, begging pardon,"}
{"doc_id": "gutenberg_1342", "para_id": 1965, "text": "protested that he never read novels. Kitty stared at him, and Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 1966, "text": "exclaimed. Other books were produced, and after some deliberation he"}
{"doc_id": "gutenberg_1342", "para_id": 1967, "text": "chose “Fordyce’s Sermons.” Lydia gaped as he opened the volume; and"}
{"doc_id": "gutenberg_1342", "para_id": 1968, "text": "before he had, with very monotonous solemnity, read three pages, she"}
{"doc_id": "gutenberg_1342", "para_id": 1969, "text": "“Do you know, mamma, that my uncle Philips talks of turning away"}
{"doc_id": "gutenberg_1342", "para_id": 1970, "text": "Richard? and if he does, Colonel Forster will hire him. My aunt told me"}
{"doc_id": "gutenberg_1342", "para_id": 1971, "text": "so herself on Saturday. I shall walk to Meryton to-morrow to hear more"}
{"doc_id": "gutenberg_1342", "para_id": 1972, "text": "about it, and to ask when Mr. Denny comes back from town.”"}
{"doc_id": "gutenberg_1342", "para_id": 1973, "text": "Lydia was bid by her two eldest sisters to hold her tongue; but Mr."}
{"doc_id": "gutenberg_1342", "para_id": 1974, "text": "Collins, much offended, laid aside his book, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 1975, "text": "“I have often observed how little young ladies are interested by books"}
{"doc_id": "gutenberg_1342", "para_id": 1976, "text": "of a serious stamp, though written solely for their benefit. It amazes"}
{"doc_id": "gutenberg_1342", "para_id": 1977, "text": "me, I confess; for certainly there can be nothing so advantageous to"}
{"doc_id": "gutenberg_1342", "para_id": 1978, "text": "them as instruction. But I will no longer importune my young cousin.”"}
{"doc_id": "gutenberg_1342", "para_id": 1979, "text": "Then, turning to Mr. Bennet, he offered himself as his antagonist at"}
{"doc_id": "gutenberg_1342", "para_id": 1980, "text": "backgammon. Mr. Bennet accepted the challenge, observing that he acted"}
{"doc_id": "gutenberg_1342", "para_id": 1981, "text": "very wisely in leaving the girls to their own trifling amusements. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 1982, "text": "Bennet and her daughters apologized most civilly for Lydia’s"}
{"doc_id": "gutenberg_1342", "para_id": 1983, "text": "interruption, and promised that it should not occur again, if he would"}
{"doc_id": "gutenberg_1342", "para_id": 1984, "text": "resume his book; but Mr. Collins, after assuring them that he bore his"}
{"doc_id": "gutenberg_1342", "para_id": 1985, "text": "young cousin no ill-will, and should never resent her behaviour as any"}
{"doc_id": "gutenberg_1342", "para_id": 1986, "text": "affront, seated himself at another table with Mr. Bennet, and prepared"}
{"doc_id": "gutenberg_1342", "para_id": 1987, "text": "Mr. Collins was not a sensible man, and the deficiency of nature had"}
{"doc_id": "gutenberg_1342", "para_id": 1988, "text": "been but little assisted by education or society; the greatest part of"}
{"doc_id": "gutenberg_1342", "para_id": 1989, "text": "his life having been spent under the guidance of an illiterate and"}
{"doc_id": "gutenberg_1342", "para_id": 1990, "text": "miserly father; and though he belonged to one of the universities, he"}
{"doc_id": "gutenberg_1342", "para_id": 1991, "text": "had merely kept the necessary terms without forming at it any useful"}
{"doc_id": "gutenberg_1342", "para_id": 1992, "text": "acquaintance. The subjection in which his father had brought him up had"}
{"doc_id": "gutenberg_1342", "para_id": 1993, "text": "given him originally great humility of manner; but it was now a good"}
{"doc_id": "gutenberg_1342", "para_id": 1994, "text": "deal counteracted by the self-conceit of a weak head, living in"}
{"doc_id": "gutenberg_1342", "para_id": 1995, "text": "retirement, and the consequential feelings of early and unexpected"}
{"doc_id": "gutenberg_1342", "para_id": 1996, "text": "prosperity. A fortunate chance had recommended him to Lady Catherine de"}
{"doc_id": "gutenberg_1342", "para_id": 1997, "text": "Bourgh when the living of Hunsford was vacant; and the respect which he"}
{"doc_id": "gutenberg_1342", "para_id": 1998, "text": "felt for her high rank, and his veneration for her as his patroness,"}
{"doc_id": "gutenberg_1342", "para_id": 1999, "text": "mingling with a very good opinion of himself, of his authority as a"}
{"doc_id": "gutenberg_1342", "para_id": 2000, "text": "clergyman, and his right as a rector, made him altogether a mixture of"}
{"doc_id": "gutenberg_1342", "para_id": 2001, "text": "pride and obsequiousness, self-importance and humility."}
{"doc_id": "gutenberg_1342", "para_id": 2002, "text": "Having now a good house and a very sufficient income, he intended to"}
{"doc_id": "gutenberg_1342", "para_id": 2003, "text": "marry; and in seeking a reconciliation with the Longbourn family he had"}
{"doc_id": "gutenberg_1342", "para_id": 2004, "text": "a wife in view, as he meant to choose one of the daughters, if he found"}
{"doc_id": "gutenberg_1342", "para_id": 2005, "text": "them as handsome and amiable as they were represented by common report."}
{"doc_id": "gutenberg_1342", "para_id": 2006, "text": "This was his plan of amends--of atonement--for inheriting their father’s"}
{"doc_id": "gutenberg_1342", "para_id": 2007, "text": "estate; and he thought it an excellent one, full of eligibility and"}
{"doc_id": "gutenberg_1342", "para_id": 2008, "text": "suitableness, and excessively generous and disinterested on his own"}
{"doc_id": "gutenberg_1342", "para_id": 2009, "text": "His plan did not vary on seeing them. Miss Bennet’s lovely face"}
{"doc_id": "gutenberg_1342", "para_id": 2010, "text": "confirmed his views, and established all his strictest notions of what"}
{"doc_id": "gutenberg_1342", "para_id": 2011, "text": "was due to seniority; and for the first evening _she_ was his settled"}
{"doc_id": "gutenberg_1342", "para_id": 2012, "text": "choice. The next morning, however, made an alteration; for in a quarter"}
{"doc_id": "gutenberg_1342", "para_id": 2013, "text": "of an hour’s _tête-à-tête_ with Mrs. Bennet before breakfast, a"}
{"doc_id": "gutenberg_1342", "para_id": 2014, "text": "conversation beginning with his parsonage-house, and leading naturally"}
{"doc_id": "gutenberg_1342", "para_id": 2015, "text": "to the avowal of his hopes, that a mistress for it might be found at"}
{"doc_id": "gutenberg_1342", "para_id": 2016, "text": "Longbourn, produced from her, amid very complaisant smiles and general"}
{"doc_id": "gutenberg_1342", "para_id": 2017, "text": "encouragement, a caution against the very Jane he had fixed on. “As to"}
{"doc_id": "gutenberg_1342", "para_id": 2018, "text": "her _younger_ daughters, she could not take upon her to say--she could"}
{"doc_id": "gutenberg_1342", "para_id": 2019, "text": "not positively answer--but she did not _know_ of any prepossession;--her"}
{"doc_id": "gutenberg_1342", "para_id": 2020, "text": "_eldest_ daughter she must just mention--she felt it incumbent on her to"}
{"doc_id": "gutenberg_1342", "para_id": 2021, "text": "Mr. Collins had only to change from Jane to Elizabeth--and it was soon"}
{"doc_id": "gutenberg_1342", "para_id": 2022, "text": "done--done while Mrs. Bennet was stirring the fire. Elizabeth, equally"}
{"doc_id": "gutenberg_1342", "para_id": 2023, "text": "next to Jane in birth and beauty, succeeded her of course."}
{"doc_id": "gutenberg_1342", "para_id": 2024, "text": "Mrs. Bennet treasured up the hint, and trusted that she might soon have"}
{"doc_id": "gutenberg_1342", "para_id": 2025, "text": "two daughters married; and the man whom she could not bear to speak of"}
{"doc_id": "gutenberg_1342", "para_id": 2026, "text": "Lydia’s intention of walking to Meryton was not forgotten: every sister"}
{"doc_id": "gutenberg_1342", "para_id": 2027, "text": "except Mary agreed to go with her; and Mr. Collins was to attend them,"}
{"doc_id": "gutenberg_1342", "para_id": 2028, "text": "at the request of Mr. Bennet, who was most anxious to get rid of him,"}
{"doc_id": "gutenberg_1342", "para_id": 2029, "text": "and have his library to himself; for thither Mr. Collins had followed"}
{"doc_id": "gutenberg_1342", "para_id": 2030, "text": "him after breakfast, and there he would continue, nominally engaged with"}
{"doc_id": "gutenberg_1342", "para_id": 2031, "text": "one of the largest folios in the collection, but really talking to Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2032, "text": "Bennet, with little cessation, of his house and garden at Hunsford. Such"}
{"doc_id": "gutenberg_1342", "para_id": 2033, "text": "doings discomposed Mr. Bennet exceedingly. In his library he had been"}
{"doc_id": "gutenberg_1342", "para_id": 2034, "text": "always sure of leisure and tranquillity; and though prepared, as he told"}
{"doc_id": "gutenberg_1342", "para_id": 2035, "text": "Elizabeth, to meet with folly and conceit in every other room in the"}
{"doc_id": "gutenberg_1342", "para_id": 2036, "text": "house, he was used to be free from them there: his civility, therefore,"}
{"doc_id": "gutenberg_1342", "para_id": 2037, "text": "was most prompt in inviting Mr. Collins to join his daughters in their"}
{"doc_id": "gutenberg_1342", "para_id": 2038, "text": "walk; and Mr. Collins, being in fact much better fitted for a walker"}
{"doc_id": "gutenberg_1342", "para_id": 2039, "text": "than a reader, was extremely well pleased to close his large book, and"}
{"doc_id": "gutenberg_1342", "para_id": 2040, "text": "In pompous nothings on his side, and civil assents on that of his"}
{"doc_id": "gutenberg_1342", "para_id": 2041, "text": "cousins, their time passed till they entered Meryton. The attention of"}
{"doc_id": "gutenberg_1342", "para_id": 2042, "text": "the younger ones was then no longer to be gained by _him_. Their eyes"}
{"doc_id": "gutenberg_1342", "para_id": 2043, "text": "were immediately wandering up the street in quest of the officers, and"}
{"doc_id": "gutenberg_1342", "para_id": 2044, "text": "nothing less than a very smart bonnet, indeed, or a really new muslin in"}
{"doc_id": "gutenberg_1342", "para_id": 2045, "text": "But the attention of every lady was soon caught by a young man, whom"}
{"doc_id": "gutenberg_1342", "para_id": 2046, "text": "they had never seen before, of most gentlemanlike appearance, walking"}
{"doc_id": "gutenberg_1342", "para_id": 2047, "text": "with an officer on the other side of the way. The officer was the very"}
{"doc_id": "gutenberg_1342", "para_id": 2048, "text": "Mr. Denny concerning whose return from London Lydia came to inquire, and"}
{"doc_id": "gutenberg_1342", "para_id": 2049, "text": "he bowed as they passed. All were struck with the stranger’s air, all"}
{"doc_id": "gutenberg_1342", "para_id": 2050, "text": "wondered who he could be; and Kitty and Lydia, determined if possible"}
{"doc_id": "gutenberg_1342", "para_id": 2051, "text": "to find out, led the way across the street, under pretence of wanting"}
{"doc_id": "gutenberg_1342", "para_id": 2052, "text": "something in an opposite shop, and fortunately had just gained the"}
{"doc_id": "gutenberg_1342", "para_id": 2053, "text": "pavement, when the two gentlemen, turning back, had reached the same"}
{"doc_id": "gutenberg_1342", "para_id": 2054, "text": "spot. Mr. Denny addressed them directly, and entreated permission to"}
{"doc_id": "gutenberg_1342", "para_id": 2055, "text": "introduce his friend, Mr. Wickham, who had returned with him the day"}
{"doc_id": "gutenberg_1342", "para_id": 2056, "text": "before from town, and, he was happy to say, had accepted a commission in"}
{"doc_id": "gutenberg_1342", "para_id": 2057, "text": "their corps. This was exactly as it should be; for the young man wanted"}
{"doc_id": "gutenberg_1342", "para_id": 2058, "text": "only regimentals to make him completely charming. His appearance was"}
{"doc_id": "gutenberg_1342", "para_id": 2059, "text": "greatly in his favour: he had all the best parts of beauty, a fine"}
{"doc_id": "gutenberg_1342", "para_id": 2060, "text": "countenance, a good figure, and very pleasing address. The introduction"}
{"doc_id": "gutenberg_1342", "para_id": 2061, "text": "was followed up on his side by a happy readiness of conversation--a"}
{"doc_id": "gutenberg_1342", "para_id": 2062, "text": "readiness at the same time perfectly correct and unassuming; and the"}
{"doc_id": "gutenberg_1342", "para_id": 2063, "text": "whole party were still standing and talking together very agreeably,"}
{"doc_id": "gutenberg_1342", "para_id": 2064, "text": "when the sound of horses drew their notice, and Darcy and Bingley were"}
{"doc_id": "gutenberg_1342", "para_id": 2065, "text": "seen riding down the street. On distinguishing the ladies of the group"}
{"doc_id": "gutenberg_1342", "para_id": 2066, "text": "the two gentlemen came directly towards them, and began the usual"}
{"doc_id": "gutenberg_1342", "para_id": 2067, "text": "civilities. Bingley was the principal spokesman, and Miss Bennet the"}
{"doc_id": "gutenberg_1342", "para_id": 2068, "text": "principal object. He was then, he said, on his way to Longbourn on"}
{"doc_id": "gutenberg_1342", "para_id": 2069, "text": "purpose to inquire after her. Mr. Darcy corroborated it with a bow, and"}
{"doc_id": "gutenberg_1342", "para_id": 2070, "text": "was beginning to determine not to fix his eyes on Elizabeth, when they"}
{"doc_id": "gutenberg_1342", "para_id": 2071, "text": "were suddenly arrested by the sight of the stranger; and Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 2072, "text": "happening to see the countenance of both as they looked at each other,"}
{"doc_id": "gutenberg_1342", "para_id": 2073, "text": "was all astonishment at the effect of the meeting. Both changed colour,"}
{"doc_id": "gutenberg_1342", "para_id": 2074, "text": "one looked white, the other red. Mr. Wickham, after a few moments,"}
{"doc_id": "gutenberg_1342", "para_id": 2075, "text": "touched his hat--a salutation which Mr. Darcy just deigned to return."}
{"doc_id": "gutenberg_1342", "para_id": 2076, "text": "What could be the meaning of it? It was impossible to imagine; it was"}
{"doc_id": "gutenberg_1342", "para_id": 2077, "text": "In another minute Mr. Bingley, but without seeming to have noticed what"}
{"doc_id": "gutenberg_1342", "para_id": 2078, "text": "Mr. Denny and Mr. Wickham walked with the young ladies to the door of"}
{"doc_id": "gutenberg_1342", "para_id": 2079, "text": "Mr. Philips’s house, and then made their bows, in spite of Miss Lydia’s"}
{"doc_id": "gutenberg_1342", "para_id": 2080, "text": "pressing entreaties that they would come in, and even in spite of Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 2081, "text": "Philips’s throwing up the parlour window, and loudly seconding the"}
{"doc_id": "gutenberg_1342", "para_id": 2082, "text": "Mrs. Philips was always glad to see her nieces; and the two eldest, from"}
{"doc_id": "gutenberg_1342", "para_id": 2083, "text": "their recent absence, were particularly welcome; and she was eagerly"}
{"doc_id": "gutenberg_1342", "para_id": 2084, "text": "expressing her surprise at their sudden return home, which, as their own"}
{"doc_id": "gutenberg_1342", "para_id": 2085, "text": "carriage had not fetched them, she should have known nothing about, if"}
{"doc_id": "gutenberg_1342", "para_id": 2086, "text": "she had not happened to see Mr. Jones’s shopboy in the street, who had"}
{"doc_id": "gutenberg_1342", "para_id": 2087, "text": "told her that they were not to send any more draughts to Netherfield,"}
{"doc_id": "gutenberg_1342", "para_id": 2088, "text": "because the Miss Bennets were come away, when her civility was claimed"}
{"doc_id": "gutenberg_1342", "para_id": 2089, "text": "towards Mr. Collins by Jane’s introduction of him. She received him with"}
{"doc_id": "gutenberg_1342", "para_id": 2090, "text": "her very best politeness, which he returned with as much more,"}
{"doc_id": "gutenberg_1342", "para_id": 2091, "text": "apologizing for his intrusion, without any previous acquaintance with"}
{"doc_id": "gutenberg_1342", "para_id": 2092, "text": "her, which he could not help flattering himself, however, might be"}
{"doc_id": "gutenberg_1342", "para_id": 2093, "text": "justified by his relationship to the young ladies who introduced him to"}
{"doc_id": "gutenberg_1342", "para_id": 2094, "text": "her notice. Mrs. Philips was quite awed by such an excess of good"}
{"doc_id": "gutenberg_1342", "para_id": 2095, "text": "breeding; but her contemplation of one stranger was soon put an end to"}
{"doc_id": "gutenberg_1342", "para_id": 2096, "text": "by exclamations and inquiries about the other, of whom, however, she"}
{"doc_id": "gutenberg_1342", "para_id": 2097, "text": "could only tell her nieces what they already knew, that Mr. Denny had"}
{"doc_id": "gutenberg_1342", "para_id": 2098, "text": "brought him from London, and that he was to have a lieutenant’s"}
{"doc_id": "gutenberg_1342", "para_id": 2099, "text": "commission in the ----shire. She had been watching him the last hour,"}
{"doc_id": "gutenberg_1342", "para_id": 2100, "text": "she said, as he walked up and down the street,--and had Mr. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 2101, "text": "appeared, Kitty and Lydia would certainly have continued the occupation;"}
{"doc_id": "gutenberg_1342", "para_id": 2102, "text": "but unluckily no one passed the windows now except a few of the"}
{"doc_id": "gutenberg_1342", "para_id": 2103, "text": "officers, who, in comparison with the stranger, were become “stupid,"}
{"doc_id": "gutenberg_1342", "para_id": 2104, "text": "disagreeable fellows.” Some of them were to dine with the Philipses the"}
{"doc_id": "gutenberg_1342", "para_id": 2105, "text": "next day, and their aunt promised to make her husband call on Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2106, "text": "Wickham, and give him an invitation also, if the family from Longbourn"}
{"doc_id": "gutenberg_1342", "para_id": 2107, "text": "would come in the evening. This was agreed to; and Mrs. Philips"}
{"doc_id": "gutenberg_1342", "para_id": 2108, "text": "protested that they would have a nice comfortable noisy game of lottery"}
{"doc_id": "gutenberg_1342", "para_id": 2109, "text": "tickets, and a little bit of hot supper afterwards. The prospect of such"}
{"doc_id": "gutenberg_1342", "para_id": 2110, "text": "delights was very cheering, and they parted in mutual good spirits. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2111, "text": "Collins repeated his apologies in quitting the room, and was assured,"}
{"doc_id": "gutenberg_1342", "para_id": 2112, "text": "with unwearying civility, that they were perfectly needless."}
{"doc_id": "gutenberg_1342", "para_id": 2113, "text": "As they walked home, Elizabeth related to Jane what she had seen pass"}
{"doc_id": "gutenberg_1342", "para_id": 2114, "text": "between the two gentlemen; but though Jane would have defended either or"}
{"doc_id": "gutenberg_1342", "para_id": 2115, "text": "both, had they appeared to be wrong, she could no more explain such"}
{"doc_id": "gutenberg_1342", "para_id": 2116, "text": "Mr. Collins on his return highly gratified Mrs. Bennet by admiring Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 2117, "text": "Philips’s manners and politeness. He protested that, except Lady"}
{"doc_id": "gutenberg_1342", "para_id": 2118, "text": "Catherine and her daughter, he had never seen a more elegant woman; for"}
{"doc_id": "gutenberg_1342", "para_id": 2119, "text": "she had not only received him with the utmost civility, but had even"}
{"doc_id": "gutenberg_1342", "para_id": 2120, "text": "pointedly included him in her invitation for the next evening, although"}
{"doc_id": "gutenberg_1342", "para_id": 2121, "text": "utterly unknown to her before. Something, he supposed, might be"}
{"doc_id": "gutenberg_1342", "para_id": 2122, "text": "attributed to his connection with them, but yet he had never met with so"}
{"doc_id": "gutenberg_1342", "para_id": 2123, "text": "As no objection was made to the young people’s engagement with their"}
{"doc_id": "gutenberg_1342", "para_id": 2124, "text": "aunt, and all Mr. Collins’s scruples of leaving Mr. and Mrs. Bennet for"}
{"doc_id": "gutenberg_1342", "para_id": 2125, "text": "a single evening during his visit were most steadily resisted, the coach"}
{"doc_id": "gutenberg_1342", "para_id": 2126, "text": "conveyed him and his five cousins at a suitable hour to Meryton; and the"}
{"doc_id": "gutenberg_1342", "para_id": 2127, "text": "girls had the pleasure of hearing, as they entered the drawing-room,"}
{"doc_id": "gutenberg_1342", "para_id": 2128, "text": "that Mr. Wickham had accepted their uncle’s invitation, and was then in"}
{"doc_id": "gutenberg_1342", "para_id": 2129, "text": "When this information was given, and they had all taken their seats, Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2130, "text": "Collins was at leisure to look around him and admire, and he was so much"}
{"doc_id": "gutenberg_1342", "para_id": 2131, "text": "struck with the size and furniture of the apartment, that he declared he"}
{"doc_id": "gutenberg_1342", "para_id": 2132, "text": "might almost have supposed himself in the small summer breakfast parlour"}
{"doc_id": "gutenberg_1342", "para_id": 2133, "text": "at Rosings; a comparison that did not at first convey much"}
{"doc_id": "gutenberg_1342", "para_id": 2134, "text": "gratification; but when Mrs. Philips understood from him what Rosings"}
{"doc_id": "gutenberg_1342", "para_id": 2135, "text": "was, and who was its proprietor, when she had listened to the"}
{"doc_id": "gutenberg_1342", "para_id": 2136, "text": "description of only one of Lady Catherine’s drawing-rooms, and found"}
{"doc_id": "gutenberg_1342", "para_id": 2137, "text": "that the chimney-piece alone had cost eight hundred pounds, she felt all"}
{"doc_id": "gutenberg_1342", "para_id": 2138, "text": "the force of the compliment, and would hardly have resented a comparison"}
{"doc_id": "gutenberg_1342", "para_id": 2139, "text": "In describing to her all the grandeur of Lady Catherine and her mansion,"}
{"doc_id": "gutenberg_1342", "para_id": 2140, "text": "with occasional digressions in praise of his own humble abode, and the"}
{"doc_id": "gutenberg_1342", "para_id": 2141, "text": "improvements it was receiving, he was happily employed until the"}
{"doc_id": "gutenberg_1342", "para_id": 2142, "text": "gentlemen joined them; and he found in Mrs. Philips a very attentive"}
{"doc_id": "gutenberg_1342", "para_id": 2143, "text": "listener, whose opinion of his consequence increased with what she"}
{"doc_id": "gutenberg_1342", "para_id": 2144, "text": "heard, and who was resolving to retail it all among her neighbours as"}
{"doc_id": "gutenberg_1342", "para_id": 2145, "text": "soon as she could. To the girls, who could not listen to their cousin,"}
{"doc_id": "gutenberg_1342", "para_id": 2146, "text": "and who had nothing to do but to wish for an instrument, and examine"}
{"doc_id": "gutenberg_1342", "para_id": 2147, "text": "their own indifferent imitations of china on the mantel-piece, the"}
{"doc_id": "gutenberg_1342", "para_id": 2148, "text": "interval of waiting appeared very long. It was over at last, however."}
{"doc_id": "gutenberg_1342", "para_id": 2149, "text": "The gentlemen did approach: and when Mr. Wickham walked into the room,"}
{"doc_id": "gutenberg_1342", "para_id": 2150, "text": "Elizabeth felt that she had neither been seeing him before, nor thinking"}
{"doc_id": "gutenberg_1342", "para_id": 2151, "text": "of him since, with the smallest degree of unreasonable admiration. The"}
{"doc_id": "gutenberg_1342", "para_id": 2152, "text": "officers of the ----shire were in general a very creditable,"}
{"doc_id": "gutenberg_1342", "para_id": 2153, "text": "gentlemanlike set and the best of them were of the present party; but"}
{"doc_id": "gutenberg_1342", "para_id": 2154, "text": "Mr, Wickham was as far beyond them all in person, countenance, air, and"}
{"doc_id": "gutenberg_1342", "para_id": 2155, "text": "walk, as _they_ were superior to the broad-faced stuffy uncle Philips,"}
{"doc_id": "gutenberg_1342", "para_id": 2156, "text": "breathing port wine, who followed them into the room."}
{"doc_id": "gutenberg_1342", "para_id": 2157, "text": "Mr. Wickham was the happy man towards whom almost every female eye was"}
{"doc_id": "gutenberg_1342", "para_id": 2158, "text": "turned, and Elizabeth was the happy woman by whom he finally seated"}
{"doc_id": "gutenberg_1342", "para_id": 2159, "text": "himself; and the agreeable manner in which he immediately fell into"}
{"doc_id": "gutenberg_1342", "para_id": 2160, "text": "conversation, though it was only on its being a wet night, and on the"}
{"doc_id": "gutenberg_1342", "para_id": 2161, "text": "probability of a rainy season, made her feel that the commonest,"}
{"doc_id": "gutenberg_1342", "para_id": 2162, "text": "dullest, most threadbare topic might be rendered interesting by the"}
{"doc_id": "gutenberg_1342", "para_id": 2163, "text": "With such rivals for the notice of the fair as Mr. Wickham and the"}
{"doc_id": "gutenberg_1342", "para_id": 2164, "text": "officers, Mr. Collins seemed to sink into insignificance; to the young"}
{"doc_id": "gutenberg_1342", "para_id": 2165, "text": "ladies he certainly was nothing; but he had still at intervals a kind"}
{"doc_id": "gutenberg_1342", "para_id": 2166, "text": "listener in Mrs. Philips, and was, by her watchfulness, most abundantly"}
{"doc_id": "gutenberg_1342", "para_id": 2167, "text": "When the card tables were placed, he had an opportunity of obliging her,"}
{"doc_id": "gutenberg_1342", "para_id": 2168, "text": "“I know little of the game at present,” said he, “but I shall be glad to"}
{"doc_id": "gutenberg_1342", "para_id": 2169, "text": "improve myself; for in my situation of life----” Mrs. Philips was very"}
{"doc_id": "gutenberg_1342", "para_id": 2170, "text": "thankful for his compliance, but could not wait for his reason."}
{"doc_id": "gutenberg_1342", "para_id": 2171, "text": "Mr. Wickham did not play at whist, and with ready delight was he"}
{"doc_id": "gutenberg_1342", "para_id": 2172, "text": "received at the other table between Elizabeth and Lydia. At first there"}
{"doc_id": "gutenberg_1342", "para_id": 2173, "text": "seemed danger of Lydia’s engrossing him entirely, for she was a most"}
{"doc_id": "gutenberg_1342", "para_id": 2174, "text": "determined talker; but being likewise extremely fond of lottery tickets,"}
{"doc_id": "gutenberg_1342", "para_id": 2175, "text": "she soon grew too much interested in the game, too eager in making bets"}
{"doc_id": "gutenberg_1342", "para_id": 2176, "text": "and exclaiming after prizes, to have attention for anyone in particular."}
{"doc_id": "gutenberg_1342", "para_id": 2177, "text": "Allowing for the common demands of the game, Mr. Wickham was therefore"}
{"doc_id": "gutenberg_1342", "para_id": 2178, "text": "at leisure to talk to Elizabeth, and she was very willing to hear him,"}
{"doc_id": "gutenberg_1342", "para_id": 2179, "text": "though what she chiefly wished to hear she could not hope to be told,"}
{"doc_id": "gutenberg_1342", "para_id": 2180, "text": "the history of his acquaintance with Mr. Darcy. She dared not even"}
{"doc_id": "gutenberg_1342", "para_id": 2181, "text": "mention that gentleman. Her curiosity, however, was unexpectedly"}
{"doc_id": "gutenberg_1342", "para_id": 2182, "text": "relieved. Mr. Wickham began the subject himself. He inquired how far"}
{"doc_id": "gutenberg_1342", "para_id": 2183, "text": "Netherfield was from Meryton; and, after receiving her answer, asked in"}
{"doc_id": "gutenberg_1342", "para_id": 2184, "text": "a hesitating manner how long Mr. Darcy had been staying there."}
{"doc_id": "gutenberg_1342", "para_id": 2185, "text": "“About a month,” said Elizabeth; and then, unwilling to let the subject"}
{"doc_id": "gutenberg_1342", "para_id": 2186, "text": "drop, added, “he is a man of very large property in Derbyshire, I"}
{"doc_id": "gutenberg_1342", "para_id": 2187, "text": "“Yes,” replied Wickham; “his estate there is a noble one. A clear ten"}
{"doc_id": "gutenberg_1342", "para_id": 2188, "text": "thousand per annum. You could not have met with a person more capable of"}
{"doc_id": "gutenberg_1342", "para_id": 2189, "text": "giving you certain information on that head than myself--for I have been"}
{"doc_id": "gutenberg_1342", "para_id": 2190, "text": "connected with his family, in a particular manner, from my infancy.”"}
{"doc_id": "gutenberg_1342", "para_id": 2191, "text": "“You may well be surprised, Miss Bennet, at such an assertion, after"}
{"doc_id": "gutenberg_1342", "para_id": 2192, "text": "seeing, as you probably might, the very cold manner of our meeting"}
{"doc_id": "gutenberg_1342", "para_id": 2193, "text": "yesterday. Are you much acquainted with Mr. Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 2194, "text": "“As much as I ever wish to be,” cried Elizabeth, warmly. “I have spent"}
{"doc_id": "gutenberg_1342", "para_id": 2195, "text": "four days in the same house with him, and I think him very"}
{"doc_id": "gutenberg_1342", "para_id": 2196, "text": "“I have no right to give _my_ opinion,” said Wickham, “as to his being"}
{"doc_id": "gutenberg_1342", "para_id": 2197, "text": "agreeable or otherwise. I am not qualified to form one. I have known him"}
{"doc_id": "gutenberg_1342", "para_id": 2198, "text": "too long and too well to be a fair judge. It is impossible for _me_ to"}
{"doc_id": "gutenberg_1342", "para_id": 2199, "text": "be impartial. But I believe your opinion of him would in general"}
{"doc_id": "gutenberg_1342", "para_id": 2200, "text": "astonish--and, perhaps, you would not express it quite so strongly"}
{"doc_id": "gutenberg_1342", "para_id": 2201, "text": "“Upon my word I say no more _here_ than I might say in any house in the"}
{"doc_id": "gutenberg_1342", "para_id": 2202, "text": "neighbourhood, except Netherfield. He is not at all liked in"}
{"doc_id": "gutenberg_1342", "para_id": 2203, "text": "Hertfordshire. Everybody is disgusted with his pride. You will not find"}
{"doc_id": "gutenberg_1342", "para_id": 2204, "text": "“I cannot pretend to be sorry,” said Wickham, after a short"}
{"doc_id": "gutenberg_1342", "para_id": 2205, "text": "interruption, “that he or that any man should not be estimated beyond"}
{"doc_id": "gutenberg_1342", "para_id": 2206, "text": "their deserts; but with _him_ I believe it does not often happen. The"}
{"doc_id": "gutenberg_1342", "para_id": 2207, "text": "world is blinded by his fortune and consequence, or frightened by his"}
{"doc_id": "gutenberg_1342", "para_id": 2208, "text": "high and imposing manners, and sees him only as he chooses to be seen.”"}
{"doc_id": "gutenberg_1342", "para_id": 2209, "text": "“I should take him, even on _my_ slight acquaintance, to be an"}
{"doc_id": "gutenberg_1342", "para_id": 2210, "text": "“I wonder,” said he, at the next opportunity of speaking, “whether he is"}
{"doc_id": "gutenberg_1342", "para_id": 2211, "text": "“I do not at all know; but I _heard_ nothing of his going away when I"}
{"doc_id": "gutenberg_1342", "para_id": 2212, "text": "was at Netherfield. I hope your plans in favour of the ----shire will"}
{"doc_id": "gutenberg_1342", "para_id": 2213, "text": "not be affected by his being in the neighbourhood.”"}
{"doc_id": "gutenberg_1342", "para_id": 2214, "text": "“Oh no--it is not for _me_ to be driven away by Mr. Darcy. If _he_"}
{"doc_id": "gutenberg_1342", "para_id": 2215, "text": "wishes to avoid seeing _me_ he must go. We are not on friendly terms,"}
{"doc_id": "gutenberg_1342", "para_id": 2216, "text": "and it always gives me pain to meet him, but I have no reason for"}
{"doc_id": "gutenberg_1342", "para_id": 2217, "text": "avoiding _him_ but what I might proclaim to all the world--a sense of"}
{"doc_id": "gutenberg_1342", "para_id": 2218, "text": "very great ill-usage, and most painful regrets at his being what he is."}
{"doc_id": "gutenberg_1342", "para_id": 2219, "text": "His father, Miss Bennet, the late Mr. Darcy, was one of the best men"}
{"doc_id": "gutenberg_1342", "para_id": 2220, "text": "that ever breathed, and the truest friend I ever had; and I can never be"}
{"doc_id": "gutenberg_1342", "para_id": 2221, "text": "in company with this Mr. Darcy without being grieved to the soul by a"}
{"doc_id": "gutenberg_1342", "para_id": 2222, "text": "thousand tender recollections. His behaviour to myself has been"}
{"doc_id": "gutenberg_1342", "para_id": 2223, "text": "scandalous; but I verily believe I could forgive him anything and"}
{"doc_id": "gutenberg_1342", "para_id": 2224, "text": "everything, rather than his disappointing the hopes and disgracing the"}
{"doc_id": "gutenberg_1342", "para_id": 2225, "text": "Elizabeth found the interest of the subject increase, and listened with"}
{"doc_id": "gutenberg_1342", "para_id": 2226, "text": "all her heart; but the delicacy of it prevented further inquiry."}
{"doc_id": "gutenberg_1342", "para_id": 2227, "text": "Mr. Wickham began to speak on more general topics, Meryton, the"}
{"doc_id": "gutenberg_1342", "para_id": 2228, "text": "neighbourhood, the society, appearing highly pleased with all that he"}
{"doc_id": "gutenberg_1342", "para_id": 2229, "text": "had yet seen, and speaking of the latter, especially, with gentle but"}
{"doc_id": "gutenberg_1342", "para_id": 2230, "text": "“It was the prospect of constant society, and good society,” he added,"}
{"doc_id": "gutenberg_1342", "para_id": 2231, "text": "“which was my chief inducement to enter the ----shire. I know it to be a"}
{"doc_id": "gutenberg_1342", "para_id": 2232, "text": "most respectable, agreeable corps; and my friend Denny tempted me"}
{"doc_id": "gutenberg_1342", "para_id": 2233, "text": "further by his account of their present quarters, and the very great"}
{"doc_id": "gutenberg_1342", "para_id": 2234, "text": "attentions and excellent acquaintance Meryton had procured them."}
{"doc_id": "gutenberg_1342", "para_id": 2235, "text": "Society, I own, is necessary to me. I have been a disappointed man, and"}
{"doc_id": "gutenberg_1342", "para_id": 2236, "text": "my spirits will not bear solitude. I _must_ have employment and society."}
{"doc_id": "gutenberg_1342", "para_id": 2237, "text": "A military life is not what I was intended for, but circumstances have"}
{"doc_id": "gutenberg_1342", "para_id": 2238, "text": "now made it eligible. The church _ought_ to have been my profession--I"}
{"doc_id": "gutenberg_1342", "para_id": 2239, "text": "was brought up for the church; and I should at this time have been in"}
{"doc_id": "gutenberg_1342", "para_id": 2240, "text": "possession of a most valuable living, had it pleased the gentleman we"}
{"doc_id": "gutenberg_1342", "para_id": 2241, "text": "“Yes--the late Mr. Darcy bequeathed me the next presentation of the best"}
{"doc_id": "gutenberg_1342", "para_id": 2242, "text": "living in his gift. He was my godfather, and excessively attached to me."}
{"doc_id": "gutenberg_1342", "para_id": 2243, "text": "I cannot do justice to his kindness. He meant to provide for me amply,"}
{"doc_id": "gutenberg_1342", "para_id": 2244, "text": "and thought he had done it; but when the living fell, it was given"}
{"doc_id": "gutenberg_1342", "para_id": 2245, "text": "“Good heavens!” cried Elizabeth; “but how could _that_ be? How could his"}
{"doc_id": "gutenberg_1342", "para_id": 2246, "text": "will be disregarded? Why did not you seek legal redress?”"}
{"doc_id": "gutenberg_1342", "para_id": 2247, "text": "“There was just such an informality in the terms of the bequest as to"}
{"doc_id": "gutenberg_1342", "para_id": 2248, "text": "give me no hope from law. A man of honour could not have doubted the"}
{"doc_id": "gutenberg_1342", "para_id": 2249, "text": "intention, but Mr. Darcy chose to doubt it--or to treat it as a merely"}
{"doc_id": "gutenberg_1342", "para_id": 2250, "text": "conditional recommendation, and to assert that I had forfeited all claim"}
{"doc_id": "gutenberg_1342", "para_id": 2251, "text": "to it by extravagance, imprudence, in short, anything or nothing."}
{"doc_id": "gutenberg_1342", "para_id": 2252, "text": "Certain it is that the living became vacant two years ago, exactly as I"}
{"doc_id": "gutenberg_1342", "para_id": 2253, "text": "was of an age to hold it, and that it was given to another man; and no"}
{"doc_id": "gutenberg_1342", "para_id": 2254, "text": "less certain is it, that I cannot accuse myself of having really done"}
{"doc_id": "gutenberg_1342", "para_id": 2255, "text": "anything to deserve to lose it. I have a warm unguarded temper, and I"}
{"doc_id": "gutenberg_1342", "para_id": 2256, "text": "may perhaps have sometimes spoken my opinion _of_ him, and _to_ him, too"}
{"doc_id": "gutenberg_1342", "para_id": 2257, "text": "freely. I can recall nothing worse. But the fact is, that we are very"}
{"doc_id": "gutenberg_1342", "para_id": 2258, "text": "“This is quite shocking! He deserves to be publicly disgraced.”"}
{"doc_id": "gutenberg_1342", "para_id": 2259, "text": "“Some time or other he _will_ be--but it shall not be by _me_. Till I"}
{"doc_id": "gutenberg_1342", "para_id": 2260, "text": "can forget his father, I can never defy or expose _him_.”"}
{"doc_id": "gutenberg_1342", "para_id": 2261, "text": "Elizabeth honoured him for such feelings, and thought him handsomer than"}
{"doc_id": "gutenberg_1342", "para_id": 2262, "text": "“But what,” said she, after a pause, “can have been his motive? what can"}
{"doc_id": "gutenberg_1342", "para_id": 2263, "text": "“A thorough, determined dislike of me--a dislike which I cannot but"}
{"doc_id": "gutenberg_1342", "para_id": 2264, "text": "attribute in some measure to jealousy. Had the late Mr. Darcy liked me"}
{"doc_id": "gutenberg_1342", "para_id": 2265, "text": "less, his son might have borne with me better; but his father’s uncommon"}
{"doc_id": "gutenberg_1342", "para_id": 2266, "text": "attachment to me irritated him, I believe, very early in life. He had"}
{"doc_id": "gutenberg_1342", "para_id": 2267, "text": "not a temper to bear the sort of competition in which we stood--the sort"}
{"doc_id": "gutenberg_1342", "para_id": 2268, "text": "“I had not thought Mr. Darcy so bad as this--though I have never liked"}
{"doc_id": "gutenberg_1342", "para_id": 2269, "text": "him, I had not thought so very ill of him--I had supposed him to be"}
{"doc_id": "gutenberg_1342", "para_id": 2270, "text": "despising his fellow-creatures in general, but did not suspect him of"}
{"doc_id": "gutenberg_1342", "para_id": 2271, "text": "descending to such malicious revenge, such injustice, such inhumanity as"}
{"doc_id": "gutenberg_1342", "para_id": 2272, "text": "After a few minutes’ reflection, however, she continued, “I _do_"}
{"doc_id": "gutenberg_1342", "para_id": 2273, "text": "remember his boasting one day, at Netherfield, of the implacability of"}
{"doc_id": "gutenberg_1342", "para_id": 2274, "text": "his resentments, of his having an unforgiving temper. His disposition"}
{"doc_id": "gutenberg_1342", "para_id": 2275, "text": "“I will not trust myself on the subject,” replied Wickham; “_I_ can"}
{"doc_id": "gutenberg_1342", "para_id": 2276, "text": "Elizabeth was again deep in thought, and after a time exclaimed, “To"}
{"doc_id": "gutenberg_1342", "para_id": 2277, "text": "treat in such a manner the godson, the friend, the favourite of his"}
{"doc_id": "gutenberg_1342", "para_id": 2278, "text": "father!” She could have added, “A young man, too, like _you_, whose very"}
{"doc_id": "gutenberg_1342", "para_id": 2279, "text": "countenance may vouch for your being amiable.” But she contented herself"}
{"doc_id": "gutenberg_1342", "para_id": 2280, "text": "with--“And one, too, who had probably been his own companion from"}
{"doc_id": "gutenberg_1342", "para_id": 2281, "text": "childhood, connected together, as I think you said, in the closest"}
{"doc_id": "gutenberg_1342", "para_id": 2282, "text": "“We were born in the same parish, within the same park; the greatest"}
{"doc_id": "gutenberg_1342", "para_id": 2283, "text": "part of our youth was passed together: inmates of the same house,"}
{"doc_id": "gutenberg_1342", "para_id": 2284, "text": "sharing the same amusements, objects of the same parental care. _My_"}
{"doc_id": "gutenberg_1342", "para_id": 2285, "text": "father began life in the profession which your uncle, Mr. Philips,"}
{"doc_id": "gutenberg_1342", "para_id": 2286, "text": "appears to do so much credit to; but he gave up everything to be of use"}
{"doc_id": "gutenberg_1342", "para_id": 2287, "text": "to the late Mr. Darcy, and devoted all his time to the care of the"}
{"doc_id": "gutenberg_1342", "para_id": 2288, "text": "Pemberley property. He was most highly esteemed by Mr. Darcy, a most"}
{"doc_id": "gutenberg_1342", "para_id": 2289, "text": "intimate, confidential friend. Mr. Darcy often acknowledged himself to"}
{"doc_id": "gutenberg_1342", "para_id": 2290, "text": "be under the greatest obligations to my father’s active superintendence;"}
{"doc_id": "gutenberg_1342", "para_id": 2291, "text": "and when, immediately before my father’s death, Mr. Darcy gave him a"}
{"doc_id": "gutenberg_1342", "para_id": 2292, "text": "voluntary promise of providing for me, I am convinced that he felt it"}
{"doc_id": "gutenberg_1342", "para_id": 2293, "text": "to be as much a debt of gratitude to _him_ as of affection to myself.”"}
{"doc_id": "gutenberg_1342", "para_id": 2294, "text": "“How strange!” cried Elizabeth. “How abominable! I wonder that the very"}
{"doc_id": "gutenberg_1342", "para_id": 2295, "text": "pride of this Mr. Darcy has not made him just to you. If from no better"}
{"doc_id": "gutenberg_1342", "para_id": 2296, "text": "motive, that he should not have been too proud to be dishonest,--for"}
{"doc_id": "gutenberg_1342", "para_id": 2297, "text": "“It _is_ wonderful,” replied Wickham; “for almost all his actions may be"}
{"doc_id": "gutenberg_1342", "para_id": 2298, "text": "traced to pride; and pride has often been his best friend. It has"}
{"doc_id": "gutenberg_1342", "para_id": 2299, "text": "connected him nearer with virtue than any other feeling. But we are none"}
{"doc_id": "gutenberg_1342", "para_id": 2300, "text": "of us consistent; and in his behaviour to me there were stronger"}
{"doc_id": "gutenberg_1342", "para_id": 2301, "text": "“Can such abominable pride as his have ever done him good?”"}
{"doc_id": "gutenberg_1342", "para_id": 2302, "text": "“Yes; it has often led him to be liberal and generous; to give his money"}
{"doc_id": "gutenberg_1342", "para_id": 2303, "text": "freely, to display hospitality, to assist his tenants, and relieve the"}
{"doc_id": "gutenberg_1342", "para_id": 2304, "text": "poor. Family pride, and _filial_ pride, for he is very proud of what his"}
{"doc_id": "gutenberg_1342", "para_id": 2305, "text": "father was, have done this. Not to appear to disgrace his family, to"}
{"doc_id": "gutenberg_1342", "para_id": 2306, "text": "degenerate from the popular qualities, or lose the influence of the"}
{"doc_id": "gutenberg_1342", "para_id": 2307, "text": "Pemberley House, is a powerful motive. He has also _brotherly_ pride,"}
{"doc_id": "gutenberg_1342", "para_id": 2308, "text": "which, with _some_ brotherly affection, makes him a very kind and"}
{"doc_id": "gutenberg_1342", "para_id": 2309, "text": "careful guardian of his sister; and you will hear him generally cried up"}
{"doc_id": "gutenberg_1342", "para_id": 2310, "text": "He shook his head. “I wish I could call her amiable. It gives me pain to"}
{"doc_id": "gutenberg_1342", "para_id": 2311, "text": "speak ill of a Darcy; but she is too much like her brother,--very, very"}
{"doc_id": "gutenberg_1342", "para_id": 2312, "text": "proud. As a child, she was affectionate and pleasing, and extremely fond"}
{"doc_id": "gutenberg_1342", "para_id": 2313, "text": "of me; and I have devoted hours and hours to her amusement. But she is"}
{"doc_id": "gutenberg_1342", "para_id": 2314, "text": "nothing to me now. She is a handsome girl, about fifteen or sixteen,"}
{"doc_id": "gutenberg_1342", "para_id": 2315, "text": "and, I understand, highly accomplished. Since her father’s death her"}
{"doc_id": "gutenberg_1342", "para_id": 2316, "text": "home has been London, where a lady lives with her, and superintends her"}
{"doc_id": "gutenberg_1342", "para_id": 2317, "text": "After many pauses and many trials of other subjects, Elizabeth could not"}
{"doc_id": "gutenberg_1342", "para_id": 2318, "text": "help reverting once more to the first, and saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 2319, "text": "“I am astonished at his intimacy with Mr. Bingley. How can Mr. Bingley,"}
{"doc_id": "gutenberg_1342", "para_id": 2320, "text": "who seems good-humour itself, and is, I really believe, truly amiable,"}
{"doc_id": "gutenberg_1342", "para_id": 2321, "text": "be in friendship with such a man? How can they suit each other? Do you"}
{"doc_id": "gutenberg_1342", "para_id": 2322, "text": "“He is a sweet-tempered, amiable, charming man. He cannot know what Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2323, "text": "“Probably not; but Mr. Darcy can please where he chooses. He does not"}
{"doc_id": "gutenberg_1342", "para_id": 2324, "text": "want abilities. He can be a conversible companion if he thinks it worth"}
{"doc_id": "gutenberg_1342", "para_id": 2325, "text": "his while. Among those who are at all his equals in consequence, he is a"}
{"doc_id": "gutenberg_1342", "para_id": 2326, "text": "very different man from what he is to the less prosperous. His pride"}
{"doc_id": "gutenberg_1342", "para_id": 2327, "text": "never deserts him; but with the rich he is liberal-minded, just,"}
{"doc_id": "gutenberg_1342", "para_id": 2328, "text": "sincere, rational, honourable, and, perhaps, agreeable,--allowing"}
{"doc_id": "gutenberg_1342", "para_id": 2329, "text": "The whist party soon afterwards breaking up, the players gathered round"}
{"doc_id": "gutenberg_1342", "para_id": 2330, "text": "the other table, and Mr. Collins took his station between his cousin"}
{"doc_id": "gutenberg_1342", "para_id": 2331, "text": "Elizabeth and Mrs. Philips. The usual inquiries as to his success were"}
{"doc_id": "gutenberg_1342", "para_id": 2332, "text": "made by the latter. It had not been very great; he had lost every point;"}
{"doc_id": "gutenberg_1342", "para_id": 2333, "text": "but when Mrs. Philips began to express her concern thereupon, he assured"}
{"doc_id": "gutenberg_1342", "para_id": 2334, "text": "her, with much earnest gravity, that it was not of the least importance;"}
{"doc_id": "gutenberg_1342", "para_id": 2335, "text": "that he considered the money as a mere trifle, and begged she would not"}
{"doc_id": "gutenberg_1342", "para_id": 2336, "text": "“I know very well, madam,” said he, “that when persons sit down to a"}
{"doc_id": "gutenberg_1342", "para_id": 2337, "text": "card table they must take their chance of these things,--and happily I"}
{"doc_id": "gutenberg_1342", "para_id": 2338, "text": "am not in such circumstances as to make five shillings any object. There"}
{"doc_id": "gutenberg_1342", "para_id": 2339, "text": "are, undoubtedly, many who could not say the same; but, thanks to Lady"}
{"doc_id": "gutenberg_1342", "para_id": 2340, "text": "Catherine de Bourgh, I am removed far beyond the necessity of regarding"}
{"doc_id": "gutenberg_1342", "para_id": 2341, "text": "Mr. Wickham’s attention was caught; and after observing Mr. Collins for"}
{"doc_id": "gutenberg_1342", "para_id": 2342, "text": "a few moments, he asked Elizabeth in a low voice whether her relations"}
{"doc_id": "gutenberg_1342", "para_id": 2343, "text": "were very intimately acquainted with the family of De Bourgh."}
{"doc_id": "gutenberg_1342", "para_id": 2344, "text": "“Lady Catherine de Bourgh,” she replied, “has very lately given him a"}
{"doc_id": "gutenberg_1342", "para_id": 2345, "text": "living. I hardly know how Mr. Collins was first introduced to her"}
{"doc_id": "gutenberg_1342", "para_id": 2346, "text": "“You know of course that Lady Catherine de Bourgh and Lady Anne Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 2347, "text": "were sisters; consequently that she is aunt to the present Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 2348, "text": "“No, indeed, I did not. I knew nothing at all of Lady Catherine’s"}
{"doc_id": "gutenberg_1342", "para_id": 2349, "text": "connections. I never heard of her existence till the day before"}
{"doc_id": "gutenberg_1342", "para_id": 2350, "text": "“Her daughter, Miss de Bourgh, will have a very large fortune, and it is"}
{"doc_id": "gutenberg_1342", "para_id": 2351, "text": "believed that she and her cousin will unite the two estates.”"}
{"doc_id": "gutenberg_1342", "para_id": 2352, "text": "This information made Elizabeth smile, as she thought of poor Miss"}
{"doc_id": "gutenberg_1342", "para_id": 2353, "text": "Bingley. Vain indeed must be all her attentions, vain and useless her"}
{"doc_id": "gutenberg_1342", "para_id": 2354, "text": "affection for his sister and her praise of himself, if he were already"}
{"doc_id": "gutenberg_1342", "para_id": 2355, "text": "“Mr. Collins,” said she, “speaks highly both of Lady Catherine and her"}
{"doc_id": "gutenberg_1342", "para_id": 2356, "text": "daughter; but, from some particulars that he has related of her"}
{"doc_id": "gutenberg_1342", "para_id": 2357, "text": "Ladyship, I suspect his gratitude misleads him; and that, in spite of"}
{"doc_id": "gutenberg_1342", "para_id": 2358, "text": "her being his patroness, she is an arrogant, conceited woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 2359, "text": "“I believe her to be both in a great degree,” replied Wickham; “I have"}
{"doc_id": "gutenberg_1342", "para_id": 2360, "text": "not seen her for many years; but I very well remember that I never liked"}
{"doc_id": "gutenberg_1342", "para_id": 2361, "text": "her, and that her manners were dictatorial and insolent. She has the"}
{"doc_id": "gutenberg_1342", "para_id": 2362, "text": "reputation of being remarkably sensible and clever; but I rather believe"}
{"doc_id": "gutenberg_1342", "para_id": 2363, "text": "she derives part of her abilities from her rank and fortune, part from"}
{"doc_id": "gutenberg_1342", "para_id": 2364, "text": "her authoritative manner, and the rest from the pride of her nephew, who"}
{"doc_id": "gutenberg_1342", "para_id": 2365, "text": "chooses that everyone connected with him should have an understanding of"}
{"doc_id": "gutenberg_1342", "para_id": 2366, "text": "Elizabeth allowed that he had given a very rational account of it, and"}
{"doc_id": "gutenberg_1342", "para_id": 2367, "text": "they continued talking together with mutual satisfaction till supper put"}
{"doc_id": "gutenberg_1342", "para_id": 2368, "text": "an end to cards, and gave the rest of the ladies their share of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2369, "text": "Wickham’s attentions. There could be no conversation in the noise of"}
{"doc_id": "gutenberg_1342", "para_id": 2370, "text": "Mrs. Philips’s supper party, but his manners recommended him to"}
{"doc_id": "gutenberg_1342", "para_id": 2371, "text": "everybody. Whatever he said, was said well; and whatever he did, done"}
{"doc_id": "gutenberg_1342", "para_id": 2372, "text": "gracefully. Elizabeth went away with her head full of him. She could"}
{"doc_id": "gutenberg_1342", "para_id": 2373, "text": "think of nothing but of Mr. Wickham, and of what he had told her, all"}
{"doc_id": "gutenberg_1342", "para_id": 2374, "text": "the way home; but there was not time for her even to mention his name as"}
{"doc_id": "gutenberg_1342", "para_id": 2375, "text": "they went, for neither Lydia nor Mr. Collins were once silent. Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 2376, "text": "talked incessantly of lottery tickets, of the fish she had lost and the"}
{"doc_id": "gutenberg_1342", "para_id": 2377, "text": "fish she had won; and Mr. Collins, in describing the civility of Mr. and"}
{"doc_id": "gutenberg_1342", "para_id": 2378, "text": "Mrs. Philips, protesting that he did not in the least regard his losses"}
{"doc_id": "gutenberg_1342", "para_id": 2379, "text": "at whist, enumerating all the dishes at supper, and repeatedly fearing"}
{"doc_id": "gutenberg_1342", "para_id": 2380, "text": "that he crowded his cousins, had more to say than he could well manage"}
{"doc_id": "gutenberg_1342", "para_id": 2381, "text": "Elizabeth related to Jane, the next day, what had passed between Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2382, "text": "Wickham and herself. Jane listened with astonishment and concern: she"}
{"doc_id": "gutenberg_1342", "para_id": 2383, "text": "knew not how to believe that Mr. Darcy could be so unworthy of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2384, "text": "Bingley’s regard; and yet it was not in her nature to question the"}
{"doc_id": "gutenberg_1342", "para_id": 2385, "text": "veracity of a young man of such amiable appearance as Wickham. The"}
{"doc_id": "gutenberg_1342", "para_id": 2386, "text": "possibility of his having really endured such unkindness was enough to"}
{"doc_id": "gutenberg_1342", "para_id": 2387, "text": "interest all her tender feelings; and nothing therefore remained to be"}
{"doc_id": "gutenberg_1342", "para_id": 2388, "text": "done but to think well of them both, to defend the conduct of each, and"}
{"doc_id": "gutenberg_1342", "para_id": 2389, "text": "throw into the account of accident or mistake whatever could not be"}
{"doc_id": "gutenberg_1342", "para_id": 2390, "text": "“They have both,” said she, “been deceived, I dare say, in some way or"}
{"doc_id": "gutenberg_1342", "para_id": 2391, "text": "other, of which we can form no idea. Interested people have perhaps"}
{"doc_id": "gutenberg_1342", "para_id": 2392, "text": "misrepresented each to the other. It is, in short, impossible for us to"}
{"doc_id": "gutenberg_1342", "para_id": 2393, "text": "conjecture the causes or circumstances which may have alienated them,"}
{"doc_id": "gutenberg_1342", "para_id": 2394, "text": "“Very true, indeed; and now, my dear Jane, what have you got to say in"}
{"doc_id": "gutenberg_1342", "para_id": 2395, "text": "behalf of the interested people who have probably been concerned in the"}
{"doc_id": "gutenberg_1342", "para_id": 2396, "text": "business? Do clear _them_, too, or we shall be obliged to think ill of"}
{"doc_id": "gutenberg_1342", "para_id": 2397, "text": "“Laugh as much as you choose, but you will not laugh me out of my"}
{"doc_id": "gutenberg_1342", "para_id": 2398, "text": "opinion. My dearest Lizzy, do but consider in what a disgraceful light"}
{"doc_id": "gutenberg_1342", "para_id": 2399, "text": "it places Mr. Darcy, to be treating his father’s favourite in such a"}
{"doc_id": "gutenberg_1342", "para_id": 2400, "text": "manner,--one whom his father had promised to provide for. It is"}
{"doc_id": "gutenberg_1342", "para_id": 2401, "text": "impossible. No man of common humanity, no man who had any value for his"}
{"doc_id": "gutenberg_1342", "para_id": 2402, "text": "character, could be capable of it. Can his most intimate friends be so"}
{"doc_id": "gutenberg_1342", "para_id": 2403, "text": "“I can much more easily believe Mr. Bingley’s being imposed on than that"}
{"doc_id": "gutenberg_1342", "para_id": 2404, "text": "Mr. Wickham should invent such a history of himself as he gave me last"}
{"doc_id": "gutenberg_1342", "para_id": 2405, "text": "night; names, facts, everything mentioned without ceremony. If it be not"}
{"doc_id": "gutenberg_1342", "para_id": 2406, "text": "so, let Mr. Darcy contradict it. Besides, there was truth in his looks.”"}
{"doc_id": "gutenberg_1342", "para_id": 2407, "text": "“It is difficult, indeed--it is distressing. One does not know what to"}
{"doc_id": "gutenberg_1342", "para_id": 2408, "text": "“I beg your pardon;--one knows exactly what to think.”"}
{"doc_id": "gutenberg_1342", "para_id": 2409, "text": "But Jane could think with certainty on only one point,--that Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2410, "text": "Bingley, if he _had been_ imposed on, would have much to suffer when"}
{"doc_id": "gutenberg_1342", "para_id": 2411, "text": "The two young ladies were summoned from the shrubbery, where this"}
{"doc_id": "gutenberg_1342", "para_id": 2412, "text": "conversation passed, by the arrival of some of the very persons of whom"}
{"doc_id": "gutenberg_1342", "para_id": 2413, "text": "they had been speaking; Mr. Bingley and his sisters came to give their"}
{"doc_id": "gutenberg_1342", "para_id": 2414, "text": "personal invitation for the long expected ball at Netherfield, which was"}
{"doc_id": "gutenberg_1342", "para_id": 2415, "text": "fixed for the following Tuesday. The two ladies were delighted to see"}
{"doc_id": "gutenberg_1342", "para_id": 2416, "text": "their dear friend again, called it an age since they had met, and"}
{"doc_id": "gutenberg_1342", "para_id": 2417, "text": "repeatedly asked what she had been doing with herself since their"}
{"doc_id": "gutenberg_1342", "para_id": 2418, "text": "separation. To the rest of the family they paid little attention;"}
{"doc_id": "gutenberg_1342", "para_id": 2419, "text": "avoiding Mrs. Bennet as much as possible, saying not much to Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 2420, "text": "and nothing at all to the others. They were soon gone again, rising from"}
{"doc_id": "gutenberg_1342", "para_id": 2421, "text": "their seats with an activity which took their brother by surprise, and"}
{"doc_id": "gutenberg_1342", "para_id": 2422, "text": "hurrying off as if eager to escape from Mrs. Bennet’s civilities."}
{"doc_id": "gutenberg_1342", "para_id": 2423, "text": "The prospect of the Netherfield ball was extremely agreeable to every"}
{"doc_id": "gutenberg_1342", "para_id": 2424, "text": "female of the family. Mrs. Bennet chose to consider it as given in"}
{"doc_id": "gutenberg_1342", "para_id": 2425, "text": "compliment to her eldest daughter, and was particularly flattered by"}
{"doc_id": "gutenberg_1342", "para_id": 2426, "text": "receiving the invitation from Mr. Bingley himself, instead of a"}
{"doc_id": "gutenberg_1342", "para_id": 2427, "text": "ceremonious card. Jane pictured to herself a happy evening in the"}
{"doc_id": "gutenberg_1342", "para_id": 2428, "text": "society of her two friends, and the attentions of their brother; and"}
{"doc_id": "gutenberg_1342", "para_id": 2429, "text": "Elizabeth thought with pleasure of dancing a great deal with Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2430, "text": "Wickham, and of seeing a confirmation of everything in Mr. Darcy’s look"}
{"doc_id": "gutenberg_1342", "para_id": 2431, "text": "and behaviour. The happiness anticipated by Catherine and Lydia depended"}
{"doc_id": "gutenberg_1342", "para_id": 2432, "text": "less on any single event, or any particular person; for though they"}
{"doc_id": "gutenberg_1342", "para_id": 2433, "text": "each, like Elizabeth, meant to dance half the evening with Mr. Wickham,"}
{"doc_id": "gutenberg_1342", "para_id": 2434, "text": "he was by no means the only partner who could satisfy them, and a ball"}
{"doc_id": "gutenberg_1342", "para_id": 2435, "text": "was, at any rate, a ball. And even Mary could assure her family that she"}
{"doc_id": "gutenberg_1342", "para_id": 2436, "text": "“While I can have my mornings to myself,” said she, “it is enough. I"}
{"doc_id": "gutenberg_1342", "para_id": 2437, "text": "think it is no sacrifice to join occasionally in evening engagements."}
{"doc_id": "gutenberg_1342", "para_id": 2438, "text": "Society has claims on us all; and I profess myself one of those who"}
{"doc_id": "gutenberg_1342", "para_id": 2439, "text": "consider intervals of recreation and amusement as desirable for"}
{"doc_id": "gutenberg_1342", "para_id": 2440, "text": "Elizabeth’s spirits were so high on the occasion, that though she did"}
{"doc_id": "gutenberg_1342", "para_id": 2441, "text": "not often speak unnecessarily to Mr. Collins, she could not help asking"}
{"doc_id": "gutenberg_1342", "para_id": 2442, "text": "him whether he intended to accept Mr. Bingley’s invitation, and if he"}
{"doc_id": "gutenberg_1342", "para_id": 2443, "text": "did, whether he would think it proper to join in the evening’s"}
{"doc_id": "gutenberg_1342", "para_id": 2444, "text": "amusement; and she was rather surprised to find that he entertained no"}
{"doc_id": "gutenberg_1342", "para_id": 2445, "text": "scruple whatever on that head, and was very far from dreading a rebuke,"}
{"doc_id": "gutenberg_1342", "para_id": 2446, "text": "either from the Archbishop or Lady Catherine de Bourgh, by venturing to"}
{"doc_id": "gutenberg_1342", "para_id": 2447, "text": "“I am by no means of opinion, I assure you,” said he, “that a ball of"}
{"doc_id": "gutenberg_1342", "para_id": 2448, "text": "this kind, given by a young man of character, to respectable people, can"}
{"doc_id": "gutenberg_1342", "para_id": 2449, "text": "have any evil tendency; and I am so far from objecting to dancing"}
{"doc_id": "gutenberg_1342", "para_id": 2450, "text": "myself, that I shall hope to be honoured with the hands of all my fair"}
{"doc_id": "gutenberg_1342", "para_id": 2451, "text": "cousins in the course of the evening; and I take this opportunity of"}
{"doc_id": "gutenberg_1342", "para_id": 2452, "text": "soliciting yours, Miss Elizabeth, for the two first dances especially; a"}
{"doc_id": "gutenberg_1342", "para_id": 2453, "text": "preference which I trust my cousin Jane will attribute to the right"}
{"doc_id": "gutenberg_1342", "para_id": 2454, "text": "Elizabeth felt herself completely taken in. She had fully proposed being"}
{"doc_id": "gutenberg_1342", "para_id": 2455, "text": "engaged by Wickham for those very dances; and to have Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 2456, "text": "instead!--her liveliness had been never worse timed. There was no help"}
{"doc_id": "gutenberg_1342", "para_id": 2457, "text": "for it, however. Mr. Wickham’s happiness and her own was perforce"}
{"doc_id": "gutenberg_1342", "para_id": 2458, "text": "delayed a little longer, and Mr. Collins’s proposal accepted with as"}
{"doc_id": "gutenberg_1342", "para_id": 2459, "text": "good a grace as she could. She was not the better pleased with his"}
{"doc_id": "gutenberg_1342", "para_id": 2460, "text": "gallantry, from the idea it suggested of something more. It now first"}
{"doc_id": "gutenberg_1342", "para_id": 2461, "text": "struck her, that _she_ was selected from among her sisters as worthy of"}
{"doc_id": "gutenberg_1342", "para_id": 2462, "text": "being the mistress of Hunsford Parsonage, and of assisting to form a"}
{"doc_id": "gutenberg_1342", "para_id": 2463, "text": "quadrille table at Rosings, in the absence of more eligible visitors."}
{"doc_id": "gutenberg_1342", "para_id": 2464, "text": "The idea soon reached to conviction, as she observed his increasing"}
{"doc_id": "gutenberg_1342", "para_id": 2465, "text": "civilities towards herself, and heard his frequent attempt at a"}
{"doc_id": "gutenberg_1342", "para_id": 2466, "text": "compliment on her wit and vivacity; and though more astonished than"}
{"doc_id": "gutenberg_1342", "para_id": 2467, "text": "gratified herself by this effect of her charms, it was not long before"}
{"doc_id": "gutenberg_1342", "para_id": 2468, "text": "her mother gave her to understand that the probability of their marriage"}
{"doc_id": "gutenberg_1342", "para_id": 2469, "text": "was exceedingly agreeable to _her_. Elizabeth, however, did not choose"}
{"doc_id": "gutenberg_1342", "para_id": 2470, "text": "to take the hint, being well aware that a serious dispute must be the"}
{"doc_id": "gutenberg_1342", "para_id": 2471, "text": "consequence of any reply. Mr. Collins might never make the offer, and,"}
{"doc_id": "gutenberg_1342", "para_id": 2472, "text": "If there had not been a Netherfield ball to prepare for and talk of, the"}
{"doc_id": "gutenberg_1342", "para_id": 2473, "text": "younger Miss Bennets would have been in a pitiable state at this time;"}
{"doc_id": "gutenberg_1342", "para_id": 2474, "text": "for, from the day of the invitation to the day of the ball, there was"}
{"doc_id": "gutenberg_1342", "para_id": 2475, "text": "such a succession of rain as prevented their walking to Meryton once. No"}
{"doc_id": "gutenberg_1342", "para_id": 2476, "text": "aunt, no officers, no news could be sought after; the very shoe-roses"}
{"doc_id": "gutenberg_1342", "para_id": 2477, "text": "for Netherfield were got by proxy. Even Elizabeth might have found some"}
{"doc_id": "gutenberg_1342", "para_id": 2478, "text": "trial of her patience in weather which totally suspended the improvement"}
{"doc_id": "gutenberg_1342", "para_id": 2479, "text": "of her acquaintance with Mr. Wickham; and nothing less than a dance on"}
{"doc_id": "gutenberg_1342", "para_id": 2480, "text": "Tuesday could have made such a Friday, Saturday, Sunday, and Monday"}
{"doc_id": "gutenberg_1342", "para_id": 2481, "text": "Till Elizabeth entered the drawing-room at Netherfield, and looked in"}
{"doc_id": "gutenberg_1342", "para_id": 2482, "text": "vain for Mr. Wickham among the cluster of red coats there assembled, a"}
{"doc_id": "gutenberg_1342", "para_id": 2483, "text": "doubt of his being present had never occurred to her. The certainty of"}
{"doc_id": "gutenberg_1342", "para_id": 2484, "text": "meeting him had not been checked by any of those recollections that"}
{"doc_id": "gutenberg_1342", "para_id": 2485, "text": "might not unreasonably have alarmed her. She had dressed with more than"}
{"doc_id": "gutenberg_1342", "para_id": 2486, "text": "usual care, and prepared in the highest spirits for the conquest of all"}
{"doc_id": "gutenberg_1342", "para_id": 2487, "text": "that remained unsubdued of his heart, trusting that it was not more than"}
{"doc_id": "gutenberg_1342", "para_id": 2488, "text": "might be won in the course of the evening. But in an instant arose the"}
{"doc_id": "gutenberg_1342", "para_id": 2489, "text": "dreadful suspicion of his being purposely omitted, for Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 2490, "text": "pleasure, in the Bingleys’ invitation to the officers; and though this"}
{"doc_id": "gutenberg_1342", "para_id": 2491, "text": "was not exactly the case, the absolute fact of his absence was"}
{"doc_id": "gutenberg_1342", "para_id": 2492, "text": "pronounced by his friend Mr. Denny, to whom Lydia eagerly applied, and"}
{"doc_id": "gutenberg_1342", "para_id": 2493, "text": "who told them that Wickham had been obliged to go to town on business"}
{"doc_id": "gutenberg_1342", "para_id": 2494, "text": "the day before, and was not yet returned; adding, with a significant"}
{"doc_id": "gutenberg_1342", "para_id": 2495, "text": "“I do not imagine his business would have called him away just now, if"}
{"doc_id": "gutenberg_1342", "para_id": 2496, "text": "he had not wished to avoid a certain gentleman here.”"}
{"doc_id": "gutenberg_1342", "para_id": 2497, "text": "This part of his intelligence, though unheard by Lydia, was caught by"}
{"doc_id": "gutenberg_1342", "para_id": 2498, "text": "Elizabeth; and, as it assured her that Darcy was not less answerable for"}
{"doc_id": "gutenberg_1342", "para_id": 2499, "text": "Wickham’s absence than if her first surmise had been just, every feeling"}
{"doc_id": "gutenberg_1342", "para_id": 2500, "text": "of displeasure against the former was so sharpened by immediate"}
{"doc_id": "gutenberg_1342", "para_id": 2501, "text": "disappointment, that she could hardly reply with tolerable civility to"}
{"doc_id": "gutenberg_1342", "para_id": 2502, "text": "the polite inquiries which he directly afterwards approached to make."}
{"doc_id": "gutenberg_1342", "para_id": 2503, "text": "Attention, forbearance, patience with Darcy, was injury to Wickham. She"}
{"doc_id": "gutenberg_1342", "para_id": 2504, "text": "was resolved against any sort of conversation with him, and turned away"}
{"doc_id": "gutenberg_1342", "para_id": 2505, "text": "with a degree of ill-humour which she could not wholly surmount even in"}
{"doc_id": "gutenberg_1342", "para_id": 2506, "text": "speaking to Mr. Bingley, whose blind partiality provoked her."}
{"doc_id": "gutenberg_1342", "para_id": 2507, "text": "But Elizabeth was not formed for ill-humour; and though every prospect"}
{"doc_id": "gutenberg_1342", "para_id": 2508, "text": "of her own was destroyed for the evening, it could not dwell long on her"}
{"doc_id": "gutenberg_1342", "para_id": 2509, "text": "spirits; and, having told all her griefs to Charlotte Lucas, whom she"}
{"doc_id": "gutenberg_1342", "para_id": 2510, "text": "had not seen for a week, she was soon able to make a voluntary"}
{"doc_id": "gutenberg_1342", "para_id": 2511, "text": "transition to the oddities of her cousin, and to point him out to her"}
{"doc_id": "gutenberg_1342", "para_id": 2512, "text": "particular notice. The two first dances, however, brought a return of"}
{"doc_id": "gutenberg_1342", "para_id": 2513, "text": "distress: they were dances of mortification. Mr. Collins, awkward and"}
{"doc_id": "gutenberg_1342", "para_id": 2514, "text": "solemn, apologizing instead of attending, and often moving wrong"}
{"doc_id": "gutenberg_1342", "para_id": 2515, "text": "without being aware of it, gave her all the shame and misery which a"}
{"doc_id": "gutenberg_1342", "para_id": 2516, "text": "disagreeable partner for a couple of dances can give. The moment of her"}
{"doc_id": "gutenberg_1342", "para_id": 2517, "text": "She danced next with an officer, and had the refreshment of talking of"}
{"doc_id": "gutenberg_1342", "para_id": 2518, "text": "Wickham, and of hearing that he was universally liked. When those dances"}
{"doc_id": "gutenberg_1342", "para_id": 2519, "text": "were over, she returned to Charlotte Lucas, and was in conversation with"}
{"doc_id": "gutenberg_1342", "para_id": 2520, "text": "her, when she found herself suddenly addressed by Mr. Darcy, who took"}
{"doc_id": "gutenberg_1342", "para_id": 2521, "text": "her so much by surprise in his application for her hand, that, without"}
{"doc_id": "gutenberg_1342", "para_id": 2522, "text": "knowing what she did, she accepted him. He walked away again"}
{"doc_id": "gutenberg_1342", "para_id": 2523, "text": "immediately, and she was left to fret over her own want of presence of"}
{"doc_id": "gutenberg_1342", "para_id": 2524, "text": "“Heaven forbid! _That_ would be the greatest misfortune of all! To find"}
{"doc_id": "gutenberg_1342", "para_id": 2525, "text": "a man agreeable whom one is determined to hate! Do not wish me such an"}
{"doc_id": "gutenberg_1342", "para_id": 2526, "text": "When the dancing recommenced, however, and Darcy approached to claim her"}
{"doc_id": "gutenberg_1342", "para_id": 2527, "text": "hand, Charlotte could not help cautioning her, in a whisper, not to be a"}
{"doc_id": "gutenberg_1342", "para_id": 2528, "text": "simpleton, and allow her fancy for Wickham to make her appear unpleasant"}
{"doc_id": "gutenberg_1342", "para_id": 2529, "text": "in the eyes of a man often times his consequence. Elizabeth made no"}
{"doc_id": "gutenberg_1342", "para_id": 2530, "text": "answer, and took her place in the set, amazed at the dignity to which"}
{"doc_id": "gutenberg_1342", "para_id": 2531, "text": "she was arrived in being allowed to stand opposite to Mr. Darcy, and"}
{"doc_id": "gutenberg_1342", "para_id": 2532, "text": "reading in her neighbours’ looks their equal amazement in beholding it."}
{"doc_id": "gutenberg_1342", "para_id": 2533, "text": "They stood for some time without speaking a word; and she began to"}
{"doc_id": "gutenberg_1342", "para_id": 2534, "text": "imagine that their silence was to last through the two dances, and, at"}
{"doc_id": "gutenberg_1342", "para_id": 2535, "text": "first, was resolved not to break it; till suddenly fancying that it"}
{"doc_id": "gutenberg_1342", "para_id": 2536, "text": "would be the greater punishment to her partner to oblige him to talk,"}
{"doc_id": "gutenberg_1342", "para_id": 2537, "text": "she made some slight observation on the dance. He replied, and was again"}
{"doc_id": "gutenberg_1342", "para_id": 2538, "text": "silent. After a pause of some minutes, she addressed him a second time,"}
{"doc_id": "gutenberg_1342", "para_id": 2539, "text": "“It is _your_ turn to say something now, Mr. Darcy. _I_ talked about the"}
{"doc_id": "gutenberg_1342", "para_id": 2540, "text": "dance, and _you_ ought to make some kind of remark on the size of the"}
{"doc_id": "gutenberg_1342", "para_id": 2541, "text": "He smiled, and assured her that whatever she wished him to say should be"}
{"doc_id": "gutenberg_1342", "para_id": 2542, "text": "“Very well; that reply will do for the present. Perhaps, by-and-by, I"}
{"doc_id": "gutenberg_1342", "para_id": 2543, "text": "may observe that private balls are much pleasanter than public ones; but"}
{"doc_id": "gutenberg_1342", "para_id": 2544, "text": "“Do you talk by rule, then, while you are dancing?”"}
{"doc_id": "gutenberg_1342", "para_id": 2545, "text": "“Sometimes. One must speak a little, you know. It would look odd to be"}
{"doc_id": "gutenberg_1342", "para_id": 2546, "text": "entirely silent for half an hour together; and yet, for the advantage of"}
{"doc_id": "gutenberg_1342", "para_id": 2547, "text": "_some_, conversation ought to be so arranged as that they may have the"}
{"doc_id": "gutenberg_1342", "para_id": 2548, "text": "“Are you consulting your own feelings in the present case, or do you"}
{"doc_id": "gutenberg_1342", "para_id": 2549, "text": "“Both,” replied Elizabeth archly; “for I have always seen a great"}
{"doc_id": "gutenberg_1342", "para_id": 2550, "text": "similarity in the turn of our minds. We are each of an unsocial,"}
{"doc_id": "gutenberg_1342", "para_id": 2551, "text": "taciturn disposition, unwilling to speak, unless we expect to say"}
{"doc_id": "gutenberg_1342", "para_id": 2552, "text": "something that will amaze the whole room, and be handed down to"}
{"doc_id": "gutenberg_1342", "para_id": 2553, "text": "“This is no very striking resemblance of your own character, I am sure,”"}
{"doc_id": "gutenberg_1342", "para_id": 2554, "text": "said he. “How near it may be to _mine_, I cannot pretend to say. _You_"}
{"doc_id": "gutenberg_1342", "para_id": 2555, "text": "He made no answer; and they were again silent till they had gone down"}
{"doc_id": "gutenberg_1342", "para_id": 2556, "text": "the dance, when he asked her if she and her sisters did not very often"}
{"doc_id": "gutenberg_1342", "para_id": 2557, "text": "walk to Meryton. She answered in the affirmative; and, unable to resist"}
{"doc_id": "gutenberg_1342", "para_id": 2558, "text": "the temptation, added, “When you met us there the other day, we had just"}
{"doc_id": "gutenberg_1342", "para_id": 2559, "text": "The effect was immediate. A deeper shade of _hauteur_ overspread his"}
{"doc_id": "gutenberg_1342", "para_id": 2560, "text": "features, but he said not a word; and Elizabeth, though blaming herself"}
{"doc_id": "gutenberg_1342", "para_id": 2561, "text": "for her own weakness, could not go on. At length Darcy spoke, and in a"}
{"doc_id": "gutenberg_1342", "para_id": 2562, "text": "“Mr. Wickham is blessed with such happy manners as may insure his"}
{"doc_id": "gutenberg_1342", "para_id": 2563, "text": "_making_ friends; whether he may be equally capable of _retaining_ them,"}
{"doc_id": "gutenberg_1342", "para_id": 2564, "text": "“He has been so unlucky as to lose your friendship,” replied Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 2565, "text": "with emphasis, “and in a manner which he is likely to suffer from all"}
{"doc_id": "gutenberg_1342", "para_id": 2566, "text": "Darcy made no answer, and seemed desirous of changing the subject. At"}
{"doc_id": "gutenberg_1342", "para_id": 2567, "text": "that moment Sir William Lucas appeared close to them, meaning to pass"}
{"doc_id": "gutenberg_1342", "para_id": 2568, "text": "through the set to the other side of the room; but, on perceiving Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2569, "text": "Darcy, he stopped, with a bow of superior courtesy, to compliment him on"}
{"doc_id": "gutenberg_1342", "para_id": 2570, "text": "“I have been most highly gratified, indeed, my dear sir; such very"}
{"doc_id": "gutenberg_1342", "para_id": 2571, "text": "superior dancing is not often seen. It is evident that you belong to the"}
{"doc_id": "gutenberg_1342", "para_id": 2572, "text": "first circles. Allow me to say, however, that your fair partner does not"}
{"doc_id": "gutenberg_1342", "para_id": 2573, "text": "disgrace you: and that I must hope to have this pleasure often repeated,"}
{"doc_id": "gutenberg_1342", "para_id": 2574, "text": "especially when a certain desirable event, my dear Miss Eliza (glancing"}
{"doc_id": "gutenberg_1342", "para_id": 2575, "text": "at her sister and Bingley), shall take place. What congratulations will"}
{"doc_id": "gutenberg_1342", "para_id": 2576, "text": "then flow in! I appeal to Mr. Darcy;--but let me not interrupt you, sir."}
{"doc_id": "gutenberg_1342", "para_id": 2577, "text": "You will not thank me for detaining you from the bewitching converse of"}
{"doc_id": "gutenberg_1342", "para_id": 2578, "text": "that young lady, whose bright eyes are also upbraiding me.”"}
{"doc_id": "gutenberg_1342", "para_id": 2579, "text": "The latter part of this address was scarcely heard by Darcy; but Sir"}
{"doc_id": "gutenberg_1342", "para_id": 2580, "text": "William’s allusion to his friend seemed to strike him forcibly, and his"}
{"doc_id": "gutenberg_1342", "para_id": 2581, "text": "eyes were directed, with a very serious expression, towards Bingley and"}
{"doc_id": "gutenberg_1342", "para_id": 2582, "text": "Jane, who were dancing together. Recovering himself, however, shortly,"}
{"doc_id": "gutenberg_1342", "para_id": 2583, "text": "“Sir William’s interruption has made me forget what we were talking"}
{"doc_id": "gutenberg_1342", "para_id": 2584, "text": "“I do not think we were speaking at all. Sir William could not have"}
{"doc_id": "gutenberg_1342", "para_id": 2585, "text": "interrupted any two people in the room who had less to say for"}
{"doc_id": "gutenberg_1342", "para_id": 2586, "text": "themselves. We have tried two or three subjects already without success,"}
{"doc_id": "gutenberg_1342", "para_id": 2587, "text": "“Books--oh no!--I am sure we never read the same, or not with the same"}
{"doc_id": "gutenberg_1342", "para_id": 2588, "text": "“I am sorry you think so; but if that be the case, there can at least be"}
{"doc_id": "gutenberg_1342", "para_id": 2589, "text": "no want of subject. We may compare our different opinions.”"}
{"doc_id": "gutenberg_1342", "para_id": 2590, "text": "“No--I cannot talk of books in a ball-room; my head is always full of"}
{"doc_id": "gutenberg_1342", "para_id": 2591, "text": "“The _present_ always occupies you in such scenes--does it?” said he,"}
{"doc_id": "gutenberg_1342", "para_id": 2592, "text": "“Yes, always,” she replied, without knowing what she said; for her"}
{"doc_id": "gutenberg_1342", "para_id": 2593, "text": "thoughts had wandered far from the subject, as soon afterwards appeared"}
{"doc_id": "gutenberg_1342", "para_id": 2594, "text": "by her suddenly exclaiming, “I remember hearing you once say, Mr. Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 2595, "text": "that you hardly ever forgave;--that your resentment, once created, was"}
{"doc_id": "gutenberg_1342", "para_id": 2596, "text": "unappeasable. You are very cautious, I suppose, as to its _being"}
{"doc_id": "gutenberg_1342", "para_id": 2597, "text": "“And never allow yourself to be blinded by prejudice?”"}
{"doc_id": "gutenberg_1342", "para_id": 2598, "text": "“It is particularly incumbent on those who never change their opinion,"}
{"doc_id": "gutenberg_1342", "para_id": 2599, "text": "“Merely to the illustration of _your_ character,” said she, endeavouring"}
{"doc_id": "gutenberg_1342", "para_id": 2600, "text": "to shake off her gravity. “I am trying to make it out.”"}
{"doc_id": "gutenberg_1342", "para_id": 2601, "text": "She shook her head. “I do not get on at all. I hear such different"}
{"doc_id": "gutenberg_1342", "para_id": 2602, "text": "“I can readily believe,” answered he, gravely, “that reports may vary"}
{"doc_id": "gutenberg_1342", "para_id": 2603, "text": "greatly with respect to me; and I could wish, Miss Bennet, that you were"}
{"doc_id": "gutenberg_1342", "para_id": 2604, "text": "not to sketch my character at the present moment, as there is reason to"}
{"doc_id": "gutenberg_1342", "para_id": 2605, "text": "fear that the performance would reflect no credit on either.”"}
{"doc_id": "gutenberg_1342", "para_id": 2606, "text": "“But if I do not take your likeness now, I may never have another"}
{"doc_id": "gutenberg_1342", "para_id": 2607, "text": "“I would by no means suspend any pleasure of yours,” he coldly replied."}
{"doc_id": "gutenberg_1342", "para_id": 2608, "text": "She said no more, and they went down the other dance and parted in"}
{"doc_id": "gutenberg_1342", "para_id": 2609, "text": "silence; on each side dissatisfied, though not to an equal degree; for"}
{"doc_id": "gutenberg_1342", "para_id": 2610, "text": "in Darcy’s breast there was a tolerably powerful feeling towards her,"}
{"doc_id": "gutenberg_1342", "para_id": 2611, "text": "which soon procured her pardon, and directed all his anger against"}
{"doc_id": "gutenberg_1342", "para_id": 2612, "text": "They had not long separated when Miss Bingley came towards her, and,"}
{"doc_id": "gutenberg_1342", "para_id": 2613, "text": "with an expression of civil disdain, thus accosted her,--"}
{"doc_id": "gutenberg_1342", "para_id": 2614, "text": "“So, Miss Eliza, I hear you are quite delighted with George Wickham?"}
{"doc_id": "gutenberg_1342", "para_id": 2615, "text": "Your sister has been talking to me about him, and asking me a thousand"}
{"doc_id": "gutenberg_1342", "para_id": 2616, "text": "questions; and I find that the young man forgot to tell you, among his"}
{"doc_id": "gutenberg_1342", "para_id": 2617, "text": "other communications, that he was the son of old Wickham, the late Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2618, "text": "Darcy’s steward. Let me recommend you, however, as a friend, not to give"}
{"doc_id": "gutenberg_1342", "para_id": 2619, "text": "implicit confidence to all his assertions; for, as to Mr. Darcy’s using"}
{"doc_id": "gutenberg_1342", "para_id": 2620, "text": "him ill, it is perfectly false: for, on the contrary, he has been always"}
{"doc_id": "gutenberg_1342", "para_id": 2621, "text": "remarkably kind to him, though George Wickham has treated Mr. Darcy in a"}
{"doc_id": "gutenberg_1342", "para_id": 2622, "text": "most infamous manner. I do not know the particulars, but I know very"}
{"doc_id": "gutenberg_1342", "para_id": 2623, "text": "well that Mr. Darcy is not in the least to blame; that he cannot bear"}
{"doc_id": "gutenberg_1342", "para_id": 2624, "text": "to hear George Wickham mentioned; and that though my brother thought he"}
{"doc_id": "gutenberg_1342", "para_id": 2625, "text": "could not well avoid including him in his invitation to the officers, he"}
{"doc_id": "gutenberg_1342", "para_id": 2626, "text": "was excessively glad to find that he had taken himself out of the way."}
{"doc_id": "gutenberg_1342", "para_id": 2627, "text": "His coming into the country at all is a most insolent thing, indeed, and"}
{"doc_id": "gutenberg_1342", "para_id": 2628, "text": "I wonder how he could presume to do it. I pity you, Miss Eliza, for this"}
{"doc_id": "gutenberg_1342", "para_id": 2629, "text": "discovery of your favourite’s guilt; but really, considering his"}
{"doc_id": "gutenberg_1342", "para_id": 2630, "text": "“His guilt and his descent appear, by your account, to be the same,”"}
{"doc_id": "gutenberg_1342", "para_id": 2631, "text": "said Elizabeth, angrily; “for I have heard you accuse him of nothing"}
{"doc_id": "gutenberg_1342", "para_id": 2632, "text": "worse than of being the son of Mr. Darcy’s steward, and of _that_, I can"}
{"doc_id": "gutenberg_1342", "para_id": 2633, "text": "“I beg your pardon,” replied Miss Bingley, turning away with a sneer."}
{"doc_id": "gutenberg_1342", "para_id": 2634, "text": "“Insolent girl!” said Elizabeth to herself. “You are much mistaken if"}
{"doc_id": "gutenberg_1342", "para_id": 2635, "text": "you expect to influence me by such a paltry attack as this. I see"}
{"doc_id": "gutenberg_1342", "para_id": 2636, "text": "nothing in it but your own wilful ignorance and the malice of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2637, "text": "Darcy.” She then sought her eldest sister, who had undertaken to make"}
{"doc_id": "gutenberg_1342", "para_id": 2638, "text": "inquiries on the same subject of Bingley. Jane met her with a smile of"}
{"doc_id": "gutenberg_1342", "para_id": 2639, "text": "such sweet complacency, a glow of such happy expression, as sufficiently"}
{"doc_id": "gutenberg_1342", "para_id": 2640, "text": "marked how well she was satisfied with the occurrences of the evening."}
{"doc_id": "gutenberg_1342", "para_id": 2641, "text": "Elizabeth instantly read her feelings; and, at that moment, solicitude"}
{"doc_id": "gutenberg_1342", "para_id": 2642, "text": "for Wickham, resentment against his enemies, and everything else, gave"}
{"doc_id": "gutenberg_1342", "para_id": 2643, "text": "way before the hope of Jane’s being in the fairest way for happiness."}
{"doc_id": "gutenberg_1342", "para_id": 2644, "text": "“I want to know,” said she, with a countenance no less smiling than her"}
{"doc_id": "gutenberg_1342", "para_id": 2645, "text": "sister’s, “what you have learnt about Mr. Wickham. But perhaps you have"}
{"doc_id": "gutenberg_1342", "para_id": 2646, "text": "been too pleasantly engaged to think of any third person, in which case"}
{"doc_id": "gutenberg_1342", "para_id": 2647, "text": "“No,” replied Jane, “I have not forgotten him; but I have nothing"}
{"doc_id": "gutenberg_1342", "para_id": 2648, "text": "satisfactory to tell you. Mr. Bingley does not know the whole of his"}
{"doc_id": "gutenberg_1342", "para_id": 2649, "text": "history, and is quite ignorant of the circumstances which have"}
{"doc_id": "gutenberg_1342", "para_id": 2650, "text": "principally offended Mr. Darcy; but he will vouch for the good conduct,"}
{"doc_id": "gutenberg_1342", "para_id": 2651, "text": "the probity and honour, of his friend, and is perfectly convinced that"}
{"doc_id": "gutenberg_1342", "para_id": 2652, "text": "Mr. Wickham has deserved much less attention from Mr. Darcy than he has"}
{"doc_id": "gutenberg_1342", "para_id": 2653, "text": "received; and I am sorry to say that by his account, as well as his"}
{"doc_id": "gutenberg_1342", "para_id": 2654, "text": "sister’s, Mr. Wickham is by no means a respectable young man. I am"}
{"doc_id": "gutenberg_1342", "para_id": 2655, "text": "afraid he has been very imprudent, and has deserved to lose Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 2656, "text": "“No; he never saw him till the other morning at Meryton.”"}
{"doc_id": "gutenberg_1342", "para_id": 2657, "text": "“This account then is what he has received from Mr. Darcy. I am"}
{"doc_id": "gutenberg_1342", "para_id": 2658, "text": "perfectly satisfied. But what does he say of the living?”"}
{"doc_id": "gutenberg_1342", "para_id": 2659, "text": "“He does not exactly recollect the circumstances, though he has heard"}
{"doc_id": "gutenberg_1342", "para_id": 2660, "text": "them from Mr. Darcy more than once, but he believes that it was left to"}
{"doc_id": "gutenberg_1342", "para_id": 2661, "text": "“I have not a doubt of Mr. Bingley’s sincerity,” said Elizabeth warmly,"}
{"doc_id": "gutenberg_1342", "para_id": 2662, "text": "“but you must excuse my not being convinced by assurances only. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2663, "text": "Bingley’s defence of his friend was a very able one, I dare say; but"}
{"doc_id": "gutenberg_1342", "para_id": 2664, "text": "since he is unacquainted with several parts of the story, and has learnt"}
{"doc_id": "gutenberg_1342", "para_id": 2665, "text": "the rest from that friend himself, I shall venture still to think of"}
{"doc_id": "gutenberg_1342", "para_id": 2666, "text": "She then changed the discourse to one more gratifying to each, and on"}
{"doc_id": "gutenberg_1342", "para_id": 2667, "text": "which there could be no difference of sentiment. Elizabeth listened with"}
{"doc_id": "gutenberg_1342", "para_id": 2668, "text": "delight to the happy though modest hopes which Jane entertained of"}
{"doc_id": "gutenberg_1342", "para_id": 2669, "text": "Bingley’s regard, and said all in her power to heighten her confidence"}
{"doc_id": "gutenberg_1342", "para_id": 2670, "text": "in it. On their being joined by Mr. Bingley himself, Elizabeth withdrew"}
{"doc_id": "gutenberg_1342", "para_id": 2671, "text": "to Miss Lucas; to whose inquiry after the pleasantness of her last"}
{"doc_id": "gutenberg_1342", "para_id": 2672, "text": "partner she had scarcely replied, before Mr. Collins came up to them,"}
{"doc_id": "gutenberg_1342", "para_id": 2673, "text": "and told her with great exultation, that he had just been so fortunate"}
{"doc_id": "gutenberg_1342", "para_id": 2674, "text": "“I have found out,” said he, “by a singular accident, that there is now"}
{"doc_id": "gutenberg_1342", "para_id": 2675, "text": "in the room a near relation to my patroness. I happened to overhear the"}
{"doc_id": "gutenberg_1342", "para_id": 2676, "text": "gentleman himself mentioning to the young lady who does the honours of"}
{"doc_id": "gutenberg_1342", "para_id": 2677, "text": "this house the names of his cousin Miss De Bourgh, and of her mother,"}
{"doc_id": "gutenberg_1342", "para_id": 2678, "text": "Lady Catherine. How wonderfully these sort of things occur! Who would"}
{"doc_id": "gutenberg_1342", "para_id": 2679, "text": "have thought of my meeting with--perhaps--a nephew of Lady Catherine de"}
{"doc_id": "gutenberg_1342", "para_id": 2680, "text": "Bourgh in this assembly! I am most thankful that the discovery is made"}
{"doc_id": "gutenberg_1342", "para_id": 2681, "text": "in time for me to pay my respects to him, which I am now going to do,"}
{"doc_id": "gutenberg_1342", "para_id": 2682, "text": "and trust he will excuse my not having done it before. My total"}
{"doc_id": "gutenberg_1342", "para_id": 2683, "text": "ignorance of the connection must plead my apology.”"}
{"doc_id": "gutenberg_1342", "para_id": 2684, "text": "“You are not going to introduce yourself to Mr. Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 2685, "text": "“Indeed I am. I shall entreat his pardon for not having done it earlier."}
{"doc_id": "gutenberg_1342", "para_id": 2686, "text": "I believe him to be Lady Catherine’s _nephew_. It will be in my power to"}
{"doc_id": "gutenberg_1342", "para_id": 2687, "text": "assure him that her Ladyship was quite well yesterday se’nnight.”"}
{"doc_id": "gutenberg_1342", "para_id": 2688, "text": "Elizabeth tried hard to dissuade him from such a scheme; assuring him"}
{"doc_id": "gutenberg_1342", "para_id": 2689, "text": "that Mr. Darcy would consider his addressing him without introduction as"}
{"doc_id": "gutenberg_1342", "para_id": 2690, "text": "an impertinent freedom, rather than a compliment to his aunt; that it"}
{"doc_id": "gutenberg_1342", "para_id": 2691, "text": "was not in the least necessary there should be any notice on either"}
{"doc_id": "gutenberg_1342", "para_id": 2692, "text": "side, and that if it were, it must belong to Mr. Darcy, the superior in"}
{"doc_id": "gutenberg_1342", "para_id": 2693, "text": "consequence, to begin the acquaintance. Mr. Collins listened to her with"}
{"doc_id": "gutenberg_1342", "para_id": 2694, "text": "the determined air of following his own inclination, and when she ceased"}
{"doc_id": "gutenberg_1342", "para_id": 2695, "text": "“My dear Miss Elizabeth, I have the highest opinion in the world of your"}
{"doc_id": "gutenberg_1342", "para_id": 2696, "text": "excellent judgment in all matters within the scope of your"}
{"doc_id": "gutenberg_1342", "para_id": 2697, "text": "understanding, but permit me to say that there must be a wide difference"}
{"doc_id": "gutenberg_1342", "para_id": 2698, "text": "between the established forms of ceremony amongst the laity and those"}
{"doc_id": "gutenberg_1342", "para_id": 2699, "text": "which regulate the clergy; for, give me leave to observe that I consider"}
{"doc_id": "gutenberg_1342", "para_id": 2700, "text": "the clerical office as equal in point of dignity with the highest rank"}
{"doc_id": "gutenberg_1342", "para_id": 2701, "text": "in the kingdom--provided that a proper humility of behaviour is at the"}
{"doc_id": "gutenberg_1342", "para_id": 2702, "text": "same time maintained. You must, therefore, allow me to follow the"}
{"doc_id": "gutenberg_1342", "para_id": 2703, "text": "dictates of my conscience on this occasion, which lead me to perform"}
{"doc_id": "gutenberg_1342", "para_id": 2704, "text": "what I look on as a point of duty. Pardon me for neglecting to profit by"}
{"doc_id": "gutenberg_1342", "para_id": 2705, "text": "your advice, which on every other subject shall be my constant guide,"}
{"doc_id": "gutenberg_1342", "para_id": 2706, "text": "though in the case before us I consider myself more fitted by education"}
{"doc_id": "gutenberg_1342", "para_id": 2707, "text": "and habitual study to decide on what is right than a young lady like"}
{"doc_id": "gutenberg_1342", "para_id": 2708, "text": "yourself;” and with a low bow he left her to attack Mr. Darcy, whose"}
{"doc_id": "gutenberg_1342", "para_id": 2709, "text": "reception of his advances she eagerly watched, and whose astonishment at"}
{"doc_id": "gutenberg_1342", "para_id": 2710, "text": "being so addressed was very evident. Her cousin prefaced his speech with"}
{"doc_id": "gutenberg_1342", "para_id": 2711, "text": "a solemn bow, and though she could not hear a word of it, she felt as if"}
{"doc_id": "gutenberg_1342", "para_id": 2712, "text": "hearing it all, and saw in the motion of his lips the words “apology,”"}
{"doc_id": "gutenberg_1342", "para_id": 2713, "text": "“Hunsford,” and “Lady Catherine de Bourgh.” It vexed her to see him"}
{"doc_id": "gutenberg_1342", "para_id": 2714, "text": "expose himself to such a man. Mr. Darcy was eyeing him with"}
{"doc_id": "gutenberg_1342", "para_id": 2715, "text": "unrestrained wonder; and when at last Mr. Collins allowed him to speak,"}
{"doc_id": "gutenberg_1342", "para_id": 2716, "text": "replied with an air of distant civility. Mr. Collins, however, was not"}
{"doc_id": "gutenberg_1342", "para_id": 2717, "text": "discouraged from speaking again, and Mr. Darcy’s contempt seemed"}
{"doc_id": "gutenberg_1342", "para_id": 2718, "text": "abundantly increasing with the length of his second speech; and at the"}
{"doc_id": "gutenberg_1342", "para_id": 2719, "text": "end of it he only made him a slight bow, and moved another way: Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2720, "text": "“I have no reason, I assure you,” said he, “to be dissatisfied with my"}
{"doc_id": "gutenberg_1342", "para_id": 2721, "text": "reception. Mr. Darcy seemed much pleased with the attention. He answered"}
{"doc_id": "gutenberg_1342", "para_id": 2722, "text": "me with the utmost civility, and even paid me the compliment of saying,"}
{"doc_id": "gutenberg_1342", "para_id": 2723, "text": "that he was so well convinced of Lady Catherine’s discernment as to be"}
{"doc_id": "gutenberg_1342", "para_id": 2724, "text": "certain she could never bestow a favour unworthily. It was really a very"}
{"doc_id": "gutenberg_1342", "para_id": 2725, "text": "handsome thought. Upon the whole, I am much pleased with him.”"}
{"doc_id": "gutenberg_1342", "para_id": 2726, "text": "As Elizabeth had no longer any interest of her own to pursue, she turned"}
{"doc_id": "gutenberg_1342", "para_id": 2727, "text": "her attention almost entirely on her sister and Mr. Bingley; and the"}
{"doc_id": "gutenberg_1342", "para_id": 2728, "text": "train of agreeable reflections which her observations gave birth to made"}
{"doc_id": "gutenberg_1342", "para_id": 2729, "text": "her perhaps almost as happy as Jane. She saw her in idea settled in that"}
{"doc_id": "gutenberg_1342", "para_id": 2730, "text": "very house, in all the felicity which a marriage of true affection could"}
{"doc_id": "gutenberg_1342", "para_id": 2731, "text": "bestow; and she felt capable, under such circumstances, of endeavouring"}
{"doc_id": "gutenberg_1342", "para_id": 2732, "text": "even to like Bingley’s two sisters. Her mother’s thoughts she plainly"}
{"doc_id": "gutenberg_1342", "para_id": 2733, "text": "saw were bent the same way, and she determined not to venture near her,"}
{"doc_id": "gutenberg_1342", "para_id": 2734, "text": "lest she might hear too much. When they sat down to supper, therefore,"}
{"doc_id": "gutenberg_1342", "para_id": 2735, "text": "she considered it a most unlucky perverseness which placed them within"}
{"doc_id": "gutenberg_1342", "para_id": 2736, "text": "one of each other; and deeply was she vexed to find that her mother was"}
{"doc_id": "gutenberg_1342", "para_id": 2737, "text": "talking to that one person (Lady Lucas) freely, openly, and of nothing"}
{"doc_id": "gutenberg_1342", "para_id": 2738, "text": "else but of her expectation that Jane would be soon married to Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2739, "text": "Bingley. It was an animating subject, and Mrs. Bennet seemed incapable"}
{"doc_id": "gutenberg_1342", "para_id": 2740, "text": "of fatigue while enumerating the advantages of the match. His being such"}
{"doc_id": "gutenberg_1342", "para_id": 2741, "text": "a charming young man, and so rich, and living but three miles from them,"}
{"doc_id": "gutenberg_1342", "para_id": 2742, "text": "were the first points of self-gratulation; and then it was such a"}
{"doc_id": "gutenberg_1342", "para_id": 2743, "text": "comfort to think how fond the two sisters were of Jane, and to be"}
{"doc_id": "gutenberg_1342", "para_id": 2744, "text": "certain that they must desire the connection as much as she could do. It"}
{"doc_id": "gutenberg_1342", "para_id": 2745, "text": "was, moreover, such a promising thing for her younger daughters, as"}
{"doc_id": "gutenberg_1342", "para_id": 2746, "text": "Jane’s marrying so greatly must throw them in the way of other rich men;"}
{"doc_id": "gutenberg_1342", "para_id": 2747, "text": "and, lastly, it was so pleasant at her time of life to be able to"}
{"doc_id": "gutenberg_1342", "para_id": 2748, "text": "consign her single daughters to the care of their sister, that she might"}
{"doc_id": "gutenberg_1342", "para_id": 2749, "text": "not be obliged to go into company more than she liked. It was necessary"}
{"doc_id": "gutenberg_1342", "para_id": 2750, "text": "to make this circumstance a matter of pleasure, because on such"}
{"doc_id": "gutenberg_1342", "para_id": 2751, "text": "occasions it is the etiquette; but no one was less likely than Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 2752, "text": "Bennet to find comfort in staying at home at any period of her life. She"}
{"doc_id": "gutenberg_1342", "para_id": 2753, "text": "concluded with many good wishes that Lady Lucas might soon be equally"}
{"doc_id": "gutenberg_1342", "para_id": 2754, "text": "fortunate, though evidently and triumphantly believing there was no"}
{"doc_id": "gutenberg_1342", "para_id": 2755, "text": "In vain did Elizabeth endeavour to check the rapidity of her mother’s"}
{"doc_id": "gutenberg_1342", "para_id": 2756, "text": "words, or persuade her to describe her felicity in a less audible"}
{"doc_id": "gutenberg_1342", "para_id": 2757, "text": "whisper; for to her inexpressible vexation she could perceive that the"}
{"doc_id": "gutenberg_1342", "para_id": 2758, "text": "chief of it was overheard by Mr. Darcy, who sat opposite to them. Her"}
{"doc_id": "gutenberg_1342", "para_id": 2759, "text": "“What is Mr. Darcy to me, pray, that I should be afraid of him? I am"}
{"doc_id": "gutenberg_1342", "para_id": 2760, "text": "sure we owe him no such particular civility as to be obliged to say"}
{"doc_id": "gutenberg_1342", "para_id": 2761, "text": "“For heaven’s sake, madam, speak lower. What advantage can it be to you"}
{"doc_id": "gutenberg_1342", "para_id": 2762, "text": "to offend Mr. Darcy? You will never recommend yourself to his friend by"}
{"doc_id": "gutenberg_1342", "para_id": 2763, "text": "Nothing that she could say, however, had any influence. Her mother would"}
{"doc_id": "gutenberg_1342", "para_id": 2764, "text": "talk of her views in the same intelligible tone. Elizabeth blushed and"}
{"doc_id": "gutenberg_1342", "para_id": 2765, "text": "blushed again with shame and vexation. She could not help frequently"}
{"doc_id": "gutenberg_1342", "para_id": 2766, "text": "glancing her eye at Mr. Darcy, though every glance convinced her of what"}
{"doc_id": "gutenberg_1342", "para_id": 2767, "text": "she dreaded; for though he was not always looking at her mother, she was"}
{"doc_id": "gutenberg_1342", "para_id": 2768, "text": "convinced that his attention was invariably fixed by her. The expression"}
{"doc_id": "gutenberg_1342", "para_id": 2769, "text": "of his face changed gradually from indignant contempt to a composed and"}
{"doc_id": "gutenberg_1342", "para_id": 2770, "text": "At length, however, Mrs. Bennet had no more to say; and Lady Lucas, who"}
{"doc_id": "gutenberg_1342", "para_id": 2771, "text": "had been long yawning at the repetition of delights which she saw no"}
{"doc_id": "gutenberg_1342", "para_id": 2772, "text": "likelihood of sharing, was left to the comforts of cold ham and chicken."}
{"doc_id": "gutenberg_1342", "para_id": 2773, "text": "Elizabeth now began to revive. But not long was the interval of"}
{"doc_id": "gutenberg_1342", "para_id": 2774, "text": "tranquillity; for when supper was over, singing was talked of, and she"}
{"doc_id": "gutenberg_1342", "para_id": 2775, "text": "had the mortification of seeing Mary, after very little entreaty,"}
{"doc_id": "gutenberg_1342", "para_id": 2776, "text": "preparing to oblige the company. By many significant looks and silent"}
{"doc_id": "gutenberg_1342", "para_id": 2777, "text": "entreaties did she endeavour to prevent such a proof of"}
{"doc_id": "gutenberg_1342", "para_id": 2778, "text": "complaisance,--but in vain; Mary would not understand them; such an"}
{"doc_id": "gutenberg_1342", "para_id": 2779, "text": "opportunity of exhibiting was delightful to her, and she began her song."}
{"doc_id": "gutenberg_1342", "para_id": 2780, "text": "Elizabeth’s eyes were fixed on her, with most painful sensations; and"}
{"doc_id": "gutenberg_1342", "para_id": 2781, "text": "she watched her progress through the several stanzas with an impatience"}
{"doc_id": "gutenberg_1342", "para_id": 2782, "text": "which was very ill rewarded at their close; for Mary, on receiving"}
{"doc_id": "gutenberg_1342", "para_id": 2783, "text": "amongst the thanks of the table the hint of a hope that she might be"}
{"doc_id": "gutenberg_1342", "para_id": 2784, "text": "prevailed on to favour them again, after the pause of half a minute"}
{"doc_id": "gutenberg_1342", "para_id": 2785, "text": "began another. Mary’s powers were by no means fitted for such a display;"}
{"doc_id": "gutenberg_1342", "para_id": 2786, "text": "her voice was weak, and her manner affected. Elizabeth was in agonies."}
{"doc_id": "gutenberg_1342", "para_id": 2787, "text": "She looked at Jane to see how she bore it; but Jane was very composedly"}
{"doc_id": "gutenberg_1342", "para_id": 2788, "text": "talking to Bingley. She looked at his two sisters, and saw them making"}
{"doc_id": "gutenberg_1342", "para_id": 2789, "text": "signs of derision at each other, and at Darcy, who continued, however,"}
{"doc_id": "gutenberg_1342", "para_id": 2790, "text": "impenetrably grave. She looked at her father to entreat his"}
{"doc_id": "gutenberg_1342", "para_id": 2791, "text": "interference, lest Mary should be singing all night. He took the hint,"}
{"doc_id": "gutenberg_1342", "para_id": 2792, "text": "and, when Mary had finished her second song, said aloud,--"}
{"doc_id": "gutenberg_1342", "para_id": 2793, "text": "“That will do extremely well, child. You have delighted us long enough."}
{"doc_id": "gutenberg_1342", "para_id": 2794, "text": "Mary, though pretending not to hear, was somewhat disconcerted; and"}
{"doc_id": "gutenberg_1342", "para_id": 2795, "text": "Elizabeth, sorry for her, and sorry for her father’s speech, was afraid"}
{"doc_id": "gutenberg_1342", "para_id": 2796, "text": "her anxiety had done no good. Others of the party were now applied to."}
{"doc_id": "gutenberg_1342", "para_id": 2797, "text": "“If I,” said Mr. Collins, “were so fortunate as to be able to sing, I"}
{"doc_id": "gutenberg_1342", "para_id": 2798, "text": "should have great pleasure, I am sure, in obliging the company with an"}
{"doc_id": "gutenberg_1342", "para_id": 2799, "text": "air; for I consider music as a very innocent diversion, and perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 2800, "text": "compatible with the profession of a clergyman. I do not mean, however,"}
{"doc_id": "gutenberg_1342", "para_id": 2801, "text": "to assert that we can be justified in devoting too much of our time to"}
{"doc_id": "gutenberg_1342", "para_id": 2802, "text": "music, for there are certainly other things to be attended to. The"}
{"doc_id": "gutenberg_1342", "para_id": 2803, "text": "rector of a parish has much to do. In the first place, he must make such"}
{"doc_id": "gutenberg_1342", "para_id": 2804, "text": "an agreement for tithes as may be beneficial to himself and not"}
{"doc_id": "gutenberg_1342", "para_id": 2805, "text": "offensive to his patron. He must write his own sermons; and the time"}
{"doc_id": "gutenberg_1342", "para_id": 2806, "text": "that remains will not be too much for his parish duties, and the care"}
{"doc_id": "gutenberg_1342", "para_id": 2807, "text": "and improvement of his dwelling, which he cannot be excused from making"}
{"doc_id": "gutenberg_1342", "para_id": 2808, "text": "as comfortable as possible. And I do not think it of light importance"}
{"doc_id": "gutenberg_1342", "para_id": 2809, "text": "that he should have attentive and conciliatory manners towards"}
{"doc_id": "gutenberg_1342", "para_id": 2810, "text": "everybody, especially towards those to whom he owes his preferment. I"}
{"doc_id": "gutenberg_1342", "para_id": 2811, "text": "cannot acquit him of that duty; nor could I think well of the man who"}
{"doc_id": "gutenberg_1342", "para_id": 2812, "text": "should omit an occasion of testifying his respect towards anybody"}
{"doc_id": "gutenberg_1342", "para_id": 2813, "text": "connected with the family.” And with a bow to Mr. Darcy, he concluded"}
{"doc_id": "gutenberg_1342", "para_id": 2814, "text": "his speech, which had been spoken so loud as to be heard by half the"}
{"doc_id": "gutenberg_1342", "para_id": 2815, "text": "room. Many stared--many smiled; but no one looked more amused than Mr."}
{"doc_id": "gutenberg_1342", "para_id": 2816, "text": "Bennet himself, while his wife seriously commended Mr. Collins for"}
{"doc_id": "gutenberg_1342", "para_id": 2817, "text": "having spoken so sensibly, and observed, in a half-whisper to Lady"}
{"doc_id": "gutenberg_1342", "para_id": 2818, "text": "Lucas, that he was a remarkably clever, good kind of young man."}
{"doc_id": "gutenberg_1342", "para_id": 2819, "text": "To Elizabeth it appeared, that had her family made an agreement to"}
{"doc_id": "gutenberg_1342", "para_id": 2820, "text": "expose themselves as much as they could during the evening, it would"}
{"doc_id": "gutenberg_1342", "para_id": 2821, "text": "have been impossible for them to play their parts with more spirit, or"}
{"doc_id": "gutenberg_1342", "para_id": 2822, "text": "finer success; and happy did she think it for Bingley and her sister"}
{"doc_id": "gutenberg_1342", "para_id": 2823, "text": "that some of the exhibition had escaped his notice, and that his"}
{"doc_id": "gutenberg_1342", "para_id": 2824, "text": "feelings were not of a sort to be much distressed by the folly which he"}
{"doc_id": "gutenberg_1342", "para_id": 2825, "text": "must have witnessed. That his two sisters and Mr. Darcy, however, should"}
{"doc_id": "gutenberg_1342", "para_id": 2826, "text": "have such an opportunity of ridiculing her relations was bad enough; and"}
{"doc_id": "gutenberg_1342", "para_id": 2827, "text": "she could not determine whether the silent contempt of the gentleman, or"}
{"doc_id": "gutenberg_1342", "para_id": 2828, "text": "the insolent smiles of the ladies, were more intolerable."}
{"doc_id": "gutenberg_1342", "para_id": 2829, "text": "The rest of the evening brought her little amusement. She was teased by"}
{"doc_id": "gutenberg_1342", "para_id": 2830, "text": "Mr. Collins, who continued most perseveringly by her side; and though he"}
{"doc_id": "gutenberg_1342", "para_id": 2831, "text": "could not prevail with her to dance with him again, put it out of her"}
{"doc_id": "gutenberg_1342", "para_id": 2832, "text": "power to dance with others. In vain did she entreat him to stand up with"}
{"doc_id": "gutenberg_1342", "para_id": 2833, "text": "somebody else, and offered to introduce him to any young lady in the"}
{"doc_id": "gutenberg_1342", "para_id": 2834, "text": "room. He assured her that, as to dancing, he was perfectly indifferent"}
{"doc_id": "gutenberg_1342", "para_id": 2835, "text": "to it; that his chief object was, by delicate attentions, to recommend"}
{"doc_id": "gutenberg_1342", "para_id": 2836, "text": "himself to her; and that he should therefore make a point of remaining"}
{"doc_id": "gutenberg_1342", "para_id": 2837, "text": "close to her the whole evening. There was no arguing upon such a"}
{"doc_id": "gutenberg_1342", "para_id": 2838, "text": "project. She owed her greatest relief to her friend Miss Lucas, who"}
{"doc_id": "gutenberg_1342", "para_id": 2839, "text": "often joined them, and good-naturedly engaged Mr. Collins’s conversation"}
{"doc_id": "gutenberg_1342", "para_id": 2840, "text": "She was at least free from the offence of Mr. Darcy’s further notice:"}
{"doc_id": "gutenberg_1342", "para_id": 2841, "text": "though often standing within a very short distance of her, quite"}
{"doc_id": "gutenberg_1342", "para_id": 2842, "text": "disengaged, he never came near enough to speak. She felt it to be the"}
{"doc_id": "gutenberg_1342", "para_id": 2843, "text": "probable consequence of her allusions to Mr. Wickham, and rejoiced in"}
{"doc_id": "gutenberg_1342", "para_id": 2844, "text": "The Longbourn party were the last of all the company to depart; and by a"}
{"doc_id": "gutenberg_1342", "para_id": 2845, "text": "manœuvre of Mrs. Bennet had to wait for their carriage a quarter of an"}
{"doc_id": "gutenberg_1342", "para_id": 2846, "text": "hour after everybody else was gone, which gave them time to see how"}
{"doc_id": "gutenberg_1342", "para_id": 2847, "text": "heartily they were wished away by some of the family. Mrs. Hurst and her"}
{"doc_id": "gutenberg_1342", "para_id": 2848, "text": "sister scarcely opened their mouths except to complain of fatigue, and"}
{"doc_id": "gutenberg_1342", "para_id": 2849, "text": "were evidently impatient to have the house to themselves. They repulsed"}
{"doc_id": "gutenberg_1342", "para_id": 2850, "text": "every attempt of Mrs. Bennet at conversation, and, by so doing, threw a"}
{"doc_id": "gutenberg_1342", "para_id": 2851, "text": "languor over the whole party, which was very little relieved by the long"}
{"doc_id": "gutenberg_1342", "para_id": 2852, "text": "speeches of Mr. Collins, who was complimenting Mr. Bingley and his"}
{"doc_id": "gutenberg_1342", "para_id": 2853, "text": "sisters on the elegance of their entertainment, and the hospitality and"}
{"doc_id": "gutenberg_1342", "para_id": 2854, "text": "politeness which had marked their behaviour to their guests. Darcy said"}
{"doc_id": "gutenberg_1342", "para_id": 2855, "text": "nothing at all. Mr. Bennet, in equal silence, was enjoying the scene."}
{"doc_id": "gutenberg_1342", "para_id": 2856, "text": "Mr. Bingley and Jane were standing together a little detached from the"}
{"doc_id": "gutenberg_1342", "para_id": 2857, "text": "rest, and talked only to each other. Elizabeth preserved as steady a"}
{"doc_id": "gutenberg_1342", "para_id": 2858, "text": "silence as either Mrs. Hurst or Miss Bingley; and even Lydia was too"}
{"doc_id": "gutenberg_1342", "para_id": 2859, "text": "much fatigued to utter more than the occasional exclamation of “Lord,"}
{"doc_id": "gutenberg_1342", "para_id": 2860, "text": "When at length they arose to take leave, Mrs. Bennet was most pressingly"}
{"doc_id": "gutenberg_1342", "para_id": 2861, "text": "civil in her hope of seeing the whole family soon at Longbourn; and"}
{"doc_id": "gutenberg_1342", "para_id": 2862, "text": "addressed herself particularly to Mr. Bingley, to assure him how happy"}
{"doc_id": "gutenberg_1342", "para_id": 2863, "text": "he would make them, by eating a family dinner with them at any time,"}
{"doc_id": "gutenberg_1342", "para_id": 2864, "text": "without the ceremony of a formal invitation. Bingley was all grateful"}
{"doc_id": "gutenberg_1342", "para_id": 2865, "text": "pleasure; and he readily engaged for taking the earliest opportunity of"}
{"doc_id": "gutenberg_1342", "para_id": 2866, "text": "waiting on her after his return from London, whither he was obliged to"}
{"doc_id": "gutenberg_1342", "para_id": 2867, "text": "Mrs. Bennet was perfectly satisfied; and quitted the house under the"}
{"doc_id": "gutenberg_1342", "para_id": 2868, "text": "delightful persuasion that, allowing for the necessary preparations of"}
{"doc_id": "gutenberg_1342", "para_id": 2869, "text": "settlements, new carriages, and wedding clothes, she should undoubtedly"}
{"doc_id": "gutenberg_1342", "para_id": 2870, "text": "see her daughter settled at Netherfield in the course of three or four"}
{"doc_id": "gutenberg_1342", "para_id": 2871, "text": "months. Of having another daughter married to Mr. Collins she thought"}
{"doc_id": "gutenberg_1342", "para_id": 2872, "text": "with equal certainty, and with considerable, though not equal, pleasure."}
{"doc_id": "gutenberg_1342", "para_id": 2873, "text": "Elizabeth was the least dear to her of all her children; and though the"}
{"doc_id": "gutenberg_1342", "para_id": 2874, "text": "man and the match were quite good enough for _her_, the worth of each"}
{"doc_id": "gutenberg_1342", "para_id": 2875, "text": "The next day opened a new scene at Longbourn. Mr. Collins made his"}
{"doc_id": "gutenberg_1342", "para_id": 2876, "text": "declaration in form. Having resolved to do it without loss of time, as"}
{"doc_id": "gutenberg_1342", "para_id": 2877, "text": "his leave of absence extended only to the following Saturday, and having"}
{"doc_id": "gutenberg_1342", "para_id": 2878, "text": "no feelings of diffidence to make it distressing to himself even at the"}
{"doc_id": "gutenberg_1342", "para_id": 2879, "text": "moment, he set about it in a very orderly manner, with all the"}
{"doc_id": "gutenberg_1342", "para_id": 2880, "text": "observances which he supposed a regular part of the business. On finding"}
{"doc_id": "gutenberg_1342", "para_id": 2881, "text": "Mrs. Bennet, Elizabeth, and one of the younger girls together, soon"}
{"doc_id": "gutenberg_1342", "para_id": 2882, "text": "after breakfast, he addressed the mother in these words,--"}
{"doc_id": "gutenberg_1342", "para_id": 2883, "text": "“May I hope, madam, for your interest with your fair daughter Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 2884, "text": "when I solicit for the honour of a private audience with her in the"}
{"doc_id": "gutenberg_1342", "para_id": 2885, "text": "Before Elizabeth had time for anything but a blush of surprise, Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 2886, "text": "“Oh dear! Yes, certainly. I am sure Lizzy will be very happy--I am sure"}
{"doc_id": "gutenberg_1342", "para_id": 2887, "text": "she can have no objection. Come, Kitty, I want you upstairs.” And"}
{"doc_id": "gutenberg_1342", "para_id": 2888, "text": "gathering her work together, she was hastening away, when Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 2889, "text": "“Dear ma’am, do not go. I beg you will not go. Mr. Collins must excuse"}
{"doc_id": "gutenberg_1342", "para_id": 2890, "text": "me. He can have nothing to say to me that anybody need not hear. I am"}
{"doc_id": "gutenberg_1342", "para_id": 2891, "text": "“No, no, nonsense, Lizzy. I desire you will stay where you are.” And"}
{"doc_id": "gutenberg_1342", "para_id": 2892, "text": "upon Elizabeth’s seeming really, with vexed and embarrassed looks, about"}
{"doc_id": "gutenberg_1342", "para_id": 2893, "text": "to escape, she added, “Lizzy, I _insist_ upon your staying and hearing"}
{"doc_id": "gutenberg_1342", "para_id": 2894, "text": "Elizabeth would not oppose such an injunction; and a moment’s"}
{"doc_id": "gutenberg_1342", "para_id": 2895, "text": "consideration making her also sensible that it would be wisest to get it"}
{"doc_id": "gutenberg_1342", "para_id": 2896, "text": "over as soon and as quietly as possible, she sat down again, and tried"}
{"doc_id": "gutenberg_1342", "para_id": 2897, "text": "to conceal, by incessant employment, the feelings which were divided"}
{"doc_id": "gutenberg_1342", "para_id": 2898, "text": "between distress and diversion. Mrs. Bennet and Kitty walked off, and as"}
{"doc_id": "gutenberg_1342", "para_id": 2899, "text": "“Believe me, my dear Miss Elizabeth, that your modesty, so far from"}
{"doc_id": "gutenberg_1342", "para_id": 2900, "text": "doing you any disservice, rather adds to your other perfections. You"}
{"doc_id": "gutenberg_1342", "para_id": 2901, "text": "would have been less amiable in my eyes had there _not_ been this little"}
{"doc_id": "gutenberg_1342", "para_id": 2902, "text": "unwillingness; but allow me to assure you that I have your respected"}
{"doc_id": "gutenberg_1342", "para_id": 2903, "text": "mother’s permission for this address. You can hardly doubt the purport"}
{"doc_id": "gutenberg_1342", "para_id": 2904, "text": "of my discourse, however your natural delicacy may lead you to"}
{"doc_id": "gutenberg_1342", "para_id": 2905, "text": "dissemble; my attentions have been too marked to be mistaken. Almost as"}
{"doc_id": "gutenberg_1342", "para_id": 2906, "text": "soon as I entered the house I singled you out as the companion of my"}
{"doc_id": "gutenberg_1342", "para_id": 2907, "text": "future life. But before I am run away with by my feelings on this"}
{"doc_id": "gutenberg_1342", "para_id": 2908, "text": "subject, perhaps it will be advisable for me to state my reasons for"}
{"doc_id": "gutenberg_1342", "para_id": 2909, "text": "marrying--and, moreover, for coming into Hertfordshire with the design"}
{"doc_id": "gutenberg_1342", "para_id": 2910, "text": "The idea of Mr. Collins, with all his solemn composure, being run away"}
{"doc_id": "gutenberg_1342", "para_id": 2911, "text": "with by his feelings, made Elizabeth so near laughing that she could not"}
{"doc_id": "gutenberg_1342", "para_id": 2912, "text": "use the short pause he allowed in any attempt to stop him farther, and"}
{"doc_id": "gutenberg_1342", "para_id": 2913, "text": "“My reasons for marrying are, first, that I think it a right thing for"}
{"doc_id": "gutenberg_1342", "para_id": 2914, "text": "every clergyman in easy circumstances (like myself) to set the example"}
{"doc_id": "gutenberg_1342", "para_id": 2915, "text": "of matrimony in his parish; secondly, that I am convinced it will add"}
{"doc_id": "gutenberg_1342", "para_id": 2916, "text": "very greatly to my happiness; and, thirdly, which perhaps I ought to"}
{"doc_id": "gutenberg_1342", "para_id": 2917, "text": "have mentioned earlier, that it is the particular advice and"}
{"doc_id": "gutenberg_1342", "para_id": 2918, "text": "recommendation of the very noble lady whom I have the honour of calling"}
{"doc_id": "gutenberg_1342", "para_id": 2919, "text": "patroness. Twice has she condescended to give me her opinion (unasked"}
{"doc_id": "gutenberg_1342", "para_id": 2920, "text": "too!) on this subject; and it was but the very Saturday night before I"}
{"doc_id": "gutenberg_1342", "para_id": 2921, "text": "left Hunsford,--between our pools at quadrille, while Mrs. Jenkinson was"}
{"doc_id": "gutenberg_1342", "para_id": 2922, "text": "arranging Miss De Bourgh’s footstool,--that she said, ‘Mr. Collins, you"}
{"doc_id": "gutenberg_1342", "para_id": 2923, "text": "must marry. A clergyman like you must marry. Choose properly, choose a"}
{"doc_id": "gutenberg_1342", "para_id": 2924, "text": "gentlewoman for _my_ sake, and for your _own_; let her be an active,"}
{"doc_id": "gutenberg_1342", "para_id": 2925, "text": "useful sort of person, not brought up high, but able to make a small"}
{"doc_id": "gutenberg_1342", "para_id": 2926, "text": "income go a good way. This is my advice. Find such a woman as soon as"}
{"doc_id": "gutenberg_1342", "para_id": 2927, "text": "you can, bring her to Hunsford, and I will visit her.’ Allow me, by the"}
{"doc_id": "gutenberg_1342", "para_id": 2928, "text": "way, to observe, my fair cousin, that I do not reckon the notice and"}
{"doc_id": "gutenberg_1342", "para_id": 2929, "text": "kindness of Lady Catherine de Bourgh as among the least of the"}
{"doc_id": "gutenberg_1342", "para_id": 2930, "text": "advantages in my power to offer. You will find her manners beyond"}
{"doc_id": "gutenberg_1342", "para_id": 2931, "text": "anything I can describe; and your wit and vivacity, I think, must be"}
{"doc_id": "gutenberg_1342", "para_id": 2932, "text": "acceptable to her, especially when tempered with the silence and respect"}
{"doc_id": "gutenberg_1342", "para_id": 2933, "text": "which her rank will inevitably excite. Thus much for my general"}
{"doc_id": "gutenberg_1342", "para_id": 2934, "text": "intention in favour of matrimony; it remains to be told why my views"}
{"doc_id": "gutenberg_1342", "para_id": 2935, "text": "were directed to Longbourn instead of my own neighbourhood, where I"}
{"doc_id": "gutenberg_1342", "para_id": 2936, "text": "assure you there are many amiable young women. But the fact is, that"}
{"doc_id": "gutenberg_1342", "para_id": 2937, "text": "being, as I am, to inherit this estate after the death of your honoured"}
{"doc_id": "gutenberg_1342", "para_id": 2938, "text": "father (who, however, may live many years longer), I could not satisfy"}
{"doc_id": "gutenberg_1342", "para_id": 2939, "text": "myself without resolving to choose a wife from among his daughters, that"}
{"doc_id": "gutenberg_1342", "para_id": 2940, "text": "the loss to them might be as little as possible when the melancholy"}
{"doc_id": "gutenberg_1342", "para_id": 2941, "text": "event takes place--which, however, as I have already said, may not be"}
{"doc_id": "gutenberg_1342", "para_id": 2942, "text": "for several years. This has been my motive, my fair cousin, and I"}
{"doc_id": "gutenberg_1342", "para_id": 2943, "text": "flatter myself it will not sink me in your esteem. And now nothing"}
{"doc_id": "gutenberg_1342", "para_id": 2944, "text": "remains for me but to assure you in the most animated language of the"}
{"doc_id": "gutenberg_1342", "para_id": 2945, "text": "violence of my affection. To fortune I am perfectly indifferent, and"}
{"doc_id": "gutenberg_1342", "para_id": 2946, "text": "shall make no demand of that nature on your father, since I am well"}
{"doc_id": "gutenberg_1342", "para_id": 2947, "text": "aware that it could not be complied with; and that one thousand pounds"}
{"doc_id": "gutenberg_1342", "para_id": 2948, "text": "in the 4 per cents., which will not be yours till after your mother’s"}
{"doc_id": "gutenberg_1342", "para_id": 2949, "text": "decease, is all that you may ever be entitled to. On that head,"}
{"doc_id": "gutenberg_1342", "para_id": 2950, "text": "therefore, I shall be uniformly silent: and you may assure yourself that"}
{"doc_id": "gutenberg_1342", "para_id": 2951, "text": "no ungenerous reproach shall ever pass my lips when we are married.”"}
{"doc_id": "gutenberg_1342", "para_id": 2952, "text": "“You are too hasty, sir,” she cried. “You forget that I have made no"}
{"doc_id": "gutenberg_1342", "para_id": 2953, "text": "answer. Let me do it without further loss of time. Accept my thanks for"}
{"doc_id": "gutenberg_1342", "para_id": 2954, "text": "the compliment you are paying me. I am very sensible of the honour of"}
{"doc_id": "gutenberg_1342", "para_id": 2955, "text": "your proposals, but it is impossible for me to do otherwise than decline"}
{"doc_id": "gutenberg_1342", "para_id": 2956, "text": "“I am not now to learn,” replied Mr. Collins, with a formal wave of the"}
{"doc_id": "gutenberg_1342", "para_id": 2957, "text": "hand, “that it is usual with young ladies to reject the addresses of the"}
{"doc_id": "gutenberg_1342", "para_id": 2958, "text": "man whom they secretly mean to accept, when he first applies for their"}
{"doc_id": "gutenberg_1342", "para_id": 2959, "text": "favour; and that sometimes the refusal is repeated a second or even a"}
{"doc_id": "gutenberg_1342", "para_id": 2960, "text": "third time. I am, therefore, by no means discouraged by what you have"}
{"doc_id": "gutenberg_1342", "para_id": 2961, "text": "just said, and shall hope to lead you to the altar ere long.”"}
{"doc_id": "gutenberg_1342", "para_id": 2962, "text": "“Upon my word, sir,” cried Elizabeth, “your hope is rather an"}
{"doc_id": "gutenberg_1342", "para_id": 2963, "text": "extraordinary one after my declaration. I do assure you that I am not"}
{"doc_id": "gutenberg_1342", "para_id": 2964, "text": "one of those young ladies (if such young ladies there are) who are so"}
{"doc_id": "gutenberg_1342", "para_id": 2965, "text": "daring as to risk their happiness on the chance of being asked a second"}
{"doc_id": "gutenberg_1342", "para_id": 2966, "text": "time. I am perfectly serious in my refusal. You could not make _me_"}
{"doc_id": "gutenberg_1342", "para_id": 2967, "text": "happy, and I am convinced that I am the last woman in the world who"}
{"doc_id": "gutenberg_1342", "para_id": 2968, "text": "would make _you_ so. Nay, were your friend Lady Catherine to know me, I"}
{"doc_id": "gutenberg_1342", "para_id": 2969, "text": "am persuaded she would find me in every respect ill qualified for the"}
{"doc_id": "gutenberg_1342", "para_id": 2970, "text": "“Were it certain that Lady Catherine would think so,” said Mr. Collins,"}
{"doc_id": "gutenberg_1342", "para_id": 2971, "text": "very gravely--“but I cannot imagine that her Ladyship would at all"}
{"doc_id": "gutenberg_1342", "para_id": 2972, "text": "disapprove of you. And you may be certain that when I have the honour of"}
{"doc_id": "gutenberg_1342", "para_id": 2973, "text": "seeing her again I shall speak in the highest terms of your modesty,"}
{"doc_id": "gutenberg_1342", "para_id": 2974, "text": "“Indeed, Mr. Collins, all praise of me will be unnecessary. You must"}
{"doc_id": "gutenberg_1342", "para_id": 2975, "text": "give me leave to judge for myself, and pay me the compliment of"}
{"doc_id": "gutenberg_1342", "para_id": 2976, "text": "believing what I say. I wish you very happy and very rich, and by"}
{"doc_id": "gutenberg_1342", "para_id": 2977, "text": "refusing your hand, do all in my power to prevent your being otherwise."}
{"doc_id": "gutenberg_1342", "para_id": 2978, "text": "In making me the offer, you must have satisfied the delicacy of your"}
{"doc_id": "gutenberg_1342", "para_id": 2979, "text": "feelings with regard to my family, and may take possession of Longbourn"}
{"doc_id": "gutenberg_1342", "para_id": 2980, "text": "estate whenever it falls, without any self-reproach. This matter may be"}
{"doc_id": "gutenberg_1342", "para_id": 2981, "text": "considered, therefore, as finally settled.” And rising as she thus"}
{"doc_id": "gutenberg_1342", "para_id": 2982, "text": "spoke, she would have quitted the room, had not Mr. Collins thus"}
{"doc_id": "gutenberg_1342", "para_id": 2983, "text": "“When I do myself the honour of speaking to you next on the subject, I"}
{"doc_id": "gutenberg_1342", "para_id": 2984, "text": "shall hope to receive a more favourable answer than you have now given"}
{"doc_id": "gutenberg_1342", "para_id": 2985, "text": "me; though I am far from accusing you of cruelty at present, because I"}
{"doc_id": "gutenberg_1342", "para_id": 2986, "text": "know it to be the established custom of your sex to reject a man on the"}
{"doc_id": "gutenberg_1342", "para_id": 2987, "text": "first application, and, perhaps, you have even now said as much to"}
{"doc_id": "gutenberg_1342", "para_id": 2988, "text": "encourage my suit as would be consistent with the true delicacy of the"}
{"doc_id": "gutenberg_1342", "para_id": 2989, "text": "“Really, Mr. Collins,” cried Elizabeth, with some warmth, “you puzzle me"}
{"doc_id": "gutenberg_1342", "para_id": 2990, "text": "exceedingly. If what I have hitherto said can appear to you in the form"}
{"doc_id": "gutenberg_1342", "para_id": 2991, "text": "of encouragement, I know not how to express my refusal in such a way as"}
{"doc_id": "gutenberg_1342", "para_id": 2992, "text": "“You must give me leave to flatter myself, my dear cousin, that your"}
{"doc_id": "gutenberg_1342", "para_id": 2993, "text": "refusal of my addresses are merely words of course. My reasons for"}
{"doc_id": "gutenberg_1342", "para_id": 2994, "text": "believing it are briefly these:--It does not appear to me that my hand"}
{"doc_id": "gutenberg_1342", "para_id": 2995, "text": "is unworthy of your acceptance, or that the establishment I can offer"}
{"doc_id": "gutenberg_1342", "para_id": 2996, "text": "would be any other than highly desirable. My situation in life, my"}
{"doc_id": "gutenberg_1342", "para_id": 2997, "text": "connections with the family of De Bourgh, and my relationship to your"}
{"doc_id": "gutenberg_1342", "para_id": 2998, "text": "own, are circumstances highly in my favour; and you should take it into"}
{"doc_id": "gutenberg_1342", "para_id": 2999, "text": "further consideration that, in spite of your manifold attractions, it is"}
{"doc_id": "gutenberg_1342", "para_id": 3000, "text": "by no means certain that another offer of marriage may ever be made you."}
{"doc_id": "gutenberg_1342", "para_id": 3001, "text": "Your portion is unhappily so small, that it will in all likelihood undo"}
{"doc_id": "gutenberg_1342", "para_id": 3002, "text": "the effects of your loveliness and amiable qualifications. As I must,"}
{"doc_id": "gutenberg_1342", "para_id": 3003, "text": "therefore, conclude that you are not serious in your rejection of me, I"}
{"doc_id": "gutenberg_1342", "para_id": 3004, "text": "shall choose to attribute it to your wish of increasing my love by"}
{"doc_id": "gutenberg_1342", "para_id": 3005, "text": "suspense, according to the usual practice of elegant females.”"}
{"doc_id": "gutenberg_1342", "para_id": 3006, "text": "“I do assure you, sir, that I have no pretensions whatever to that kind"}
{"doc_id": "gutenberg_1342", "para_id": 3007, "text": "of elegance which consists in tormenting a respectable man. I would"}
{"doc_id": "gutenberg_1342", "para_id": 3008, "text": "rather be paid the compliment of being believed sincere. I thank you"}
{"doc_id": "gutenberg_1342", "para_id": 3009, "text": "again and again for the honour you have done me in your proposals, but"}
{"doc_id": "gutenberg_1342", "para_id": 3010, "text": "to accept them is absolutely impossible. My feelings in every respect"}
{"doc_id": "gutenberg_1342", "para_id": 3011, "text": "forbid it. Can I speak plainer? Do not consider me now as an elegant"}
{"doc_id": "gutenberg_1342", "para_id": 3012, "text": "female intending to plague you, but as a rational creature speaking the"}
{"doc_id": "gutenberg_1342", "para_id": 3013, "text": "“You are uniformly charming!” cried he, with an air of awkward"}
{"doc_id": "gutenberg_1342", "para_id": 3014, "text": "gallantry; “and I am persuaded that, when sanctioned by the express"}
{"doc_id": "gutenberg_1342", "para_id": 3015, "text": "authority of both your excellent parents, my proposals will not fail of"}
{"doc_id": "gutenberg_1342", "para_id": 3016, "text": "To such perseverance in wilful self-deception Elizabeth would make no"}
{"doc_id": "gutenberg_1342", "para_id": 3017, "text": "reply, and immediately and in silence withdrew; determined, that if he"}
{"doc_id": "gutenberg_1342", "para_id": 3018, "text": "persisted in considering her repeated refusals as flattering"}
{"doc_id": "gutenberg_1342", "para_id": 3019, "text": "encouragement, to apply to her father, whose negative might be uttered"}
{"doc_id": "gutenberg_1342", "para_id": 3020, "text": "in such a manner as must be decisive, and whose behaviour at least could"}
{"doc_id": "gutenberg_1342", "para_id": 3021, "text": "not be mistaken for the affectation and coquetry of an elegant female."}
{"doc_id": "gutenberg_1342", "para_id": 3022, "text": "Mr. Collins was not left long to the silent contemplation of his"}
{"doc_id": "gutenberg_1342", "para_id": 3023, "text": "successful love; for Mrs. Bennet, having dawdled about in the vestibule"}
{"doc_id": "gutenberg_1342", "para_id": 3024, "text": "to watch for the end of the conference, no sooner saw Elizabeth open the"}
{"doc_id": "gutenberg_1342", "para_id": 3025, "text": "door and with quick step pass her towards the staircase, than she"}
{"doc_id": "gutenberg_1342", "para_id": 3026, "text": "entered the breakfast-room, and congratulated both him and herself in"}
{"doc_id": "gutenberg_1342", "para_id": 3027, "text": "warm terms on the happy prospect of their nearer connection. Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 3028, "text": "received and returned these felicitations with equal pleasure, and then"}
{"doc_id": "gutenberg_1342", "para_id": 3029, "text": "proceeded to relate the particulars of their interview, with the result"}
{"doc_id": "gutenberg_1342", "para_id": 3030, "text": "of which he trusted he had every reason to be satisfied, since the"}
{"doc_id": "gutenberg_1342", "para_id": 3031, "text": "refusal which his cousin had steadfastly given him would naturally flow"}
{"doc_id": "gutenberg_1342", "para_id": 3032, "text": "from her bashful modesty and the genuine delicacy of her character."}
{"doc_id": "gutenberg_1342", "para_id": 3033, "text": "This information, however, startled Mrs. Bennet: she would have been"}
{"doc_id": "gutenberg_1342", "para_id": 3034, "text": "glad to be equally satisfied that her daughter had meant to encourage"}
{"doc_id": "gutenberg_1342", "para_id": 3035, "text": "him by protesting against his proposals, but she dared not believe it,"}
{"doc_id": "gutenberg_1342", "para_id": 3036, "text": "“But depend upon it, Mr. Collins,” she added, “that Lizzy shall be"}
{"doc_id": "gutenberg_1342", "para_id": 3037, "text": "brought to reason. I will speak to her about it myself directly. She is"}
{"doc_id": "gutenberg_1342", "para_id": 3038, "text": "a very headstrong, foolish girl, and does not know her own interest; but"}
{"doc_id": "gutenberg_1342", "para_id": 3039, "text": "“Pardon me for interrupting you, madam,” cried Mr. Collins; “but if she"}
{"doc_id": "gutenberg_1342", "para_id": 3040, "text": "is really headstrong and foolish, I know not whether she would"}
{"doc_id": "gutenberg_1342", "para_id": 3041, "text": "altogether be a very desirable wife to a man in my situation, who"}
{"doc_id": "gutenberg_1342", "para_id": 3042, "text": "naturally looks for happiness in the marriage state. If, therefore, she"}
{"doc_id": "gutenberg_1342", "para_id": 3043, "text": "actually persists in rejecting my suit, perhaps it were better not to"}
{"doc_id": "gutenberg_1342", "para_id": 3044, "text": "force her into accepting me, because, if liable to such defects of"}
{"doc_id": "gutenberg_1342", "para_id": 3045, "text": "temper, she could not contribute much to my felicity.”"}
{"doc_id": "gutenberg_1342", "para_id": 3046, "text": "“Sir, you quite misunderstand me,” said Mrs. Bennet, alarmed. “Lizzy is"}
{"doc_id": "gutenberg_1342", "para_id": 3047, "text": "only headstrong in such matters as these. In everything else she is as"}
{"doc_id": "gutenberg_1342", "para_id": 3048, "text": "good-natured a girl as ever lived. I will go directly to Mr. Bennet, and"}
{"doc_id": "gutenberg_1342", "para_id": 3049, "text": "She would not give him time to reply, but hurrying instantly to her"}
{"doc_id": "gutenberg_1342", "para_id": 3050, "text": "“Oh, Mr. Bennet, you are wanted immediately; we are all in an uproar."}
{"doc_id": "gutenberg_1342", "para_id": 3051, "text": "You must come and make Lizzy marry Mr. Collins, for she vows she will"}
{"doc_id": "gutenberg_1342", "para_id": 3052, "text": "not have him; and if you do not make haste he will change his mind and"}
{"doc_id": "gutenberg_1342", "para_id": 3053, "text": "Mr. Bennet raised his eyes from his book as she entered, and fixed them"}
{"doc_id": "gutenberg_1342", "para_id": 3054, "text": "on her face with a calm unconcern, which was not in the least altered by"}
{"doc_id": "gutenberg_1342", "para_id": 3055, "text": "“I have not the pleasure of understanding you,” said he, when she had"}
{"doc_id": "gutenberg_1342", "para_id": 3056, "text": "“Of Mr. Collins and Lizzy. Lizzy declares she will not have Mr. Collins,"}
{"doc_id": "gutenberg_1342", "para_id": 3057, "text": "and Mr. Collins begins to say that he will not have Lizzy.”"}
{"doc_id": "gutenberg_1342", "para_id": 3058, "text": "“And what am I to do on the occasion? It seems a hopeless business.”"}
{"doc_id": "gutenberg_1342", "para_id": 3059, "text": "“Speak to Lizzy about it yourself. Tell her that you insist upon her"}
{"doc_id": "gutenberg_1342", "para_id": 3060, "text": "“Let her be called down. She shall hear my opinion.”"}
{"doc_id": "gutenberg_1342", "para_id": 3061, "text": "Mrs. Bennet rang the bell, and Miss Elizabeth was summoned to the"}
{"doc_id": "gutenberg_1342", "para_id": 3062, "text": "“Come here, child,” cried her father as she appeared. “I have sent for"}
{"doc_id": "gutenberg_1342", "para_id": 3063, "text": "you on an affair of importance. I understand that Mr. Collins has made"}
{"doc_id": "gutenberg_1342", "para_id": 3064, "text": "“Very well--and this offer of marriage you have refused?”"}
{"doc_id": "gutenberg_1342", "para_id": 3065, "text": "“Very well. We now come to the point. Your mother insists upon your"}
{"doc_id": "gutenberg_1342", "para_id": 3066, "text": "“An unhappy alternative is before you, Elizabeth. From this day you must"}
{"doc_id": "gutenberg_1342", "para_id": 3067, "text": "be a stranger to one of your parents. Your mother will never see you"}
{"doc_id": "gutenberg_1342", "para_id": 3068, "text": "again if you do _not_ marry Mr. Collins, and I will never see you again"}
{"doc_id": "gutenberg_1342", "para_id": 3069, "text": "Elizabeth could not but smile at such a conclusion of such a beginning;"}
{"doc_id": "gutenberg_1342", "para_id": 3070, "text": "but Mrs. Bennet, who had persuaded herself that her husband regarded the"}
{"doc_id": "gutenberg_1342", "para_id": 3071, "text": "affair as she wished, was excessively disappointed."}
{"doc_id": "gutenberg_1342", "para_id": 3072, "text": "“What do you mean, Mr. Bennet, by talking in this way? You promised me"}
{"doc_id": "gutenberg_1342", "para_id": 3073, "text": "“My dear,” replied her husband, “I have two small favours to request."}
{"doc_id": "gutenberg_1342", "para_id": 3074, "text": "First, that you will allow me the free use of my understanding on the"}
{"doc_id": "gutenberg_1342", "para_id": 3075, "text": "present occasion; and, secondly, of my room. I shall be glad to have the"}
{"doc_id": "gutenberg_1342", "para_id": 3076, "text": "Not yet, however, in spite of her disappointment in her husband, did"}
{"doc_id": "gutenberg_1342", "para_id": 3077, "text": "Mrs. Bennet give up the point. She talked to Elizabeth again and again;"}
{"doc_id": "gutenberg_1342", "para_id": 3078, "text": "coaxed and threatened her by turns. She endeavoured to secure Jane in"}
{"doc_id": "gutenberg_1342", "para_id": 3079, "text": "her interest, but Jane, with all possible mildness, declined"}
{"doc_id": "gutenberg_1342", "para_id": 3080, "text": "interfering; and Elizabeth, sometimes with real earnestness, and"}
{"doc_id": "gutenberg_1342", "para_id": 3081, "text": "sometimes with playful gaiety, replied to her attacks. Though her manner"}
{"doc_id": "gutenberg_1342", "para_id": 3082, "text": "Mr. Collins, meanwhile, was meditating in solitude on what had passed."}
{"doc_id": "gutenberg_1342", "para_id": 3083, "text": "He thought too well of himself to comprehend on what motive his cousin"}
{"doc_id": "gutenberg_1342", "para_id": 3084, "text": "could refuse him; and though his pride was hurt, he suffered in no other"}
{"doc_id": "gutenberg_1342", "para_id": 3085, "text": "way. His regard for her was quite imaginary; and the possibility of her"}
{"doc_id": "gutenberg_1342", "para_id": 3086, "text": "deserving her mother’s reproach prevented his feeling any regret."}
{"doc_id": "gutenberg_1342", "para_id": 3087, "text": "While the family were in this confusion, Charlotte Lucas came to spend"}
{"doc_id": "gutenberg_1342", "para_id": 3088, "text": "the day with them. She was met in the vestibule by Lydia, who, flying to"}
{"doc_id": "gutenberg_1342", "para_id": 3089, "text": "her, cried in a half whisper, “I am glad you are come, for there is such"}
{"doc_id": "gutenberg_1342", "para_id": 3090, "text": "fun here! What do you think has happened this morning? Mr. Collins has"}
{"doc_id": "gutenberg_1342", "para_id": 3091, "text": "made an offer to Lizzy, and she will not have him.”"}
{"doc_id": "gutenberg_1342", "para_id": 3092, "text": "Charlotte had hardly time to answer before they were joined by Kitty,"}
{"doc_id": "gutenberg_1342", "para_id": 3093, "text": "who came to tell the same news; and no sooner had they entered the"}
{"doc_id": "gutenberg_1342", "para_id": 3094, "text": "breakfast-room, where Mrs. Bennet was alone, than she likewise began on"}
{"doc_id": "gutenberg_1342", "para_id": 3095, "text": "the subject, calling on Miss Lucas for her compassion, and entreating"}
{"doc_id": "gutenberg_1342", "para_id": 3096, "text": "her to persuade her friend Lizzy to comply with the wishes of her"}
{"doc_id": "gutenberg_1342", "para_id": 3097, "text": "family. “Pray do, my dear Miss Lucas,” she added, in a melancholy tone;"}
{"doc_id": "gutenberg_1342", "para_id": 3098, "text": "“for nobody is on my side, nobody takes part with me; I am cruelly used,"}
{"doc_id": "gutenberg_1342", "para_id": 3099, "text": "Charlotte’s reply was spared by the entrance of Jane and Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 3100, "text": "“Ay, there she comes,” continued Mrs. Bennet, “looking as unconcerned as"}
{"doc_id": "gutenberg_1342", "para_id": 3101, "text": "may be, and caring no more for us than if we were at York, provided she"}
{"doc_id": "gutenberg_1342", "para_id": 3102, "text": "can have her own way. But I tell you what, Miss Lizzy, if you take it"}
{"doc_id": "gutenberg_1342", "para_id": 3103, "text": "into your head to go on refusing every offer of marriage in this way,"}
{"doc_id": "gutenberg_1342", "para_id": 3104, "text": "you will never get a husband at all--and I am sure I do not know who is"}
{"doc_id": "gutenberg_1342", "para_id": 3105, "text": "to maintain you when your father is dead. _I_ shall not be able to keep"}
{"doc_id": "gutenberg_1342", "para_id": 3106, "text": "you--and so I warn you. I have done with you from this very day. I told"}
{"doc_id": "gutenberg_1342", "para_id": 3107, "text": "you in the library, you know, that I should never speak to you again,"}
{"doc_id": "gutenberg_1342", "para_id": 3108, "text": "and you will find me as good as my word. I have no pleasure in talking"}
{"doc_id": "gutenberg_1342", "para_id": 3109, "text": "to undutiful children. Not that I have much pleasure, indeed, in talking"}
{"doc_id": "gutenberg_1342", "para_id": 3110, "text": "to anybody. People who suffer as I do from nervous complaints can have"}
{"doc_id": "gutenberg_1342", "para_id": 3111, "text": "no great inclination for talking. Nobody can tell what I suffer! But it"}
{"doc_id": "gutenberg_1342", "para_id": 3112, "text": "is always so. Those who do not complain are never pitied.”"}
{"doc_id": "gutenberg_1342", "para_id": 3113, "text": "Her daughters listened in silence to this effusion, sensible that any"}
{"doc_id": "gutenberg_1342", "para_id": 3114, "text": "attempt to reason with or soothe her would only increase the irritation."}
{"doc_id": "gutenberg_1342", "para_id": 3115, "text": "She talked on, therefore, without interruption from any of them till"}
{"doc_id": "gutenberg_1342", "para_id": 3116, "text": "they were joined by Mr. Collins, who entered with an air more stately"}
{"doc_id": "gutenberg_1342", "para_id": 3117, "text": "than usual, and on perceiving whom, she said to the girls,--"}
{"doc_id": "gutenberg_1342", "para_id": 3118, "text": "“Now, I do insist upon it, that you, all of you, hold your tongues, and"}
{"doc_id": "gutenberg_1342", "para_id": 3119, "text": "let Mr. Collins and me have a little conversation together.”"}
{"doc_id": "gutenberg_1342", "para_id": 3120, "text": "Elizabeth passed quietly out of the room, Jane and Kitty followed, but"}
{"doc_id": "gutenberg_1342", "para_id": 3121, "text": "Lydia stood her ground, determined to hear all she could; and Charlotte,"}
{"doc_id": "gutenberg_1342", "para_id": 3122, "text": "detained first by the civility of Mr. Collins, whose inquiries after"}
{"doc_id": "gutenberg_1342", "para_id": 3123, "text": "herself and all her family were very minute, and then by a little"}
{"doc_id": "gutenberg_1342", "para_id": 3124, "text": "curiosity, satisfied herself with walking to the window and pretending"}
{"doc_id": "gutenberg_1342", "para_id": 3125, "text": "not to hear. In a doleful voice Mrs. Bennet thus began the projected"}
{"doc_id": "gutenberg_1342", "para_id": 3126, "text": "“My dear madam,” replied he, “let us be for ever silent on this point."}
{"doc_id": "gutenberg_1342", "para_id": 3127, "text": "Far be it from me,” he presently continued, in a voice that marked his"}
{"doc_id": "gutenberg_1342", "para_id": 3128, "text": "displeasure, “to resent the behaviour of your daughter. Resignation to"}
{"doc_id": "gutenberg_1342", "para_id": 3129, "text": "inevitable evils is the duty of us all: the peculiar duty of a young man"}
{"doc_id": "gutenberg_1342", "para_id": 3130, "text": "who has been so fortunate as I have been, in early preferment; and, I"}
{"doc_id": "gutenberg_1342", "para_id": 3131, "text": "trust, I am resigned. Perhaps not the less so from feeling a doubt of my"}
{"doc_id": "gutenberg_1342", "para_id": 3132, "text": "positive happiness had my fair cousin honoured me with her hand; for I"}
{"doc_id": "gutenberg_1342", "para_id": 3133, "text": "have often observed, that resignation is never so perfect as when the"}
{"doc_id": "gutenberg_1342", "para_id": 3134, "text": "blessing denied begins to lose somewhat of its value in our estimation."}
{"doc_id": "gutenberg_1342", "para_id": 3135, "text": "You will not, I hope, consider me as showing any disrespect to your"}
{"doc_id": "gutenberg_1342", "para_id": 3136, "text": "family, my dear madam, by thus withdrawing my pretensions to your"}
{"doc_id": "gutenberg_1342", "para_id": 3137, "text": "daughter’s favour, without having paid yourself and Mr. Bennet the"}
{"doc_id": "gutenberg_1342", "para_id": 3138, "text": "compliment of requesting you to interpose your authority in my behalf."}
{"doc_id": "gutenberg_1342", "para_id": 3139, "text": "My conduct may, I fear, be objectionable in having accepted my"}
{"doc_id": "gutenberg_1342", "para_id": 3140, "text": "dismission from your daughter’s lips instead of your own; but we are all"}
{"doc_id": "gutenberg_1342", "para_id": 3141, "text": "liable to error. I have certainly meant well through the whole affair."}
{"doc_id": "gutenberg_1342", "para_id": 3142, "text": "My object has been to secure an amiable companion for myself, with due"}
{"doc_id": "gutenberg_1342", "para_id": 3143, "text": "consideration for the advantage of all your family; and if my _manner_"}
{"doc_id": "gutenberg_1342", "para_id": 3144, "text": "has been at all reprehensible, I here beg leave to apologize.”"}
{"doc_id": "gutenberg_1342", "para_id": 3145, "text": "The discussion of Mr. Collins’s offer was now nearly at an end, and"}
{"doc_id": "gutenberg_1342", "para_id": 3146, "text": "Elizabeth had only to suffer from the uncomfortable feelings necessarily"}
{"doc_id": "gutenberg_1342", "para_id": 3147, "text": "attending it, and occasionally from some peevish allusion of her mother."}
{"doc_id": "gutenberg_1342", "para_id": 3148, "text": "As for the gentleman himself, _his_ feelings were chiefly expressed, not"}
{"doc_id": "gutenberg_1342", "para_id": 3149, "text": "by embarrassment or dejection, or by trying to avoid her, but by"}
{"doc_id": "gutenberg_1342", "para_id": 3150, "text": "stiffness of manner and resentful silence. He scarcely ever spoke to"}
{"doc_id": "gutenberg_1342", "para_id": 3151, "text": "her; and the assiduous attentions which he had been so sensible of"}
{"doc_id": "gutenberg_1342", "para_id": 3152, "text": "himself were transferred for the rest of the day to Miss Lucas, whose"}
{"doc_id": "gutenberg_1342", "para_id": 3153, "text": "civility in listening to him was a seasonable relief to them all, and"}
{"doc_id": "gutenberg_1342", "para_id": 3154, "text": "The morrow produced no abatement of Mrs. Bennet’s ill humour or ill"}
{"doc_id": "gutenberg_1342", "para_id": 3155, "text": "health. Mr. Collins was also in the same state of angry pride. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 3156, "text": "had hoped that his resentment might shorten his visit, but his plan did"}
{"doc_id": "gutenberg_1342", "para_id": 3157, "text": "not appear in the least affected by it. He was always to have gone on"}
{"doc_id": "gutenberg_1342", "para_id": 3158, "text": "After breakfast, the girls walked to Meryton, to inquire if Mr. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 3159, "text": "were returned, and to lament over his absence from the Netherfield ball."}
{"doc_id": "gutenberg_1342", "para_id": 3160, "text": "He joined them on their entering the town, and attended them to their"}
{"doc_id": "gutenberg_1342", "para_id": 3161, "text": "aunt’s, where his regret and vexation and the concern of everybody were"}
{"doc_id": "gutenberg_1342", "para_id": 3162, "text": "well talked over. To Elizabeth, however, he voluntarily acknowledged"}
{"doc_id": "gutenberg_1342", "para_id": 3163, "text": "that the necessity of his absence _had_ been self-imposed."}
{"doc_id": "gutenberg_1342", "para_id": 3164, "text": "“I found,” said he, “as the time drew near, that I had better not meet"}
{"doc_id": "gutenberg_1342", "para_id": 3165, "text": "Mr. Darcy;--that to be in the same room, the same party with him for so"}
{"doc_id": "gutenberg_1342", "para_id": 3166, "text": "many hours together, might be more than I could bear, and that scenes"}
{"doc_id": "gutenberg_1342", "para_id": 3167, "text": "She highly approved his forbearance; and they had leisure for a full"}
{"doc_id": "gutenberg_1342", "para_id": 3168, "text": "discussion of it, and for all the commendations which they civilly"}
{"doc_id": "gutenberg_1342", "para_id": 3169, "text": "bestowed on each other, as Wickham and another officer walked back with"}
{"doc_id": "gutenberg_1342", "para_id": 3170, "text": "them to Longbourn, and during the walk he particularly attended to her."}
{"doc_id": "gutenberg_1342", "para_id": 3171, "text": "His accompanying them was a double advantage: she felt all the"}
{"doc_id": "gutenberg_1342", "para_id": 3172, "text": "compliment it offered to herself; and it was most acceptable as an"}
{"doc_id": "gutenberg_1342", "para_id": 3173, "text": "occasion of introducing him to her father and mother."}
{"doc_id": "gutenberg_1342", "para_id": 3174, "text": "Soon after their return, a letter was delivered to Miss Bennet; it came"}
{"doc_id": "gutenberg_1342", "para_id": 3175, "text": "from Netherfield, and was opened immediately. The envelope contained a"}
{"doc_id": "gutenberg_1342", "para_id": 3176, "text": "sheet of elegant, little, hot-pressed paper, well covered with a lady’s"}
{"doc_id": "gutenberg_1342", "para_id": 3177, "text": "fair, flowing hand; and Elizabeth saw her sister’s countenance change as"}
{"doc_id": "gutenberg_1342", "para_id": 3178, "text": "she read it, and saw her dwelling intently on some particular passages."}
{"doc_id": "gutenberg_1342", "para_id": 3179, "text": "Jane recollected herself soon; and putting the letter away, tried to"}
{"doc_id": "gutenberg_1342", "para_id": 3180, "text": "join, with her usual cheerfulness, in the general conversation: but"}
{"doc_id": "gutenberg_1342", "para_id": 3181, "text": "Elizabeth felt an anxiety on the subject which drew off her attention"}
{"doc_id": "gutenberg_1342", "para_id": 3182, "text": "even from Wickham; and no sooner had he and his companion taken leave,"}
{"doc_id": "gutenberg_1342", "para_id": 3183, "text": "than a glance from Jane invited her to follow her upstairs. When they"}
{"doc_id": "gutenberg_1342", "para_id": 3184, "text": "had gained their own room, Jane, taking out her letter, said, “This is"}
{"doc_id": "gutenberg_1342", "para_id": 3185, "text": "from Caroline Bingley: what it contains has surprised me a good deal."}
{"doc_id": "gutenberg_1342", "para_id": 3186, "text": "The whole party have left Netherfield by this time, and are on their way"}
{"doc_id": "gutenberg_1342", "para_id": 3187, "text": "to town; and without any intention of coming back again. You shall hear"}
{"doc_id": "gutenberg_1342", "para_id": 3188, "text": "She then read the first sentence aloud, which comprised the information"}
{"doc_id": "gutenberg_1342", "para_id": 3189, "text": "of their having just resolved to follow their brother to town directly,"}
{"doc_id": "gutenberg_1342", "para_id": 3190, "text": "and of their meaning to dine that day in Grosvenor Street, where Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3191, "text": "Hurst had a house. The next was in these words:--“‘I do not pretend to"}
{"doc_id": "gutenberg_1342", "para_id": 3192, "text": "regret anything I shall leave in Hertfordshire except your society, my"}
{"doc_id": "gutenberg_1342", "para_id": 3193, "text": "dearest friend; but we will hope, at some future period, to enjoy many"}
{"doc_id": "gutenberg_1342", "para_id": 3194, "text": "returns of that delightful intercourse we have known, and in the"}
{"doc_id": "gutenberg_1342", "para_id": 3195, "text": "meanwhile may lessen the pain of separation by a very frequent and most"}
{"doc_id": "gutenberg_1342", "para_id": 3196, "text": "unreserved correspondence. I depend on you for that.’” To these"}
{"doc_id": "gutenberg_1342", "para_id": 3197, "text": "high-flown expressions Elizabeth listened with all the insensibility of"}
{"doc_id": "gutenberg_1342", "para_id": 3198, "text": "distrust; and though the suddenness of their removal surprised her, she"}
{"doc_id": "gutenberg_1342", "para_id": 3199, "text": "saw nothing in it really to lament: it was not to be supposed that their"}
{"doc_id": "gutenberg_1342", "para_id": 3200, "text": "absence from Netherfield would prevent Mr. Bingley’s being there; and as"}
{"doc_id": "gutenberg_1342", "para_id": 3201, "text": "to the loss of their society, she was persuaded that Jane must soon"}
{"doc_id": "gutenberg_1342", "para_id": 3202, "text": "“It is unlucky,” said she, after a short pause, “that you should not be"}
{"doc_id": "gutenberg_1342", "para_id": 3203, "text": "able to see your friends before they leave the country. But may we not"}
{"doc_id": "gutenberg_1342", "para_id": 3204, "text": "hope that the period of future happiness, to which Miss Bingley looks"}
{"doc_id": "gutenberg_1342", "para_id": 3205, "text": "forward, may arrive earlier than she is aware, and that the delightful"}
{"doc_id": "gutenberg_1342", "para_id": 3206, "text": "intercourse you have known as friends will be renewed with yet greater"}
{"doc_id": "gutenberg_1342", "para_id": 3207, "text": "satisfaction as sisters? Mr. Bingley will not be detained in London by"}
{"doc_id": "gutenberg_1342", "para_id": 3208, "text": "“Caroline decidedly says that none of the party will return into"}
{"doc_id": "gutenberg_1342", "para_id": 3209, "text": "“‘When my brother left us yesterday, he imagined that the business which"}
{"doc_id": "gutenberg_1342", "para_id": 3210, "text": "took him to London might be concluded in three or four days; but as we"}
{"doc_id": "gutenberg_1342", "para_id": 3211, "text": "are certain it cannot be so, and at the same time convinced that when"}
{"doc_id": "gutenberg_1342", "para_id": 3212, "text": "Charles gets to town he will be in no hurry to leave it again, we have"}
{"doc_id": "gutenberg_1342", "para_id": 3213, "text": "determined on following him thither, that he may not be obliged to spend"}
{"doc_id": "gutenberg_1342", "para_id": 3214, "text": "his vacant hours in a comfortless hotel. Many of my acquaintance are"}
{"doc_id": "gutenberg_1342", "para_id": 3215, "text": "already there for the winter: I wish I could hear that you, my dearest"}
{"doc_id": "gutenberg_1342", "para_id": 3216, "text": "friend, had any intention of making one in the crowd, but of that I"}
{"doc_id": "gutenberg_1342", "para_id": 3217, "text": "despair. I sincerely hope your Christmas in Hertfordshire may abound in"}
{"doc_id": "gutenberg_1342", "para_id": 3218, "text": "the gaieties which that season generally brings, and that your beaux"}
{"doc_id": "gutenberg_1342", "para_id": 3219, "text": "will be so numerous as to prevent your feeling the loss of the three of"}
{"doc_id": "gutenberg_1342", "para_id": 3220, "text": "“It is evident by this,” added Jane, “that he comes back no more this"}
{"doc_id": "gutenberg_1342", "para_id": 3221, "text": "“It is only evident that Miss Bingley does not mean he _should_.”"}
{"doc_id": "gutenberg_1342", "para_id": 3222, "text": "“Why will you think so? It must be his own doing; he is his own master."}
{"doc_id": "gutenberg_1342", "para_id": 3223, "text": "But you do not know _all_. I _will_ read you the passage which"}
{"doc_id": "gutenberg_1342", "para_id": 3224, "text": "particularly hurts me. I will have no reserves from _you_. ‘Mr. Darcy is"}
{"doc_id": "gutenberg_1342", "para_id": 3225, "text": "impatient to see his sister; and to confess the truth, _we_ are scarcely"}
{"doc_id": "gutenberg_1342", "para_id": 3226, "text": "less eager to meet her again. I really do not think Georgiana Darcy has"}
{"doc_id": "gutenberg_1342", "para_id": 3227, "text": "her equal for beauty, elegance, and accomplishments; and the affection"}
{"doc_id": "gutenberg_1342", "para_id": 3228, "text": "she inspires in Louisa and myself is heightened into something still"}
{"doc_id": "gutenberg_1342", "para_id": 3229, "text": "more interesting from the hope we dare to entertain of her being"}
{"doc_id": "gutenberg_1342", "para_id": 3230, "text": "hereafter our sister. I do not know whether I ever before mentioned to"}
{"doc_id": "gutenberg_1342", "para_id": 3231, "text": "you my feelings on this subject, but I will not leave the country"}
{"doc_id": "gutenberg_1342", "para_id": 3232, "text": "without confiding them, and I trust you will not esteem them"}
{"doc_id": "gutenberg_1342", "para_id": 3233, "text": "unreasonable. My brother admires her greatly already; he will have"}
{"doc_id": "gutenberg_1342", "para_id": 3234, "text": "frequent opportunity now of seeing her on the most intimate footing; her"}
{"doc_id": "gutenberg_1342", "para_id": 3235, "text": "relations all wish the connection as much as his own; and a sister’s"}
{"doc_id": "gutenberg_1342", "para_id": 3236, "text": "partiality is not misleading me, I think, when I call Charles most"}
{"doc_id": "gutenberg_1342", "para_id": 3237, "text": "capable of engaging any woman’s heart. With all these circumstances to"}
{"doc_id": "gutenberg_1342", "para_id": 3238, "text": "favour an attachment, and nothing to prevent it, am I wrong, my dearest"}
{"doc_id": "gutenberg_1342", "para_id": 3239, "text": "Jane, in indulging the hope of an event which will secure the happiness"}
{"doc_id": "gutenberg_1342", "para_id": 3240, "text": "of so many?’ What think you of _this_ sentence, my dear Lizzy?” said"}
{"doc_id": "gutenberg_1342", "para_id": 3241, "text": "Jane, as she finished it. “Is it not clear enough? Does it not expressly"}
{"doc_id": "gutenberg_1342", "para_id": 3242, "text": "declare that Caroline neither expects nor wishes me to be her sister;"}
{"doc_id": "gutenberg_1342", "para_id": 3243, "text": "that she is perfectly convinced of her brother’s indifference; and that"}
{"doc_id": "gutenberg_1342", "para_id": 3244, "text": "if she suspects the nature of my feelings for him she means (most"}
{"doc_id": "gutenberg_1342", "para_id": 3245, "text": "kindly!) to put me on my guard. Can there be any other opinion on the"}
{"doc_id": "gutenberg_1342", "para_id": 3246, "text": "“Yes, there can; for mine is totally different. Will you hear it?”"}
{"doc_id": "gutenberg_1342", "para_id": 3247, "text": "“You shall have it in a few words. Miss Bingley sees that her brother is"}
{"doc_id": "gutenberg_1342", "para_id": 3248, "text": "in love with you and wants him to marry Miss Darcy. She follows him to"}
{"doc_id": "gutenberg_1342", "para_id": 3249, "text": "town in the hope of keeping him there, and tries to persuade you that he"}
{"doc_id": "gutenberg_1342", "para_id": 3250, "text": "“Indeed, Jane, you ought to believe me. No one who has ever seen you"}
{"doc_id": "gutenberg_1342", "para_id": 3251, "text": "together can doubt his affection; Miss Bingley, I am sure, cannot: she"}
{"doc_id": "gutenberg_1342", "para_id": 3252, "text": "is not such a simpleton. Could she have seen half as much love in Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3253, "text": "Darcy for herself, she would have ordered her wedding clothes. But the"}
{"doc_id": "gutenberg_1342", "para_id": 3254, "text": "case is this:--we are not rich enough or grand enough for them; and she"}
{"doc_id": "gutenberg_1342", "para_id": 3255, "text": "is the more anxious to get Miss Darcy for her brother, from the notion"}
{"doc_id": "gutenberg_1342", "para_id": 3256, "text": "that when there has been _one_ inter-marriage, she may have less trouble"}
{"doc_id": "gutenberg_1342", "para_id": 3257, "text": "in achieving a second; in which there is certainly some ingenuity, and I"}
{"doc_id": "gutenberg_1342", "para_id": 3258, "text": "dare say it would succeed if Miss de Bourgh were out of the way. But, my"}
{"doc_id": "gutenberg_1342", "para_id": 3259, "text": "dearest Jane, you cannot seriously imagine that, because Miss Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 3260, "text": "tells you her brother greatly admires Miss Darcy, he is in the smallest"}
{"doc_id": "gutenberg_1342", "para_id": 3261, "text": "degree less sensible of _your_ merit than when he took leave of you on"}
{"doc_id": "gutenberg_1342", "para_id": 3262, "text": "Tuesday; or that it will be in her power to persuade him that, instead"}
{"doc_id": "gutenberg_1342", "para_id": 3263, "text": "of being in love with you, he is very much in love with her friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 3264, "text": "“If we thought alike of Miss Bingley,” replied Jane, “your"}
{"doc_id": "gutenberg_1342", "para_id": 3265, "text": "representation of all this might make me quite easy. But I know the"}
{"doc_id": "gutenberg_1342", "para_id": 3266, "text": "foundation is unjust. Caroline is incapable of wilfully deceiving"}
{"doc_id": "gutenberg_1342", "para_id": 3267, "text": "anyone; and all that I can hope in this case is, that she is deceived"}
{"doc_id": "gutenberg_1342", "para_id": 3268, "text": "“That is right. You could not have started a more happy idea, since you"}
{"doc_id": "gutenberg_1342", "para_id": 3269, "text": "will not take comfort in mine: believe her to be deceived, by all means."}
{"doc_id": "gutenberg_1342", "para_id": 3270, "text": "You have now done your duty by her, and must fret no longer.”"}
{"doc_id": "gutenberg_1342", "para_id": 3271, "text": "“But, my dear sister, can I be happy, even supposing the best, in"}
{"doc_id": "gutenberg_1342", "para_id": 3272, "text": "accepting a man whose sisters and friends are all wishing him to marry"}
{"doc_id": "gutenberg_1342", "para_id": 3273, "text": "“You must decide for yourself,” said Elizabeth; “and if, upon mature"}
{"doc_id": "gutenberg_1342", "para_id": 3274, "text": "deliberation, you find that the misery of disobliging his two sisters is"}
{"doc_id": "gutenberg_1342", "para_id": 3275, "text": "more than equivalent to the happiness of being his wife, I advise you,"}
{"doc_id": "gutenberg_1342", "para_id": 3276, "text": "“How can you talk so?” said Jane, faintly smiling; “you must know, that,"}
{"doc_id": "gutenberg_1342", "para_id": 3277, "text": "though I should be exceedingly grieved at their disapprobation, I could"}
{"doc_id": "gutenberg_1342", "para_id": 3278, "text": "“I did not think you would; and that being the case, I cannot consider"}
{"doc_id": "gutenberg_1342", "para_id": 3279, "text": "“But if he returns no more this winter, my choice will never be"}
{"doc_id": "gutenberg_1342", "para_id": 3280, "text": "required. A thousand things may arise in six months.”"}
{"doc_id": "gutenberg_1342", "para_id": 3281, "text": "The idea of his returning no more Elizabeth treated with the utmost"}
{"doc_id": "gutenberg_1342", "para_id": 3282, "text": "contempt. It appeared to her merely the suggestion of Caroline’s"}
{"doc_id": "gutenberg_1342", "para_id": 3283, "text": "interested wishes; and she could not for a moment suppose that those"}
{"doc_id": "gutenberg_1342", "para_id": 3284, "text": "wishes, however openly or artfully spoken, could influence a young man"}
{"doc_id": "gutenberg_1342", "para_id": 3285, "text": "She represented to her sister, as forcibly as possible, what she felt on"}
{"doc_id": "gutenberg_1342", "para_id": 3286, "text": "the subject, and had soon the pleasure of seeing its happy effect."}
{"doc_id": "gutenberg_1342", "para_id": 3287, "text": "Jane’s temper was not desponding; and she was gradually led to hope,"}
{"doc_id": "gutenberg_1342", "para_id": 3288, "text": "though the diffidence of affection sometimes overcame the hope, that"}
{"doc_id": "gutenberg_1342", "para_id": 3289, "text": "Bingley would return to Netherfield, and answer every wish of her heart."}
{"doc_id": "gutenberg_1342", "para_id": 3290, "text": "They agreed that Mrs. Bennet should only hear of the departure of the"}
{"doc_id": "gutenberg_1342", "para_id": 3291, "text": "family, without being alarmed on the score of the gentleman’s conduct;"}
{"doc_id": "gutenberg_1342", "para_id": 3292, "text": "but even this partial communication gave her a great deal of concern,"}
{"doc_id": "gutenberg_1342", "para_id": 3293, "text": "and she bewailed it as exceedingly unlucky that the ladies should happen"}
{"doc_id": "gutenberg_1342", "para_id": 3294, "text": "to go away just as they were all getting so intimate together. After"}
{"doc_id": "gutenberg_1342", "para_id": 3295, "text": "lamenting it, however, at some length, she had the consolation of"}
{"doc_id": "gutenberg_1342", "para_id": 3296, "text": "thinking that Mr. Bingley would be soon down again, and soon dining at"}
{"doc_id": "gutenberg_1342", "para_id": 3297, "text": "Longbourn; and the conclusion of all was the comfortable declaration,"}
{"doc_id": "gutenberg_1342", "para_id": 3298, "text": "that, though he had been invited only to a family dinner, she would take"}
{"doc_id": "gutenberg_1342", "para_id": 3299, "text": "The Bennets were engaged to dine with the Lucases; and again, during the"}
{"doc_id": "gutenberg_1342", "para_id": 3300, "text": "chief of the day, was Miss Lucas so kind as to listen to Mr. Collins."}
{"doc_id": "gutenberg_1342", "para_id": 3301, "text": "Elizabeth took an opportunity of thanking her. “It keeps him in good"}
{"doc_id": "gutenberg_1342", "para_id": 3302, "text": "humour,” said she, “and I am more obliged to you than I can express.”"}
{"doc_id": "gutenberg_1342", "para_id": 3303, "text": "Charlotte assured her friend of her satisfaction in being useful, and"}
{"doc_id": "gutenberg_1342", "para_id": 3304, "text": "that it amply repaid her for the little sacrifice of her time. This was"}
{"doc_id": "gutenberg_1342", "para_id": 3305, "text": "very amiable; but Charlotte’s kindness extended farther than Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 3306, "text": "had any conception of:--its object was nothing less than to secure her"}
{"doc_id": "gutenberg_1342", "para_id": 3307, "text": "from any return of Mr. Collins’s addresses, by engaging them towards"}
{"doc_id": "gutenberg_1342", "para_id": 3308, "text": "herself. Such was Miss Lucas’s scheme; and appearances were so"}
{"doc_id": "gutenberg_1342", "para_id": 3309, "text": "favourable, that when they parted at night, she would have felt almost"}
{"doc_id": "gutenberg_1342", "para_id": 3310, "text": "sure of success if he had not been to leave Hertfordshire so very soon."}
{"doc_id": "gutenberg_1342", "para_id": 3311, "text": "But here she did injustice to the fire and independence of his"}
{"doc_id": "gutenberg_1342", "para_id": 3312, "text": "character; for it led him to escape out of Longbourn House the next"}
{"doc_id": "gutenberg_1342", "para_id": 3313, "text": "morning with admirable slyness, and hasten to Lucas Lodge to throw"}
{"doc_id": "gutenberg_1342", "para_id": 3314, "text": "himself at her feet. He was anxious to avoid the notice of his cousins,"}
{"doc_id": "gutenberg_1342", "para_id": 3315, "text": "from a conviction that, if they saw him depart, they could not fail to"}
{"doc_id": "gutenberg_1342", "para_id": 3316, "text": "conjecture his design, and he was not willing to have the attempt known"}
{"doc_id": "gutenberg_1342", "para_id": 3317, "text": "till its success could be known likewise; for, though feeling almost"}
{"doc_id": "gutenberg_1342", "para_id": 3318, "text": "secure, and with reason, for Charlotte had been tolerably encouraging,"}
{"doc_id": "gutenberg_1342", "para_id": 3319, "text": "he was comparatively diffident since the adventure of Wednesday. His"}
{"doc_id": "gutenberg_1342", "para_id": 3320, "text": "reception, however, was of the most flattering kind. Miss Lucas"}
{"doc_id": "gutenberg_1342", "para_id": 3321, "text": "perceived him from an upper window as he walked towards the house, and"}
{"doc_id": "gutenberg_1342", "para_id": 3322, "text": "instantly set out to meet him accidentally in the lane. But little had"}
{"doc_id": "gutenberg_1342", "para_id": 3323, "text": "she dared to hope that so much love and eloquence awaited her there."}
{"doc_id": "gutenberg_1342", "para_id": 3324, "text": "In as short a time as Mr. Collins’s long speeches would allow,"}
{"doc_id": "gutenberg_1342", "para_id": 3325, "text": "everything was settled between them to the satisfaction of both; and as"}
{"doc_id": "gutenberg_1342", "para_id": 3326, "text": "they entered the house, he earnestly entreated her to name the day that"}
{"doc_id": "gutenberg_1342", "para_id": 3327, "text": "was to make him the happiest of men; and though such a solicitation must"}
{"doc_id": "gutenberg_1342", "para_id": 3328, "text": "be waived for the present, the lady felt no inclination to trifle with"}
{"doc_id": "gutenberg_1342", "para_id": 3329, "text": "his happiness. The stupidity with which he was favoured by nature must"}
{"doc_id": "gutenberg_1342", "para_id": 3330, "text": "guard his courtship from any charm that could make a woman wish for its"}
{"doc_id": "gutenberg_1342", "para_id": 3331, "text": "continuance; and Miss Lucas, who accepted him solely from the pure and"}
{"doc_id": "gutenberg_1342", "para_id": 3332, "text": "disinterested desire of an establishment, cared not how soon that"}
{"doc_id": "gutenberg_1342", "para_id": 3333, "text": "Sir William and Lady Lucas were speedily applied to for their consent;"}
{"doc_id": "gutenberg_1342", "para_id": 3334, "text": "and it was bestowed with a most joyful alacrity. Mr. Collins’s present"}
{"doc_id": "gutenberg_1342", "para_id": 3335, "text": "circumstances made it a most eligible match for their daughter, to whom"}
{"doc_id": "gutenberg_1342", "para_id": 3336, "text": "they could give little fortune; and his prospects of future wealth were"}
{"doc_id": "gutenberg_1342", "para_id": 3337, "text": "exceedingly fair. Lady Lucas began directly to calculate, with more"}
{"doc_id": "gutenberg_1342", "para_id": 3338, "text": "excited before, how many years longer Mr. Bennet was likely to live; and"}
{"doc_id": "gutenberg_1342", "para_id": 3339, "text": "Sir William gave it as his decided opinion, that whenever Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 3340, "text": "should be in possession of the Longbourn estate, it would be highly"}
{"doc_id": "gutenberg_1342", "para_id": 3341, "text": "expedient that both he and his wife should make their appearance at St."}
{"doc_id": "gutenberg_1342", "para_id": 3342, "text": "James’s. The whole family in short were properly overjoyed on the"}
{"doc_id": "gutenberg_1342", "para_id": 3343, "text": "occasion. The younger girls formed hopes of _coming out_ a year or two"}
{"doc_id": "gutenberg_1342", "para_id": 3344, "text": "sooner than they might otherwise have done; and the boys were relieved"}
{"doc_id": "gutenberg_1342", "para_id": 3345, "text": "from their apprehension of Charlotte’s dying an old maid. Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 3346, "text": "herself was tolerably composed. She had gained her point, and had time"}
{"doc_id": "gutenberg_1342", "para_id": 3347, "text": "to consider of it. Her reflections were in general satisfactory. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3348, "text": "Collins, to be sure, was neither sensible nor agreeable: his society was"}
{"doc_id": "gutenberg_1342", "para_id": 3349, "text": "irksome, and his attachment to her must be imaginary. But still he would"}
{"doc_id": "gutenberg_1342", "para_id": 3350, "text": "be her husband. Without thinking highly either of men or of matrimony,"}
{"doc_id": "gutenberg_1342", "para_id": 3351, "text": "marriage had always been her object: it was the only honourable"}
{"doc_id": "gutenberg_1342", "para_id": 3352, "text": "provision for well-educated young women of small fortune, and, however"}
{"doc_id": "gutenberg_1342", "para_id": 3353, "text": "uncertain of giving happiness, must be their pleasantest preservative"}
{"doc_id": "gutenberg_1342", "para_id": 3354, "text": "from want. This preservative she had now obtained; and at the age of"}
{"doc_id": "gutenberg_1342", "para_id": 3355, "text": "twenty-seven, without having ever been handsome, she felt all the good"}
{"doc_id": "gutenberg_1342", "para_id": 3356, "text": "luck of it. The least agreeable circumstance in the business was the"}
{"doc_id": "gutenberg_1342", "para_id": 3357, "text": "surprise it must occasion to Elizabeth Bennet, whose friendship she"}
{"doc_id": "gutenberg_1342", "para_id": 3358, "text": "valued beyond that of any other person. Elizabeth would wonder, and"}
{"doc_id": "gutenberg_1342", "para_id": 3359, "text": "probably would blame her; and though her resolution was not to be"}
{"doc_id": "gutenberg_1342", "para_id": 3360, "text": "shaken, her feelings must be hurt by such a disapprobation. She resolved"}
{"doc_id": "gutenberg_1342", "para_id": 3361, "text": "to give her the information herself; and therefore charged Mr. Collins,"}
{"doc_id": "gutenberg_1342", "para_id": 3362, "text": "when he returned to Longbourn to dinner, to drop no hint of what had"}
{"doc_id": "gutenberg_1342", "para_id": 3363, "text": "passed before any of the family. A promise of secrecy was of course very"}
{"doc_id": "gutenberg_1342", "para_id": 3364, "text": "dutifully given, but it could not be kept without difficulty; for the"}
{"doc_id": "gutenberg_1342", "para_id": 3365, "text": "curiosity excited by his long absence burst forth in such very direct"}
{"doc_id": "gutenberg_1342", "para_id": 3366, "text": "questions on his return, as required some ingenuity to evade, and he was"}
{"doc_id": "gutenberg_1342", "para_id": 3367, "text": "at the same time exercising great self-denial, for he was longing to"}
{"doc_id": "gutenberg_1342", "para_id": 3368, "text": "As he was to begin his journey too early on the morrow to see any of"}
{"doc_id": "gutenberg_1342", "para_id": 3369, "text": "the family, the ceremony of leave-taking was performed when the ladies"}
{"doc_id": "gutenberg_1342", "para_id": 3370, "text": "moved for the night; and Mrs. Bennet, with great politeness and"}
{"doc_id": "gutenberg_1342", "para_id": 3371, "text": "cordiality, said how happy they should be to see him at Longbourn again,"}
{"doc_id": "gutenberg_1342", "para_id": 3372, "text": "whenever his other engagements might allow him to visit them."}
{"doc_id": "gutenberg_1342", "para_id": 3373, "text": "“My dear madam,” he replied, “this invitation is particularly"}
{"doc_id": "gutenberg_1342", "para_id": 3374, "text": "gratifying, because it is what I have been hoping to receive; and you"}
{"doc_id": "gutenberg_1342", "para_id": 3375, "text": "may be very certain that I shall avail myself of it as soon as"}
{"doc_id": "gutenberg_1342", "para_id": 3376, "text": "They were all astonished; and Mr. Bennet, who could by no means wish for"}
{"doc_id": "gutenberg_1342", "para_id": 3377, "text": "“But is there not danger of Lady Catherine’s disapprobation here, my"}
{"doc_id": "gutenberg_1342", "para_id": 3378, "text": "good sir? You had better neglect your relations than run the risk of"}
{"doc_id": "gutenberg_1342", "para_id": 3379, "text": "“My dear sir,” replied Mr. Collins, “I am particularly obliged to you"}
{"doc_id": "gutenberg_1342", "para_id": 3380, "text": "for this friendly caution, and you may depend upon my not taking so"}
{"doc_id": "gutenberg_1342", "para_id": 3381, "text": "material a step without her Ladyship’s concurrence.”"}
{"doc_id": "gutenberg_1342", "para_id": 3382, "text": "“You cannot be too much on your guard. Risk anything rather than her"}
{"doc_id": "gutenberg_1342", "para_id": 3383, "text": "displeasure; and if you find it likely to be raised by your coming to us"}
{"doc_id": "gutenberg_1342", "para_id": 3384, "text": "again, which I should think exceedingly probable, stay quietly at home,"}
{"doc_id": "gutenberg_1342", "para_id": 3385, "text": "“Believe me, my dear sir, my gratitude is warmly excited by such"}
{"doc_id": "gutenberg_1342", "para_id": 3386, "text": "affectionate attention; and, depend upon it, you will speedily receive"}
{"doc_id": "gutenberg_1342", "para_id": 3387, "text": "from me a letter of thanks for this as well as for every other mark of"}
{"doc_id": "gutenberg_1342", "para_id": 3388, "text": "your regard during my stay in Hertfordshire. As for my fair cousins,"}
{"doc_id": "gutenberg_1342", "para_id": 3389, "text": "though my absence may not be long enough to render it necessary, I shall"}
{"doc_id": "gutenberg_1342", "para_id": 3390, "text": "now take the liberty of wishing them health and happiness, not excepting"}
{"doc_id": "gutenberg_1342", "para_id": 3391, "text": "With proper civilities, the ladies then withdrew; all of them equally"}
{"doc_id": "gutenberg_1342", "para_id": 3392, "text": "surprised to find that he meditated a quick return. Mrs. Bennet wished"}
{"doc_id": "gutenberg_1342", "para_id": 3393, "text": "to understand by it that he thought of paying his addresses to one of"}
{"doc_id": "gutenberg_1342", "para_id": 3394, "text": "her younger girls, and Mary might have been prevailed on to accept him."}
{"doc_id": "gutenberg_1342", "para_id": 3395, "text": "She rated his abilities much higher than any of the others: there was a"}
{"doc_id": "gutenberg_1342", "para_id": 3396, "text": "solidity in his reflections which often struck her; and though by no"}
{"doc_id": "gutenberg_1342", "para_id": 3397, "text": "means so clever as herself, she thought that, if encouraged to read and"}
{"doc_id": "gutenberg_1342", "para_id": 3398, "text": "improve himself by such an example as hers, he might become a very"}
{"doc_id": "gutenberg_1342", "para_id": 3399, "text": "agreeable companion. But on the following morning every hope of this"}
{"doc_id": "gutenberg_1342", "para_id": 3400, "text": "kind was done away. Miss Lucas called soon after breakfast, and in a"}
{"doc_id": "gutenberg_1342", "para_id": 3401, "text": "private conference with Elizabeth related the event of the day before."}
{"doc_id": "gutenberg_1342", "para_id": 3402, "text": "The possibility of Mr. Collins’s fancying himself in love with her"}
{"doc_id": "gutenberg_1342", "para_id": 3403, "text": "friend had once occurred to Elizabeth within the last day or two: but"}
{"doc_id": "gutenberg_1342", "para_id": 3404, "text": "that Charlotte could encourage him seemed almost as far from possibility"}
{"doc_id": "gutenberg_1342", "para_id": 3405, "text": "as that she could encourage him herself; and her astonishment was"}
{"doc_id": "gutenberg_1342", "para_id": 3406, "text": "consequently so great as to overcome at first the bounds of decorum, and"}
{"doc_id": "gutenberg_1342", "para_id": 3407, "text": "“Engaged to Mr. Collins! my dear Charlotte, impossible!”"}
{"doc_id": "gutenberg_1342", "para_id": 3408, "text": "The steady countenance which Miss Lucas had commanded in telling her"}
{"doc_id": "gutenberg_1342", "para_id": 3409, "text": "story gave way to a momentary confusion here on receiving so direct a"}
{"doc_id": "gutenberg_1342", "para_id": 3410, "text": "reproach; though, as it was no more than she expected, she soon regained"}
{"doc_id": "gutenberg_1342", "para_id": 3411, "text": "“Why should you be surprised, my dear Eliza? Do you think it incredible"}
{"doc_id": "gutenberg_1342", "para_id": 3412, "text": "that Mr. Collins should be able to procure any woman’s good opinion,"}
{"doc_id": "gutenberg_1342", "para_id": 3413, "text": "because he was not so happy as to succeed with you?”"}
{"doc_id": "gutenberg_1342", "para_id": 3414, "text": "But Elizabeth had now recollected herself; and, making a strong effort"}
{"doc_id": "gutenberg_1342", "para_id": 3415, "text": "for it, was able to assure her, with tolerable firmness, that the"}
{"doc_id": "gutenberg_1342", "para_id": 3416, "text": "prospect of their relationship was highly grateful to her, and that she"}
{"doc_id": "gutenberg_1342", "para_id": 3417, "text": "“I see what you are feeling,” replied Charlotte; “you must be surprised,"}
{"doc_id": "gutenberg_1342", "para_id": 3418, "text": "very much surprised, so lately as Mr. Collins was wishing to marry you."}
{"doc_id": "gutenberg_1342", "para_id": 3419, "text": "But when you have had time to think it all over, I hope you will be"}
{"doc_id": "gutenberg_1342", "para_id": 3420, "text": "satisfied with what I have done. I am not romantic, you know. I never"}
{"doc_id": "gutenberg_1342", "para_id": 3421, "text": "was. I ask only a comfortable home; and, considering Mr. Collins’s"}
{"doc_id": "gutenberg_1342", "para_id": 3422, "text": "character, connections, and situation in life, I am convinced that my"}
{"doc_id": "gutenberg_1342", "para_id": 3423, "text": "chance of happiness with him is as fair as most people can boast on"}
{"doc_id": "gutenberg_1342", "para_id": 3424, "text": "Elizabeth quietly answered “undoubtedly;” and, after an awkward pause,"}
{"doc_id": "gutenberg_1342", "para_id": 3425, "text": "they returned to the rest of the family. Charlotte did not stay much"}
{"doc_id": "gutenberg_1342", "para_id": 3426, "text": "longer; and Elizabeth was then left to reflect on what she had heard. It"}
{"doc_id": "gutenberg_1342", "para_id": 3427, "text": "was a long time before she became at all reconciled to the idea of so"}
{"doc_id": "gutenberg_1342", "para_id": 3428, "text": "unsuitable a match. The strangeness of Mr. Collins’s making two offers"}
{"doc_id": "gutenberg_1342", "para_id": 3429, "text": "of marriage within three days was nothing in comparison of his being now"}
{"doc_id": "gutenberg_1342", "para_id": 3430, "text": "accepted. She had always felt that Charlotte’s opinion of matrimony was"}
{"doc_id": "gutenberg_1342", "para_id": 3431, "text": "not exactly like her own; but she could not have supposed it possible"}
{"doc_id": "gutenberg_1342", "para_id": 3432, "text": "that, when called into action, she would have sacrificed every better"}
{"doc_id": "gutenberg_1342", "para_id": 3433, "text": "feeling to worldly advantage. Charlotte, the wife of Mr. Collins, was a"}
{"doc_id": "gutenberg_1342", "para_id": 3434, "text": "most humiliating picture! And to the pang of a friend disgracing"}
{"doc_id": "gutenberg_1342", "para_id": 3435, "text": "herself, and sunk in her esteem, was added the distressing conviction"}
{"doc_id": "gutenberg_1342", "para_id": 3436, "text": "that it was impossible for that friend to be tolerably happy in the lot"}
{"doc_id": "gutenberg_1342", "para_id": 3437, "text": "Elizabeth was sitting with her mother and sisters, reflecting on what"}
{"doc_id": "gutenberg_1342", "para_id": 3438, "text": "she had heard, and doubting whether she was authorized to mention it,"}
{"doc_id": "gutenberg_1342", "para_id": 3439, "text": "when Sir William Lucas himself appeared, sent by his daughter to"}
{"doc_id": "gutenberg_1342", "para_id": 3440, "text": "announce her engagement to the family. With many compliments to them,"}
{"doc_id": "gutenberg_1342", "para_id": 3441, "text": "and much self-gratulation on the prospect of a connection between the"}
{"doc_id": "gutenberg_1342", "para_id": 3442, "text": "houses, he unfolded the matter,--to an audience not merely wondering,"}
{"doc_id": "gutenberg_1342", "para_id": 3443, "text": "but incredulous; for Mrs. Bennet, with more perseverance than"}
{"doc_id": "gutenberg_1342", "para_id": 3444, "text": "politeness, protested he must be entirely mistaken; and Lydia, always"}
{"doc_id": "gutenberg_1342", "para_id": 3445, "text": "unguarded and often uncivil, boisterously exclaimed,--"}
{"doc_id": "gutenberg_1342", "para_id": 3446, "text": "“Good Lord! Sir William, how can you tell such a story? Do not you know"}
{"doc_id": "gutenberg_1342", "para_id": 3447, "text": "Nothing less than the complaisance of a courtier could have borne"}
{"doc_id": "gutenberg_1342", "para_id": 3448, "text": "without anger such treatment: but Sir William’s good-breeding carried"}
{"doc_id": "gutenberg_1342", "para_id": 3449, "text": "him through it all; and though he begged leave to be positive as to the"}
{"doc_id": "gutenberg_1342", "para_id": 3450, "text": "truth of his information, he listened to all their impertinence with the"}
{"doc_id": "gutenberg_1342", "para_id": 3451, "text": "Elizabeth, feeling it incumbent on her to relieve him from so unpleasant"}
{"doc_id": "gutenberg_1342", "para_id": 3452, "text": "a situation, now put herself forward to confirm his account, by"}
{"doc_id": "gutenberg_1342", "para_id": 3453, "text": "mentioning her prior knowledge of it from Charlotte herself; and"}
{"doc_id": "gutenberg_1342", "para_id": 3454, "text": "endeavoured to put a stop to the exclamations of her mother and sisters,"}
{"doc_id": "gutenberg_1342", "para_id": 3455, "text": "by the earnestness of her congratulations to Sir William, in which she"}
{"doc_id": "gutenberg_1342", "para_id": 3456, "text": "was readily joined by Jane, and by making a variety of remarks on the"}
{"doc_id": "gutenberg_1342", "para_id": 3457, "text": "happiness that might be expected from the match, the excellent character"}
{"doc_id": "gutenberg_1342", "para_id": 3458, "text": "of Mr. Collins, and the convenient distance of Hunsford from London."}
{"doc_id": "gutenberg_1342", "para_id": 3459, "text": "Mrs. Bennet was, in fact, too much overpowered to say a great deal while"}
{"doc_id": "gutenberg_1342", "para_id": 3460, "text": "Sir William remained; but no sooner had he left them than her feelings"}
{"doc_id": "gutenberg_1342", "para_id": 3461, "text": "found a rapid vent. In the first place, she persisted in disbelieving"}
{"doc_id": "gutenberg_1342", "para_id": 3462, "text": "the whole of the matter; secondly, she was very sure that Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 3463, "text": "had been taken in; thirdly, she trusted that they would never be happy"}
{"doc_id": "gutenberg_1342", "para_id": 3464, "text": "together; and, fourthly, that the match might be broken off. Two"}
{"doc_id": "gutenberg_1342", "para_id": 3465, "text": "inferences, however, were plainly deduced from the whole: one, that"}
{"doc_id": "gutenberg_1342", "para_id": 3466, "text": "Elizabeth was the real cause of all the mischief; and the other, that"}
{"doc_id": "gutenberg_1342", "para_id": 3467, "text": "she herself had been barbarously used by them all; and on these two"}
{"doc_id": "gutenberg_1342", "para_id": 3468, "text": "points she principally dwelt during the rest of the day. Nothing could"}
{"doc_id": "gutenberg_1342", "para_id": 3469, "text": "console and nothing appease her. Nor did that day wear out her"}
{"doc_id": "gutenberg_1342", "para_id": 3470, "text": "resentment. A week elapsed before she could see Elizabeth without"}
{"doc_id": "gutenberg_1342", "para_id": 3471, "text": "scolding her: a month passed away before she could speak to Sir William"}
{"doc_id": "gutenberg_1342", "para_id": 3472, "text": "or Lady Lucas without being rude; and many months were gone before she"}
{"doc_id": "gutenberg_1342", "para_id": 3473, "text": "Mr. Bennet’s emotions were much more tranquil on the occasion, and such"}
{"doc_id": "gutenberg_1342", "para_id": 3474, "text": "as he did experience he pronounced to be of a most agreeable sort; for"}
{"doc_id": "gutenberg_1342", "para_id": 3475, "text": "it gratified him, he said, to discover that Charlotte Lucas, whom he had"}
{"doc_id": "gutenberg_1342", "para_id": 3476, "text": "been used to think tolerably sensible, was as foolish as his wife, and"}
{"doc_id": "gutenberg_1342", "para_id": 3477, "text": "Jane confessed herself a little surprised at the match: but she said"}
{"doc_id": "gutenberg_1342", "para_id": 3478, "text": "less of her astonishment than of her earnest desire for their happiness;"}
{"doc_id": "gutenberg_1342", "para_id": 3479, "text": "nor could Elizabeth persuade her to consider it as improbable. Kitty and"}
{"doc_id": "gutenberg_1342", "para_id": 3480, "text": "Lydia were far from envying Miss Lucas, for Mr. Collins was only a"}
{"doc_id": "gutenberg_1342", "para_id": 3481, "text": "clergyman; and it affected them in no other way than as a piece of news"}
{"doc_id": "gutenberg_1342", "para_id": 3482, "text": "Lady Lucas could not be insensible of triumph on being able to retort on"}
{"doc_id": "gutenberg_1342", "para_id": 3483, "text": "Mrs. Bennet the comfort of having a daughter well married; and she"}
{"doc_id": "gutenberg_1342", "para_id": 3484, "text": "called at Longbourn rather oftener than usual to say how happy she was,"}
{"doc_id": "gutenberg_1342", "para_id": 3485, "text": "though Mrs. Bennet’s sour looks and ill-natured remarks might have been"}
{"doc_id": "gutenberg_1342", "para_id": 3486, "text": "Between Elizabeth and Charlotte there was a restraint which kept them"}
{"doc_id": "gutenberg_1342", "para_id": 3487, "text": "mutually silent on the subject; and Elizabeth felt persuaded that no"}
{"doc_id": "gutenberg_1342", "para_id": 3488, "text": "real confidence could ever subsist between them again. Her"}
{"doc_id": "gutenberg_1342", "para_id": 3489, "text": "disappointment in Charlotte made her turn with fonder regard to her"}
{"doc_id": "gutenberg_1342", "para_id": 3490, "text": "sister, of whose rectitude and delicacy she was sure her opinion could"}
{"doc_id": "gutenberg_1342", "para_id": 3491, "text": "never be shaken, and for whose happiness she grew daily more anxious, as"}
{"doc_id": "gutenberg_1342", "para_id": 3492, "text": "Bingley had now been gone a week, and nothing was heard of his return."}
{"doc_id": "gutenberg_1342", "para_id": 3493, "text": "Jane had sent Caroline an early answer to her letter, and was counting"}
{"doc_id": "gutenberg_1342", "para_id": 3494, "text": "the days till she might reasonably hope to hear again. The promised"}
{"doc_id": "gutenberg_1342", "para_id": 3495, "text": "letter of thanks from Mr. Collins arrived on Tuesday, addressed to their"}
{"doc_id": "gutenberg_1342", "para_id": 3496, "text": "father, and written with all the solemnity of gratitude which a"}
{"doc_id": "gutenberg_1342", "para_id": 3497, "text": "twelve-month’s abode in the family might have prompted. After"}
{"doc_id": "gutenberg_1342", "para_id": 3498, "text": "discharging his conscience on that head, he proceeded to inform them,"}
{"doc_id": "gutenberg_1342", "para_id": 3499, "text": "with many rapturous expressions, of his happiness in having obtained the"}
{"doc_id": "gutenberg_1342", "para_id": 3500, "text": "affection of their amiable neighbour, Miss Lucas, and then explained"}
{"doc_id": "gutenberg_1342", "para_id": 3501, "text": "that it was merely with the view of enjoying her society that he had"}
{"doc_id": "gutenberg_1342", "para_id": 3502, "text": "been so ready to close with their kind wish of seeing him again at"}
{"doc_id": "gutenberg_1342", "para_id": 3503, "text": "Longbourn, whither he hoped to be able to return on Monday fortnight;"}
{"doc_id": "gutenberg_1342", "para_id": 3504, "text": "for Lady Catherine, he added, so heartily approved his marriage, that"}
{"doc_id": "gutenberg_1342", "para_id": 3505, "text": "she wished it to take place as soon as possible, which he trusted would"}
{"doc_id": "gutenberg_1342", "para_id": 3506, "text": "be an unanswerable argument with his amiable Charlotte to name an early"}
{"doc_id": "gutenberg_1342", "para_id": 3507, "text": "Mr. Collins’s return into Hertfordshire was no longer a matter of"}
{"doc_id": "gutenberg_1342", "para_id": 3508, "text": "pleasure to Mrs. Bennet. On the contrary, she was as much disposed to"}
{"doc_id": "gutenberg_1342", "para_id": 3509, "text": "complain of it as her husband. It was very strange that he should come"}
{"doc_id": "gutenberg_1342", "para_id": 3510, "text": "to Longbourn instead of to Lucas Lodge; it was also very inconvenient"}
{"doc_id": "gutenberg_1342", "para_id": 3511, "text": "and exceedingly troublesome. She hated having visitors in the house"}
{"doc_id": "gutenberg_1342", "para_id": 3512, "text": "while her health was so indifferent, and lovers were of all people the"}
{"doc_id": "gutenberg_1342", "para_id": 3513, "text": "most disagreeable. Such were the gentle murmurs of Mrs. Bennet, and they"}
{"doc_id": "gutenberg_1342", "para_id": 3514, "text": "gave way only to the greater distress of Mr. Bingley’s continued"}
{"doc_id": "gutenberg_1342", "para_id": 3515, "text": "Neither Jane nor Elizabeth were comfortable on this subject. Day after"}
{"doc_id": "gutenberg_1342", "para_id": 3516, "text": "day passed away without bringing any other tidings of him than the"}
{"doc_id": "gutenberg_1342", "para_id": 3517, "text": "report which shortly prevailed in Meryton of his coming no more to"}
{"doc_id": "gutenberg_1342", "para_id": 3518, "text": "Netherfield the whole winter; a report which highly incensed Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 3519, "text": "Bennet, and which she never failed to contradict as a most scandalous"}
{"doc_id": "gutenberg_1342", "para_id": 3520, "text": "Even Elizabeth began to fear--not that Bingley was indifferent--but that"}
{"doc_id": "gutenberg_1342", "para_id": 3521, "text": "his sisters would be successful in keeping him away. Unwilling as she"}
{"doc_id": "gutenberg_1342", "para_id": 3522, "text": "was to admit an idea so destructive to Jane’s happiness, and so"}
{"doc_id": "gutenberg_1342", "para_id": 3523, "text": "dishonourable to the stability of her lover, she could not prevent its"}
{"doc_id": "gutenberg_1342", "para_id": 3524, "text": "frequently recurring. The united efforts of his two unfeeling sisters,"}
{"doc_id": "gutenberg_1342", "para_id": 3525, "text": "and of his overpowering friend, assisted by the attractions of Miss"}
{"doc_id": "gutenberg_1342", "para_id": 3526, "text": "Darcy and the amusements of London, might be too much, she feared, for"}
{"doc_id": "gutenberg_1342", "para_id": 3527, "text": "As for Jane, _her_ anxiety under this suspense was, of course, more"}
{"doc_id": "gutenberg_1342", "para_id": 3528, "text": "painful than Elizabeth’s: but whatever she felt she was desirous of"}
{"doc_id": "gutenberg_1342", "para_id": 3529, "text": "concealing; and between herself and Elizabeth, therefore, the subject"}
{"doc_id": "gutenberg_1342", "para_id": 3530, "text": "was never alluded to. But as no such delicacy restrained her mother, an"}
{"doc_id": "gutenberg_1342", "para_id": 3531, "text": "hour seldom passed in which she did not talk of Bingley, express her"}
{"doc_id": "gutenberg_1342", "para_id": 3532, "text": "impatience for his arrival, or even require Jane to confess that if he"}
{"doc_id": "gutenberg_1342", "para_id": 3533, "text": "did not come back she should think herself very ill-used. It needed all"}
{"doc_id": "gutenberg_1342", "para_id": 3534, "text": "Jane’s steady mildness to bear these attacks with tolerable"}
{"doc_id": "gutenberg_1342", "para_id": 3535, "text": "Mr. Collins returned most punctually on the Monday fortnight, but his"}
{"doc_id": "gutenberg_1342", "para_id": 3536, "text": "reception at Longbourn was not quite so gracious as it had been on his"}
{"doc_id": "gutenberg_1342", "para_id": 3537, "text": "first introduction. He was too happy, however, to need much attention;"}
{"doc_id": "gutenberg_1342", "para_id": 3538, "text": "and, luckily for the others, the business of love-making relieved them"}
{"doc_id": "gutenberg_1342", "para_id": 3539, "text": "from a great deal of his company. The chief of every day was spent by"}
{"doc_id": "gutenberg_1342", "para_id": 3540, "text": "him at Lucas Lodge, and he sometimes returned to Longbourn only in time"}
{"doc_id": "gutenberg_1342", "para_id": 3541, "text": "to make an apology for his absence before the family went to bed."}
{"doc_id": "gutenberg_1342", "para_id": 3542, "text": "Mrs. Bennet was really in a most pitiable state. The very mention of"}
{"doc_id": "gutenberg_1342", "para_id": 3543, "text": "anything concerning the match threw her into an agony of ill-humour, and"}
{"doc_id": "gutenberg_1342", "para_id": 3544, "text": "wherever she went she was sure of hearing it talked of. The sight of"}
{"doc_id": "gutenberg_1342", "para_id": 3545, "text": "Miss Lucas was odious to her. As her successor in that house, she"}
{"doc_id": "gutenberg_1342", "para_id": 3546, "text": "regarded her with jealous abhorrence. Whenever Charlotte came to see"}
{"doc_id": "gutenberg_1342", "para_id": 3547, "text": "them, she concluded her to be anticipating the hour of possession; and"}
{"doc_id": "gutenberg_1342", "para_id": 3548, "text": "whenever she spoke in a low voice to Mr. Collins, was convinced that"}
{"doc_id": "gutenberg_1342", "para_id": 3549, "text": "they were talking of the Longbourn estate, and resolving to turn herself"}
{"doc_id": "gutenberg_1342", "para_id": 3550, "text": "and her daughters out of the house as soon as Mr. Bennet was dead. She"}
{"doc_id": "gutenberg_1342", "para_id": 3551, "text": "“Indeed, Mr. Bennet,” said she, “it is very hard to think that Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 3552, "text": "Lucas should ever be mistress of this house, that _I_ should be forced"}
{"doc_id": "gutenberg_1342", "para_id": 3553, "text": "to make way for _her_, and live to see her take my place in it!”"}
{"doc_id": "gutenberg_1342", "para_id": 3554, "text": "“My dear, do not give way to such gloomy thoughts. Let us hope for"}
{"doc_id": "gutenberg_1342", "para_id": 3555, "text": "better things. Let us flatter ourselves that _I_ may be the survivor.”"}
{"doc_id": "gutenberg_1342", "para_id": 3556, "text": "This was not very consoling to Mrs. Bennet; and, therefore, instead of"}
{"doc_id": "gutenberg_1342", "para_id": 3557, "text": "“I cannot bear to think that they should have all this estate. If it was"}
{"doc_id": "gutenberg_1342", "para_id": 3558, "text": "“Let us be thankful that you are preserved from a state of such"}
{"doc_id": "gutenberg_1342", "para_id": 3559, "text": "“I never can be thankful, Mr. Bennet, for anything about the entail. How"}
{"doc_id": "gutenberg_1342", "para_id": 3560, "text": "anyone could have the conscience to entail away an estate from one’s own"}
{"doc_id": "gutenberg_1342", "para_id": 3561, "text": "daughters I cannot understand; and all for the sake of Mr. Collins, too!"}
{"doc_id": "gutenberg_1342", "para_id": 3562, "text": "“I leave it to yourself to determine,” said Mr. Bennet."}
{"doc_id": "gutenberg_1342", "para_id": 3563, "text": "Miss Bingley’s letter arrived, and put an end to doubt. The very first"}
{"doc_id": "gutenberg_1342", "para_id": 3564, "text": "sentence conveyed the assurance of their being all settled in London for"}
{"doc_id": "gutenberg_1342", "para_id": 3565, "text": "the winter, and concluded with her brother’s regret at not having had"}
{"doc_id": "gutenberg_1342", "para_id": 3566, "text": "time to pay his respects to his friends in Hertfordshire before he left"}
{"doc_id": "gutenberg_1342", "para_id": 3567, "text": "Hope was over, entirely over; and when Jane could attend to the rest of"}
{"doc_id": "gutenberg_1342", "para_id": 3568, "text": "the letter, she found little, except the professed affection of the"}
{"doc_id": "gutenberg_1342", "para_id": 3569, "text": "writer, that could give her any comfort. Miss Darcy’s praise occupied"}
{"doc_id": "gutenberg_1342", "para_id": 3570, "text": "the chief of it. Her many attractions were again dwelt on; and Caroline"}
{"doc_id": "gutenberg_1342", "para_id": 3571, "text": "boasted joyfully of their increasing intimacy, and ventured to predict"}
{"doc_id": "gutenberg_1342", "para_id": 3572, "text": "the accomplishment of the wishes which had been unfolded in her former"}
{"doc_id": "gutenberg_1342", "para_id": 3573, "text": "letter. She wrote also with great pleasure of her brother’s being an"}
{"doc_id": "gutenberg_1342", "para_id": 3574, "text": "inmate of Mr. Darcy’s house, and mentioned with raptures some plans of"}
{"doc_id": "gutenberg_1342", "para_id": 3575, "text": "Elizabeth, to whom Jane very soon communicated the chief of all this,"}
{"doc_id": "gutenberg_1342", "para_id": 3576, "text": "heard it in silent indignation. Her heart was divided between concern"}
{"doc_id": "gutenberg_1342", "para_id": 3577, "text": "for her sister and resentment against all others. To Caroline’s"}
{"doc_id": "gutenberg_1342", "para_id": 3578, "text": "assertion of her brother’s being partial to Miss Darcy, she paid no"}
{"doc_id": "gutenberg_1342", "para_id": 3579, "text": "credit. That he was really fond of Jane, she doubted no more than she"}
{"doc_id": "gutenberg_1342", "para_id": 3580, "text": "had ever done; and much as she had always been disposed to like him, she"}
{"doc_id": "gutenberg_1342", "para_id": 3581, "text": "could not think without anger, hardly without contempt, on that easiness"}
{"doc_id": "gutenberg_1342", "para_id": 3582, "text": "of temper, that want of proper resolution, which now made him the slave"}
{"doc_id": "gutenberg_1342", "para_id": 3583, "text": "of his designing friends, and led him to sacrifice his own happiness to"}
{"doc_id": "gutenberg_1342", "para_id": 3584, "text": "the caprice of their inclinations. Had his own happiness, however, been"}
{"doc_id": "gutenberg_1342", "para_id": 3585, "text": "the only sacrifice, he might have been allowed to sport with it in"}
{"doc_id": "gutenberg_1342", "para_id": 3586, "text": "whatever manner he thought best; but her sister’s was involved in it, as"}
{"doc_id": "gutenberg_1342", "para_id": 3587, "text": "she thought he must be sensible himself. It was a subject, in short, on"}
{"doc_id": "gutenberg_1342", "para_id": 3588, "text": "which reflection would be long indulged, and must be unavailing. She"}
{"doc_id": "gutenberg_1342", "para_id": 3589, "text": "could think of nothing else; and yet, whether Bingley’s regard had"}
{"doc_id": "gutenberg_1342", "para_id": 3590, "text": "really died away, or were suppressed by his friends’ interference;"}
{"doc_id": "gutenberg_1342", "para_id": 3591, "text": "whether he had been aware of Jane’s attachment, or whether it had"}
{"doc_id": "gutenberg_1342", "para_id": 3592, "text": "escaped his observation; whichever were the case, though her opinion of"}
{"doc_id": "gutenberg_1342", "para_id": 3593, "text": "him must be materially affected by the difference, her sister’s"}
{"doc_id": "gutenberg_1342", "para_id": 3594, "text": "situation remained the same, her peace equally wounded."}
{"doc_id": "gutenberg_1342", "para_id": 3595, "text": "A day or two passed before Jane had courage to speak of her feelings to"}
{"doc_id": "gutenberg_1342", "para_id": 3596, "text": "Elizabeth; but at last, on Mrs. Bennet’s leaving them together, after a"}
{"doc_id": "gutenberg_1342", "para_id": 3597, "text": "longer irritation than usual about Netherfield and its master, she could"}
{"doc_id": "gutenberg_1342", "para_id": 3598, "text": "“O that my dear mother had more command over herself! she can have no"}
{"doc_id": "gutenberg_1342", "para_id": 3599, "text": "idea of the pain she gives me by her continual reflections on him. But I"}
{"doc_id": "gutenberg_1342", "para_id": 3600, "text": "will not repine. It cannot last long. He will be forgot, and we shall"}
{"doc_id": "gutenberg_1342", "para_id": 3601, "text": "Elizabeth looked at her sister with incredulous solicitude, but said"}
{"doc_id": "gutenberg_1342", "para_id": 3602, "text": "“You doubt me,” cried Jane, slightly colouring; “indeed, you have no"}
{"doc_id": "gutenberg_1342", "para_id": 3603, "text": "reason. He may live in my memory as the most amiable man of my"}
{"doc_id": "gutenberg_1342", "para_id": 3604, "text": "acquaintance but that is all. I have nothing either to hope or fear, and"}
{"doc_id": "gutenberg_1342", "para_id": 3605, "text": "nothing to reproach him with. Thank God I have not _that_ pain. A little"}
{"doc_id": "gutenberg_1342", "para_id": 3606, "text": "time, therefore--I shall certainly try to get the better----”"}
{"doc_id": "gutenberg_1342", "para_id": 3607, "text": "With a stronger voice she soon added, “I have this comfort immediately,"}
{"doc_id": "gutenberg_1342", "para_id": 3608, "text": "that it has not been more than an error of fancy on my side, and that it"}
{"doc_id": "gutenberg_1342", "para_id": 3609, "text": "“My dear Jane,” exclaimed Elizabeth, “you are too good. Your sweetness"}
{"doc_id": "gutenberg_1342", "para_id": 3610, "text": "and disinterestedness are really angelic; I do not know what to say to"}
{"doc_id": "gutenberg_1342", "para_id": 3611, "text": "you. I feel as if I had never done you justice, or loved you as you"}
{"doc_id": "gutenberg_1342", "para_id": 3612, "text": "Miss Bennet eagerly disclaimed all extraordinary merit, and threw back"}
{"doc_id": "gutenberg_1342", "para_id": 3613, "text": "“Nay,” said Elizabeth, “this is not fair. _You_ wish to think all the"}
{"doc_id": "gutenberg_1342", "para_id": 3614, "text": "world respectable, and are hurt if I speak ill of anybody. _I_ only want"}
{"doc_id": "gutenberg_1342", "para_id": 3615, "text": "to think _you_ perfect, and you set yourself against it. Do not be"}
{"doc_id": "gutenberg_1342", "para_id": 3616, "text": "afraid of my running into any excess, of my encroaching on your"}
{"doc_id": "gutenberg_1342", "para_id": 3617, "text": "privilege of universal good-will. You need not. There are few people"}
{"doc_id": "gutenberg_1342", "para_id": 3618, "text": "whom I really love, and still fewer of whom I think well. The more I see"}
{"doc_id": "gutenberg_1342", "para_id": 3619, "text": "of the world the more am I dissatisfied with it; and every day confirms"}
{"doc_id": "gutenberg_1342", "para_id": 3620, "text": "my belief of the inconsistency of all human characters, and of the"}
{"doc_id": "gutenberg_1342", "para_id": 3621, "text": "little dependence that can be placed on the appearance of either merit"}
{"doc_id": "gutenberg_1342", "para_id": 3622, "text": "or sense. I have met with two instances lately: one I will not mention,"}
{"doc_id": "gutenberg_1342", "para_id": 3623, "text": "the other is Charlotte’s marriage. It is unaccountable! in every view it"}
{"doc_id": "gutenberg_1342", "para_id": 3624, "text": "“My dear Lizzy, do not give way to such feelings as these. They will"}
{"doc_id": "gutenberg_1342", "para_id": 3625, "text": "ruin your happiness. You do not make allowance enough for difference of"}
{"doc_id": "gutenberg_1342", "para_id": 3626, "text": "situation and temper. Consider Mr. Collins’s respectability, and"}
{"doc_id": "gutenberg_1342", "para_id": 3627, "text": "Charlotte’s prudent, steady character. Remember that she is one of a"}
{"doc_id": "gutenberg_1342", "para_id": 3628, "text": "large family; that as to fortune it is a most eligible match; and be"}
{"doc_id": "gutenberg_1342", "para_id": 3629, "text": "ready to believe, for everybody’s sake, that she may feel something like"}
{"doc_id": "gutenberg_1342", "para_id": 3630, "text": "“To oblige you, I would try to believe almost anything, but no one else"}
{"doc_id": "gutenberg_1342", "para_id": 3631, "text": "could be benefited by such a belief as this; for were I persuaded that"}
{"doc_id": "gutenberg_1342", "para_id": 3632, "text": "Charlotte had any regard for him, I should only think worse of her"}
{"doc_id": "gutenberg_1342", "para_id": 3633, "text": "understanding than I now do of her heart. My dear Jane, Mr. Collins is a"}
{"doc_id": "gutenberg_1342", "para_id": 3634, "text": "conceited, pompous, narrow-minded, silly man: you know he is, as well as"}
{"doc_id": "gutenberg_1342", "para_id": 3635, "text": "I do; and you must feel, as well as I do, that the woman who marries him"}
{"doc_id": "gutenberg_1342", "para_id": 3636, "text": "cannot have a proper way of thinking. You shall not defend her, though"}
{"doc_id": "gutenberg_1342", "para_id": 3637, "text": "it is Charlotte Lucas. You shall not, for the sake of one individual,"}
{"doc_id": "gutenberg_1342", "para_id": 3638, "text": "change the meaning of principle and integrity, nor endeavour to persuade"}
{"doc_id": "gutenberg_1342", "para_id": 3639, "text": "yourself or me, that selfishness is prudence, and insensibility of"}
{"doc_id": "gutenberg_1342", "para_id": 3640, "text": "“I must think your language too strong in speaking of both,” replied"}
{"doc_id": "gutenberg_1342", "para_id": 3641, "text": "Jane; “and I hope you will be convinced of it, by seeing them happy"}
{"doc_id": "gutenberg_1342", "para_id": 3642, "text": "together. But enough of this. You alluded to something else. You"}
{"doc_id": "gutenberg_1342", "para_id": 3643, "text": "mentioned _two_ instances. I cannot misunderstand you, but I entreat"}
{"doc_id": "gutenberg_1342", "para_id": 3644, "text": "you, dear Lizzy, not to pain me by thinking _that person_ to blame, and"}
{"doc_id": "gutenberg_1342", "para_id": 3645, "text": "saying your opinion of him is sunk. We must not be so ready to fancy"}
{"doc_id": "gutenberg_1342", "para_id": 3646, "text": "ourselves intentionally injured. We must not expect a lively young man"}
{"doc_id": "gutenberg_1342", "para_id": 3647, "text": "to be always so guarded and circumspect. It is very often nothing but"}
{"doc_id": "gutenberg_1342", "para_id": 3648, "text": "our own vanity that deceives us. Women fancy admiration means more than"}
{"doc_id": "gutenberg_1342", "para_id": 3649, "text": "“If it is designedly done, they cannot be justified; but I have no idea"}
{"doc_id": "gutenberg_1342", "para_id": 3650, "text": "of there being so much design in the world as some persons imagine.”"}
{"doc_id": "gutenberg_1342", "para_id": 3651, "text": "“I am far from attributing any part of Mr. Bingley’s conduct to design,”"}
{"doc_id": "gutenberg_1342", "para_id": 3652, "text": "said Elizabeth; “but, without scheming to do wrong, or to make others"}
{"doc_id": "gutenberg_1342", "para_id": 3653, "text": "unhappy, there may be error and there may be misery. Thoughtlessness,"}
{"doc_id": "gutenberg_1342", "para_id": 3654, "text": "want of attention to other people’s feelings, and want of resolution,"}
{"doc_id": "gutenberg_1342", "para_id": 3655, "text": "“Yes; to the last. But if I go on I shall displease you by saying what I"}
{"doc_id": "gutenberg_1342", "para_id": 3656, "text": "think of persons you esteem. Stop me, whilst you can.”"}
{"doc_id": "gutenberg_1342", "para_id": 3657, "text": "“You persist, then, in supposing his sisters influence him?”"}
{"doc_id": "gutenberg_1342", "para_id": 3658, "text": "“I cannot believe it. Why should they try to influence him? They can"}
{"doc_id": "gutenberg_1342", "para_id": 3659, "text": "only wish his happiness; and if he is attached to me no other woman can"}
{"doc_id": "gutenberg_1342", "para_id": 3660, "text": "“Your first position is false. They may wish many things besides his"}
{"doc_id": "gutenberg_1342", "para_id": 3661, "text": "happiness: they may wish his increase of wealth and consequence; they"}
{"doc_id": "gutenberg_1342", "para_id": 3662, "text": "may wish him to marry a girl who has all the importance of money, great"}
{"doc_id": "gutenberg_1342", "para_id": 3663, "text": "“Beyond a doubt they do wish him to choose Miss Darcy,” replied Jane;"}
{"doc_id": "gutenberg_1342", "para_id": 3664, "text": "“but this may be from better feelings than you are supposing. They have"}
{"doc_id": "gutenberg_1342", "para_id": 3665, "text": "known her much longer than they have known me; no wonder if they love"}
{"doc_id": "gutenberg_1342", "para_id": 3666, "text": "her better. But, whatever may be their own wishes, it is very unlikely"}
{"doc_id": "gutenberg_1342", "para_id": 3667, "text": "they should have opposed their brother’s. What sister would think"}
{"doc_id": "gutenberg_1342", "para_id": 3668, "text": "herself at liberty to do it, unless there were something very"}
{"doc_id": "gutenberg_1342", "para_id": 3669, "text": "objectionable? If they believed him attached to me they would not try to"}
{"doc_id": "gutenberg_1342", "para_id": 3670, "text": "part us; if he were so, they could not succeed. By supposing such an"}
{"doc_id": "gutenberg_1342", "para_id": 3671, "text": "affection, you make everybody acting unnaturally and wrong, and me most"}
{"doc_id": "gutenberg_1342", "para_id": 3672, "text": "unhappy. Do not distress me by the idea. I am not ashamed of having been"}
{"doc_id": "gutenberg_1342", "para_id": 3673, "text": "mistaken--or, at least, it is slight, it is nothing in comparison of"}
{"doc_id": "gutenberg_1342", "para_id": 3674, "text": "what I should feel in thinking ill of him or his sisters. Let me take it"}
{"doc_id": "gutenberg_1342", "para_id": 3675, "text": "in the best light, in the light in which it may be understood.”"}
{"doc_id": "gutenberg_1342", "para_id": 3676, "text": "Elizabeth could not oppose such a wish; and from this time Mr. Bingley’s"}
{"doc_id": "gutenberg_1342", "para_id": 3677, "text": "Mrs. Bennet still continued to wonder and repine at his returning no"}
{"doc_id": "gutenberg_1342", "para_id": 3678, "text": "more; and though a day seldom passed in which Elizabeth did not account"}
{"doc_id": "gutenberg_1342", "para_id": 3679, "text": "for it clearly, there seemed little chance of her ever considering it"}
{"doc_id": "gutenberg_1342", "para_id": 3680, "text": "with less perplexity. Her daughter endeavoured to convince her of what"}
{"doc_id": "gutenberg_1342", "para_id": 3681, "text": "she did not believe herself, that his attentions to Jane had been merely"}
{"doc_id": "gutenberg_1342", "para_id": 3682, "text": "the effect of a common and transient liking, which ceased when he saw"}
{"doc_id": "gutenberg_1342", "para_id": 3683, "text": "her no more; but though the probability of the statement was admitted at"}
{"doc_id": "gutenberg_1342", "para_id": 3684, "text": "the time, she had the same story to repeat every day. Mrs. Bennet’s best"}
{"doc_id": "gutenberg_1342", "para_id": 3685, "text": "comfort was, that Mr. Bingley must be down again in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 3686, "text": "Mr. Bennet treated the matter differently. “So, Lizzy,” said he, one"}
{"doc_id": "gutenberg_1342", "para_id": 3687, "text": "day, “your sister is crossed in love, I find. I congratulate her. Next"}
{"doc_id": "gutenberg_1342", "para_id": 3688, "text": "to being married, a girl likes to be crossed in love a little now and"}
{"doc_id": "gutenberg_1342", "para_id": 3689, "text": "then. It is something to think of, and gives her a sort of distinction"}
{"doc_id": "gutenberg_1342", "para_id": 3690, "text": "among her companions. When is your turn to come? You will hardly bear to"}
{"doc_id": "gutenberg_1342", "para_id": 3691, "text": "be long outdone by Jane. Now is your time. Here are officers enough at"}
{"doc_id": "gutenberg_1342", "para_id": 3692, "text": "Meryton to disappoint all the young ladies in the country. Let Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 3693, "text": "be your man. He is a pleasant fellow, and would jilt you creditably.”"}
{"doc_id": "gutenberg_1342", "para_id": 3694, "text": "“Thank you, sir, but a less agreeable man would satisfy me. We must not"}
{"doc_id": "gutenberg_1342", "para_id": 3695, "text": "“True,” said Mr. Bennet; “but it is a comfort to think that, whatever of"}
{"doc_id": "gutenberg_1342", "para_id": 3696, "text": "that kind may befall you, you have an affectionate mother who will"}
{"doc_id": "gutenberg_1342", "para_id": 3697, "text": "Mr. Wickham’s society was of material service in dispelling the gloom"}
{"doc_id": "gutenberg_1342", "para_id": 3698, "text": "which the late perverse occurrences had thrown on many of the Longbourn"}
{"doc_id": "gutenberg_1342", "para_id": 3699, "text": "family. They saw him often, and to his other recommendations was now"}
{"doc_id": "gutenberg_1342", "para_id": 3700, "text": "added that of general unreserve. The whole of what Elizabeth had already"}
{"doc_id": "gutenberg_1342", "para_id": 3701, "text": "heard, his claims on Mr. Darcy, and all that he had suffered from him,"}
{"doc_id": "gutenberg_1342", "para_id": 3702, "text": "was now openly acknowledged and publicly canvassed; and everybody was"}
{"doc_id": "gutenberg_1342", "para_id": 3703, "text": "pleased to think how much they had always disliked Mr. Darcy before they"}
{"doc_id": "gutenberg_1342", "para_id": 3704, "text": "Miss Bennet was the only creature who could suppose there might be any"}
{"doc_id": "gutenberg_1342", "para_id": 3705, "text": "extenuating circumstances in the case unknown to the society of"}
{"doc_id": "gutenberg_1342", "para_id": 3706, "text": "Hertfordshire: her mild and steady candour always pleaded for"}
{"doc_id": "gutenberg_1342", "para_id": 3707, "text": "allowances, and urged the possibility of mistakes; but by everybody else"}
{"doc_id": "gutenberg_1342", "para_id": 3708, "text": "After a week spent in professions of love and schemes of felicity, Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3709, "text": "Collins was called from his amiable Charlotte by the arrival of"}
{"doc_id": "gutenberg_1342", "para_id": 3710, "text": "Saturday. The pain of separation, however, might be alleviated on his"}
{"doc_id": "gutenberg_1342", "para_id": 3711, "text": "side by preparations for the reception of his bride, as he had reason to"}
{"doc_id": "gutenberg_1342", "para_id": 3712, "text": "hope, that shortly after his next return into Hertfordshire, the day"}
{"doc_id": "gutenberg_1342", "para_id": 3713, "text": "would be fixed that was to make him the happiest of men. He took leave"}
{"doc_id": "gutenberg_1342", "para_id": 3714, "text": "of his relations at Longbourn with as much solemnity as before; wished"}
{"doc_id": "gutenberg_1342", "para_id": 3715, "text": "his fair cousins health and happiness again, and promised their father"}
{"doc_id": "gutenberg_1342", "para_id": 3716, "text": "On the following Monday, Mrs. Bennet had the pleasure of receiving her"}
{"doc_id": "gutenberg_1342", "para_id": 3717, "text": "brother and his wife, who came, as usual, to spend the Christmas at"}
{"doc_id": "gutenberg_1342", "para_id": 3718, "text": "Longbourn. Mr. Gardiner was a sensible, gentlemanlike man, greatly"}
{"doc_id": "gutenberg_1342", "para_id": 3719, "text": "superior to his sister, as well by nature as education. The Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 3720, "text": "ladies would have had difficulty in believing that a man who lived by"}
{"doc_id": "gutenberg_1342", "para_id": 3721, "text": "trade, and within view of his own warehouses, could have been so"}
{"doc_id": "gutenberg_1342", "para_id": 3722, "text": "well-bred and agreeable. Mrs. Gardiner, who was several years younger"}
{"doc_id": "gutenberg_1342", "para_id": 3723, "text": "than Mrs. Bennet and Mrs. Philips, was an amiable, intelligent, elegant"}
{"doc_id": "gutenberg_1342", "para_id": 3724, "text": "woman, and a great favourite with her Longbourn nieces. Between the two"}
{"doc_id": "gutenberg_1342", "para_id": 3725, "text": "eldest and herself especially, there subsisted a very particular regard."}
{"doc_id": "gutenberg_1342", "para_id": 3726, "text": "The first part of Mrs. Gardiner’s business, on her arrival, was to"}
{"doc_id": "gutenberg_1342", "para_id": 3727, "text": "distribute her presents and describe the newest fashions. When this was"}
{"doc_id": "gutenberg_1342", "para_id": 3728, "text": "done, she had a less active part to play. It became her turn to listen."}
{"doc_id": "gutenberg_1342", "para_id": 3729, "text": "Mrs. Bennet had many grievances to relate, and much to complain of. They"}
{"doc_id": "gutenberg_1342", "para_id": 3730, "text": "had all been very ill-used since she last saw her sister. Two of her"}
{"doc_id": "gutenberg_1342", "para_id": 3731, "text": "girls had been on the point of marriage, and after all there was nothing"}
{"doc_id": "gutenberg_1342", "para_id": 3732, "text": "“I do not blame Jane,” she continued, “for Jane would have got Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3733, "text": "Bingley if she could. But, Lizzy! Oh, sister! it is very hard to think"}
{"doc_id": "gutenberg_1342", "para_id": 3734, "text": "that she might have been Mr. Collins’s wife by this time, had not it"}
{"doc_id": "gutenberg_1342", "para_id": 3735, "text": "been for her own perverseness. He made her an offer in this very room,"}
{"doc_id": "gutenberg_1342", "para_id": 3736, "text": "and she refused him. The consequence of it is, that Lady Lucas will have"}
{"doc_id": "gutenberg_1342", "para_id": 3737, "text": "a daughter married before I have, and that Longbourn estate is just as"}
{"doc_id": "gutenberg_1342", "para_id": 3738, "text": "much entailed as ever. The Lucases are very artful people, indeed,"}
{"doc_id": "gutenberg_1342", "para_id": 3739, "text": "sister. They are all for what they can get. I am sorry to say it of"}
{"doc_id": "gutenberg_1342", "para_id": 3740, "text": "them, but so it is. It makes me very nervous and poorly, to be thwarted"}
{"doc_id": "gutenberg_1342", "para_id": 3741, "text": "so in my own family, and to have neighbours who think of themselves"}
{"doc_id": "gutenberg_1342", "para_id": 3742, "text": "before anybody else. However, your coming just at this time is the"}
{"doc_id": "gutenberg_1342", "para_id": 3743, "text": "greatest of comforts, and I am very glad to hear what you tell us of"}
{"doc_id": "gutenberg_1342", "para_id": 3744, "text": "Mrs. Gardiner, to whom the chief of this news had been given before, in"}
{"doc_id": "gutenberg_1342", "para_id": 3745, "text": "the course of Jane and Elizabeth’s correspondence with her, made her"}
{"doc_id": "gutenberg_1342", "para_id": 3746, "text": "sister a slight answer, and, in compassion to her nieces, turned the"}
{"doc_id": "gutenberg_1342", "para_id": 3747, "text": "When alone with Elizabeth afterwards, she spoke more on the subject."}
{"doc_id": "gutenberg_1342", "para_id": 3748, "text": "“It seems likely to have been a desirable match for Jane,” said she. “I"}
{"doc_id": "gutenberg_1342", "para_id": 3749, "text": "am sorry it went off. But these things happen so often! A young man,"}
{"doc_id": "gutenberg_1342", "para_id": 3750, "text": "such as you describe Mr. Bingley, so easily falls in love with a pretty"}
{"doc_id": "gutenberg_1342", "para_id": 3751, "text": "girl for a few weeks, and, when accident separates them, so easily"}
{"doc_id": "gutenberg_1342", "para_id": 3752, "text": "forgets her, that these sort of inconstancies are very frequent.”"}
{"doc_id": "gutenberg_1342", "para_id": 3753, "text": "“An excellent consolation in its way,” said Elizabeth; “but it will not"}
{"doc_id": "gutenberg_1342", "para_id": 3754, "text": "do for _us_. We do not suffer by accident. It does not often happen"}
{"doc_id": "gutenberg_1342", "para_id": 3755, "text": "that the interference of friends will persuade a young man of"}
{"doc_id": "gutenberg_1342", "para_id": 3756, "text": "independent fortune to think no more of a girl whom he was violently in"}
{"doc_id": "gutenberg_1342", "para_id": 3757, "text": "“But that expression of ‘violently in love’ is so hackneyed, so"}
{"doc_id": "gutenberg_1342", "para_id": 3758, "text": "doubtful, so indefinite, that it gives me very little idea. It is as"}
{"doc_id": "gutenberg_1342", "para_id": 3759, "text": "often applied to feelings which arise only from a half hour’s"}
{"doc_id": "gutenberg_1342", "para_id": 3760, "text": "acquaintance, as to a real, strong attachment. Pray, how _violent was_"}
{"doc_id": "gutenberg_1342", "para_id": 3761, "text": "“I never saw a more promising inclination; he was growing quite"}
{"doc_id": "gutenberg_1342", "para_id": 3762, "text": "inattentive to other people, and wholly engrossed by her. Every time"}
{"doc_id": "gutenberg_1342", "para_id": 3763, "text": "they met, it was more decided and remarkable. At his own ball he"}
{"doc_id": "gutenberg_1342", "para_id": 3764, "text": "offended two or three young ladies by not asking them to dance; and I"}
{"doc_id": "gutenberg_1342", "para_id": 3765, "text": "spoke to him twice myself without receiving an answer. Could there be"}
{"doc_id": "gutenberg_1342", "para_id": 3766, "text": "finer symptoms? Is not general incivility the very essence of love?”"}
{"doc_id": "gutenberg_1342", "para_id": 3767, "text": "“Oh, yes! of that kind of love which I suppose him to have felt. Poor"}
{"doc_id": "gutenberg_1342", "para_id": 3768, "text": "Jane! I am sorry for her, because, with her disposition, she may not get"}
{"doc_id": "gutenberg_1342", "para_id": 3769, "text": "over it immediately. It had better have happened to _you_, Lizzy; you"}
{"doc_id": "gutenberg_1342", "para_id": 3770, "text": "would have laughed yourself out of it sooner. But do you think she would"}
{"doc_id": "gutenberg_1342", "para_id": 3771, "text": "be prevailed on to go back with us? Change of scene might be of"}
{"doc_id": "gutenberg_1342", "para_id": 3772, "text": "service--and perhaps a little relief from home may be as useful as"}
{"doc_id": "gutenberg_1342", "para_id": 3773, "text": "Elizabeth was exceedingly pleased with this proposal, and felt persuaded"}
{"doc_id": "gutenberg_1342", "para_id": 3774, "text": "“I hope,” added Mrs. Gardiner, “that no consideration with regard to"}
{"doc_id": "gutenberg_1342", "para_id": 3775, "text": "this young man will influence her. We live in so different a part of"}
{"doc_id": "gutenberg_1342", "para_id": 3776, "text": "town, all our connections are so different, and, as you well know, we go"}
{"doc_id": "gutenberg_1342", "para_id": 3777, "text": "out so little, that it is very improbable they should meet at all,"}
{"doc_id": "gutenberg_1342", "para_id": 3778, "text": "“And _that_ is quite impossible; for he is now in the custody of his"}
{"doc_id": "gutenberg_1342", "para_id": 3779, "text": "friend, and Mr. Darcy would no more suffer him to call on Jane in such a"}
{"doc_id": "gutenberg_1342", "para_id": 3780, "text": "part of London! My dear aunt, how could you think of it? Mr. Darcy may,"}
{"doc_id": "gutenberg_1342", "para_id": 3781, "text": "perhaps, have _heard_ of such a place as Gracechurch Street, but he"}
{"doc_id": "gutenberg_1342", "para_id": 3782, "text": "would hardly think a month’s ablution enough to cleanse him from its"}
{"doc_id": "gutenberg_1342", "para_id": 3783, "text": "impurities, were he once to enter it; and, depend upon it, Mr. Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 3784, "text": "“So much the better. I hope they will not meet at all. But does not Jane"}
{"doc_id": "gutenberg_1342", "para_id": 3785, "text": "correspond with his sister? _She_ will not be able to help calling.”"}
{"doc_id": "gutenberg_1342", "para_id": 3786, "text": "But, in spite of the certainty in which Elizabeth affected to place this"}
{"doc_id": "gutenberg_1342", "para_id": 3787, "text": "point, as well as the still more interesting one of Bingley’s being"}
{"doc_id": "gutenberg_1342", "para_id": 3788, "text": "withheld from seeing Jane, she felt a solicitude on the subject which"}
{"doc_id": "gutenberg_1342", "para_id": 3789, "text": "convinced her, on examination, that she did not consider it entirely"}
{"doc_id": "gutenberg_1342", "para_id": 3790, "text": "hopeless. It was possible, and sometimes she thought it probable, that"}
{"doc_id": "gutenberg_1342", "para_id": 3791, "text": "his affection might be re-animated, and the influence of his friends"}
{"doc_id": "gutenberg_1342", "para_id": 3792, "text": "successfully combated by the more natural influence of Jane’s"}
{"doc_id": "gutenberg_1342", "para_id": 3793, "text": "Miss Bennet accepted her aunt’s invitation with pleasure; and the"}
{"doc_id": "gutenberg_1342", "para_id": 3794, "text": "Bingleys were no otherwise in her thoughts at the same time than as she"}
{"doc_id": "gutenberg_1342", "para_id": 3795, "text": "hoped, by Caroline’s not living in the same house with her brother, she"}
{"doc_id": "gutenberg_1342", "para_id": 3796, "text": "might occasionally spend a morning with her, without any danger of"}
{"doc_id": "gutenberg_1342", "para_id": 3797, "text": "The Gardiners stayed a week at Longbourn; and what with the Philipses,"}
{"doc_id": "gutenberg_1342", "para_id": 3798, "text": "the Lucases, and the officers, there was not a day without its"}
{"doc_id": "gutenberg_1342", "para_id": 3799, "text": "engagement. Mrs. Bennet had so carefully provided for the entertainment"}
{"doc_id": "gutenberg_1342", "para_id": 3800, "text": "of her brother and sister, that they did not once sit down to a family"}
{"doc_id": "gutenberg_1342", "para_id": 3801, "text": "dinner. When the engagement was for home, some of the officers always"}
{"doc_id": "gutenberg_1342", "para_id": 3802, "text": "made part of it, of which officers Mr. Wickham was sure to be one; and"}
{"doc_id": "gutenberg_1342", "para_id": 3803, "text": "on these occasions Mrs. Gardiner, rendered suspicious by Elizabeth’s"}
{"doc_id": "gutenberg_1342", "para_id": 3804, "text": "warm commendation of him, narrowly observed them both. Without supposing"}
{"doc_id": "gutenberg_1342", "para_id": 3805, "text": "them, from what she saw, to be very seriously in love, their preference"}
{"doc_id": "gutenberg_1342", "para_id": 3806, "text": "of each other was plain enough to make her a little uneasy; and she"}
{"doc_id": "gutenberg_1342", "para_id": 3807, "text": "resolved to speak to Elizabeth on the subject before she left"}
{"doc_id": "gutenberg_1342", "para_id": 3808, "text": "Hertfordshire, and represent to her the imprudence of encouraging such"}
{"doc_id": "gutenberg_1342", "para_id": 3809, "text": "To Mrs. Gardiner, Wickham had one means of affording pleasure,"}
{"doc_id": "gutenberg_1342", "para_id": 3810, "text": "unconnected with his general powers. About ten or a dozen years ago,"}
{"doc_id": "gutenberg_1342", "para_id": 3811, "text": "before her marriage, she had spent a considerable time in that very part"}
{"doc_id": "gutenberg_1342", "para_id": 3812, "text": "of Derbyshire to which he belonged. They had, therefore, many"}
{"doc_id": "gutenberg_1342", "para_id": 3813, "text": "acquaintance in common; and, though Wickham had been little there since"}
{"doc_id": "gutenberg_1342", "para_id": 3814, "text": "the death of Darcy’s father, five years before, it was yet in his power"}
{"doc_id": "gutenberg_1342", "para_id": 3815, "text": "to give her fresher intelligence of her former friends than she had been"}
{"doc_id": "gutenberg_1342", "para_id": 3816, "text": "Mrs. Gardiner had seen Pemberley, and known the late Mr. Darcy by"}
{"doc_id": "gutenberg_1342", "para_id": 3817, "text": "character perfectly well. Here, consequently, was an inexhaustible"}
{"doc_id": "gutenberg_1342", "para_id": 3818, "text": "subject of discourse. In comparing her recollection of Pemberley with"}
{"doc_id": "gutenberg_1342", "para_id": 3819, "text": "the minute description which Wickham could give, and in bestowing her"}
{"doc_id": "gutenberg_1342", "para_id": 3820, "text": "tribute of praise on the character of its late possessor, she was"}
{"doc_id": "gutenberg_1342", "para_id": 3821, "text": "delighting both him and herself. On being made acquainted with the"}
{"doc_id": "gutenberg_1342", "para_id": 3822, "text": "present Mr. Darcy’s treatment of him, she tried to remember something of"}
{"doc_id": "gutenberg_1342", "para_id": 3823, "text": "that gentleman’s reputed disposition, when quite a lad, which might"}
{"doc_id": "gutenberg_1342", "para_id": 3824, "text": "agree with it; and was confident, at last, that she recollected having"}
{"doc_id": "gutenberg_1342", "para_id": 3825, "text": "heard Mr. Fitzwilliam Darcy formerly spoken of as a very proud,"}
{"doc_id": "gutenberg_1342", "para_id": 3826, "text": "Mrs. Gardiner’s caution to Elizabeth was punctually and kindly given on"}
{"doc_id": "gutenberg_1342", "para_id": 3827, "text": "the first favourable opportunity of speaking to her alone: after"}
{"doc_id": "gutenberg_1342", "para_id": 3828, "text": "honestly telling her what she thought, she thus went on:--"}
{"doc_id": "gutenberg_1342", "para_id": 3829, "text": "“You are too sensible a girl, Lizzy, to fall in love merely because you"}
{"doc_id": "gutenberg_1342", "para_id": 3830, "text": "are warned against it; and, therefore, I am not afraid of speaking"}
{"doc_id": "gutenberg_1342", "para_id": 3831, "text": "openly. Seriously, I would have you be on your guard. Do not involve"}
{"doc_id": "gutenberg_1342", "para_id": 3832, "text": "yourself, or endeavour to involve him, in an affection which the want of"}
{"doc_id": "gutenberg_1342", "para_id": 3833, "text": "fortune would make so very imprudent. I have nothing to say against"}
{"doc_id": "gutenberg_1342", "para_id": 3834, "text": "_him_: he is a most interesting young man; and if he had the fortune he"}
{"doc_id": "gutenberg_1342", "para_id": 3835, "text": "ought to have, I should think you could not do better. But as it is--you"}
{"doc_id": "gutenberg_1342", "para_id": 3836, "text": "must not let your fancy run away with you. You have sense, and we all"}
{"doc_id": "gutenberg_1342", "para_id": 3837, "text": "expect you to use it. Your father would depend on _your_ resolution and"}
{"doc_id": "gutenberg_1342", "para_id": 3838, "text": "good conduct, I am sure. You must not disappoint your father.”"}
{"doc_id": "gutenberg_1342", "para_id": 3839, "text": "“Yes, and I hope to engage you to be serious likewise.”"}
{"doc_id": "gutenberg_1342", "para_id": 3840, "text": "“Well, then, you need not be under any alarm. I will take care of"}
{"doc_id": "gutenberg_1342", "para_id": 3841, "text": "myself, and of Mr. Wickham too. He shall not be in love with me, if I"}
{"doc_id": "gutenberg_1342", "para_id": 3842, "text": "“I beg your pardon. I will try again. At present I am not in love with"}
{"doc_id": "gutenberg_1342", "para_id": 3843, "text": "Mr. Wickham; no, I certainly am not. But he is, beyond all comparison,"}
{"doc_id": "gutenberg_1342", "para_id": 3844, "text": "the most agreeable man I ever saw--and if he becomes really attached to"}
{"doc_id": "gutenberg_1342", "para_id": 3845, "text": "me--I believe it will be better that he should not. I see the imprudence"}
{"doc_id": "gutenberg_1342", "para_id": 3846, "text": "of it. Oh, _that_ abominable Mr. Darcy! My father’s opinion of me does"}
{"doc_id": "gutenberg_1342", "para_id": 3847, "text": "me the greatest honour; and I should be miserable to forfeit it. My"}
{"doc_id": "gutenberg_1342", "para_id": 3848, "text": "father, however, is partial to Mr. Wickham. In short, my dear aunt, I"}
{"doc_id": "gutenberg_1342", "para_id": 3849, "text": "should be very sorry to be the means of making any of you unhappy; but"}
{"doc_id": "gutenberg_1342", "para_id": 3850, "text": "since we see, every day, that where there is affection young people are"}
{"doc_id": "gutenberg_1342", "para_id": 3851, "text": "seldom withheld, by immediate want of fortune, from entering into"}
{"doc_id": "gutenberg_1342", "para_id": 3852, "text": "engagements with each other, how can I promise to be wiser than so many"}
{"doc_id": "gutenberg_1342", "para_id": 3853, "text": "of my fellow-creatures, if I am tempted, or how am I even to know that"}
{"doc_id": "gutenberg_1342", "para_id": 3854, "text": "it would be wiser to resist? All that I can promise you, therefore, is"}
{"doc_id": "gutenberg_1342", "para_id": 3855, "text": "not to be in a hurry. I will not be in a hurry to believe myself his"}
{"doc_id": "gutenberg_1342", "para_id": 3856, "text": "first object. When I am in company with him, I will not be wishing. In"}
{"doc_id": "gutenberg_1342", "para_id": 3857, "text": "“Perhaps it will be as well if you discourage his coming here so very"}
{"doc_id": "gutenberg_1342", "para_id": 3858, "text": "often. At least you should not _remind_ your mother of inviting him.”"}
{"doc_id": "gutenberg_1342", "para_id": 3859, "text": "“As I did the other day,” said Elizabeth, with a conscious smile; “very"}
{"doc_id": "gutenberg_1342", "para_id": 3860, "text": "true, it will be wise in me to refrain from _that_. But do not imagine"}
{"doc_id": "gutenberg_1342", "para_id": 3861, "text": "that he is always here so often. It is on your account that he has been"}
{"doc_id": "gutenberg_1342", "para_id": 3862, "text": "so frequently invited this week. You know my mother’s ideas as to the"}
{"doc_id": "gutenberg_1342", "para_id": 3863, "text": "necessity of constant company for her friends. But really, and upon my"}
{"doc_id": "gutenberg_1342", "para_id": 3864, "text": "honour, I will try to do what I think to be wisest; and now I hope you"}
{"doc_id": "gutenberg_1342", "para_id": 3865, "text": "Her aunt assured her that she was; and Elizabeth, having thanked her for"}
{"doc_id": "gutenberg_1342", "para_id": 3866, "text": "the kindness of her hints, they parted,--a wonderful instance of advice"}
{"doc_id": "gutenberg_1342", "para_id": 3867, "text": "being given on such a point without being resented."}
{"doc_id": "gutenberg_1342", "para_id": 3868, "text": "Mr. Collins returned into Hertfordshire soon after it had been quitted"}
{"doc_id": "gutenberg_1342", "para_id": 3869, "text": "by the Gardiners and Jane; but, as he took up his abode with the"}
{"doc_id": "gutenberg_1342", "para_id": 3870, "text": "Lucases, his arrival was no great inconvenience to Mrs. Bennet. His"}
{"doc_id": "gutenberg_1342", "para_id": 3871, "text": "marriage was now fast approaching; and she was at length so far resigned"}
{"doc_id": "gutenberg_1342", "para_id": 3872, "text": "as to think it inevitable, and even repeatedly to say, in an ill-natured"}
{"doc_id": "gutenberg_1342", "para_id": 3873, "text": "tone, that she “_wished_ they might be happy.” Thursday was to be the"}
{"doc_id": "gutenberg_1342", "para_id": 3874, "text": "wedding-day, and on Wednesday Miss Lucas paid her farewell visit; and"}
{"doc_id": "gutenberg_1342", "para_id": 3875, "text": "when she rose to take leave, Elizabeth, ashamed of her mother’s"}
{"doc_id": "gutenberg_1342", "para_id": 3876, "text": "ungracious and reluctant good wishes, and sincerely affected herself,"}
{"doc_id": "gutenberg_1342", "para_id": 3877, "text": "accompanied her out of the room. As they went down stairs together,"}
{"doc_id": "gutenberg_1342", "para_id": 3878, "text": "“I shall depend on hearing from you very often, Eliza.”"}
{"doc_id": "gutenberg_1342", "para_id": 3879, "text": "“And I have another favour to ask. Will you come and see me?”"}
{"doc_id": "gutenberg_1342", "para_id": 3880, "text": "“I am not likely to leave Kent for some time. Promise me, therefore, to"}
{"doc_id": "gutenberg_1342", "para_id": 3881, "text": "Elizabeth could not refuse, though she foresaw little pleasure in the"}
{"doc_id": "gutenberg_1342", "para_id": 3882, "text": "“My father and Maria are to come to me in March,” added Charlotte, “and"}
{"doc_id": "gutenberg_1342", "para_id": 3883, "text": "I hope you will consent to be of the party. Indeed, Eliza, you will be"}
{"doc_id": "gutenberg_1342", "para_id": 3884, "text": "The wedding took place: the bride and bridegroom set off for Kent from"}
{"doc_id": "gutenberg_1342", "para_id": 3885, "text": "the church door, and everybody had as much to say or to hear on the"}
{"doc_id": "gutenberg_1342", "para_id": 3886, "text": "subject as usual. Elizabeth soon heard from her friend, and their"}
{"doc_id": "gutenberg_1342", "para_id": 3887, "text": "correspondence was as regular and frequent as it ever had been: that it"}
{"doc_id": "gutenberg_1342", "para_id": 3888, "text": "should be equally unreserved was impossible. Elizabeth could never"}
{"doc_id": "gutenberg_1342", "para_id": 3889, "text": "address her without feeling that all the comfort of intimacy was over;"}
{"doc_id": "gutenberg_1342", "para_id": 3890, "text": "and, though determined not to slacken as a correspondent, it was for the"}
{"doc_id": "gutenberg_1342", "para_id": 3891, "text": "sake of what had been rather than what was. Charlotte’s first letters"}
{"doc_id": "gutenberg_1342", "para_id": 3892, "text": "were received with a good deal of eagerness: there could not but be"}
{"doc_id": "gutenberg_1342", "para_id": 3893, "text": "curiosity to know how she would speak of her new home, how she would"}
{"doc_id": "gutenberg_1342", "para_id": 3894, "text": "like Lady Catherine, and how happy she would dare pronounce herself to"}
{"doc_id": "gutenberg_1342", "para_id": 3895, "text": "be; though, when the letters were read, Elizabeth felt that Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 3896, "text": "expressed herself on every point exactly as she might have foreseen. She"}
{"doc_id": "gutenberg_1342", "para_id": 3897, "text": "wrote cheerfully, seemed surrounded with comforts, and mentioned nothing"}
{"doc_id": "gutenberg_1342", "para_id": 3898, "text": "which she could not praise. The house, furniture, neighbourhood, and"}
{"doc_id": "gutenberg_1342", "para_id": 3899, "text": "roads, were all to her taste, and Lady Catherine’s behaviour was most"}
{"doc_id": "gutenberg_1342", "para_id": 3900, "text": "friendly and obliging. It was Mr. Collins’s picture of Hunsford and"}
{"doc_id": "gutenberg_1342", "para_id": 3901, "text": "Rosings rationally softened; and Elizabeth perceived that she must wait"}
{"doc_id": "gutenberg_1342", "para_id": 3902, "text": "Jane had already written a few lines to her sister, to announce their"}
{"doc_id": "gutenberg_1342", "para_id": 3903, "text": "safe arrival in London; and when she wrote again, Elizabeth hoped it"}
{"doc_id": "gutenberg_1342", "para_id": 3904, "text": "would be in her power to say something of the Bingleys."}
{"doc_id": "gutenberg_1342", "para_id": 3905, "text": "Her impatience for this second letter was as well rewarded as impatience"}
{"doc_id": "gutenberg_1342", "para_id": 3906, "text": "generally is. Jane had been a week in town, without either seeing or"}
{"doc_id": "gutenberg_1342", "para_id": 3907, "text": "hearing from Caroline. She accounted for it, however, by supposing that"}
{"doc_id": "gutenberg_1342", "para_id": 3908, "text": "her last letter to her friend from Longbourn had by some accident been"}
{"doc_id": "gutenberg_1342", "para_id": 3909, "text": "“My aunt,” she continued, “is going to-morrow into that part of the"}
{"doc_id": "gutenberg_1342", "para_id": 3910, "text": "town, and I shall take the opportunity of calling in Grosvenor Street.”"}
{"doc_id": "gutenberg_1342", "para_id": 3911, "text": "She wrote again when the visit was paid, and she had seen Miss Bingley."}
{"doc_id": "gutenberg_1342", "para_id": 3912, "text": "“I did not think Caroline in spirits,” were her words, “but she was very"}
{"doc_id": "gutenberg_1342", "para_id": 3913, "text": "glad to see me, and reproached me for giving her no notice of my coming"}
{"doc_id": "gutenberg_1342", "para_id": 3914, "text": "to London. I was right, therefore; my last letter had never reached her."}
{"doc_id": "gutenberg_1342", "para_id": 3915, "text": "I inquired after their brother, of course. He was well, but so much"}
{"doc_id": "gutenberg_1342", "para_id": 3916, "text": "engaged with Mr. Darcy that they scarcely ever saw him. I found that"}
{"doc_id": "gutenberg_1342", "para_id": 3917, "text": "Miss Darcy was expected to dinner: I wish I could see her. My visit was"}
{"doc_id": "gutenberg_1342", "para_id": 3918, "text": "not long, as Caroline and Mrs. Hurst were going out. I dare say I shall"}
{"doc_id": "gutenberg_1342", "para_id": 3919, "text": "Elizabeth shook her head over this letter. It convinced her that"}
{"doc_id": "gutenberg_1342", "para_id": 3920, "text": "accident only could discover to Mr. Bingley her sister’s being in town."}
{"doc_id": "gutenberg_1342", "para_id": 3921, "text": "Four weeks passed away, and Jane saw nothing of him. She endeavoured to"}
{"doc_id": "gutenberg_1342", "para_id": 3922, "text": "persuade herself that she did not regret it; but she could no longer be"}
{"doc_id": "gutenberg_1342", "para_id": 3923, "text": "blind to Miss Bingley’s inattention. After waiting at home every morning"}
{"doc_id": "gutenberg_1342", "para_id": 3924, "text": "for a fortnight, and inventing every evening a fresh excuse for her, the"}
{"doc_id": "gutenberg_1342", "para_id": 3925, "text": "visitor did at last appear; but the shortness of her stay, and, yet"}
{"doc_id": "gutenberg_1342", "para_id": 3926, "text": "more, the alteration of her manner, would allow Jane to deceive herself"}
{"doc_id": "gutenberg_1342", "para_id": 3927, "text": "no longer. The letter which she wrote on this occasion to her sister"}
{"doc_id": "gutenberg_1342", "para_id": 3928, "text": "“My dearest Lizzy will, I am sure, be incapable of triumphing in"}
{"doc_id": "gutenberg_1342", "para_id": 3929, "text": "her better judgment, at my expense, when I confess myself to have"}
{"doc_id": "gutenberg_1342", "para_id": 3930, "text": "been entirely deceived in Miss Bingley’s regard for me. But, my"}
{"doc_id": "gutenberg_1342", "para_id": 3931, "text": "dear sister, though the event has proved you right, do not think me"}
{"doc_id": "gutenberg_1342", "para_id": 3932, "text": "obstinate if I still assert that, considering what her behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 3933, "text": "was, my confidence was as natural as your suspicion. I do not at"}
{"doc_id": "gutenberg_1342", "para_id": 3934, "text": "all comprehend her reason for wishing to be intimate with me; but,"}
{"doc_id": "gutenberg_1342", "para_id": 3935, "text": "if the same circumstances were to happen again, I am sure I should"}
{"doc_id": "gutenberg_1342", "para_id": 3936, "text": "be deceived again. Caroline did not return my visit till yesterday;"}
{"doc_id": "gutenberg_1342", "para_id": 3937, "text": "and not a note, not a line, did I receive in the meantime. When she"}
{"doc_id": "gutenberg_1342", "para_id": 3938, "text": "did come, it was very evident that she had no pleasure in it; she"}
{"doc_id": "gutenberg_1342", "para_id": 3939, "text": "made a slight, formal apology for not calling before, said not a"}
{"doc_id": "gutenberg_1342", "para_id": 3940, "text": "word of wishing to see me again, and was, in every respect, so"}
{"doc_id": "gutenberg_1342", "para_id": 3941, "text": "altered a creature, that when she went away I was perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 3942, "text": "resolved to continue the acquaintance no longer. I pity, though I"}
{"doc_id": "gutenberg_1342", "para_id": 3943, "text": "cannot help blaming, her. She was very wrong in singling me out as"}
{"doc_id": "gutenberg_1342", "para_id": 3944, "text": "she did; I can safely say, that every advance to intimacy began on"}
{"doc_id": "gutenberg_1342", "para_id": 3945, "text": "her side. But I pity her, because she must feel that she has been"}
{"doc_id": "gutenberg_1342", "para_id": 3946, "text": "acting wrong, and because I am very sure that anxiety for her"}
{"doc_id": "gutenberg_1342", "para_id": 3947, "text": "brother is the cause of it. I need not explain myself farther; and"}
{"doc_id": "gutenberg_1342", "para_id": 3948, "text": "though _we_ know this anxiety to be quite needless, yet if she"}
{"doc_id": "gutenberg_1342", "para_id": 3949, "text": "feels it, it will easily account for her behaviour to me; and so"}
{"doc_id": "gutenberg_1342", "para_id": 3950, "text": "deservedly dear as he is to his sister, whatever anxiety she may"}
{"doc_id": "gutenberg_1342", "para_id": 3951, "text": "feel on his behalf is natural and amiable. I cannot but wonder,"}
{"doc_id": "gutenberg_1342", "para_id": 3952, "text": "however, at her having any such fears now, because if he had at all"}
{"doc_id": "gutenberg_1342", "para_id": 3953, "text": "cared about me, we must have met long, long ago. He knows of my"}
{"doc_id": "gutenberg_1342", "para_id": 3954, "text": "being in town, I am certain, from something she said herself; and"}
{"doc_id": "gutenberg_1342", "para_id": 3955, "text": "yet it would seem, by her manner of talking, as if she wanted to"}
{"doc_id": "gutenberg_1342", "para_id": 3956, "text": "persuade herself that he is really partial to Miss Darcy. I cannot"}
{"doc_id": "gutenberg_1342", "para_id": 3957, "text": "understand it. If I were not afraid of judging harshly, I should be"}
{"doc_id": "gutenberg_1342", "para_id": 3958, "text": "almost tempted to say, that there is a strong appearance of"}
{"doc_id": "gutenberg_1342", "para_id": 3959, "text": "duplicity in all this. I will endeavour to banish every painful"}
{"doc_id": "gutenberg_1342", "para_id": 3960, "text": "thought, and think only of what will make me happy, your affection,"}
{"doc_id": "gutenberg_1342", "para_id": 3961, "text": "and the invariable kindness of my dear uncle and aunt. Let me hear"}
{"doc_id": "gutenberg_1342", "para_id": 3962, "text": "from you very soon. Miss Bingley said something of his never"}
{"doc_id": "gutenberg_1342", "para_id": 3963, "text": "returning to Netherfield again, of giving up the house, but not"}
{"doc_id": "gutenberg_1342", "para_id": 3964, "text": "with any certainty. We had better not mention it. I am extremely"}
{"doc_id": "gutenberg_1342", "para_id": 3965, "text": "glad that you have such pleasant accounts from our friends at"}
{"doc_id": "gutenberg_1342", "para_id": 3966, "text": "Hunsford. Pray go to see them, with Sir William and Maria. I am"}
{"doc_id": "gutenberg_1342", "para_id": 3967, "text": "This letter gave Elizabeth some pain; but her spirits returned, as she"}
{"doc_id": "gutenberg_1342", "para_id": 3968, "text": "considered that Jane would no longer be duped, by the sister at least."}
{"doc_id": "gutenberg_1342", "para_id": 3969, "text": "All expectation from the brother was now absolutely over. She would not"}
{"doc_id": "gutenberg_1342", "para_id": 3970, "text": "even wish for any renewal of his attentions. His character sunk on every"}
{"doc_id": "gutenberg_1342", "para_id": 3971, "text": "review of it; and, as a punishment for him, as well as a possible"}
{"doc_id": "gutenberg_1342", "para_id": 3972, "text": "advantage to Jane, she seriously hoped he might really soon marry Mr."}
{"doc_id": "gutenberg_1342", "para_id": 3973, "text": "Darcy’s sister, as, by Wickham’s account, she would make him abundantly"}
{"doc_id": "gutenberg_1342", "para_id": 3974, "text": "Mrs. Gardiner about this time reminded Elizabeth of her promise"}
{"doc_id": "gutenberg_1342", "para_id": 3975, "text": "concerning that gentleman, and required information; and Elizabeth had"}
{"doc_id": "gutenberg_1342", "para_id": 3976, "text": "such to send as might rather give contentment to her aunt than to"}
{"doc_id": "gutenberg_1342", "para_id": 3977, "text": "herself. His apparent partiality had subsided, his attentions were over,"}
{"doc_id": "gutenberg_1342", "para_id": 3978, "text": "he was the admirer of some one else. Elizabeth was watchful enough to"}
{"doc_id": "gutenberg_1342", "para_id": 3979, "text": "see it all, but she could see it and write of it without material pain."}
{"doc_id": "gutenberg_1342", "para_id": 3980, "text": "Her heart had been but slightly touched, and her vanity was satisfied"}
{"doc_id": "gutenberg_1342", "para_id": 3981, "text": "with believing that _she_ would have been his only choice, had fortune"}
{"doc_id": "gutenberg_1342", "para_id": 3982, "text": "permitted it. The sudden acquisition of ten thousand pounds was the most"}
{"doc_id": "gutenberg_1342", "para_id": 3983, "text": "remarkable charm of the young lady to whom he was now rendering himself"}
{"doc_id": "gutenberg_1342", "para_id": 3984, "text": "agreeable; but Elizabeth, less clear-sighted perhaps in this case than"}
{"doc_id": "gutenberg_1342", "para_id": 3985, "text": "in Charlotte’s, did not quarrel with him for his wish of independence."}
{"doc_id": "gutenberg_1342", "para_id": 3986, "text": "Nothing, on the contrary, could be more natural; and, while able to"}
{"doc_id": "gutenberg_1342", "para_id": 3987, "text": "suppose that it cost him a few struggles to relinquish her, she was"}
{"doc_id": "gutenberg_1342", "para_id": 3988, "text": "ready to allow it a wise and desirable measure for both, and could very"}
{"doc_id": "gutenberg_1342", "para_id": 3989, "text": "All this was acknowledged to Mrs. Gardiner; and, after relating the"}
{"doc_id": "gutenberg_1342", "para_id": 3990, "text": "circumstances, she thus went on:--“I am now convinced, my dear aunt,"}
{"doc_id": "gutenberg_1342", "para_id": 3991, "text": "that I have never been much in love; for had I really experienced that"}
{"doc_id": "gutenberg_1342", "para_id": 3992, "text": "pure and elevating passion, I should at present detest his very name,"}
{"doc_id": "gutenberg_1342", "para_id": 3993, "text": "and wish him all manner of evil. But my feelings are not only cordial"}
{"doc_id": "gutenberg_1342", "para_id": 3994, "text": "towards _him_, they are even impartial towards Miss King. I cannot find"}
{"doc_id": "gutenberg_1342", "para_id": 3995, "text": "out that I hate her at all, or that I am in the least unwilling to think"}
{"doc_id": "gutenberg_1342", "para_id": 3996, "text": "her a very good sort of girl. There can be no love in all this. My"}
{"doc_id": "gutenberg_1342", "para_id": 3997, "text": "watchfulness has been effectual; and though I should certainly be a more"}
{"doc_id": "gutenberg_1342", "para_id": 3998, "text": "interesting object to all my acquaintance, were I distractedly in love"}
{"doc_id": "gutenberg_1342", "para_id": 3999, "text": "with him, I cannot say that I regret my comparative insignificance."}
{"doc_id": "gutenberg_1342", "para_id": 4000, "text": "Importance may sometimes be purchased too dearly. Kitty and Lydia take"}
{"doc_id": "gutenberg_1342", "para_id": 4001, "text": "his defection much more to heart than I do. They are young in the ways"}
{"doc_id": "gutenberg_1342", "para_id": 4002, "text": "of the world, and not yet open to the mortifying conviction that"}
{"doc_id": "gutenberg_1342", "para_id": 4003, "text": "handsome young men must have something to live on as well as the"}
{"doc_id": "gutenberg_1342", "para_id": 4004, "text": "With no greater events than these in the Longbourn family, and otherwise"}
{"doc_id": "gutenberg_1342", "para_id": 4005, "text": "diversified by little beyond the walks to Meryton, sometimes dirty and"}
{"doc_id": "gutenberg_1342", "para_id": 4006, "text": "sometimes cold, did January and February pass away. March was to take"}
{"doc_id": "gutenberg_1342", "para_id": 4007, "text": "Elizabeth to Hunsford. She had not at first thought very seriously of"}
{"doc_id": "gutenberg_1342", "para_id": 4008, "text": "going thither; but Charlotte, she soon found, was depending on the"}
{"doc_id": "gutenberg_1342", "para_id": 4009, "text": "plan, and she gradually learned to consider it herself with greater"}
{"doc_id": "gutenberg_1342", "para_id": 4010, "text": "pleasure as well as greater certainty. Absence had increased her desire"}
{"doc_id": "gutenberg_1342", "para_id": 4011, "text": "of seeing Charlotte again, and weakened her disgust of Mr. Collins."}
{"doc_id": "gutenberg_1342", "para_id": 4012, "text": "There was novelty in the scheme; and as, with such a mother and such"}
{"doc_id": "gutenberg_1342", "para_id": 4013, "text": "uncompanionable sisters, home could not be faultless, a little change"}
{"doc_id": "gutenberg_1342", "para_id": 4014, "text": "was not unwelcome for its own sake. The journey would, moreover, give"}
{"doc_id": "gutenberg_1342", "para_id": 4015, "text": "her a peep at Jane; and, in short, as the time drew near, she would have"}
{"doc_id": "gutenberg_1342", "para_id": 4016, "text": "been very sorry for any delay. Everything, however, went on smoothly,"}
{"doc_id": "gutenberg_1342", "para_id": 4017, "text": "and was finally settled according to Charlotte’s first sketch. She was"}
{"doc_id": "gutenberg_1342", "para_id": 4018, "text": "to accompany Sir William and his second daughter. The improvement of"}
{"doc_id": "gutenberg_1342", "para_id": 4019, "text": "spending a night in London was added in time, and the plan became as"}
{"doc_id": "gutenberg_1342", "para_id": 4020, "text": "The only pain was in leaving her father, who would certainly miss her,"}
{"doc_id": "gutenberg_1342", "para_id": 4021, "text": "and who, when it came to the point, so little liked her going, that he"}
{"doc_id": "gutenberg_1342", "para_id": 4022, "text": "told her to write to him, and almost promised to answer her letter."}
{"doc_id": "gutenberg_1342", "para_id": 4023, "text": "The farewell between herself and Mr. Wickham was perfectly friendly; on"}
{"doc_id": "gutenberg_1342", "para_id": 4024, "text": "his side even more. His present pursuit could not make him forget that"}
{"doc_id": "gutenberg_1342", "para_id": 4025, "text": "Elizabeth had been the first to excite and to deserve his attention, the"}
{"doc_id": "gutenberg_1342", "para_id": 4026, "text": "first to listen and to pity, the first to be admired; and in his manner"}
{"doc_id": "gutenberg_1342", "para_id": 4027, "text": "of bidding her adieu, wishing her every enjoyment, reminding her of what"}
{"doc_id": "gutenberg_1342", "para_id": 4028, "text": "she was to expect in Lady Catherine de Bourgh, and trusting their"}
{"doc_id": "gutenberg_1342", "para_id": 4029, "text": "opinion of her--their opinion of everybody--would always coincide, there"}
{"doc_id": "gutenberg_1342", "para_id": 4030, "text": "was a solicitude, an interest, which she felt must ever attach her to"}
{"doc_id": "gutenberg_1342", "para_id": 4031, "text": "him with a most sincere regard; and she parted from him convinced, that,"}
{"doc_id": "gutenberg_1342", "para_id": 4032, "text": "whether married or single, he must always be her model of the amiable"}
{"doc_id": "gutenberg_1342", "para_id": 4033, "text": "Her fellow-travellers the next day were not of a kind to make her think"}
{"doc_id": "gutenberg_1342", "para_id": 4034, "text": "him less agreeable. Sir William Lucas, and his daughter Maria, a"}
{"doc_id": "gutenberg_1342", "para_id": 4035, "text": "good-humoured girl, but as empty-headed as himself, had nothing to say"}
{"doc_id": "gutenberg_1342", "para_id": 4036, "text": "that could be worth hearing, and were listened to with about as much"}
{"doc_id": "gutenberg_1342", "para_id": 4037, "text": "delight as the rattle of the chaise. Elizabeth loved absurdities, but"}
{"doc_id": "gutenberg_1342", "para_id": 4038, "text": "she had known Sir William’s too long. He could tell her nothing new of"}
{"doc_id": "gutenberg_1342", "para_id": 4039, "text": "the wonders of his presentation and knighthood; and his civilities were"}
{"doc_id": "gutenberg_1342", "para_id": 4040, "text": "It was a journey of only twenty-four miles, and they began it so early"}
{"doc_id": "gutenberg_1342", "para_id": 4041, "text": "as to be in Gracechurch Street by noon. As they drove to Mr. Gardiner’s"}
{"doc_id": "gutenberg_1342", "para_id": 4042, "text": "door, Jane was at a drawing-room window watching their arrival: when"}
{"doc_id": "gutenberg_1342", "para_id": 4043, "text": "they entered the passage, she was there to welcome them, and Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 4044, "text": "looking earnestly in her face, was pleased to see it healthful and"}
{"doc_id": "gutenberg_1342", "para_id": 4045, "text": "lovely as ever. On the stairs were a troop of little boys and girls,"}
{"doc_id": "gutenberg_1342", "para_id": 4046, "text": "whose eagerness for their cousin’s appearance would not allow them to"}
{"doc_id": "gutenberg_1342", "para_id": 4047, "text": "wait in the drawing-room, and whose shyness, as they had not seen her"}
{"doc_id": "gutenberg_1342", "para_id": 4048, "text": "for a twelvemonth, prevented their coming lower. All was joy and"}
{"doc_id": "gutenberg_1342", "para_id": 4049, "text": "kindness. The day passed most pleasantly away; the morning in bustle and"}
{"doc_id": "gutenberg_1342", "para_id": 4050, "text": "Elizabeth then contrived to sit by her aunt. Their first subject was her"}
{"doc_id": "gutenberg_1342", "para_id": 4051, "text": "sister; and she was more grieved than astonished to hear, in reply to"}
{"doc_id": "gutenberg_1342", "para_id": 4052, "text": "her minute inquiries, that though Jane always struggled to support her"}
{"doc_id": "gutenberg_1342", "para_id": 4053, "text": "spirits, there were periods of dejection. It was reasonable, however, to"}
{"doc_id": "gutenberg_1342", "para_id": 4054, "text": "hope that they would not continue long. Mrs. Gardiner gave her the"}
{"doc_id": "gutenberg_1342", "para_id": 4055, "text": "particulars also of Miss Bingley’s visit in Gracechurch Street, and"}
{"doc_id": "gutenberg_1342", "para_id": 4056, "text": "repeated conversations occurring at different times between Jane and"}
{"doc_id": "gutenberg_1342", "para_id": 4057, "text": "herself, which proved that the former had, from her heart, given up the"}
{"doc_id": "gutenberg_1342", "para_id": 4058, "text": "Mrs. Gardiner then rallied her niece on Wickham’s desertion, and"}
{"doc_id": "gutenberg_1342", "para_id": 4059, "text": "“But, my dear Elizabeth,” she added, “what sort of girl is Miss King? I"}
{"doc_id": "gutenberg_1342", "para_id": 4060, "text": "“Pray, my dear aunt, what is the difference in matrimonial affairs,"}
{"doc_id": "gutenberg_1342", "para_id": 4061, "text": "between the mercenary and the prudent motive? Where does discretion end,"}
{"doc_id": "gutenberg_1342", "para_id": 4062, "text": "and avarice begin? Last Christmas you were afraid of his marrying me,"}
{"doc_id": "gutenberg_1342", "para_id": 4063, "text": "because it would be imprudent; and now, because he is trying to get a"}
{"doc_id": "gutenberg_1342", "para_id": 4064, "text": "girl with only ten thousand pounds, you want to find out that he is"}
{"doc_id": "gutenberg_1342", "para_id": 4065, "text": "“If you will only tell me what sort of girl Miss King is, I shall know"}
{"doc_id": "gutenberg_1342", "para_id": 4066, "text": "“She is a very good kind of girl, I believe. I know no harm of her.”"}
{"doc_id": "gutenberg_1342", "para_id": 4067, "text": "“But he paid her not the smallest attention till her grandfather’s death"}
{"doc_id": "gutenberg_1342", "para_id": 4068, "text": "“No--why should he? If it were not allowable for him to gain _my_"}
{"doc_id": "gutenberg_1342", "para_id": 4069, "text": "affections, because I had no money, what occasion could there be for"}
{"doc_id": "gutenberg_1342", "para_id": 4070, "text": "making love to a girl whom he did not care about, and who was equally"}
{"doc_id": "gutenberg_1342", "para_id": 4071, "text": "“But there seems indelicacy in directing his attentions towards her so"}
{"doc_id": "gutenberg_1342", "para_id": 4072, "text": "“A man in distressed circumstances has not time for all those elegant"}
{"doc_id": "gutenberg_1342", "para_id": 4073, "text": "decorums which other people may observe. If _she_ does not object to it,"}
{"doc_id": "gutenberg_1342", "para_id": 4074, "text": "“_Her_ not objecting does not justify _him_. It only shows her being"}
{"doc_id": "gutenberg_1342", "para_id": 4075, "text": "“Well,” cried Elizabeth, “have it as you choose. _He_ shall be"}
{"doc_id": "gutenberg_1342", "para_id": 4076, "text": "“No, Lizzy, that is what I do _not_ choose. I should be sorry, you know,"}
{"doc_id": "gutenberg_1342", "para_id": 4077, "text": "to think ill of a young man who has lived so long in Derbyshire.”"}
{"doc_id": "gutenberg_1342", "para_id": 4078, "text": "“Oh, if that is all, I have a very poor opinion of young men who live in"}
{"doc_id": "gutenberg_1342", "para_id": 4079, "text": "Derbyshire; and their intimate friends who live in Hertfordshire are not"}
{"doc_id": "gutenberg_1342", "para_id": 4080, "text": "much better. I am sick of them all. Thank heaven! I am going to-morrow"}
{"doc_id": "gutenberg_1342", "para_id": 4081, "text": "where I shall find a man who has not one agreeable quality, who has"}
{"doc_id": "gutenberg_1342", "para_id": 4082, "text": "neither manners nor sense to recommend him. Stupid men are the only ones"}
{"doc_id": "gutenberg_1342", "para_id": 4083, "text": "“Take care, Lizzy; that speech savours strongly of disappointment.”"}
{"doc_id": "gutenberg_1342", "para_id": 4084, "text": "Before they were separated by the conclusion of the play, she had the"}
{"doc_id": "gutenberg_1342", "para_id": 4085, "text": "unexpected happiness of an invitation to accompany her uncle and aunt in"}
{"doc_id": "gutenberg_1342", "para_id": 4086, "text": "a tour of pleasure which they proposed taking in the summer."}
{"doc_id": "gutenberg_1342", "para_id": 4087, "text": "“We have not quite determined how far it shall carry us,” said Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 4088, "text": "No scheme could have been more agreeable to Elizabeth, and her"}
{"doc_id": "gutenberg_1342", "para_id": 4089, "text": "acceptance of the invitation was most ready and grateful. “My dear, dear"}
{"doc_id": "gutenberg_1342", "para_id": 4090, "text": "aunt,” she rapturously cried, “what delight! what felicity! You give me"}
{"doc_id": "gutenberg_1342", "para_id": 4091, "text": "fresh life and vigour. Adieu to disappointment and spleen. What are men"}
{"doc_id": "gutenberg_1342", "para_id": 4092, "text": "to rocks and mountains? Oh, what hours of transport we shall spend! And"}
{"doc_id": "gutenberg_1342", "para_id": 4093, "text": "when we _do_ return, it shall not be like other travellers, without"}
{"doc_id": "gutenberg_1342", "para_id": 4094, "text": "being able to give one accurate idea of anything. We _will_ know where"}
{"doc_id": "gutenberg_1342", "para_id": 4095, "text": "we have gone--we _will_ recollect what we have seen. Lakes, mountains,"}
{"doc_id": "gutenberg_1342", "para_id": 4096, "text": "and rivers, shall not be jumbled together in our imaginations; nor, when"}
{"doc_id": "gutenberg_1342", "para_id": 4097, "text": "we attempt to describe any particular scene, will we begin quarrelling"}
{"doc_id": "gutenberg_1342", "para_id": 4098, "text": "about its relative situation. Let _our_ first effusions be less"}
{"doc_id": "gutenberg_1342", "para_id": 4099, "text": "insupportable than those of the generality of travellers.”"}
{"doc_id": "gutenberg_1342", "para_id": 4100, "text": "Every object in the next day’s journey was new and interesting to"}
{"doc_id": "gutenberg_1342", "para_id": 4101, "text": "Elizabeth; and her spirits were in a state of enjoyment; for she had"}
{"doc_id": "gutenberg_1342", "para_id": 4102, "text": "seen her sister looking so well as to banish all fear for her health,"}
{"doc_id": "gutenberg_1342", "para_id": 4103, "text": "and the prospect of her northern tour was a constant source of delight."}
{"doc_id": "gutenberg_1342", "para_id": 4104, "text": "When they left the high road for the lane to Hunsford, every eye was in"}
{"doc_id": "gutenberg_1342", "para_id": 4105, "text": "search of the Parsonage, and every turning expected to bring it in view."}
{"doc_id": "gutenberg_1342", "para_id": 4106, "text": "The paling of Rosings park was their boundary on one side. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 4107, "text": "smiled at the recollection of all that she had heard of its inhabitants."}
{"doc_id": "gutenberg_1342", "para_id": 4108, "text": "At length the Parsonage was discernible. The garden sloping to the"}
{"doc_id": "gutenberg_1342", "para_id": 4109, "text": "road, the house standing in it, the green pales and the laurel hedge,"}
{"doc_id": "gutenberg_1342", "para_id": 4110, "text": "everything declared they were arriving. Mr. Collins and Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 4111, "text": "appeared at the door, and the carriage stopped at the small gate, which"}
{"doc_id": "gutenberg_1342", "para_id": 4112, "text": "led by a short gravel walk to the house, amidst the nods and smiles of"}
{"doc_id": "gutenberg_1342", "para_id": 4113, "text": "the whole party. In a moment they were all out of the chaise, rejoicing"}
{"doc_id": "gutenberg_1342", "para_id": 4114, "text": "at the sight of each other. Mrs. Collins welcomed her friend with the"}
{"doc_id": "gutenberg_1342", "para_id": 4115, "text": "liveliest pleasure, and Elizabeth was more and more satisfied with"}
{"doc_id": "gutenberg_1342", "para_id": 4116, "text": "coming, when she found herself so affectionately received. She saw"}
{"doc_id": "gutenberg_1342", "para_id": 4117, "text": "instantly that her cousin’s manners were not altered by his marriage:"}
{"doc_id": "gutenberg_1342", "para_id": 4118, "text": "his formal civility was just what it had been; and he detained her some"}
{"doc_id": "gutenberg_1342", "para_id": 4119, "text": "minutes at the gate to hear and satisfy his inquiries after all her"}
{"doc_id": "gutenberg_1342", "para_id": 4120, "text": "family. They were then, with no other delay than his pointing out the"}
{"doc_id": "gutenberg_1342", "para_id": 4121, "text": "neatness of the entrance, taken into the house; and as soon as they were"}
{"doc_id": "gutenberg_1342", "para_id": 4122, "text": "in the parlour, he welcomed them a second time, with ostentatious"}
{"doc_id": "gutenberg_1342", "para_id": 4123, "text": "formality, to his humble abode, and punctually repeated all his wife’s"}
{"doc_id": "gutenberg_1342", "para_id": 4124, "text": "Elizabeth was prepared to see him in his glory; and she could not help"}
{"doc_id": "gutenberg_1342", "para_id": 4125, "text": "fancying that in displaying the good proportion of the room, its aspect,"}
{"doc_id": "gutenberg_1342", "para_id": 4126, "text": "and its furniture, he addressed himself particularly to her, as if"}
{"doc_id": "gutenberg_1342", "para_id": 4127, "text": "wishing to make her feel what she had lost in refusing him. But though"}
{"doc_id": "gutenberg_1342", "para_id": 4128, "text": "everything seemed neat and comfortable, she was not able to gratify him"}
{"doc_id": "gutenberg_1342", "para_id": 4129, "text": "by any sigh of repentance; and rather looked with wonder at her friend,"}
{"doc_id": "gutenberg_1342", "para_id": 4130, "text": "that she could have so cheerful an air with such a companion. When Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4131, "text": "Collins said anything of which his wife might reasonably be ashamed,"}
{"doc_id": "gutenberg_1342", "para_id": 4132, "text": "which certainly was not seldom, she involuntarily turned her eye on"}
{"doc_id": "gutenberg_1342", "para_id": 4133, "text": "Charlotte. Once or twice she could discern a faint blush; but in general"}
{"doc_id": "gutenberg_1342", "para_id": 4134, "text": "Charlotte wisely did not hear. After sitting long enough to admire"}
{"doc_id": "gutenberg_1342", "para_id": 4135, "text": "every article of furniture in the room, from the sideboard to the"}
{"doc_id": "gutenberg_1342", "para_id": 4136, "text": "fender, to give an account of their journey, and of all that had"}
{"doc_id": "gutenberg_1342", "para_id": 4137, "text": "happened in London, Mr. Collins invited them to take a stroll in the"}
{"doc_id": "gutenberg_1342", "para_id": 4138, "text": "garden, which was large and well laid out, and to the cultivation of"}
{"doc_id": "gutenberg_1342", "para_id": 4139, "text": "which he attended himself. To work in his garden was one of his most"}
{"doc_id": "gutenberg_1342", "para_id": 4140, "text": "respectable pleasures; and Elizabeth admired the command of countenance"}
{"doc_id": "gutenberg_1342", "para_id": 4141, "text": "with which Charlotte talked of the healthfulness of the exercise, and"}
{"doc_id": "gutenberg_1342", "para_id": 4142, "text": "owned she encouraged it as much as possible. Here, leading the way"}
{"doc_id": "gutenberg_1342", "para_id": 4143, "text": "through every walk and cross walk, and scarcely allowing them an"}
{"doc_id": "gutenberg_1342", "para_id": 4144, "text": "interval to utter the praises he asked for, every view was pointed out"}
{"doc_id": "gutenberg_1342", "para_id": 4145, "text": "with a minuteness which left beauty entirely behind. He could number the"}
{"doc_id": "gutenberg_1342", "para_id": 4146, "text": "fields in every direction, and could tell how many trees there were in"}
{"doc_id": "gutenberg_1342", "para_id": 4147, "text": "the most distant clump. But of all the views which his garden, or which"}
{"doc_id": "gutenberg_1342", "para_id": 4148, "text": "the country or the kingdom could boast, none were to be compared with"}
{"doc_id": "gutenberg_1342", "para_id": 4149, "text": "the prospect of Rosings, afforded by an opening in the trees that"}
{"doc_id": "gutenberg_1342", "para_id": 4150, "text": "bordered the park nearly opposite the front of his house. It was a"}
{"doc_id": "gutenberg_1342", "para_id": 4151, "text": "handsome modern building, well situated on rising ground."}
{"doc_id": "gutenberg_1342", "para_id": 4152, "text": "From his garden, Mr. Collins would have led them round his two meadows;"}
{"doc_id": "gutenberg_1342", "para_id": 4153, "text": "but the ladies, not having shoes to encounter the remains of a white"}
{"doc_id": "gutenberg_1342", "para_id": 4154, "text": "frost, turned back; and while Sir William accompanied him, Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 4155, "text": "took her sister and friend over the house, extremely well pleased,"}
{"doc_id": "gutenberg_1342", "para_id": 4156, "text": "probably, to have the opportunity of showing it without her husband’s"}
{"doc_id": "gutenberg_1342", "para_id": 4157, "text": "help. It was rather small, but well built and convenient; and everything"}
{"doc_id": "gutenberg_1342", "para_id": 4158, "text": "was fitted up and arranged with a neatness and consistency, of which"}
{"doc_id": "gutenberg_1342", "para_id": 4159, "text": "Elizabeth gave Charlotte all the credit. When Mr. Collins could be"}
{"doc_id": "gutenberg_1342", "para_id": 4160, "text": "forgotten, there was really a great air of comfort throughout, and by"}
{"doc_id": "gutenberg_1342", "para_id": 4161, "text": "Charlotte’s evident enjoyment of it, Elizabeth supposed he must be often"}
{"doc_id": "gutenberg_1342", "para_id": 4162, "text": "She had already learnt that Lady Catherine was still in the country. It"}
{"doc_id": "gutenberg_1342", "para_id": 4163, "text": "was spoken of again while they were at dinner, when Mr. Collins joining"}
{"doc_id": "gutenberg_1342", "para_id": 4164, "text": "“Yes, Miss Elizabeth, you will have the honour of seeing Lady Catherine"}
{"doc_id": "gutenberg_1342", "para_id": 4165, "text": "de Bourgh on the ensuing Sunday at church, and I need not say you will"}
{"doc_id": "gutenberg_1342", "para_id": 4166, "text": "be delighted with her. She is all affability and condescension, and I"}
{"doc_id": "gutenberg_1342", "para_id": 4167, "text": "doubt not but you will be honoured with some portion of her notice when"}
{"doc_id": "gutenberg_1342", "para_id": 4168, "text": "service is over. I have scarcely any hesitation in saying that she will"}
{"doc_id": "gutenberg_1342", "para_id": 4169, "text": "include you and my sister Maria in every invitation with which she"}
{"doc_id": "gutenberg_1342", "para_id": 4170, "text": "honours us during your stay here. Her behaviour to my dear Charlotte is"}
{"doc_id": "gutenberg_1342", "para_id": 4171, "text": "charming. We dine at Rosings twice every week, and are never allowed to"}
{"doc_id": "gutenberg_1342", "para_id": 4172, "text": "walk home. Her Ladyship’s carriage is regularly ordered for us. I"}
{"doc_id": "gutenberg_1342", "para_id": 4173, "text": "_should_ say, one of her Ladyship’s carriages, for she has several.”"}
{"doc_id": "gutenberg_1342", "para_id": 4174, "text": "“Lady Catherine is a very respectable, sensible woman, indeed,” added"}
{"doc_id": "gutenberg_1342", "para_id": 4175, "text": "“Very true, my dear, that is exactly what I say. She is the sort of"}
{"doc_id": "gutenberg_1342", "para_id": 4176, "text": "woman whom one cannot regard with too much deference.”"}
{"doc_id": "gutenberg_1342", "para_id": 4177, "text": "The evening was spent chiefly in talking over Hertfordshire news, and"}
{"doc_id": "gutenberg_1342", "para_id": 4178, "text": "telling again what had been already written; and when it closed,"}
{"doc_id": "gutenberg_1342", "para_id": 4179, "text": "Elizabeth, in the solitude of her chamber, had to meditate upon"}
{"doc_id": "gutenberg_1342", "para_id": 4180, "text": "Charlotte’s degree of contentment, to understand her address in guiding,"}
{"doc_id": "gutenberg_1342", "para_id": 4181, "text": "and composure in bearing with, her husband, and to acknowledge that it"}
{"doc_id": "gutenberg_1342", "para_id": 4182, "text": "was all done very well. She had also to anticipate how her visit would"}
{"doc_id": "gutenberg_1342", "para_id": 4183, "text": "pass, the quiet tenour of their usual employments, the vexatious"}
{"doc_id": "gutenberg_1342", "para_id": 4184, "text": "interruptions of Mr. Collins, and the gaieties of their intercourse"}
{"doc_id": "gutenberg_1342", "para_id": 4185, "text": "with Rosings. A lively imagination soon settled it all."}
{"doc_id": "gutenberg_1342", "para_id": 4186, "text": "About the middle of the next day, as she was in her room getting ready"}
{"doc_id": "gutenberg_1342", "para_id": 4187, "text": "for a walk, a sudden noise below seemed to speak the whole house in"}
{"doc_id": "gutenberg_1342", "para_id": 4188, "text": "confusion; and, after listening a moment, she heard somebody running"}
{"doc_id": "gutenberg_1342", "para_id": 4189, "text": "upstairs in a violent hurry, and calling loudly after her. She opened"}
{"doc_id": "gutenberg_1342", "para_id": 4190, "text": "the door, and met Maria in the landing-place, who, breathless with"}
{"doc_id": "gutenberg_1342", "para_id": 4191, "text": "“Oh, my dear Eliza! pray make haste and come into the dining-room, for"}
{"doc_id": "gutenberg_1342", "para_id": 4192, "text": "there is such a sight to be seen! I will not tell you what it is. Make"}
{"doc_id": "gutenberg_1342", "para_id": 4193, "text": "Elizabeth asked questions in vain; Maria would tell her nothing more;"}
{"doc_id": "gutenberg_1342", "para_id": 4194, "text": "and down they ran into the dining-room which fronted the lane, in quest"}
{"doc_id": "gutenberg_1342", "para_id": 4195, "text": "of this wonder; it was two ladies, stopping in a low phaeton at the"}
{"doc_id": "gutenberg_1342", "para_id": 4196, "text": "“And is this all?” cried Elizabeth. “I expected at least that the pigs"}
{"doc_id": "gutenberg_1342", "para_id": 4197, "text": "were got into the garden, and here is nothing but Lady Catherine and her"}
{"doc_id": "gutenberg_1342", "para_id": 4198, "text": "“La! my dear,” said Maria, quite shocked at the mistake, “it is not Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4199, "text": "Catherine. The old lady is Mrs. Jenkinson, who lives with them. The"}
{"doc_id": "gutenberg_1342", "para_id": 4200, "text": "other is Miss De Bourgh. Only look at her. She is quite a little"}
{"doc_id": "gutenberg_1342", "para_id": 4201, "text": "creature. Who would have thought she could be so thin and small!”"}
{"doc_id": "gutenberg_1342", "para_id": 4202, "text": "“She is abominably rude to keep Charlotte out of doors in all this wind."}
{"doc_id": "gutenberg_1342", "para_id": 4203, "text": "“Oh, Charlotte says she hardly ever does. It is the greatest of favours"}
{"doc_id": "gutenberg_1342", "para_id": 4204, "text": "“I like her appearance,” said Elizabeth, struck with other ideas. “She"}
{"doc_id": "gutenberg_1342", "para_id": 4205, "text": "looks sickly and cross. Yes, she will do for him very well. She will"}
{"doc_id": "gutenberg_1342", "para_id": 4206, "text": "Mr. Collins and Charlotte were both standing at the gate in conversation"}
{"doc_id": "gutenberg_1342", "para_id": 4207, "text": "with the ladies; and Sir William, to Elizabeth’s high diversion, was"}
{"doc_id": "gutenberg_1342", "para_id": 4208, "text": "stationed in the doorway, in earnest contemplation of the greatness"}
{"doc_id": "gutenberg_1342", "para_id": 4209, "text": "before him, and constantly bowing whenever Miss De Bourgh looked that"}
{"doc_id": "gutenberg_1342", "para_id": 4210, "text": "At length there was nothing more to be said; the ladies drove on, and"}
{"doc_id": "gutenberg_1342", "para_id": 4211, "text": "the others returned into the house. Mr. Collins no sooner saw the two"}
{"doc_id": "gutenberg_1342", "para_id": 4212, "text": "girls than he began to congratulate them on their good fortune, which"}
{"doc_id": "gutenberg_1342", "para_id": 4213, "text": "Charlotte explained by letting them know that the whole party was asked"}
{"doc_id": "gutenberg_1342", "para_id": 4214, "text": "‘Lady Catherine, said she, you have given me a treasure.’"}
{"doc_id": "gutenberg_1342", "para_id": 4215, "text": "Mr. Collins’s triumph, in consequence of this invitation, was complete."}
{"doc_id": "gutenberg_1342", "para_id": 4216, "text": "The power of displaying the grandeur of his patroness to his wondering"}
{"doc_id": "gutenberg_1342", "para_id": 4217, "text": "visitors, and of letting them see her civility towards himself and his"}
{"doc_id": "gutenberg_1342", "para_id": 4218, "text": "wife, was exactly what he had wished for; and that an opportunity of"}
{"doc_id": "gutenberg_1342", "para_id": 4219, "text": "doing it should be given so soon was such an instance of Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4220, "text": "Catherine’s condescension as he knew not how to admire enough."}
{"doc_id": "gutenberg_1342", "para_id": 4221, "text": "“I confess,” said he, “that I should not have been at all surprised by"}
{"doc_id": "gutenberg_1342", "para_id": 4222, "text": "her Ladyship’s asking us on Sunday to drink tea and spend the evening"}
{"doc_id": "gutenberg_1342", "para_id": 4223, "text": "at Rosings. I rather expected, from my knowledge of her affability, that"}
{"doc_id": "gutenberg_1342", "para_id": 4224, "text": "it would happen. But who could have foreseen such an attention as this?"}
{"doc_id": "gutenberg_1342", "para_id": 4225, "text": "Who could have imagined that we should receive an invitation to dine"}
{"doc_id": "gutenberg_1342", "para_id": 4226, "text": "there (an invitation, moreover, including the whole party) so"}
{"doc_id": "gutenberg_1342", "para_id": 4227, "text": "“I am the less surprised at what has happened,” replied Sir William,"}
{"doc_id": "gutenberg_1342", "para_id": 4228, "text": "“from that knowledge of what the manners of the great really are, which"}
{"doc_id": "gutenberg_1342", "para_id": 4229, "text": "my situation in life has allowed me to acquire. About the court, such"}
{"doc_id": "gutenberg_1342", "para_id": 4230, "text": "Scarcely anything was talked of the whole day or next morning but their"}
{"doc_id": "gutenberg_1342", "para_id": 4231, "text": "visit to Rosings. Mr. Collins was carefully instructing them in what"}
{"doc_id": "gutenberg_1342", "para_id": 4232, "text": "they were to expect, that the sight of such rooms, so many servants, and"}
{"doc_id": "gutenberg_1342", "para_id": 4233, "text": "so splendid a dinner, might not wholly overpower them."}
{"doc_id": "gutenberg_1342", "para_id": 4234, "text": "When the ladies were separating for the toilette, he said to"}
{"doc_id": "gutenberg_1342", "para_id": 4235, "text": "“Do not make yourself uneasy, my dear cousin, about your apparel. Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4236, "text": "Catherine is far from requiring that elegance of dress in us which"}
{"doc_id": "gutenberg_1342", "para_id": 4237, "text": "becomes herself and daughter. I would advise you merely to put on"}
{"doc_id": "gutenberg_1342", "para_id": 4238, "text": "whatever of your clothes is superior to the rest--there is no occasion"}
{"doc_id": "gutenberg_1342", "para_id": 4239, "text": "for anything more. Lady Catherine will not think the worse of you for"}
{"doc_id": "gutenberg_1342", "para_id": 4240, "text": "being simply dressed. She likes to have the distinction of rank"}
{"doc_id": "gutenberg_1342", "para_id": 4241, "text": "While they were dressing, he came two or three times to their different"}
{"doc_id": "gutenberg_1342", "para_id": 4242, "text": "doors, to recommend their being quick, as Lady Catherine very much"}
{"doc_id": "gutenberg_1342", "para_id": 4243, "text": "objected to be kept waiting for her dinner. Such formidable accounts of"}
{"doc_id": "gutenberg_1342", "para_id": 4244, "text": "her Ladyship, and her manner of living, quite frightened Maria Lucas,"}
{"doc_id": "gutenberg_1342", "para_id": 4245, "text": "who had been little used to company; and she looked forward to her"}
{"doc_id": "gutenberg_1342", "para_id": 4246, "text": "introduction at Rosings with as much apprehension as her father had done"}
{"doc_id": "gutenberg_1342", "para_id": 4247, "text": "As the weather was fine, they had a pleasant walk of about half a mile"}
{"doc_id": "gutenberg_1342", "para_id": 4248, "text": "across the park. Every park has its beauty and its prospects; and"}
{"doc_id": "gutenberg_1342", "para_id": 4249, "text": "Elizabeth saw much to be pleased with, though she could not be in such"}
{"doc_id": "gutenberg_1342", "para_id": 4250, "text": "raptures as Mr. Collins expected the scene to inspire, and was but"}
{"doc_id": "gutenberg_1342", "para_id": 4251, "text": "slightly affected by his enumeration of the windows in front of the"}
{"doc_id": "gutenberg_1342", "para_id": 4252, "text": "house, and his relation of what the glazing altogether had originally"}
{"doc_id": "gutenberg_1342", "para_id": 4253, "text": "When they ascended the steps to the hall, Maria’s alarm was every moment"}
{"doc_id": "gutenberg_1342", "para_id": 4254, "text": "increasing, and even Sir William did not look perfectly calm."}
{"doc_id": "gutenberg_1342", "para_id": 4255, "text": "Elizabeth’s courage did not fail her. She had heard nothing of Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4256, "text": "Catherine that spoke her awful from any extraordinary talents or"}
{"doc_id": "gutenberg_1342", "para_id": 4257, "text": "miraculous virtue, and the mere stateliness of money and rank she"}
{"doc_id": "gutenberg_1342", "para_id": 4258, "text": "From the entrance hall, of which Mr. Collins pointed out, with a"}
{"doc_id": "gutenberg_1342", "para_id": 4259, "text": "rapturous air, the fine proportion and finished ornaments, they followed"}
{"doc_id": "gutenberg_1342", "para_id": 4260, "text": "the servants through an antechamber to the room where Lady Catherine,"}
{"doc_id": "gutenberg_1342", "para_id": 4261, "text": "her daughter, and Mrs. Jenkinson were sitting. Her Ladyship, with great"}
{"doc_id": "gutenberg_1342", "para_id": 4262, "text": "condescension, arose to receive them; and as Mrs. Collins had settled it"}
{"doc_id": "gutenberg_1342", "para_id": 4263, "text": "with her husband that the office of introduction should be hers, it was"}
{"doc_id": "gutenberg_1342", "para_id": 4264, "text": "performed in a proper manner, without any of those apologies and thanks"}
{"doc_id": "gutenberg_1342", "para_id": 4265, "text": "In spite of having been at St. James’s, Sir William was so completely"}
{"doc_id": "gutenberg_1342", "para_id": 4266, "text": "awed by the grandeur surrounding him, that he had but just courage"}
{"doc_id": "gutenberg_1342", "para_id": 4267, "text": "enough to make a very low bow, and take his seat without saying a word;"}
{"doc_id": "gutenberg_1342", "para_id": 4268, "text": "and his daughter, frightened almost out of her senses, sat on the edge"}
{"doc_id": "gutenberg_1342", "para_id": 4269, "text": "of her chair, not knowing which way to look. Elizabeth found herself"}
{"doc_id": "gutenberg_1342", "para_id": 4270, "text": "quite equal to the scene, and could observe the three ladies before her"}
{"doc_id": "gutenberg_1342", "para_id": 4271, "text": "composedly. Lady Catherine was a tall, large woman, with strongly-marked"}
{"doc_id": "gutenberg_1342", "para_id": 4272, "text": "features, which might once have been handsome. Her air was not"}
{"doc_id": "gutenberg_1342", "para_id": 4273, "text": "conciliating, nor was her manner of receiving them such as to make her"}
{"doc_id": "gutenberg_1342", "para_id": 4274, "text": "visitors forget their inferior rank. She was not rendered formidable by"}
{"doc_id": "gutenberg_1342", "para_id": 4275, "text": "silence: but whatever she said was spoken in so authoritative a tone as"}
{"doc_id": "gutenberg_1342", "para_id": 4276, "text": "marked her self-importance, and brought Mr. Wickham immediately to"}
{"doc_id": "gutenberg_1342", "para_id": 4277, "text": "Elizabeth’s mind; and, from the observation of the day altogether, she"}
{"doc_id": "gutenberg_1342", "para_id": 4278, "text": "believed Lady Catherine to be exactly what he had represented."}
{"doc_id": "gutenberg_1342", "para_id": 4279, "text": "When, after examining the mother, in whose countenance and deportment"}
{"doc_id": "gutenberg_1342", "para_id": 4280, "text": "she soon found some resemblance of Mr. Darcy, she turned her eyes on the"}
{"doc_id": "gutenberg_1342", "para_id": 4281, "text": "daughter, she could almost have joined in Maria’s astonishment at her"}
{"doc_id": "gutenberg_1342", "para_id": 4282, "text": "being so thin and so small. There was neither in figure nor face any"}
{"doc_id": "gutenberg_1342", "para_id": 4283, "text": "likeness between the ladies. Miss de Bourgh was pale and sickly: her"}
{"doc_id": "gutenberg_1342", "para_id": 4284, "text": "features, though not plain, were insignificant; and she spoke very"}
{"doc_id": "gutenberg_1342", "para_id": 4285, "text": "little, except in a low voice, to Mrs. Jenkinson, in whose appearance"}
{"doc_id": "gutenberg_1342", "para_id": 4286, "text": "there was nothing remarkable, and who was entirely engaged in listening"}
{"doc_id": "gutenberg_1342", "para_id": 4287, "text": "to what she said, and placing a screen in the proper direction before"}
{"doc_id": "gutenberg_1342", "para_id": 4288, "text": "After sitting a few minutes, they were all sent to one of the windows to"}
{"doc_id": "gutenberg_1342", "para_id": 4289, "text": "admire the view, Mr. Collins attending them to point out its beauties,"}
{"doc_id": "gutenberg_1342", "para_id": 4290, "text": "and Lady Catherine kindly informing them that it was much better worth"}
{"doc_id": "gutenberg_1342", "para_id": 4291, "text": "The dinner was exceedingly handsome, and there were all the servants,"}
{"doc_id": "gutenberg_1342", "para_id": 4292, "text": "and all the articles of plate which Mr. Collins had promised; and, as he"}
{"doc_id": "gutenberg_1342", "para_id": 4293, "text": "had likewise foretold, he took his seat at the bottom of the table, by"}
{"doc_id": "gutenberg_1342", "para_id": 4294, "text": "her Ladyship’s desire, and looked as if he felt that life could furnish"}
{"doc_id": "gutenberg_1342", "para_id": 4295, "text": "nothing greater. He carved and ate and praised with delighted alacrity;"}
{"doc_id": "gutenberg_1342", "para_id": 4296, "text": "and every dish was commended first by him, and then by Sir William, who"}
{"doc_id": "gutenberg_1342", "para_id": 4297, "text": "was now enough recovered to echo whatever his son-in-law said, in a"}
{"doc_id": "gutenberg_1342", "para_id": 4298, "text": "manner which Elizabeth wondered Lady Catherine could bear. But Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4299, "text": "Catherine seemed gratified by their excessive admiration, and gave most"}
{"doc_id": "gutenberg_1342", "para_id": 4300, "text": "gracious smiles, especially when any dish on the table proved a novelty"}
{"doc_id": "gutenberg_1342", "para_id": 4301, "text": "to them. The party did not supply much conversation. Elizabeth was ready"}
{"doc_id": "gutenberg_1342", "para_id": 4302, "text": "to speak whenever there was an opening, but she was seated between"}
{"doc_id": "gutenberg_1342", "para_id": 4303, "text": "Charlotte and Miss de Bourgh--the former of whom was engaged in"}
{"doc_id": "gutenberg_1342", "para_id": 4304, "text": "listening to Lady Catherine, and the latter said not a word to her all"}
{"doc_id": "gutenberg_1342", "para_id": 4305, "text": "the dinnertime. Mrs. Jenkinson was chiefly employed in watching how"}
{"doc_id": "gutenberg_1342", "para_id": 4306, "text": "little Miss de Bourgh ate, pressing her to try some other dish and"}
{"doc_id": "gutenberg_1342", "para_id": 4307, "text": "fearing she was indisposed. Maria thought speaking out of the question,"}
{"doc_id": "gutenberg_1342", "para_id": 4308, "text": "When the ladies returned to the drawing-room, there was little to be"}
{"doc_id": "gutenberg_1342", "para_id": 4309, "text": "done but to hear Lady Catherine talk, which she did without any"}
{"doc_id": "gutenberg_1342", "para_id": 4310, "text": "intermission till coffee came in, delivering her opinion on every"}
{"doc_id": "gutenberg_1342", "para_id": 4311, "text": "subject in so decisive a manner as proved that she was not used to have"}
{"doc_id": "gutenberg_1342", "para_id": 4312, "text": "her judgment controverted. She inquired into Charlotte’s domestic"}
{"doc_id": "gutenberg_1342", "para_id": 4313, "text": "concerns familiarly and minutely, and gave her a great deal of advice as"}
{"doc_id": "gutenberg_1342", "para_id": 4314, "text": "to the management of them all; told her how everything ought to be"}
{"doc_id": "gutenberg_1342", "para_id": 4315, "text": "regulated in so small a family as hers, and instructed her as to the"}
{"doc_id": "gutenberg_1342", "para_id": 4316, "text": "care of her cows and her poultry. Elizabeth found that nothing was"}
{"doc_id": "gutenberg_1342", "para_id": 4317, "text": "beneath this great lady’s attention which could furnish her with an"}
{"doc_id": "gutenberg_1342", "para_id": 4318, "text": "occasion for dictating to others. In the intervals of her discourse with"}
{"doc_id": "gutenberg_1342", "para_id": 4319, "text": "Mrs. Collins, she addressed a variety of questions to Maria and"}
{"doc_id": "gutenberg_1342", "para_id": 4320, "text": "Elizabeth, but especially to the latter, of whose connections she knew"}
{"doc_id": "gutenberg_1342", "para_id": 4321, "text": "the least, and who, she observed to Mrs. Collins, was a very genteel,"}
{"doc_id": "gutenberg_1342", "para_id": 4322, "text": "pretty kind of girl. She asked her at different times how many sisters"}
{"doc_id": "gutenberg_1342", "para_id": 4323, "text": "she had, whether they were older or younger than herself, whether any of"}
{"doc_id": "gutenberg_1342", "para_id": 4324, "text": "them were likely to be married, whether they were handsome, where they"}
{"doc_id": "gutenberg_1342", "para_id": 4325, "text": "had been educated, what carriage her father kept, and what had been her"}
{"doc_id": "gutenberg_1342", "para_id": 4326, "text": "mother’s maiden name? Elizabeth felt all the impertinence of her"}
{"doc_id": "gutenberg_1342", "para_id": 4327, "text": "questions, but answered them very composedly. Lady Catherine then"}
{"doc_id": "gutenberg_1342", "para_id": 4328, "text": "“Your father’s estate is entailed on Mr. Collins, I think? For your"}
{"doc_id": "gutenberg_1342", "para_id": 4329, "text": "sake,” turning to Charlotte, “I am glad of it; but otherwise I see no"}
{"doc_id": "gutenberg_1342", "para_id": 4330, "text": "occasion for entailing estates from the female line. It was not thought"}
{"doc_id": "gutenberg_1342", "para_id": 4331, "text": "necessary in Sir Lewis de Bourgh’s family. Do you play and sing, Miss"}
{"doc_id": "gutenberg_1342", "para_id": 4332, "text": "“Oh then--some time or other we shall be happy to hear you. Our"}
{"doc_id": "gutenberg_1342", "para_id": 4333, "text": "instrument is a capital one, probably superior to ---- you shall try it"}
{"doc_id": "gutenberg_1342", "para_id": 4334, "text": "“Why did not you all learn? You ought all to have learned. The Miss"}
{"doc_id": "gutenberg_1342", "para_id": 4335, "text": "Webbs all play, and their father has not so good an income as yours. Do"}
{"doc_id": "gutenberg_1342", "para_id": 4336, "text": "“That is very strange. But I suppose you had no opportunity. Your mother"}
{"doc_id": "gutenberg_1342", "para_id": 4337, "text": "should have taken you to town every spring for the benefit of masters.”"}
{"doc_id": "gutenberg_1342", "para_id": 4338, "text": "“My mother would have no objection, but my father hates London.”"}
{"doc_id": "gutenberg_1342", "para_id": 4339, "text": "“No governess! How was that possible? Five daughters brought up at home"}
{"doc_id": "gutenberg_1342", "para_id": 4340, "text": "without a governess! I never heard of such a thing. Your mother must"}
{"doc_id": "gutenberg_1342", "para_id": 4341, "text": "Elizabeth could hardly help smiling, as she assured her that had not"}
{"doc_id": "gutenberg_1342", "para_id": 4342, "text": "“Then who taught you? who attended to you? Without a governess, you must"}
{"doc_id": "gutenberg_1342", "para_id": 4343, "text": "“Compared with some families, I believe we were; but such of us as"}
{"doc_id": "gutenberg_1342", "para_id": 4344, "text": "wished to learn never wanted the means. We were always encouraged to"}
{"doc_id": "gutenberg_1342", "para_id": 4345, "text": "read, and had all the masters that were necessary. Those who chose to be"}
{"doc_id": "gutenberg_1342", "para_id": 4346, "text": "“Ay, no doubt: but that is what a governess will prevent; and if I had"}
{"doc_id": "gutenberg_1342", "para_id": 4347, "text": "known your mother, I should have advised her most strenuously to engage"}
{"doc_id": "gutenberg_1342", "para_id": 4348, "text": "one. I always say that nothing is to be done in education without steady"}
{"doc_id": "gutenberg_1342", "para_id": 4349, "text": "and regular instruction, and nobody but a governess can give it. It is"}
{"doc_id": "gutenberg_1342", "para_id": 4350, "text": "wonderful how many families I have been the means of supplying in that"}
{"doc_id": "gutenberg_1342", "para_id": 4351, "text": "way. I am always glad to get a young person well placed out. Four nieces"}
{"doc_id": "gutenberg_1342", "para_id": 4352, "text": "of Mrs. Jenkinson are most delightfully situated through my means; and"}
{"doc_id": "gutenberg_1342", "para_id": 4353, "text": "it was but the other day that I recommended another young person, who"}
{"doc_id": "gutenberg_1342", "para_id": 4354, "text": "was merely accidentally mentioned to me, and the family are quite"}
{"doc_id": "gutenberg_1342", "para_id": 4355, "text": "delighted with her. Mrs. Collins, did I tell you of Lady Metcalfe’s"}
{"doc_id": "gutenberg_1342", "para_id": 4356, "text": "calling yesterday to thank me? She finds Miss Pope a treasure. ‘Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4357, "text": "Catherine,’ said she, ‘you have given me a treasure.’ Are any of your"}
{"doc_id": "gutenberg_1342", "para_id": 4358, "text": "“All! What, all five out at once? Very odd! And you only the second. The"}
{"doc_id": "gutenberg_1342", "para_id": 4359, "text": "younger ones out before the elder are married! Your younger sisters must"}
{"doc_id": "gutenberg_1342", "para_id": 4360, "text": "“Yes, my youngest is not sixteen. Perhaps _she_ is full young to be much"}
{"doc_id": "gutenberg_1342", "para_id": 4361, "text": "in company. But really, ma’am, I think it would be very hard upon"}
{"doc_id": "gutenberg_1342", "para_id": 4362, "text": "younger sisters that they should not have their share of society and"}
{"doc_id": "gutenberg_1342", "para_id": 4363, "text": "amusement, because the elder may not have the means or inclination to"}
{"doc_id": "gutenberg_1342", "para_id": 4364, "text": "marry early. The last born has as good a right to the pleasures of youth"}
{"doc_id": "gutenberg_1342", "para_id": 4365, "text": "as the first. And to be kept back on _such_ a motive! I think it would"}
{"doc_id": "gutenberg_1342", "para_id": 4366, "text": "not be very likely to promote sisterly affection or delicacy of mind.”"}
{"doc_id": "gutenberg_1342", "para_id": 4367, "text": "“Upon my word,” said her Ladyship, “you give your opinion very decidedly"}
{"doc_id": "gutenberg_1342", "para_id": 4368, "text": "“With three younger sisters grown up,” replied Elizabeth, smiling, “your"}
{"doc_id": "gutenberg_1342", "para_id": 4369, "text": "Lady Catherine seemed quite astonished at not receiving a direct answer;"}
{"doc_id": "gutenberg_1342", "para_id": 4370, "text": "and Elizabeth suspected herself to be the first creature who had ever"}
{"doc_id": "gutenberg_1342", "para_id": 4371, "text": "dared to trifle with so much dignified impertinence."}
{"doc_id": "gutenberg_1342", "para_id": 4372, "text": "“You cannot be more than twenty, I am sure,--therefore you need not"}
{"doc_id": "gutenberg_1342", "para_id": 4373, "text": "When the gentlemen had joined them, and tea was over, the card tables"}
{"doc_id": "gutenberg_1342", "para_id": 4374, "text": "were placed. Lady Catherine, Sir William, and Mr. and Mrs. Collins sat"}
{"doc_id": "gutenberg_1342", "para_id": 4375, "text": "down to quadrille; and as Miss De Bourgh chose to play at cassino, the"}
{"doc_id": "gutenberg_1342", "para_id": 4376, "text": "two girls had the honour of assisting Mrs. Jenkinson to make up her"}
{"doc_id": "gutenberg_1342", "para_id": 4377, "text": "party. Their table was superlatively stupid. Scarcely a syllable was"}
{"doc_id": "gutenberg_1342", "para_id": 4378, "text": "uttered that did not relate to the game, except when Mrs. Jenkinson"}
{"doc_id": "gutenberg_1342", "para_id": 4379, "text": "expressed her fears of Miss De Bourgh’s being too hot or too cold, or"}
{"doc_id": "gutenberg_1342", "para_id": 4380, "text": "having too much or too little light. A great deal more passed at the"}
{"doc_id": "gutenberg_1342", "para_id": 4381, "text": "other table. Lady Catherine was generally speaking--stating the mistakes"}
{"doc_id": "gutenberg_1342", "para_id": 4382, "text": "of the three others, or relating some anecdote of herself. Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 4383, "text": "was employed in agreeing to everything her Ladyship said, thanking her"}
{"doc_id": "gutenberg_1342", "para_id": 4384, "text": "for every fish he won, and apologizing if he thought he won too many."}
{"doc_id": "gutenberg_1342", "para_id": 4385, "text": "Sir William did not say much. He was storing his memory with anecdotes"}
{"doc_id": "gutenberg_1342", "para_id": 4386, "text": "When Lady Catherine and her daughter had played as long as they chose,"}
{"doc_id": "gutenberg_1342", "para_id": 4387, "text": "the tables were broken up, the carriage was offered to Mrs. Collins,"}
{"doc_id": "gutenberg_1342", "para_id": 4388, "text": "gratefully accepted, and immediately ordered. The party then gathered"}
{"doc_id": "gutenberg_1342", "para_id": 4389, "text": "round the fire to hear Lady Catherine determine what weather they were"}
{"doc_id": "gutenberg_1342", "para_id": 4390, "text": "to have on the morrow. From these instructions they were summoned by the"}
{"doc_id": "gutenberg_1342", "para_id": 4391, "text": "arrival of the coach; and with many speeches of thankfulness on Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4392, "text": "Collins’s side, and as many bows on Sir William’s, they departed. As"}
{"doc_id": "gutenberg_1342", "para_id": 4393, "text": "soon as they had driven from the door, Elizabeth was called on by her"}
{"doc_id": "gutenberg_1342", "para_id": 4394, "text": "cousin to give her opinion of all that she had seen at Rosings, which,"}
{"doc_id": "gutenberg_1342", "para_id": 4395, "text": "for Charlotte’s sake, she made more favourable than it really was. But"}
{"doc_id": "gutenberg_1342", "para_id": 4396, "text": "her commendation, though costing her some trouble, could by no means"}
{"doc_id": "gutenberg_1342", "para_id": 4397, "text": "satisfy Mr. Collins, and he was very soon obliged to take her Ladyship’s"}
{"doc_id": "gutenberg_1342", "para_id": 4398, "text": "Sir William stayed only a week at Hunsford; but his visit was long"}
{"doc_id": "gutenberg_1342", "para_id": 4399, "text": "enough to convince him of his daughter’s being most comfortably settled,"}
{"doc_id": "gutenberg_1342", "para_id": 4400, "text": "and of her possessing such a husband and such a neighbour as were not"}
{"doc_id": "gutenberg_1342", "para_id": 4401, "text": "often met with. While Sir William was with them, Mr. Collins devoted his"}
{"doc_id": "gutenberg_1342", "para_id": 4402, "text": "mornings to driving him out in his gig, and showing him the country: but"}
{"doc_id": "gutenberg_1342", "para_id": 4403, "text": "when he went away, the whole family returned to their usual employments,"}
{"doc_id": "gutenberg_1342", "para_id": 4404, "text": "and Elizabeth was thankful to find that they did not see more of her"}
{"doc_id": "gutenberg_1342", "para_id": 4405, "text": "cousin by the alteration; for the chief of the time between breakfast"}
{"doc_id": "gutenberg_1342", "para_id": 4406, "text": "and dinner was now passed by him either at work in the garden, or in"}
{"doc_id": "gutenberg_1342", "para_id": 4407, "text": "reading and writing, and looking out of window in his own book room,"}
{"doc_id": "gutenberg_1342", "para_id": 4408, "text": "which fronted the road. The room in which the ladies sat was backwards."}
{"doc_id": "gutenberg_1342", "para_id": 4409, "text": "Elizabeth at first had rather wondered that Charlotte should not prefer"}
{"doc_id": "gutenberg_1342", "para_id": 4410, "text": "the dining parlour for common use; it was a better sized room, and had a"}
{"doc_id": "gutenberg_1342", "para_id": 4411, "text": "pleasanter aspect: but she soon saw that her friend had an excellent"}
{"doc_id": "gutenberg_1342", "para_id": 4412, "text": "reason for what she did, for Mr. Collins would undoubtedly have been"}
{"doc_id": "gutenberg_1342", "para_id": 4413, "text": "much less in his own apartment had they sat in one equally lively; and"}
{"doc_id": "gutenberg_1342", "para_id": 4414, "text": "From the drawing-room they could distinguish nothing in the lane, and"}
{"doc_id": "gutenberg_1342", "para_id": 4415, "text": "were indebted to Mr. Collins for the knowledge of what carriages went"}
{"doc_id": "gutenberg_1342", "para_id": 4416, "text": "along, and how often especially Miss De Bourgh drove by in her phaeton,"}
{"doc_id": "gutenberg_1342", "para_id": 4417, "text": "which he never failed coming to inform them of, though it happened"}
{"doc_id": "gutenberg_1342", "para_id": 4418, "text": "almost every day. She not unfrequently stopped at the Parsonage, and had"}
{"doc_id": "gutenberg_1342", "para_id": 4419, "text": "a few minutes’ conversation with Charlotte, but was scarcely ever"}
{"doc_id": "gutenberg_1342", "para_id": 4420, "text": "Very few days passed in which Mr. Collins did not walk to Rosings, and"}
{"doc_id": "gutenberg_1342", "para_id": 4421, "text": "not many in which his wife did not think it necessary to go likewise;"}
{"doc_id": "gutenberg_1342", "para_id": 4422, "text": "and till Elizabeth recollected that there might be other family livings"}
{"doc_id": "gutenberg_1342", "para_id": 4423, "text": "to be disposed of, she could not understand the sacrifice of so many"}
{"doc_id": "gutenberg_1342", "para_id": 4424, "text": "hours. Now and then they were honoured with a call from her Ladyship,"}
{"doc_id": "gutenberg_1342", "para_id": 4425, "text": "and nothing escaped her observation that was passing in the room during"}
{"doc_id": "gutenberg_1342", "para_id": 4426, "text": "these visits. She examined into their employments, looked at their work,"}
{"doc_id": "gutenberg_1342", "para_id": 4427, "text": "and advised them to do it differently; found fault with the arrangement"}
{"doc_id": "gutenberg_1342", "para_id": 4428, "text": "of the furniture, or detected the housemaid in negligence; and if she"}
{"doc_id": "gutenberg_1342", "para_id": 4429, "text": "accepted any refreshment, seemed to do it only for the sake of finding"}
{"doc_id": "gutenberg_1342", "para_id": 4430, "text": "out that Mrs. Collins’s joints of meat were too large for her family."}
{"doc_id": "gutenberg_1342", "para_id": 4431, "text": "Elizabeth soon perceived, that though this great lady was not in the"}
{"doc_id": "gutenberg_1342", "para_id": 4432, "text": "commission of the peace for the county, she was a most active magistrate"}
{"doc_id": "gutenberg_1342", "para_id": 4433, "text": "in her own parish, the minutest concerns of which were carried to her by"}
{"doc_id": "gutenberg_1342", "para_id": 4434, "text": "Mr. Collins; and whenever any of the cottagers were disposed to be"}
{"doc_id": "gutenberg_1342", "para_id": 4435, "text": "quarrelsome, discontented, or too poor, she sallied forth into the"}
{"doc_id": "gutenberg_1342", "para_id": 4436, "text": "village to settle their differences, silence their complaints, and scold"}
{"doc_id": "gutenberg_1342", "para_id": 4437, "text": "The entertainment of dining at Rosings was repeated about twice a week;"}
{"doc_id": "gutenberg_1342", "para_id": 4438, "text": "and, allowing for the loss of Sir William, and there being only one"}
{"doc_id": "gutenberg_1342", "para_id": 4439, "text": "card-table in the evening, every such entertainment was the counterpart"}
{"doc_id": "gutenberg_1342", "para_id": 4440, "text": "of the first. Their other engagements were few, as the style of living"}
{"doc_id": "gutenberg_1342", "para_id": 4441, "text": "of the neighbourhood in general was beyond the Collinses’ reach. This,"}
{"doc_id": "gutenberg_1342", "para_id": 4442, "text": "however, was no evil to Elizabeth, and upon the whole she spent her time"}
{"doc_id": "gutenberg_1342", "para_id": 4443, "text": "comfortably enough: there were half hours of pleasant conversation with"}
{"doc_id": "gutenberg_1342", "para_id": 4444, "text": "Charlotte, and the weather was so fine for the time of year, that she"}
{"doc_id": "gutenberg_1342", "para_id": 4445, "text": "had often great enjoyment out of doors. Her favourite walk, and where"}
{"doc_id": "gutenberg_1342", "para_id": 4446, "text": "she frequently went while the others were calling on Lady Catherine, was"}
{"doc_id": "gutenberg_1342", "para_id": 4447, "text": "along the open grove which edged that side of the park, where there was"}
{"doc_id": "gutenberg_1342", "para_id": 4448, "text": "a nice sheltered path, which no one seemed to value but herself, and"}
{"doc_id": "gutenberg_1342", "para_id": 4449, "text": "where she felt beyond the reach of Lady Catherine’s curiosity."}
{"doc_id": "gutenberg_1342", "para_id": 4450, "text": "In this quiet way the first fortnight of her visit soon passed away."}
{"doc_id": "gutenberg_1342", "para_id": 4451, "text": "Easter was approaching, and the week preceding it was to bring an"}
{"doc_id": "gutenberg_1342", "para_id": 4452, "text": "addition to the family at Rosings, which in so small a circle must be"}
{"doc_id": "gutenberg_1342", "para_id": 4453, "text": "important. Elizabeth had heard, soon after her arrival, that Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 4454, "text": "was expected there in the course of a few weeks; and though there were"}
{"doc_id": "gutenberg_1342", "para_id": 4455, "text": "not many of her acquaintance whom she did not prefer, his coming would"}
{"doc_id": "gutenberg_1342", "para_id": 4456, "text": "furnish one comparatively new to look at in their Rosings parties, and"}
{"doc_id": "gutenberg_1342", "para_id": 4457, "text": "she might be amused in seeing how hopeless Miss Bingley’s designs on him"}
{"doc_id": "gutenberg_1342", "para_id": 4458, "text": "were, by his behaviour to his cousin, for whom he was evidently destined"}
{"doc_id": "gutenberg_1342", "para_id": 4459, "text": "by Lady Catherine, who talked of his coming with the greatest"}
{"doc_id": "gutenberg_1342", "para_id": 4460, "text": "satisfaction, spoke of him in terms of the highest admiration, and"}
{"doc_id": "gutenberg_1342", "para_id": 4461, "text": "seemed almost angry to find that he had already been frequently seen by"}
{"doc_id": "gutenberg_1342", "para_id": 4462, "text": "His arrival was soon known at the Parsonage; for Mr. Collins was walking"}
{"doc_id": "gutenberg_1342", "para_id": 4463, "text": "the whole morning within view of the lodges opening into Hunsford Lane,"}
{"doc_id": "gutenberg_1342", "para_id": 4464, "text": "the earliest assurance of it; and, after making his bow as the carriage"}
{"doc_id": "gutenberg_1342", "para_id": 4465, "text": "turned into the park, hurried home with the great intelligence. On the"}
{"doc_id": "gutenberg_1342", "para_id": 4466, "text": "following morning he hastened to Rosings to pay his respects. There were"}
{"doc_id": "gutenberg_1342", "para_id": 4467, "text": "two nephews of Lady Catherine to require them, for Mr. Darcy had brought"}
{"doc_id": "gutenberg_1342", "para_id": 4468, "text": "with him a Colonel Fitzwilliam, the younger son of his uncle, Lord ----;"}
{"doc_id": "gutenberg_1342", "para_id": 4469, "text": "and, to the great surprise of all the party, when Mr. Collins returned,"}
{"doc_id": "gutenberg_1342", "para_id": 4470, "text": "the gentlemen accompanied him. Charlotte had seen them from her"}
{"doc_id": "gutenberg_1342", "para_id": 4471, "text": "husband’s room, crossing the road, and immediately running into the"}
{"doc_id": "gutenberg_1342", "para_id": 4472, "text": "other, told the girls what an honour they might expect, adding,--"}
{"doc_id": "gutenberg_1342", "para_id": 4473, "text": "“I may thank you, Eliza, for this piece of civility. Mr. Darcy would"}
{"doc_id": "gutenberg_1342", "para_id": 4474, "text": "Elizabeth had scarcely time to disclaim all right to the compliment"}
{"doc_id": "gutenberg_1342", "para_id": 4475, "text": "before their approach was announced by the door-bell, and shortly"}
{"doc_id": "gutenberg_1342", "para_id": 4476, "text": "afterwards the three gentlemen entered the room. Colonel Fitzwilliam,"}
{"doc_id": "gutenberg_1342", "para_id": 4477, "text": "who led the way, was about thirty, not handsome, but in person and"}
{"doc_id": "gutenberg_1342", "para_id": 4478, "text": "address most truly the gentleman. Mr. Darcy looked just as he had been"}
{"doc_id": "gutenberg_1342", "para_id": 4479, "text": "used to look in Hertfordshire, paid his compliments, with his usual"}
{"doc_id": "gutenberg_1342", "para_id": 4480, "text": "reserve, to Mrs. Collins; and whatever might be his feelings towards her"}
{"doc_id": "gutenberg_1342", "para_id": 4481, "text": "friend, met her with every appearance of composure. Elizabeth merely"}
{"doc_id": "gutenberg_1342", "para_id": 4482, "text": "Colonel Fitzwilliam entered into conversation directly, with the"}
{"doc_id": "gutenberg_1342", "para_id": 4483, "text": "readiness and ease of a well-bred man, and talked very pleasantly; but"}
{"doc_id": "gutenberg_1342", "para_id": 4484, "text": "his cousin, after having addressed a slight observation on the house and"}
{"doc_id": "gutenberg_1342", "para_id": 4485, "text": "garden to Mrs. Collins, sat for some time without speaking to anybody."}
{"doc_id": "gutenberg_1342", "para_id": 4486, "text": "At length, however, his civility was so far awakened as to inquire of"}
{"doc_id": "gutenberg_1342", "para_id": 4487, "text": "Elizabeth after the health of her family. She answered him in the usual"}
{"doc_id": "gutenberg_1342", "para_id": 4488, "text": "“My eldest sister has been in town these three months. Have you never"}
{"doc_id": "gutenberg_1342", "para_id": 4489, "text": "She was perfectly sensible that he never had: but she wished to see"}
{"doc_id": "gutenberg_1342", "para_id": 4490, "text": "whether he would betray any consciousness of what had passed between the"}
{"doc_id": "gutenberg_1342", "para_id": 4491, "text": "Bingleys and Jane; and she thought he looked a little confused as he"}
{"doc_id": "gutenberg_1342", "para_id": 4492, "text": "answered that he had never been so fortunate as to meet Miss Bennet. The"}
{"doc_id": "gutenberg_1342", "para_id": 4493, "text": "subject was pursued no further, and the gentlemen soon afterwards went"}
{"doc_id": "gutenberg_1342", "para_id": 4494, "text": "Colonel Fitzwilliam’s manners were very much admired at the Parsonage,"}
{"doc_id": "gutenberg_1342", "para_id": 4495, "text": "and the ladies all felt that he must add considerably to the pleasure of"}
{"doc_id": "gutenberg_1342", "para_id": 4496, "text": "their engagements at Rosings. It was some days, however, before they"}
{"doc_id": "gutenberg_1342", "para_id": 4497, "text": "received any invitation thither, for while there were visitors in the"}
{"doc_id": "gutenberg_1342", "para_id": 4498, "text": "house they could not be necessary; and it was not till Easter-day,"}
{"doc_id": "gutenberg_1342", "para_id": 4499, "text": "almost a week after the gentlemen’s arrival, that they were honoured by"}
{"doc_id": "gutenberg_1342", "para_id": 4500, "text": "such an attention, and then they were merely asked on leaving church to"}
{"doc_id": "gutenberg_1342", "para_id": 4501, "text": "come there in the evening. For the last week they had seen very little"}
{"doc_id": "gutenberg_1342", "para_id": 4502, "text": "of either Lady Catherine or her daughter. Colonel Fitzwilliam had called"}
{"doc_id": "gutenberg_1342", "para_id": 4503, "text": "at the Parsonage more than once during the time, but Mr. Darcy they had"}
{"doc_id": "gutenberg_1342", "para_id": 4504, "text": "The invitation was accepted, of course, and at a proper hour they joined"}
{"doc_id": "gutenberg_1342", "para_id": 4505, "text": "the party in Lady Catherine’s drawing-room. Her Ladyship received them"}
{"doc_id": "gutenberg_1342", "para_id": 4506, "text": "civilly, but it was plain that their company was by no means so"}
{"doc_id": "gutenberg_1342", "para_id": 4507, "text": "acceptable as when she could get nobody else; and she was, in fact,"}
{"doc_id": "gutenberg_1342", "para_id": 4508, "text": "almost engrossed by her nephews, speaking to them, especially to Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 4509, "text": "Colonel Fitzwilliam seemed really glad to see them: anything was a"}
{"doc_id": "gutenberg_1342", "para_id": 4510, "text": "welcome relief to him at Rosings; and Mrs. Collins’s pretty friend had,"}
{"doc_id": "gutenberg_1342", "para_id": 4511, "text": "moreover, caught his fancy very much. He now seated himself by her, and"}
{"doc_id": "gutenberg_1342", "para_id": 4512, "text": "talked so agreeably of Kent and Hertfordshire, of travelling and staying"}
{"doc_id": "gutenberg_1342", "para_id": 4513, "text": "at home, of new books and music, that Elizabeth had never been half so"}
{"doc_id": "gutenberg_1342", "para_id": 4514, "text": "well entertained in that room before; and they conversed with so much"}
{"doc_id": "gutenberg_1342", "para_id": 4515, "text": "spirit and flow as to draw the attention of Lady Catherine herself, as"}
{"doc_id": "gutenberg_1342", "para_id": 4516, "text": "well as of Mr. Darcy. _His_ eyes had been soon and repeatedly turned"}
{"doc_id": "gutenberg_1342", "para_id": 4517, "text": "towards them with a look of curiosity; and that her Ladyship, after a"}
{"doc_id": "gutenberg_1342", "para_id": 4518, "text": "while, shared the feeling, was more openly acknowledged, for she did not"}
{"doc_id": "gutenberg_1342", "para_id": 4519, "text": "“What is that you are saying, Fitzwilliam? What is it you are talking"}
{"doc_id": "gutenberg_1342", "para_id": 4520, "text": "of? What are you telling Miss Bennet? Let me hear what it is.”"}
{"doc_id": "gutenberg_1342", "para_id": 4521, "text": "“We were talking of music, madam,” said he, when no longer able to avoid"}
{"doc_id": "gutenberg_1342", "para_id": 4522, "text": "“Of music! Then pray speak aloud. It is of all subjects my delight. I"}
{"doc_id": "gutenberg_1342", "para_id": 4523, "text": "must have my share in the conversation, if you are speaking of music."}
{"doc_id": "gutenberg_1342", "para_id": 4524, "text": "There are few people in England, I suppose, who have more true"}
{"doc_id": "gutenberg_1342", "para_id": 4525, "text": "enjoyment of music than myself, or a better natural taste. If I had ever"}
{"doc_id": "gutenberg_1342", "para_id": 4526, "text": "learnt, I should have been a great proficient. And so would Anne, if her"}
{"doc_id": "gutenberg_1342", "para_id": 4527, "text": "health had allowed her to apply. I am confident that she would have"}
{"doc_id": "gutenberg_1342", "para_id": 4528, "text": "performed delightfully. How does Georgiana get on, Darcy?”"}
{"doc_id": "gutenberg_1342", "para_id": 4529, "text": "Mr. Darcy spoke with affectionate praise of his sister’s proficiency."}
{"doc_id": "gutenberg_1342", "para_id": 4530, "text": "“I am very glad to hear such a good account of her,” said Lady"}
{"doc_id": "gutenberg_1342", "para_id": 4531, "text": "Catherine; “and pray tell her from me, that she cannot expect to excel,"}
{"doc_id": "gutenberg_1342", "para_id": 4532, "text": "“I assure you, madam,” he replied, “that she does not need such advice."}
{"doc_id": "gutenberg_1342", "para_id": 4533, "text": "“So much the better. It cannot be done too much; and when I next write"}
{"doc_id": "gutenberg_1342", "para_id": 4534, "text": "to her, I shall charge her not to neglect it on any account. I often"}
{"doc_id": "gutenberg_1342", "para_id": 4535, "text": "tell young ladies, that no excellence in music is to be acquired without"}
{"doc_id": "gutenberg_1342", "para_id": 4536, "text": "constant practice. I have told Miss Bennet several times, that she will"}
{"doc_id": "gutenberg_1342", "para_id": 4537, "text": "never play really well, unless she practises more; and though Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 4538, "text": "Collins has no instrument, she is very welcome, as I have often told"}
{"doc_id": "gutenberg_1342", "para_id": 4539, "text": "her, to come to Rosings every day, and play on the pianoforte in Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 4540, "text": "Jenkinson’s room. She would be in nobody’s way, you know, in that part"}
{"doc_id": "gutenberg_1342", "para_id": 4541, "text": "Mr. Darcy looked a little ashamed of his aunt’s ill-breeding, and made"}
{"doc_id": "gutenberg_1342", "para_id": 4542, "text": "When coffee was over, Colonel Fitzwilliam reminded Elizabeth of having"}
{"doc_id": "gutenberg_1342", "para_id": 4543, "text": "promised to play to him; and she sat down directly to the instrument. He"}
{"doc_id": "gutenberg_1342", "para_id": 4544, "text": "drew a chair near her. Lady Catherine listened to half a song, and then"}
{"doc_id": "gutenberg_1342", "para_id": 4545, "text": "talked, as before, to her other nephew; till the latter walked away from"}
{"doc_id": "gutenberg_1342", "para_id": 4546, "text": "her, and moving with his usual deliberation towards the pianoforte,"}
{"doc_id": "gutenberg_1342", "para_id": 4547, "text": "stationed himself so as to command a full view of the fair performer’s"}
{"doc_id": "gutenberg_1342", "para_id": 4548, "text": "countenance. Elizabeth saw what he was doing, and at the first"}
{"doc_id": "gutenberg_1342", "para_id": 4549, "text": "convenient pause turned to him with an arch smile, and said,--"}
{"doc_id": "gutenberg_1342", "para_id": 4550, "text": "“You mean to frighten me, Mr. Darcy, by coming in all this state to hear"}
{"doc_id": "gutenberg_1342", "para_id": 4551, "text": "me. But I will not be alarmed, though your sister _does_ play so well."}
{"doc_id": "gutenberg_1342", "para_id": 4552, "text": "There is a stubbornness about me that never can bear to be frightened at"}
{"doc_id": "gutenberg_1342", "para_id": 4553, "text": "the will of others. My courage always rises with every attempt to"}
{"doc_id": "gutenberg_1342", "para_id": 4554, "text": "“I shall not say that you are mistaken,” he replied, “because you could"}
{"doc_id": "gutenberg_1342", "para_id": 4555, "text": "not really believe me to entertain any design of alarming you; and I"}
{"doc_id": "gutenberg_1342", "para_id": 4556, "text": "have had the pleasure of your acquaintance long enough to know, that you"}
{"doc_id": "gutenberg_1342", "para_id": 4557, "text": "find great enjoyment in occasionally professing opinions which, in fact,"}
{"doc_id": "gutenberg_1342", "para_id": 4558, "text": "Elizabeth laughed heartily at this picture of herself, and said to"}
{"doc_id": "gutenberg_1342", "para_id": 4559, "text": "Colonel Fitzwilliam, “Your cousin will give you a very pretty notion of"}
{"doc_id": "gutenberg_1342", "para_id": 4560, "text": "me, and teach you not to believe a word I say. I am particularly unlucky"}
{"doc_id": "gutenberg_1342", "para_id": 4561, "text": "in meeting with a person so well able to expose my real character, in a"}
{"doc_id": "gutenberg_1342", "para_id": 4562, "text": "part of the world where I had hoped to pass myself off with some degree"}
{"doc_id": "gutenberg_1342", "para_id": 4563, "text": "of credit. Indeed, Mr. Darcy, it is very ungenerous in you to mention"}
{"doc_id": "gutenberg_1342", "para_id": 4564, "text": "all that you knew to my disadvantage in Hertfordshire--and, give me"}
{"doc_id": "gutenberg_1342", "para_id": 4565, "text": "leave to say, very impolitic too--for it is provoking me to retaliate,"}
{"doc_id": "gutenberg_1342", "para_id": 4566, "text": "and such things may come out as will shock your relations to hear.”"}
{"doc_id": "gutenberg_1342", "para_id": 4567, "text": "“Pray let me hear what you have to accuse him of,” cried Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 4568, "text": "Fitzwilliam. “I should like to know how he behaves among strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 4569, "text": "“You shall hear, then--but prepare for something very dreadful. The"}
{"doc_id": "gutenberg_1342", "para_id": 4570, "text": "first time of my ever seeing him in Hertfordshire, you must know, was at"}
{"doc_id": "gutenberg_1342", "para_id": 4571, "text": "a ball--and at this ball, what do you think he did? He danced only four"}
{"doc_id": "gutenberg_1342", "para_id": 4572, "text": "dances! I am sorry to pain you, but so it was. He danced only four"}
{"doc_id": "gutenberg_1342", "para_id": 4573, "text": "dances, though gentlemen were scarce; and, to my certain knowledge, more"}
{"doc_id": "gutenberg_1342", "para_id": 4574, "text": "than one young lady was sitting down in want of a partner. Mr. Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 4575, "text": "“I had not at that time the honour of knowing any lady in the assembly"}
{"doc_id": "gutenberg_1342", "para_id": 4576, "text": "“True; and nobody can ever be introduced in a ball-room. Well, Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 4577, "text": "Fitzwilliam, what do I play next? My fingers wait your orders.”"}
{"doc_id": "gutenberg_1342", "para_id": 4578, "text": "“Perhaps,” said Darcy, “I should have judged better had I sought an"}
{"doc_id": "gutenberg_1342", "para_id": 4579, "text": "introduction, but I am ill-qualified to recommend myself to strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 4580, "text": "“Shall we ask your cousin the reason of this?” said Elizabeth, still"}
{"doc_id": "gutenberg_1342", "para_id": 4581, "text": "addressing Colonel Fitzwilliam. “Shall we ask him why a man of sense and"}
{"doc_id": "gutenberg_1342", "para_id": 4582, "text": "education, and who has lived in the world, is ill-qualified to recommend"}
{"doc_id": "gutenberg_1342", "para_id": 4583, "text": "“I can answer your question,” said Fitzwilliam, “without applying to"}
{"doc_id": "gutenberg_1342", "para_id": 4584, "text": "him. It is because he will not give himself the trouble.”"}
{"doc_id": "gutenberg_1342", "para_id": 4585, "text": "“I certainly have not the talent which some people possess,” said Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 4586, "text": "“of conversing easily with those I have never seen before. I cannot"}
{"doc_id": "gutenberg_1342", "para_id": 4587, "text": "catch their tone of conversation, or appear interested in their"}
{"doc_id": "gutenberg_1342", "para_id": 4588, "text": "“My fingers,” said Elizabeth, “do not move over this instrument in the"}
{"doc_id": "gutenberg_1342", "para_id": 4589, "text": "masterly manner which I see so many women’s do. They have not the same"}
{"doc_id": "gutenberg_1342", "para_id": 4590, "text": "force or rapidity, and do not produce the same expression. But then I"}
{"doc_id": "gutenberg_1342", "para_id": 4591, "text": "have always supposed it to be my own fault--because I would not take"}
{"doc_id": "gutenberg_1342", "para_id": 4592, "text": "the trouble of practising. It is not that I do not believe _my_ fingers"}
{"doc_id": "gutenberg_1342", "para_id": 4593, "text": "as capable as any other woman’s of superior execution.”"}
{"doc_id": "gutenberg_1342", "para_id": 4594, "text": "Darcy smiled and said, “You are perfectly right. You have employed your"}
{"doc_id": "gutenberg_1342", "para_id": 4595, "text": "time much better. No one admitted to the privilege of hearing you can"}
{"doc_id": "gutenberg_1342", "para_id": 4596, "text": "think anything wanting. We neither of us perform to strangers.”"}
{"doc_id": "gutenberg_1342", "para_id": 4597, "text": "Here they were interrupted by Lady Catherine, who called out to know"}
{"doc_id": "gutenberg_1342", "para_id": 4598, "text": "what they were talking of. Elizabeth immediately began playing again."}
{"doc_id": "gutenberg_1342", "para_id": 4599, "text": "Lady Catherine approached, and, after listening for a few minutes, said"}
{"doc_id": "gutenberg_1342", "para_id": 4600, "text": "“Miss Bennet would not play at all amiss if she practised more, and"}
{"doc_id": "gutenberg_1342", "para_id": 4601, "text": "could have the advantage of a London master. She has a very good notion"}
{"doc_id": "gutenberg_1342", "para_id": 4602, "text": "of fingering, though her taste is not equal to Anne’s. Anne would have"}
{"doc_id": "gutenberg_1342", "para_id": 4603, "text": "been a delightful performer, had her health allowed her to learn.”"}
{"doc_id": "gutenberg_1342", "para_id": 4604, "text": "Elizabeth looked at Darcy, to see how cordially he assented to his"}
{"doc_id": "gutenberg_1342", "para_id": 4605, "text": "cousin’s praise: but neither at that moment nor at any other could she"}
{"doc_id": "gutenberg_1342", "para_id": 4606, "text": "discern any symptom of love; and from the whole of his behaviour to Miss"}
{"doc_id": "gutenberg_1342", "para_id": 4607, "text": "De Bourgh she derived this comfort for Miss Bingley, that he might have"}
{"doc_id": "gutenberg_1342", "para_id": 4608, "text": "been just as likely to marry _her_, had she been his relation."}
{"doc_id": "gutenberg_1342", "para_id": 4609, "text": "Lady Catherine continued her remarks on Elizabeth’s performance, mixing"}
{"doc_id": "gutenberg_1342", "para_id": 4610, "text": "with them many instructions on execution and taste. Elizabeth received"}
{"doc_id": "gutenberg_1342", "para_id": 4611, "text": "them with all the forbearance of civility; and at the request of the"}
{"doc_id": "gutenberg_1342", "para_id": 4612, "text": "gentlemen remained at the instrument till her Ladyship’s carriage was"}
{"doc_id": "gutenberg_1342", "para_id": 4613, "text": "Elizabeth was sitting by herself the next morning, and writing to Jane,"}
{"doc_id": "gutenberg_1342", "para_id": 4614, "text": "while Mrs. Collins and Maria were gone on business into the village,"}
{"doc_id": "gutenberg_1342", "para_id": 4615, "text": "when she was startled by a ring at the door, the certain signal of a"}
{"doc_id": "gutenberg_1342", "para_id": 4616, "text": "visitor. As she had heard no carriage, she thought it not unlikely to be"}
{"doc_id": "gutenberg_1342", "para_id": 4617, "text": "Lady Catherine; and under that apprehension was putting away her"}
{"doc_id": "gutenberg_1342", "para_id": 4618, "text": "half-finished letter, that she might escape all impertinent questions,"}
{"doc_id": "gutenberg_1342", "para_id": 4619, "text": "when the door opened, and to her very great surprise Mr. Darcy, and Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4620, "text": "He seemed astonished too on finding her alone, and apologized for his"}
{"doc_id": "gutenberg_1342", "para_id": 4621, "text": "intrusion, by letting her know that he had understood all the ladies to"}
{"doc_id": "gutenberg_1342", "para_id": 4622, "text": "They then sat down, and when her inquiries after Rosings were made,"}
{"doc_id": "gutenberg_1342", "para_id": 4623, "text": "seemed in danger of sinking into total silence. It was absolutely"}
{"doc_id": "gutenberg_1342", "para_id": 4624, "text": "necessary, therefore, to think of something; and in this emergency"}
{"doc_id": "gutenberg_1342", "para_id": 4625, "text": "recollecting _when_ she had seen him last in Hertfordshire, and feeling"}
{"doc_id": "gutenberg_1342", "para_id": 4626, "text": "curious to know what he would say on the subject of their hasty"}
{"doc_id": "gutenberg_1342", "para_id": 4627, "text": "“How very suddenly you all quitted Netherfield last November, Mr. Darcy!"}
{"doc_id": "gutenberg_1342", "para_id": 4628, "text": "It must have been a most agreeable surprise to Mr. Bingley to see you"}
{"doc_id": "gutenberg_1342", "para_id": 4629, "text": "all after him so soon; for, if I recollect right, he went but the day"}
{"doc_id": "gutenberg_1342", "para_id": 4630, "text": "before. He and his sisters were well, I hope, when you left London?”"}
{"doc_id": "gutenberg_1342", "para_id": 4631, "text": "She found that she was to receive no other answer; and, after a short"}
{"doc_id": "gutenberg_1342", "para_id": 4632, "text": "“I think I have understood that Mr. Bingley has not much idea of ever"}
{"doc_id": "gutenberg_1342", "para_id": 4633, "text": "“I have never heard him say so; but it is probable that he may spend"}
{"doc_id": "gutenberg_1342", "para_id": 4634, "text": "very little of his time there in future. He has many friends, and he is"}
{"doc_id": "gutenberg_1342", "para_id": 4635, "text": "at a time of life when friends and engagements are continually"}
{"doc_id": "gutenberg_1342", "para_id": 4636, "text": "“If he means to be but little at Netherfield, it would be better for the"}
{"doc_id": "gutenberg_1342", "para_id": 4637, "text": "neighbourhood that he should give up the place entirely, for then we"}
{"doc_id": "gutenberg_1342", "para_id": 4638, "text": "might possibly get a settled family there. But, perhaps, Mr. Bingley did"}
{"doc_id": "gutenberg_1342", "para_id": 4639, "text": "not take the house so much for the convenience of the neighbourhood as"}
{"doc_id": "gutenberg_1342", "para_id": 4640, "text": "for his own, and we must expect him to keep or quit it on the same"}
{"doc_id": "gutenberg_1342", "para_id": 4641, "text": "“I should not be surprised,” said Darcy, “if he were to give it up as"}
{"doc_id": "gutenberg_1342", "para_id": 4642, "text": "Elizabeth made no answer. She was afraid of talking longer of his"}
{"doc_id": "gutenberg_1342", "para_id": 4643, "text": "friend; and, having nothing else to say, was now determined to leave the"}
{"doc_id": "gutenberg_1342", "para_id": 4644, "text": "He took the hint and soon began with, “This seems a very comfortable"}
{"doc_id": "gutenberg_1342", "para_id": 4645, "text": "house. Lady Catherine, I believe, did a great deal to it when Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4646, "text": "“I believe she did--and I am sure she could not have bestowed her"}
{"doc_id": "gutenberg_1342", "para_id": 4647, "text": "“Mr. Collins appears very fortunate in his choice of a wife.”"}
{"doc_id": "gutenberg_1342", "para_id": 4648, "text": "“Yes, indeed; his friends may well rejoice in his having met with one of"}
{"doc_id": "gutenberg_1342", "para_id": 4649, "text": "the very few sensible women who would have accepted him, or have made"}
{"doc_id": "gutenberg_1342", "para_id": 4650, "text": "him happy if they had. My friend has an excellent understanding--though"}
{"doc_id": "gutenberg_1342", "para_id": 4651, "text": "I am not certain that I consider her marrying Mr. Collins as the wisest"}
{"doc_id": "gutenberg_1342", "para_id": 4652, "text": "thing she ever did. She seems perfectly happy, however; and, in a"}
{"doc_id": "gutenberg_1342", "para_id": 4653, "text": "prudential light, it is certainly a very good match for her.”"}
{"doc_id": "gutenberg_1342", "para_id": 4654, "text": "“It must be very agreeable to her to be settled within so easy a"}
{"doc_id": "gutenberg_1342", "para_id": 4655, "text": "“An easy distance do you call it? It is nearly fifty miles.”"}
{"doc_id": "gutenberg_1342", "para_id": 4656, "text": "“And what is fifty miles of good road? Little more than half a day’s"}
{"doc_id": "gutenberg_1342", "para_id": 4657, "text": "“I should never have considered the distance as one of the _advantages_"}
{"doc_id": "gutenberg_1342", "para_id": 4658, "text": "of the match,” cried Elizabeth. “I should never have said Mrs. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 4659, "text": "“It is a proof of your own attachment to Hertfordshire. Anything beyond"}
{"doc_id": "gutenberg_1342", "para_id": 4660, "text": "the very neighbourhood of Longbourn, I suppose, would appear far.”"}
{"doc_id": "gutenberg_1342", "para_id": 4661, "text": "As he spoke there was a sort of smile, which Elizabeth fancied she"}
{"doc_id": "gutenberg_1342", "para_id": 4662, "text": "understood; he must be supposing her to be thinking of Jane and"}
{"doc_id": "gutenberg_1342", "para_id": 4663, "text": "“I do not mean to say that a woman may not be settled too near her"}
{"doc_id": "gutenberg_1342", "para_id": 4664, "text": "family. The far and the near must be relative, and depend on many"}
{"doc_id": "gutenberg_1342", "para_id": 4665, "text": "varying circumstances. Where there is fortune to make the expense of"}
{"doc_id": "gutenberg_1342", "para_id": 4666, "text": "travelling unimportant, distance becomes no evil. But that is not the"}
{"doc_id": "gutenberg_1342", "para_id": 4667, "text": "case _here_. Mr. and Mrs. Collins have a comfortable income, but not"}
{"doc_id": "gutenberg_1342", "para_id": 4668, "text": "such a one as will allow of frequent journeys--and I am persuaded my"}
{"doc_id": "gutenberg_1342", "para_id": 4669, "text": "friend would not call herself _near_ her family under less than _half_"}
{"doc_id": "gutenberg_1342", "para_id": 4670, "text": "Mr. Darcy drew his chair a little towards her, and said, “_You_ cannot"}
{"doc_id": "gutenberg_1342", "para_id": 4671, "text": "have a right to such very strong local attachment. _You_ cannot have"}
{"doc_id": "gutenberg_1342", "para_id": 4672, "text": "Elizabeth looked surprised. The gentleman experienced some change of"}
{"doc_id": "gutenberg_1342", "para_id": 4673, "text": "feeling; he drew back his chair, took a newspaper from the table, and,"}
{"doc_id": "gutenberg_1342", "para_id": 4674, "text": "A short dialogue on the subject of the country ensued, on either side"}
{"doc_id": "gutenberg_1342", "para_id": 4675, "text": "calm and concise--and soon put an end to by the entrance of Charlotte"}
{"doc_id": "gutenberg_1342", "para_id": 4676, "text": "and her sister, just returned from their walk. The _tête-à-tête_"}
{"doc_id": "gutenberg_1342", "para_id": 4677, "text": "surprised them. Mr. Darcy related the mistake which had occasioned his"}
{"doc_id": "gutenberg_1342", "para_id": 4678, "text": "intruding on Miss Bennet, and, after sitting a few minutes longer,"}
{"doc_id": "gutenberg_1342", "para_id": 4679, "text": "“What can be the meaning of this?” said Charlotte, as soon as he was"}
{"doc_id": "gutenberg_1342", "para_id": 4680, "text": "gone. “My dear Eliza, he must be in love with you, or he would never"}
{"doc_id": "gutenberg_1342", "para_id": 4681, "text": "But when Elizabeth told of his silence, it did not seem very likely,"}
{"doc_id": "gutenberg_1342", "para_id": 4682, "text": "even to Charlotte’s wishes, to be the case; and, after various"}
{"doc_id": "gutenberg_1342", "para_id": 4683, "text": "conjectures, they could at last only suppose his visit to proceed from"}
{"doc_id": "gutenberg_1342", "para_id": 4684, "text": "the difficulty of finding anything to do, which was the more probable"}
{"doc_id": "gutenberg_1342", "para_id": 4685, "text": "from the time of year. All field sports were over. Within doors there"}
{"doc_id": "gutenberg_1342", "para_id": 4686, "text": "was Lady Catherine, books, and a billiard table, but gentlemen cannot be"}
{"doc_id": "gutenberg_1342", "para_id": 4687, "text": "always within doors; and in the nearness of the Parsonage, or the"}
{"doc_id": "gutenberg_1342", "para_id": 4688, "text": "pleasantness of the walk to it, or of the people who lived in it, the"}
{"doc_id": "gutenberg_1342", "para_id": 4689, "text": "two cousins found a temptation from this period of walking thither"}
{"doc_id": "gutenberg_1342", "para_id": 4690, "text": "almost every day. They called at various times of the morning, sometimes"}
{"doc_id": "gutenberg_1342", "para_id": 4691, "text": "separately, sometimes together, and now and then accompanied by their"}
{"doc_id": "gutenberg_1342", "para_id": 4692, "text": "aunt. It was plain to them all that Colonel Fitzwilliam came because he"}
{"doc_id": "gutenberg_1342", "para_id": 4693, "text": "had pleasure in their society, a persuasion which of course recommended"}
{"doc_id": "gutenberg_1342", "para_id": 4694, "text": "him still more; and Elizabeth was reminded by her own satisfaction in"}
{"doc_id": "gutenberg_1342", "para_id": 4695, "text": "being with him, as well as by his evident admiration, of her former"}
{"doc_id": "gutenberg_1342", "para_id": 4696, "text": "favourite, George Wickham; and though, in comparing them, she saw there"}
{"doc_id": "gutenberg_1342", "para_id": 4697, "text": "was less captivating softness in Colonel Fitzwilliam’s manners, she"}
{"doc_id": "gutenberg_1342", "para_id": 4698, "text": "But why Mr. Darcy came so often to the Parsonage it was more difficult"}
{"doc_id": "gutenberg_1342", "para_id": 4699, "text": "to understand. It could not be for society, as he frequently sat there"}
{"doc_id": "gutenberg_1342", "para_id": 4700, "text": "ten minutes together without opening his lips; and when he did speak, it"}
{"doc_id": "gutenberg_1342", "para_id": 4701, "text": "seemed the effect of necessity rather than of choice--a sacrifice to"}
{"doc_id": "gutenberg_1342", "para_id": 4702, "text": "propriety, not a pleasure to himself. He seldom appeared really"}
{"doc_id": "gutenberg_1342", "para_id": 4703, "text": "animated. Mrs. Collins knew not what to make of him. Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 4704, "text": "Fitzwilliam’s occasionally laughing at his stupidity proved that he was"}
{"doc_id": "gutenberg_1342", "para_id": 4705, "text": "generally different, which her own knowledge of him could not have told"}
{"doc_id": "gutenberg_1342", "para_id": 4706, "text": "her; and as she would have liked to believe this change the effect of"}
{"doc_id": "gutenberg_1342", "para_id": 4707, "text": "love, and the object of that love her friend Eliza, she set herself"}
{"doc_id": "gutenberg_1342", "para_id": 4708, "text": "seriously to work to find it out: she watched him whenever they were at"}
{"doc_id": "gutenberg_1342", "para_id": 4709, "text": "Rosings, and whenever he came to Hunsford; but without much success. He"}
{"doc_id": "gutenberg_1342", "para_id": 4710, "text": "certainly looked at her friend a great deal, but the expression of that"}
{"doc_id": "gutenberg_1342", "para_id": 4711, "text": "look was disputable. It was an earnest, steadfast gaze, but she often"}
{"doc_id": "gutenberg_1342", "para_id": 4712, "text": "doubted whether there were much admiration in it, and sometimes it"}
{"doc_id": "gutenberg_1342", "para_id": 4713, "text": "She had once or twice suggested to Elizabeth the possibility of his"}
{"doc_id": "gutenberg_1342", "para_id": 4714, "text": "being partial to her, but Elizabeth always laughed at the idea; and Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 4715, "text": "Collins did not think it right to press the subject, from the danger of"}
{"doc_id": "gutenberg_1342", "para_id": 4716, "text": "raising expectations which might only end in disappointment; for in her"}
{"doc_id": "gutenberg_1342", "para_id": 4717, "text": "opinion it admitted not of a doubt, that all her friend’s dislike would"}
{"doc_id": "gutenberg_1342", "para_id": 4718, "text": "vanish, if she could suppose him to be in her power."}
{"doc_id": "gutenberg_1342", "para_id": 4719, "text": "In her kind schemes for Elizabeth, she sometimes planned her marrying"}
{"doc_id": "gutenberg_1342", "para_id": 4720, "text": "Colonel Fitzwilliam. He was, beyond comparison, the pleasantest man: he"}
{"doc_id": "gutenberg_1342", "para_id": 4721, "text": "certainly admired her, and his situation in life was most eligible; but,"}
{"doc_id": "gutenberg_1342", "para_id": 4722, "text": "to counterbalance these advantages, Mr. Darcy had considerable patronage"}
{"doc_id": "gutenberg_1342", "para_id": 4723, "text": "in the church, and his cousin could have none at all."}
{"doc_id": "gutenberg_1342", "para_id": 4724, "text": "More than once did Elizabeth, in her ramble within the park,"}
{"doc_id": "gutenberg_1342", "para_id": 4725, "text": "unexpectedly meet Mr. Darcy. She felt all the perverseness of the"}
{"doc_id": "gutenberg_1342", "para_id": 4726, "text": "mischance that should bring him where no one else was brought; and, to"}
{"doc_id": "gutenberg_1342", "para_id": 4727, "text": "prevent its ever happening again, took care to inform him, at first,"}
{"doc_id": "gutenberg_1342", "para_id": 4728, "text": "that it was a favourite haunt of hers. How it could occur a second time,"}
{"doc_id": "gutenberg_1342", "para_id": 4729, "text": "therefore, was very odd! Yet it did, and even the third. It seemed like"}
{"doc_id": "gutenberg_1342", "para_id": 4730, "text": "wilful ill-nature, or a voluntary penance; for on these occasions it was"}
{"doc_id": "gutenberg_1342", "para_id": 4731, "text": "not merely a few formal inquiries and an awkward pause and then away,"}
{"doc_id": "gutenberg_1342", "para_id": 4732, "text": "but he actually thought it necessary to turn back and walk with her. He"}
{"doc_id": "gutenberg_1342", "para_id": 4733, "text": "never said a great deal, nor did she give herself the trouble of talking"}
{"doc_id": "gutenberg_1342", "para_id": 4734, "text": "or of listening much; but it struck her in the course of their third"}
{"doc_id": "gutenberg_1342", "para_id": 4735, "text": "encounter that he was asking some odd unconnected questions--about her"}
{"doc_id": "gutenberg_1342", "para_id": 4736, "text": "pleasure in being at Hunsford, her love of solitary walks, and her"}
{"doc_id": "gutenberg_1342", "para_id": 4737, "text": "opinion of Mr. and Mrs. Collins’s happiness; and that in speaking of"}
{"doc_id": "gutenberg_1342", "para_id": 4738, "text": "Rosings, and her not perfectly understanding the house, he seemed to"}
{"doc_id": "gutenberg_1342", "para_id": 4739, "text": "expect that whenever she came into Kent again she would be staying"}
{"doc_id": "gutenberg_1342", "para_id": 4740, "text": "_there_ too. His words seemed to imply it. Could he have Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 4741, "text": "Fitzwilliam in his thoughts? She supposed, if he meant anything, he must"}
{"doc_id": "gutenberg_1342", "para_id": 4742, "text": "mean an allusion to what might arise in that quarter. It distressed her"}
{"doc_id": "gutenberg_1342", "para_id": 4743, "text": "a little, and she was quite glad to find herself at the gate in the"}
{"doc_id": "gutenberg_1342", "para_id": 4744, "text": "She was engaged one day, as she walked, in re-perusing Jane’s last"}
{"doc_id": "gutenberg_1342", "para_id": 4745, "text": "letter, and dwelling on some passages which proved that Jane had not"}
{"doc_id": "gutenberg_1342", "para_id": 4746, "text": "written in spirits, when, instead of being again surprised by Mr. Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 4747, "text": "she saw, on looking up, that Colonel Fitzwilliam was meeting her."}
{"doc_id": "gutenberg_1342", "para_id": 4748, "text": "Putting away the letter immediately, and forcing a smile, she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 4749, "text": "“I did not know before that you ever walked this way.”"}
{"doc_id": "gutenberg_1342", "para_id": 4750, "text": "“I have been making the tour of the park,” he replied, “as I generally"}
{"doc_id": "gutenberg_1342", "para_id": 4751, "text": "do every year, and intended to close it with a call at the Parsonage."}
{"doc_id": "gutenberg_1342", "para_id": 4752, "text": "And accordingly she did turn, and they walked towards the Parsonage"}
{"doc_id": "gutenberg_1342", "para_id": 4753, "text": "“Do you certainly leave Kent on Saturday?” said she."}
{"doc_id": "gutenberg_1342", "para_id": 4754, "text": "“Yes--if Darcy does not put it off again. But I am at his disposal. He"}
{"doc_id": "gutenberg_1342", "para_id": 4755, "text": "“And if not able to please himself in the arrangement, he has at least"}
{"doc_id": "gutenberg_1342", "para_id": 4756, "text": "great pleasure in the power of choice. I do not know anybody who seems"}
{"doc_id": "gutenberg_1342", "para_id": 4757, "text": "more to enjoy the power of doing what he likes than Mr. Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 4758, "text": "“He likes to have his own way very well,” replied Colonel Fitzwilliam."}
{"doc_id": "gutenberg_1342", "para_id": 4759, "text": "“But so we all do. It is only that he has better means of having it than"}
{"doc_id": "gutenberg_1342", "para_id": 4760, "text": "many others, because he is rich, and many others are poor. I speak"}
{"doc_id": "gutenberg_1342", "para_id": 4761, "text": "feelingly. A younger son, you know, must be inured to self-denial and"}
{"doc_id": "gutenberg_1342", "para_id": 4762, "text": "“In my opinion, the younger son of an earl can know very little of"}
{"doc_id": "gutenberg_1342", "para_id": 4763, "text": "either. Now, seriously, what have you ever known of self-denial and"}
{"doc_id": "gutenberg_1342", "para_id": 4764, "text": "dependence? When have you been prevented by want of money from going"}
{"doc_id": "gutenberg_1342", "para_id": 4765, "text": "wherever you chose or procuring anything you had a fancy for?”"}
{"doc_id": "gutenberg_1342", "para_id": 4766, "text": "“These are home questions--and perhaps I cannot say that I have"}
{"doc_id": "gutenberg_1342", "para_id": 4767, "text": "experienced many hardships of that nature. But in matters of greater"}
{"doc_id": "gutenberg_1342", "para_id": 4768, "text": "weight, I may suffer from the want of money. Younger sons cannot marry"}
{"doc_id": "gutenberg_1342", "para_id": 4769, "text": "“Unless where they like women of fortune, which I think they very often"}
{"doc_id": "gutenberg_1342", "para_id": 4770, "text": "“Our habits of expense make us too dependent, and there are not many in"}
{"doc_id": "gutenberg_1342", "para_id": 4771, "text": "my rank of life who can afford to marry without some attention to"}
{"doc_id": "gutenberg_1342", "para_id": 4772, "text": "“Is this,” thought Elizabeth, “meant for me?” and she coloured at the"}
{"doc_id": "gutenberg_1342", "para_id": 4773, "text": "idea; but, recovering herself, said in a lively tone, “And pray, what is"}
{"doc_id": "gutenberg_1342", "para_id": 4774, "text": "the usual price of an earl’s younger son? Unless the elder brother is"}
{"doc_id": "gutenberg_1342", "para_id": 4775, "text": "very sickly, I suppose you would not ask above fifty thousand pounds.”"}
{"doc_id": "gutenberg_1342", "para_id": 4776, "text": "He answered her in the same style, and the subject dropped. To interrupt"}
{"doc_id": "gutenberg_1342", "para_id": 4777, "text": "a silence which might make him fancy her affected with what had passed,"}
{"doc_id": "gutenberg_1342", "para_id": 4778, "text": "“I imagine your cousin brought you down with him chiefly for the sake of"}
{"doc_id": "gutenberg_1342", "para_id": 4779, "text": "having somebody at his disposal. I wonder he does not marry, to secure a"}
{"doc_id": "gutenberg_1342", "para_id": 4780, "text": "lasting convenience of that kind. But, perhaps, his sister does as well"}
{"doc_id": "gutenberg_1342", "para_id": 4781, "text": "for the present; and, as she is under his sole care, he may do what he"}
{"doc_id": "gutenberg_1342", "para_id": 4782, "text": "“No,” said Colonel Fitzwilliam, “that is an advantage which he must"}
{"doc_id": "gutenberg_1342", "para_id": 4783, "text": "divide with me. I am joined with him in the guardianship of Miss Darcy.”"}
{"doc_id": "gutenberg_1342", "para_id": 4784, "text": "“Are you, indeed? And pray what sort of a guardian do you make? Does"}
{"doc_id": "gutenberg_1342", "para_id": 4785, "text": "your charge give you much trouble? Young ladies of her age are sometimes"}
{"doc_id": "gutenberg_1342", "para_id": 4786, "text": "a little difficult to manage; and if she has the true Darcy spirit, she"}
{"doc_id": "gutenberg_1342", "para_id": 4787, "text": "As she spoke, she observed him looking at her earnestly; and the manner"}
{"doc_id": "gutenberg_1342", "para_id": 4788, "text": "in which he immediately asked her why she supposed Miss Darcy likely to"}
{"doc_id": "gutenberg_1342", "para_id": 4789, "text": "give them any uneasiness, convinced her that she had somehow or other"}
{"doc_id": "gutenberg_1342", "para_id": 4790, "text": "“You need not be frightened. I never heard any harm of her; and I dare"}
{"doc_id": "gutenberg_1342", "para_id": 4791, "text": "say she is one of the most tractable creatures in the world. She is a"}
{"doc_id": "gutenberg_1342", "para_id": 4792, "text": "very great favourite with some ladies of my acquaintance, Mrs. Hurst and"}
{"doc_id": "gutenberg_1342", "para_id": 4793, "text": "Miss Bingley. I think I have heard you say that you know them.”"}
{"doc_id": "gutenberg_1342", "para_id": 4794, "text": "“I know them a little. Their brother is a pleasant, gentlemanlike"}
{"doc_id": "gutenberg_1342", "para_id": 4795, "text": "“Oh yes,” said Elizabeth drily--“Mr. Darcy is uncommonly kind to Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4796, "text": "Bingley, and takes a prodigious deal of care of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 4797, "text": "“Care of him! Yes, I really believe Darcy _does_ take care of him in"}
{"doc_id": "gutenberg_1342", "para_id": 4798, "text": "those points where he most wants care. From something that he told me"}
{"doc_id": "gutenberg_1342", "para_id": 4799, "text": "in our journey hither, I have reason to think Bingley very much indebted"}
{"doc_id": "gutenberg_1342", "para_id": 4800, "text": "to him. But I ought to beg his pardon, for I have no right to suppose"}
{"doc_id": "gutenberg_1342", "para_id": 4801, "text": "that Bingley was the person meant. It was all conjecture.”"}
{"doc_id": "gutenberg_1342", "para_id": 4802, "text": "“It is a circumstance which Darcy of course could not wish to be"}
{"doc_id": "gutenberg_1342", "para_id": 4803, "text": "generally known, because if it were to get round to the lady’s family it"}
{"doc_id": "gutenberg_1342", "para_id": 4804, "text": "“And remember that I have not much reason for supposing it to be"}
{"doc_id": "gutenberg_1342", "para_id": 4805, "text": "Bingley. What he told me was merely this: that he congratulated himself"}
{"doc_id": "gutenberg_1342", "para_id": 4806, "text": "on having lately saved a friend from the inconveniences of a most"}
{"doc_id": "gutenberg_1342", "para_id": 4807, "text": "imprudent marriage, but without mentioning names or any other"}
{"doc_id": "gutenberg_1342", "para_id": 4808, "text": "particulars; and I only suspected it to be Bingley from believing him"}
{"doc_id": "gutenberg_1342", "para_id": 4809, "text": "the kind of young man to get into a scrape of that sort, and from"}
{"doc_id": "gutenberg_1342", "para_id": 4810, "text": "knowing them to have been together the whole of last summer.”"}
{"doc_id": "gutenberg_1342", "para_id": 4811, "text": "“Did Mr. Darcy give you his reasons for this interference?”"}
{"doc_id": "gutenberg_1342", "para_id": 4812, "text": "“I understood that there were some very strong objections against the"}
{"doc_id": "gutenberg_1342", "para_id": 4813, "text": "“He did not talk to me of his own arts,” said Fitzwilliam, smiling. “He"}
{"doc_id": "gutenberg_1342", "para_id": 4814, "text": "Elizabeth made no answer, and walked on, her heart swelling with"}
{"doc_id": "gutenberg_1342", "para_id": 4815, "text": "indignation. After watching her a little, Fitzwilliam asked her why she"}
{"doc_id": "gutenberg_1342", "para_id": 4816, "text": "“I am thinking of what you have been telling me,” said she. “Your"}
{"doc_id": "gutenberg_1342", "para_id": 4817, "text": "cousin’s conduct does not suit my feelings. Why was he to be the"}
{"doc_id": "gutenberg_1342", "para_id": 4818, "text": "“You are rather disposed to call his interference officious?”"}
{"doc_id": "gutenberg_1342", "para_id": 4819, "text": "“I do not see what right Mr. Darcy had to decide on the propriety of his"}
{"doc_id": "gutenberg_1342", "para_id": 4820, "text": "friend’s inclination; or why, upon his own judgment alone, he was to"}
{"doc_id": "gutenberg_1342", "para_id": 4821, "text": "determine and direct in what manner that friend was to be happy. But,”"}
{"doc_id": "gutenberg_1342", "para_id": 4822, "text": "she continued, recollecting herself, “as we know none of the"}
{"doc_id": "gutenberg_1342", "para_id": 4823, "text": "particulars, it is not fair to condemn him. It is not to be supposed"}
{"doc_id": "gutenberg_1342", "para_id": 4824, "text": "“That is not an unnatural surmise,” said Fitzwilliam; “but it is"}
{"doc_id": "gutenberg_1342", "para_id": 4825, "text": "lessening the honour of my cousin’s triumph very sadly.”"}
{"doc_id": "gutenberg_1342", "para_id": 4826, "text": "This was spoken jestingly, but it appeared to her so just a picture of"}
{"doc_id": "gutenberg_1342", "para_id": 4827, "text": "Mr. Darcy, that she would not trust herself with an answer; and,"}
{"doc_id": "gutenberg_1342", "para_id": 4828, "text": "therefore, abruptly changing the conversation, talked on indifferent"}
{"doc_id": "gutenberg_1342", "para_id": 4829, "text": "matters till they reached the Parsonage. There, shut into her own room,"}
{"doc_id": "gutenberg_1342", "para_id": 4830, "text": "as soon as their visitor left them, she could think without interruption"}
{"doc_id": "gutenberg_1342", "para_id": 4831, "text": "of all that she had heard. It was not to be supposed that any other"}
{"doc_id": "gutenberg_1342", "para_id": 4832, "text": "people could be meant than those with whom she was connected. There"}
{"doc_id": "gutenberg_1342", "para_id": 4833, "text": "could not exist in the world _two_ men over whom Mr. Darcy could have"}
{"doc_id": "gutenberg_1342", "para_id": 4834, "text": "such boundless influence. That he had been concerned in the measures"}
{"doc_id": "gutenberg_1342", "para_id": 4835, "text": "taken to separate Mr. Bingley and Jane, she had never doubted; but she"}
{"doc_id": "gutenberg_1342", "para_id": 4836, "text": "had always attributed to Miss Bingley the principal design and"}
{"doc_id": "gutenberg_1342", "para_id": 4837, "text": "arrangement of them. If his own vanity, however, did not mislead him,"}
{"doc_id": "gutenberg_1342", "para_id": 4838, "text": "_he_ was the cause--his pride and caprice were the cause--of all that"}
{"doc_id": "gutenberg_1342", "para_id": 4839, "text": "Jane had suffered, and still continued to suffer. He had ruined for a"}
{"doc_id": "gutenberg_1342", "para_id": 4840, "text": "while every hope of happiness for the most affectionate, generous heart"}
{"doc_id": "gutenberg_1342", "para_id": 4841, "text": "in the world; and no one could say how lasting an evil he might have"}
{"doc_id": "gutenberg_1342", "para_id": 4842, "text": "“There were some very strong objections against the lady,” were Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 4843, "text": "Fitzwilliam’s words; and these strong objections probably were, her"}
{"doc_id": "gutenberg_1342", "para_id": 4844, "text": "having one uncle who was a country attorney, and another who was in"}
{"doc_id": "gutenberg_1342", "para_id": 4845, "text": "“To Jane herself,” she exclaimed, “there could be no possibility of"}
{"doc_id": "gutenberg_1342", "para_id": 4846, "text": "objection,--all loveliness and goodness as she is! Her understanding"}
{"doc_id": "gutenberg_1342", "para_id": 4847, "text": "excellent, her mind improved, and her manners captivating. Neither could"}
{"doc_id": "gutenberg_1342", "para_id": 4848, "text": "anything be urged against my father, who, though with some"}
{"doc_id": "gutenberg_1342", "para_id": 4849, "text": "peculiarities, has abilities which Mr. Darcy himself need not disdain,"}
{"doc_id": "gutenberg_1342", "para_id": 4850, "text": "and respectability which he will probably never reach.” When she thought"}
{"doc_id": "gutenberg_1342", "para_id": 4851, "text": "of her mother, indeed, her confidence gave way a little; but she would"}
{"doc_id": "gutenberg_1342", "para_id": 4852, "text": "not allow that any objections _there_ had material weight with Mr."}
{"doc_id": "gutenberg_1342", "para_id": 4853, "text": "Darcy, whose pride, she was convinced, would receive a deeper wound from"}
{"doc_id": "gutenberg_1342", "para_id": 4854, "text": "the want of importance in his friend’s connections than from their want"}
{"doc_id": "gutenberg_1342", "para_id": 4855, "text": "of sense; and she was quite decided, at last, that he had been partly"}
{"doc_id": "gutenberg_1342", "para_id": 4856, "text": "governed by this worst kind of pride, and partly by the wish of"}
{"doc_id": "gutenberg_1342", "para_id": 4857, "text": "The agitation and tears which the subject occasioned brought on a"}
{"doc_id": "gutenberg_1342", "para_id": 4858, "text": "headache; and it grew so much worse towards the evening that, added to"}
{"doc_id": "gutenberg_1342", "para_id": 4859, "text": "her unwillingness to see Mr. Darcy, it determined her not to attend her"}
{"doc_id": "gutenberg_1342", "para_id": 4860, "text": "cousins to Rosings, where they were engaged to drink tea. Mrs. Collins,"}
{"doc_id": "gutenberg_1342", "para_id": 4861, "text": "seeing that she was really unwell, did not press her to go, and as much"}
{"doc_id": "gutenberg_1342", "para_id": 4862, "text": "as possible prevented her husband from pressing her; but Mr. Collins"}
{"doc_id": "gutenberg_1342", "para_id": 4863, "text": "could not conceal his apprehension of Lady Catherine’s being rather"}
{"doc_id": "gutenberg_1342", "para_id": 4864, "text": "When they were gone, Elizabeth, as if intending to exasperate herself as"}
{"doc_id": "gutenberg_1342", "para_id": 4865, "text": "much as possible against Mr. Darcy, chose for her employment the"}
{"doc_id": "gutenberg_1342", "para_id": 4866, "text": "examination of all the letters which Jane had written to her since her"}
{"doc_id": "gutenberg_1342", "para_id": 4867, "text": "being in Kent. They contained no actual complaint, nor was there any"}
{"doc_id": "gutenberg_1342", "para_id": 4868, "text": "revival of past occurrences, or any communication of present suffering."}
{"doc_id": "gutenberg_1342", "para_id": 4869, "text": "But in all, and in almost every line of each, there was a want of that"}
{"doc_id": "gutenberg_1342", "para_id": 4870, "text": "cheerfulness which had been used to characterize her style, and which,"}
{"doc_id": "gutenberg_1342", "para_id": 4871, "text": "proceeding from the serenity of a mind at ease with itself, and kindly"}
{"doc_id": "gutenberg_1342", "para_id": 4872, "text": "disposed towards everyone, had been scarcely ever clouded. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 4873, "text": "noticed every sentence conveying the idea of uneasiness, with an"}
{"doc_id": "gutenberg_1342", "para_id": 4874, "text": "attention which it had hardly received on the first perusal. Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 4875, "text": "shameful boast of what misery he had been able to inflict gave her a"}
{"doc_id": "gutenberg_1342", "para_id": 4876, "text": "keener sense of her sister’s sufferings. It was some consolation to"}
{"doc_id": "gutenberg_1342", "para_id": 4877, "text": "think that his visit to Rosings was to end on the day after the next,"}
{"doc_id": "gutenberg_1342", "para_id": 4878, "text": "and a still greater that in less than a fortnight she should herself be"}
{"doc_id": "gutenberg_1342", "para_id": 4879, "text": "with Jane again, and enabled to contribute to the recovery of her"}
{"doc_id": "gutenberg_1342", "para_id": 4880, "text": "She could not think of Darcy’s leaving Kent without remembering that his"}
{"doc_id": "gutenberg_1342", "para_id": 4881, "text": "cousin was to go with him; but Colonel Fitzwilliam had made it clear"}
{"doc_id": "gutenberg_1342", "para_id": 4882, "text": "that he had no intentions at all, and, agreeable as he was, she did not"}
{"doc_id": "gutenberg_1342", "para_id": 4883, "text": "While settling this point, she was suddenly roused by the sound of the"}
{"doc_id": "gutenberg_1342", "para_id": 4884, "text": "door-bell; and her spirits were a little fluttered by the idea of its"}
{"doc_id": "gutenberg_1342", "para_id": 4885, "text": "being Colonel Fitzwilliam himself, who had once before called late in"}
{"doc_id": "gutenberg_1342", "para_id": 4886, "text": "the evening, and might now come to inquire particularly after her. But"}
{"doc_id": "gutenberg_1342", "para_id": 4887, "text": "this idea was soon banished, and her spirits were very differently"}
{"doc_id": "gutenberg_1342", "para_id": 4888, "text": "affected, when, to her utter amazement, she saw Mr. Darcy walk into the"}
{"doc_id": "gutenberg_1342", "para_id": 4889, "text": "room. In a hurried manner he immediately began an inquiry after her"}
{"doc_id": "gutenberg_1342", "para_id": 4890, "text": "health, imputing his visit to a wish of hearing that she were better."}
{"doc_id": "gutenberg_1342", "para_id": 4891, "text": "She answered him with cold civility. He sat down for a few moments, and"}
{"doc_id": "gutenberg_1342", "para_id": 4892, "text": "then getting up walked about the room. Elizabeth was surprised, but"}
{"doc_id": "gutenberg_1342", "para_id": 4893, "text": "said not a word. After a silence of several minutes, he came towards her"}
{"doc_id": "gutenberg_1342", "para_id": 4894, "text": "“In vain have I struggled. It will not do. My feelings will not be"}
{"doc_id": "gutenberg_1342", "para_id": 4895, "text": "repressed. You must allow me to tell you how ardently I admire and love"}
{"doc_id": "gutenberg_1342", "para_id": 4896, "text": "Elizabeth’s astonishment was beyond expression. She stared, coloured,"}
{"doc_id": "gutenberg_1342", "para_id": 4897, "text": "doubted, and was silent. This he considered sufficient encouragement,"}
{"doc_id": "gutenberg_1342", "para_id": 4898, "text": "and the avowal of all that he felt and had long felt for her immediately"}
{"doc_id": "gutenberg_1342", "para_id": 4899, "text": "followed. He spoke well; but there were feelings besides those of the"}
{"doc_id": "gutenberg_1342", "para_id": 4900, "text": "heart to be detailed, and he was not more eloquent on the subject of"}
{"doc_id": "gutenberg_1342", "para_id": 4901, "text": "tenderness than of pride. His sense of her inferiority, of its being a"}
{"doc_id": "gutenberg_1342", "para_id": 4902, "text": "degradation, of the family obstacles which judgment had always opposed"}
{"doc_id": "gutenberg_1342", "para_id": 4903, "text": "to inclination, were dwelt on with a warmth which seemed due to the"}
{"doc_id": "gutenberg_1342", "para_id": 4904, "text": "consequence he was wounding, but was very unlikely to recommend his"}
{"doc_id": "gutenberg_1342", "para_id": 4905, "text": "In spite of her deeply-rooted dislike, she could not be insensible to"}
{"doc_id": "gutenberg_1342", "para_id": 4906, "text": "the compliment of such a man’s affection, and though her intentions did"}
{"doc_id": "gutenberg_1342", "para_id": 4907, "text": "not vary for an instant, she was at first sorry for the pain he was to"}
{"doc_id": "gutenberg_1342", "para_id": 4908, "text": "receive; till roused to resentment by his subsequent language, she lost"}
{"doc_id": "gutenberg_1342", "para_id": 4909, "text": "all compassion in anger. She tried, however, to compose herself to"}
{"doc_id": "gutenberg_1342", "para_id": 4910, "text": "answer him with patience, when he should have done. He concluded with"}
{"doc_id": "gutenberg_1342", "para_id": 4911, "text": "representing to her the strength of that attachment which in spite of"}
{"doc_id": "gutenberg_1342", "para_id": 4912, "text": "all his endeavours he had found impossible to conquer; and with"}
{"doc_id": "gutenberg_1342", "para_id": 4913, "text": "expressing his hope that it would now be rewarded by her acceptance of"}
{"doc_id": "gutenberg_1342", "para_id": 4914, "text": "his hand. As he said this she could easily see that he had no doubt of a"}
{"doc_id": "gutenberg_1342", "para_id": 4915, "text": "favourable answer. He _spoke_ of apprehension and anxiety, but his"}
{"doc_id": "gutenberg_1342", "para_id": 4916, "text": "countenance expressed real security. Such a circumstance could only"}
{"doc_id": "gutenberg_1342", "para_id": 4917, "text": "exasperate farther; and when he ceased the colour rose into her cheeks"}
{"doc_id": "gutenberg_1342", "para_id": 4918, "text": "“In such cases as this, it is, I believe, the established mode to"}
{"doc_id": "gutenberg_1342", "para_id": 4919, "text": "express a sense of obligation for the sentiments avowed, however"}
{"doc_id": "gutenberg_1342", "para_id": 4920, "text": "unequally they may be returned. It is natural that obligation should be"}
{"doc_id": "gutenberg_1342", "para_id": 4921, "text": "felt, and if I could _feel_ gratitude, I would now thank you. But I"}
{"doc_id": "gutenberg_1342", "para_id": 4922, "text": "cannot--I have never desired your good opinion, and you have certainly"}
{"doc_id": "gutenberg_1342", "para_id": 4923, "text": "bestowed it most unwillingly. I am sorry to have occasioned pain to"}
{"doc_id": "gutenberg_1342", "para_id": 4924, "text": "anyone. It has been most unconsciously done, however, and I hope will be"}
{"doc_id": "gutenberg_1342", "para_id": 4925, "text": "of short duration. The feelings which you tell me have long prevented"}
{"doc_id": "gutenberg_1342", "para_id": 4926, "text": "the acknowledgment of your regard can have little difficulty in"}
{"doc_id": "gutenberg_1342", "para_id": 4927, "text": "Mr. Darcy, who was leaning against the mantel-piece with his eyes fixed"}
{"doc_id": "gutenberg_1342", "para_id": 4928, "text": "on her face, seemed to catch her words with no less resentment than"}
{"doc_id": "gutenberg_1342", "para_id": 4929, "text": "surprise. His complexion became pale with anger, and the disturbance of"}
{"doc_id": "gutenberg_1342", "para_id": 4930, "text": "his mind was visible in every feature. He was struggling for the"}
{"doc_id": "gutenberg_1342", "para_id": 4931, "text": "appearance of composure, and would not open his lips till he believed"}
{"doc_id": "gutenberg_1342", "para_id": 4932, "text": "himself to have attained it. The pause was to Elizabeth’s feelings"}
{"doc_id": "gutenberg_1342", "para_id": 4933, "text": "dreadful. At length, in a voice of forced calmness, he said,--"}
{"doc_id": "gutenberg_1342", "para_id": 4934, "text": "“And this is all the reply which I am to have the honour of expecting! I"}
{"doc_id": "gutenberg_1342", "para_id": 4935, "text": "might, perhaps, wish to be informed why, with so little _endeavour_ at"}
{"doc_id": "gutenberg_1342", "para_id": 4936, "text": "civility, I am thus rejected. But it is of small importance.”"}
{"doc_id": "gutenberg_1342", "para_id": 4937, "text": "“I might as well inquire,” replied she, “why, with so evident a design"}
{"doc_id": "gutenberg_1342", "para_id": 4938, "text": "of offending and insulting me, you chose to tell me that you liked me"}
{"doc_id": "gutenberg_1342", "para_id": 4939, "text": "against your will, against your reason, and even against your character?"}
{"doc_id": "gutenberg_1342", "para_id": 4940, "text": "Was not this some excuse for incivility, if I _was_ uncivil? But I have"}
{"doc_id": "gutenberg_1342", "para_id": 4941, "text": "other provocations. You know I have. Had not my own feelings decided"}
{"doc_id": "gutenberg_1342", "para_id": 4942, "text": "against you, had they been indifferent, or had they even been"}
{"doc_id": "gutenberg_1342", "para_id": 4943, "text": "favourable, do you think that any consideration would tempt me to accept"}
{"doc_id": "gutenberg_1342", "para_id": 4944, "text": "the man who has been the means of ruining, perhaps for ever, the"}
{"doc_id": "gutenberg_1342", "para_id": 4945, "text": "As she pronounced these words, Mr. Darcy changed colour; but the emotion"}
{"doc_id": "gutenberg_1342", "para_id": 4946, "text": "was short, and he listened without attempting to interrupt her while she"}
{"doc_id": "gutenberg_1342", "para_id": 4947, "text": "“I have every reason in the world to think ill of you. No motive can"}
{"doc_id": "gutenberg_1342", "para_id": 4948, "text": "excuse the unjust and ungenerous part you acted _there_. You dare not,"}
{"doc_id": "gutenberg_1342", "para_id": 4949, "text": "you cannot deny that you have been the principal, if not the only means"}
{"doc_id": "gutenberg_1342", "para_id": 4950, "text": "of dividing them from each other, of exposing one to the censure of the"}
{"doc_id": "gutenberg_1342", "para_id": 4951, "text": "world for caprice and instability, the other to its derision for"}
{"doc_id": "gutenberg_1342", "para_id": 4952, "text": "disappointed hopes, and involving them both in misery of the acutest"}
{"doc_id": "gutenberg_1342", "para_id": 4953, "text": "She paused, and saw with no slight indignation that he was listening"}
{"doc_id": "gutenberg_1342", "para_id": 4954, "text": "with an air which proved him wholly unmoved by any feeling of remorse."}
{"doc_id": "gutenberg_1342", "para_id": 4955, "text": "He even looked at her with a smile of affected incredulity."}
{"doc_id": "gutenberg_1342", "para_id": 4956, "text": "“Can you deny that you have done it?” she repeated."}
{"doc_id": "gutenberg_1342", "para_id": 4957, "text": "With assumed tranquillity he then replied, “I have no wish of denying"}
{"doc_id": "gutenberg_1342", "para_id": 4958, "text": "that I did everything in my power to separate my friend from your"}
{"doc_id": "gutenberg_1342", "para_id": 4959, "text": "sister, or that I rejoice in my success. Towards _him_ I have been"}
{"doc_id": "gutenberg_1342", "para_id": 4960, "text": "Elizabeth disdained the appearance of noticing this civil reflection,"}
{"doc_id": "gutenberg_1342", "para_id": 4961, "text": "but its meaning did not escape, nor was it likely to conciliate her."}
{"doc_id": "gutenberg_1342", "para_id": 4962, "text": "“But it is not merely this affair,” she continued, “on which my dislike"}
{"doc_id": "gutenberg_1342", "para_id": 4963, "text": "is founded. Long before it had taken place, my opinion of you was"}
{"doc_id": "gutenberg_1342", "para_id": 4964, "text": "decided. Your character was unfolded in the recital which I received"}
{"doc_id": "gutenberg_1342", "para_id": 4965, "text": "many months ago from Mr. Wickham. On this subject, what can you have to"}
{"doc_id": "gutenberg_1342", "para_id": 4966, "text": "say? In what imaginary act of friendship can you here defend yourself?"}
{"doc_id": "gutenberg_1342", "para_id": 4967, "text": "or under what misrepresentation can you here impose upon others?”"}
{"doc_id": "gutenberg_1342", "para_id": 4968, "text": "“You take an eager interest in that gentleman’s concerns,” said Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 4969, "text": "in a less tranquil tone, and with a heightened colour."}
{"doc_id": "gutenberg_1342", "para_id": 4970, "text": "“Who that knows what his misfortunes have been can help feeling an"}
{"doc_id": "gutenberg_1342", "para_id": 4971, "text": "“His misfortunes!” repeated Darcy, contemptuously,--“yes, his"}
{"doc_id": "gutenberg_1342", "para_id": 4972, "text": "“And of your infliction,” cried Elizabeth, with energy; “You have"}
{"doc_id": "gutenberg_1342", "para_id": 4973, "text": "reduced him to his present state of poverty--comparative poverty. You"}
{"doc_id": "gutenberg_1342", "para_id": 4974, "text": "have withheld the advantages which you must know to have been designed"}
{"doc_id": "gutenberg_1342", "para_id": 4975, "text": "for him. You have deprived the best years of his life of that"}
{"doc_id": "gutenberg_1342", "para_id": 4976, "text": "independence which was no less his due than his desert. You have done"}
{"doc_id": "gutenberg_1342", "para_id": 4977, "text": "all this! and yet you can treat the mention of his misfortunes with"}
{"doc_id": "gutenberg_1342", "para_id": 4978, "text": "“And this,” cried Darcy, as he walked with quick steps across the room,"}
{"doc_id": "gutenberg_1342", "para_id": 4979, "text": "“is your opinion of me! This is the estimation in which you hold me! I"}
{"doc_id": "gutenberg_1342", "para_id": 4980, "text": "thank you for explaining it so fully. My faults, according to this"}
{"doc_id": "gutenberg_1342", "para_id": 4981, "text": "calculation, are heavy indeed! But, perhaps,” added he, stopping in his"}
{"doc_id": "gutenberg_1342", "para_id": 4982, "text": "walk, and turning towards her, “these offences might have been"}
{"doc_id": "gutenberg_1342", "para_id": 4983, "text": "overlooked, had not your pride been hurt by my honest confession of the"}
{"doc_id": "gutenberg_1342", "para_id": 4984, "text": "scruples that had long prevented my forming any serious design. These"}
{"doc_id": "gutenberg_1342", "para_id": 4985, "text": "bitter accusations might have been suppressed, had I, with greater"}
{"doc_id": "gutenberg_1342", "para_id": 4986, "text": "policy, concealed my struggles, and flattered you into the belief of my"}
{"doc_id": "gutenberg_1342", "para_id": 4987, "text": "being impelled by unqualified, unalloyed inclination; by reason, by"}
{"doc_id": "gutenberg_1342", "para_id": 4988, "text": "reflection, by everything. But disguise of every sort is my abhorrence."}
{"doc_id": "gutenberg_1342", "para_id": 4989, "text": "Nor am I ashamed of the feelings I related. They were natural and just."}
{"doc_id": "gutenberg_1342", "para_id": 4990, "text": "Could you expect me to rejoice in the inferiority of your"}
{"doc_id": "gutenberg_1342", "para_id": 4991, "text": "connections?--to congratulate myself on the hope of relations whose"}
{"doc_id": "gutenberg_1342", "para_id": 4992, "text": "Elizabeth felt herself growing more angry every moment; yet she tried to"}
{"doc_id": "gutenberg_1342", "para_id": 4993, "text": "the utmost to speak with composure when she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 4994, "text": "“You are mistaken, Mr. Darcy, if you suppose that the mode of your"}
{"doc_id": "gutenberg_1342", "para_id": 4995, "text": "declaration affected me in any other way than as it spared me the"}
{"doc_id": "gutenberg_1342", "para_id": 4996, "text": "concern which I might have felt in refusing you, had you behaved in a"}
{"doc_id": "gutenberg_1342", "para_id": 4997, "text": "She saw him start at this; but he said nothing, and she continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 4998, "text": "“You could not have made me the offer of your hand in any possible way"}
{"doc_id": "gutenberg_1342", "para_id": 4999, "text": "Again his astonishment was obvious; and he looked at her with an"}
{"doc_id": "gutenberg_1342", "para_id": 5000, "text": "expression of mingled incredulity and mortification. She went on,--"}
{"doc_id": "gutenberg_1342", "para_id": 5001, "text": "“From the very beginning, from the first moment, I may almost say, of my"}
{"doc_id": "gutenberg_1342", "para_id": 5002, "text": "acquaintance with you, your manners impressing me with the fullest"}
{"doc_id": "gutenberg_1342", "para_id": 5003, "text": "belief of your arrogance, your conceit, and your selfish disdain of the"}
{"doc_id": "gutenberg_1342", "para_id": 5004, "text": "feelings of others, were such as to form that groundwork of"}
{"doc_id": "gutenberg_1342", "para_id": 5005, "text": "disapprobation, on which succeeding events have built so immovable a"}
{"doc_id": "gutenberg_1342", "para_id": 5006, "text": "dislike; and I had not known you a month before I felt that you were the"}
{"doc_id": "gutenberg_1342", "para_id": 5007, "text": "last man in the world whom I could ever be prevailed on to marry.”"}
{"doc_id": "gutenberg_1342", "para_id": 5008, "text": "“You have said quite enough, madam. I perfectly comprehend your"}
{"doc_id": "gutenberg_1342", "para_id": 5009, "text": "feelings, and have now only to be ashamed of what my own have been."}
{"doc_id": "gutenberg_1342", "para_id": 5010, "text": "Forgive me for having taken up so much of your time, and accept my best"}
{"doc_id": "gutenberg_1342", "para_id": 5011, "text": "And with these words he hastily left the room, and Elizabeth heard him"}
{"doc_id": "gutenberg_1342", "para_id": 5012, "text": "the next moment open the front door and quit the house. The tumult of"}
{"doc_id": "gutenberg_1342", "para_id": 5013, "text": "her mind was now painfully great. She knew not how to support herself,"}
{"doc_id": "gutenberg_1342", "para_id": 5014, "text": "and, from actual weakness, sat down and cried for half an hour. Her"}
{"doc_id": "gutenberg_1342", "para_id": 5015, "text": "astonishment, as she reflected on what had passed, was increased by"}
{"doc_id": "gutenberg_1342", "para_id": 5016, "text": "every review of it. That she should receive an offer of marriage from"}
{"doc_id": "gutenberg_1342", "para_id": 5017, "text": "Mr. Darcy! that he should have been in love with her for so many months!"}
{"doc_id": "gutenberg_1342", "para_id": 5018, "text": "so much in love as to wish to marry her in spite of all the objections"}
{"doc_id": "gutenberg_1342", "para_id": 5019, "text": "which had made him prevent his friend’s marrying her sister, and which"}
{"doc_id": "gutenberg_1342", "para_id": 5020, "text": "must appear at least with equal force in his own case, was almost"}
{"doc_id": "gutenberg_1342", "para_id": 5021, "text": "incredible! it was gratifying to have inspired unconsciously so strong"}
{"doc_id": "gutenberg_1342", "para_id": 5022, "text": "an affection. But his pride, his abominable pride, his shameless avowal"}
{"doc_id": "gutenberg_1342", "para_id": 5023, "text": "of what he had done with respect to Jane, his unpardonable assurance in"}
{"doc_id": "gutenberg_1342", "para_id": 5024, "text": "acknowledging, though he could not justify it, and the unfeeling manner"}
{"doc_id": "gutenberg_1342", "para_id": 5025, "text": "which he had mentioned Mr. Wickham, his cruelty towards whom he had not"}
{"doc_id": "gutenberg_1342", "para_id": 5026, "text": "attempted to deny, soon overcame the pity which the consideration of his"}
{"doc_id": "gutenberg_1342", "para_id": 5027, "text": "She continued in very agitating reflections till the sound of Lady"}
{"doc_id": "gutenberg_1342", "para_id": 5028, "text": "Catherine’s carriage made her feel how unequal she was to encounter"}
{"doc_id": "gutenberg_1342", "para_id": 5029, "text": "Charlotte’s observation, and hurried her away to her room."}
{"doc_id": "gutenberg_1342", "para_id": 5030, "text": "Elizabeth awoke the next morning to the same thoughts and meditations"}
{"doc_id": "gutenberg_1342", "para_id": 5031, "text": "which had at length closed her eyes. She could not yet recover from the"}
{"doc_id": "gutenberg_1342", "para_id": 5032, "text": "surprise of what had happened: it was impossible to think of anything"}
{"doc_id": "gutenberg_1342", "para_id": 5033, "text": "else; and, totally indisposed for employment, she resolved soon after"}
{"doc_id": "gutenberg_1342", "para_id": 5034, "text": "breakfast to indulge herself in air and exercise. She was proceeding"}
{"doc_id": "gutenberg_1342", "para_id": 5035, "text": "directly to her favourite walk, when the recollection of Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 5036, "text": "sometimes coming there stopped her, and instead of entering the park,"}
{"doc_id": "gutenberg_1342", "para_id": 5037, "text": "she turned up the lane which led her farther from the turnpike road. The"}
{"doc_id": "gutenberg_1342", "para_id": 5038, "text": "park paling was still the boundary on one side, and she soon passed one"}
{"doc_id": "gutenberg_1342", "para_id": 5039, "text": "After walking two or three times along that part of the lane, she was"}
{"doc_id": "gutenberg_1342", "para_id": 5040, "text": "tempted, by the pleasantness of the morning, to stop at the gates and"}
{"doc_id": "gutenberg_1342", "para_id": 5041, "text": "look into the park. The five weeks which she had now passed in Kent had"}
{"doc_id": "gutenberg_1342", "para_id": 5042, "text": "made a great difference in the country, and every day was adding to the"}
{"doc_id": "gutenberg_1342", "para_id": 5043, "text": "verdure of the early trees. She was on the point of continuing her"}
{"doc_id": "gutenberg_1342", "para_id": 5044, "text": "walk, when she caught a glimpse of a gentleman within the sort of grove"}
{"doc_id": "gutenberg_1342", "para_id": 5045, "text": "which edged the park: he was moving that way; and fearful of its being"}
{"doc_id": "gutenberg_1342", "para_id": 5046, "text": "Mr. Darcy, she was directly retreating. But the person who advanced was"}
{"doc_id": "gutenberg_1342", "para_id": 5047, "text": "now near enough to see her, and stepping forward with eagerness,"}
{"doc_id": "gutenberg_1342", "para_id": 5048, "text": "pronounced her name. She had turned away; but on hearing herself called,"}
{"doc_id": "gutenberg_1342", "para_id": 5049, "text": "though in a voice which proved it to be Mr. Darcy, she moved again"}
{"doc_id": "gutenberg_1342", "para_id": 5050, "text": "towards the gate. He had by that time reached it also; and, holding out"}
{"doc_id": "gutenberg_1342", "para_id": 5051, "text": "a letter, which she instinctively took, said, with a look of haughty"}
{"doc_id": "gutenberg_1342", "para_id": 5052, "text": "composure, “I have been walking in the grove some time, in the hope of"}
{"doc_id": "gutenberg_1342", "para_id": 5053, "text": "meeting you. Will you do me the honour of reading that letter?” and"}
{"doc_id": "gutenberg_1342", "para_id": 5054, "text": "then, with a slight bow, turned again into the plantation, and was soon"}
{"doc_id": "gutenberg_1342", "para_id": 5055, "text": "With no expectation of pleasure, but with the strongest curiosity,"}
{"doc_id": "gutenberg_1342", "para_id": 5056, "text": "Elizabeth opened the letter, and to her still increasing wonder,"}
{"doc_id": "gutenberg_1342", "para_id": 5057, "text": "perceived an envelope containing two sheets of letter paper, written"}
{"doc_id": "gutenberg_1342", "para_id": 5058, "text": "quite through, in a very close hand. The envelope itself was likewise"}
{"doc_id": "gutenberg_1342", "para_id": 5059, "text": "full. Pursuing her way along the lane, she then began it. It was dated"}
{"doc_id": "gutenberg_1342", "para_id": 5060, "text": "from Rosings, at eight o’clock in the morning, and was as follows:--"}
{"doc_id": "gutenberg_1342", "para_id": 5061, "text": "“Be not alarmed, madam, on receiving this letter, by the apprehension of"}
{"doc_id": "gutenberg_1342", "para_id": 5062, "text": "its containing any repetition of those sentiments, or renewal of those"}
{"doc_id": "gutenberg_1342", "para_id": 5063, "text": "offers, which were last night so disgusting to you. I write without any"}
{"doc_id": "gutenberg_1342", "para_id": 5064, "text": "intention of paining you, or humbling myself, by dwelling on wishes,"}
{"doc_id": "gutenberg_1342", "para_id": 5065, "text": "which, for the happiness of both, cannot be too soon forgotten; and the"}
{"doc_id": "gutenberg_1342", "para_id": 5066, "text": "effort which the formation and the perusal of this letter must occasion,"}
{"doc_id": "gutenberg_1342", "para_id": 5067, "text": "should have been spared, had not my character required it to be written"}
{"doc_id": "gutenberg_1342", "para_id": 5068, "text": "and read. You must, therefore, pardon the freedom with which I demand"}
{"doc_id": "gutenberg_1342", "para_id": 5069, "text": "your attention; your feelings, I know, will bestow it unwillingly, but I"}
{"doc_id": "gutenberg_1342", "para_id": 5070, "text": "“Two offences of a very different nature, and by no means of equal"}
{"doc_id": "gutenberg_1342", "para_id": 5071, "text": "magnitude, you last night laid to my charge. The first mentioned was,"}
{"doc_id": "gutenberg_1342", "para_id": 5072, "text": "that, regardless of the sentiments of either, I had detached Mr. Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 5073, "text": "from your sister,--and the other, that I had, in defiance of various"}
{"doc_id": "gutenberg_1342", "para_id": 5074, "text": "claims, in defiance of honour and humanity, ruined the immediate"}
{"doc_id": "gutenberg_1342", "para_id": 5075, "text": "prosperity and blasted the prospects of Mr. Wickham. Wilfully and"}
{"doc_id": "gutenberg_1342", "para_id": 5076, "text": "wantonly to have thrown off the companion of my youth, the acknowledged"}
{"doc_id": "gutenberg_1342", "para_id": 5077, "text": "favourite of my father, a young man who had scarcely any other"}
{"doc_id": "gutenberg_1342", "para_id": 5078, "text": "dependence than on our patronage, and who had been brought up to expect"}
{"doc_id": "gutenberg_1342", "para_id": 5079, "text": "its exertion, would be a depravity, to which the separation of two young"}
{"doc_id": "gutenberg_1342", "para_id": 5080, "text": "persons whose affection could be the growth of only a few weeks, could"}
{"doc_id": "gutenberg_1342", "para_id": 5081, "text": "bear no comparison. But from the severity of that blame which was last"}
{"doc_id": "gutenberg_1342", "para_id": 5082, "text": "night so liberally bestowed, respecting each circumstance, I shall hope"}
{"doc_id": "gutenberg_1342", "para_id": 5083, "text": "to be in future secured, when the following account of my actions and"}
{"doc_id": "gutenberg_1342", "para_id": 5084, "text": "their motives has been read. If, in the explanation of them which is due"}
{"doc_id": "gutenberg_1342", "para_id": 5085, "text": "to myself, I am under the necessity of relating feelings which may be"}
{"doc_id": "gutenberg_1342", "para_id": 5086, "text": "offensive to yours, I can only say that I am sorry. The necessity must"}
{"doc_id": "gutenberg_1342", "para_id": 5087, "text": "be obeyed, and further apology would be absurd. I had not been long in"}
{"doc_id": "gutenberg_1342", "para_id": 5088, "text": "Hertfordshire before I saw, in common with others, that Bingley"}
{"doc_id": "gutenberg_1342", "para_id": 5089, "text": "preferred your elder sister to any other young woman in the country. But"}
{"doc_id": "gutenberg_1342", "para_id": 5090, "text": "it was not till the evening of the dance at Netherfield that I had any"}
{"doc_id": "gutenberg_1342", "para_id": 5091, "text": "apprehension of his feeling a serious attachment. I had often seen him"}
{"doc_id": "gutenberg_1342", "para_id": 5092, "text": "in love before. At that ball, while I had the honour of dancing with"}
{"doc_id": "gutenberg_1342", "para_id": 5093, "text": "you, I was first made acquainted, by Sir William Lucas’s accidental"}
{"doc_id": "gutenberg_1342", "para_id": 5094, "text": "information, that Bingley’s attentions to your sister had given rise to"}
{"doc_id": "gutenberg_1342", "para_id": 5095, "text": "a general expectation of their marriage. He spoke of it as a certain"}
{"doc_id": "gutenberg_1342", "para_id": 5096, "text": "event, of which the time alone could be undecided. From that moment I"}
{"doc_id": "gutenberg_1342", "para_id": 5097, "text": "observed my friend’s behaviour attentively; and I could then perceive"}
{"doc_id": "gutenberg_1342", "para_id": 5098, "text": "that his partiality for Miss Bennet was beyond what I had ever witnessed"}
{"doc_id": "gutenberg_1342", "para_id": 5099, "text": "in him. Your sister I also watched. Her look and manners were open,"}
{"doc_id": "gutenberg_1342", "para_id": 5100, "text": "cheerful, and engaging as ever, but without any symptom of peculiar"}
{"doc_id": "gutenberg_1342", "para_id": 5101, "text": "regard; and I remained convinced, from the evening’s scrutiny, that"}
{"doc_id": "gutenberg_1342", "para_id": 5102, "text": "though she received his attentions with pleasure, she did not invite"}
{"doc_id": "gutenberg_1342", "para_id": 5103, "text": "them by any participation of sentiment. If _you_ have not been mistaken"}
{"doc_id": "gutenberg_1342", "para_id": 5104, "text": "here, _I_ must have been in an error. Your superior knowledge of your"}
{"doc_id": "gutenberg_1342", "para_id": 5105, "text": "sister must make the latter probable. If it be so, if I have been misled"}
{"doc_id": "gutenberg_1342", "para_id": 5106, "text": "by such error to inflict pain on her, your resentment has not been"}
{"doc_id": "gutenberg_1342", "para_id": 5107, "text": "unreasonable. But I shall not scruple to assert, that the serenity of"}
{"doc_id": "gutenberg_1342", "para_id": 5108, "text": "your sister’s countenance and air was such as might have given the most"}
{"doc_id": "gutenberg_1342", "para_id": 5109, "text": "acute observer a conviction that, however amiable her temper, her heart"}
{"doc_id": "gutenberg_1342", "para_id": 5110, "text": "was not likely to be easily touched. That I was desirous of believing"}
{"doc_id": "gutenberg_1342", "para_id": 5111, "text": "her indifferent is certain; but I will venture to say that my"}
{"doc_id": "gutenberg_1342", "para_id": 5112, "text": "investigations and decisions are not usually influenced by my hopes or"}
{"doc_id": "gutenberg_1342", "para_id": 5113, "text": "fears. I did not believe her to be indifferent because I wished it; I"}
{"doc_id": "gutenberg_1342", "para_id": 5114, "text": "believed it on impartial conviction, as truly as I wished it in reason."}
{"doc_id": "gutenberg_1342", "para_id": 5115, "text": "My objections to the marriage were not merely those which I last night"}
{"doc_id": "gutenberg_1342", "para_id": 5116, "text": "acknowledged to have required the utmost force of passion to put aside"}
{"doc_id": "gutenberg_1342", "para_id": 5117, "text": "in my own case; the want of connection could not be so great an evil to"}
{"doc_id": "gutenberg_1342", "para_id": 5118, "text": "my friend as to me. But there were other causes of repugnance; causes"}
{"doc_id": "gutenberg_1342", "para_id": 5119, "text": "which, though still existing, and existing to an equal degree in both"}
{"doc_id": "gutenberg_1342", "para_id": 5120, "text": "instances, I had myself endeavoured to forget, because they were not"}
{"doc_id": "gutenberg_1342", "para_id": 5121, "text": "immediately before me. These causes must be stated, though briefly. The"}
{"doc_id": "gutenberg_1342", "para_id": 5122, "text": "situation of your mother’s family, though objectionable, was nothing in"}
{"doc_id": "gutenberg_1342", "para_id": 5123, "text": "comparison of that total want of propriety so frequently, so almost"}
{"doc_id": "gutenberg_1342", "para_id": 5124, "text": "uniformly betrayed by herself, by your three younger sisters, and"}
{"doc_id": "gutenberg_1342", "para_id": 5125, "text": "occasionally even by your father:--pardon me,--it pains me to offend"}
{"doc_id": "gutenberg_1342", "para_id": 5126, "text": "you. But amidst your concern for the defects of your nearest relations,"}
{"doc_id": "gutenberg_1342", "para_id": 5127, "text": "and your displeasure at this representation of them, let it give you"}
{"doc_id": "gutenberg_1342", "para_id": 5128, "text": "consolation to consider that to have conducted yourselves so as to avoid"}
{"doc_id": "gutenberg_1342", "para_id": 5129, "text": "any share of the like censure is praise no less generally bestowed on"}
{"doc_id": "gutenberg_1342", "para_id": 5130, "text": "you and your eldest sister than it is honourable to the sense and"}
{"doc_id": "gutenberg_1342", "para_id": 5131, "text": "disposition of both. I will only say, farther, that from what passed"}
{"doc_id": "gutenberg_1342", "para_id": 5132, "text": "that evening my opinion of all parties was confirmed, and every"}
{"doc_id": "gutenberg_1342", "para_id": 5133, "text": "inducement heightened, which could have led me before to preserve my"}
{"doc_id": "gutenberg_1342", "para_id": 5134, "text": "friend from what I esteemed a most unhappy connection. He left"}
{"doc_id": "gutenberg_1342", "para_id": 5135, "text": "Netherfield for London on the day following, as you, I am certain,"}
{"doc_id": "gutenberg_1342", "para_id": 5136, "text": "remember, with the design of soon returning. The part which I acted is"}
{"doc_id": "gutenberg_1342", "para_id": 5137, "text": "now to be explained. His sisters’ uneasiness had been equally excited"}
{"doc_id": "gutenberg_1342", "para_id": 5138, "text": "with my own: our coincidence of feeling was soon discovered; and, alike"}
{"doc_id": "gutenberg_1342", "para_id": 5139, "text": "sensible that no time was to be lost in detaching their brother, we"}
{"doc_id": "gutenberg_1342", "para_id": 5140, "text": "shortly resolved on joining him directly in London. We accordingly"}
{"doc_id": "gutenberg_1342", "para_id": 5141, "text": "went--and there I readily engaged in the office of pointing out to my"}
{"doc_id": "gutenberg_1342", "para_id": 5142, "text": "friend the certain evils of such a choice. I described and enforced them"}
{"doc_id": "gutenberg_1342", "para_id": 5143, "text": "earnestly. But however this remonstrance might have staggered or delayed"}
{"doc_id": "gutenberg_1342", "para_id": 5144, "text": "his determination, I do not suppose that it would ultimately have"}
{"doc_id": "gutenberg_1342", "para_id": 5145, "text": "prevented the marriage, had it not been seconded by the assurance, which"}
{"doc_id": "gutenberg_1342", "para_id": 5146, "text": "I hesitated not in giving, of your sister’s indifference. He had before"}
{"doc_id": "gutenberg_1342", "para_id": 5147, "text": "believed her to return his affection with sincere, if not with equal,"}
{"doc_id": "gutenberg_1342", "para_id": 5148, "text": "regard. But Bingley has great natural modesty, with a stronger"}
{"doc_id": "gutenberg_1342", "para_id": 5149, "text": "dependence on my judgment than on his own. To convince him, therefore,"}
{"doc_id": "gutenberg_1342", "para_id": 5150, "text": "that he had deceived himself was no very difficult point. To persuade"}
{"doc_id": "gutenberg_1342", "para_id": 5151, "text": "him against returning into Hertfordshire, when that conviction had been"}
{"doc_id": "gutenberg_1342", "para_id": 5152, "text": "given, was scarcely the work of a moment. I cannot blame myself for"}
{"doc_id": "gutenberg_1342", "para_id": 5153, "text": "having done thus much. There is but one part of my conduct, in the whole"}
{"doc_id": "gutenberg_1342", "para_id": 5154, "text": "affair, on which I do not reflect with satisfaction; it is that I"}
{"doc_id": "gutenberg_1342", "para_id": 5155, "text": "condescended to adopt the measures of art so far as to conceal from him"}
{"doc_id": "gutenberg_1342", "para_id": 5156, "text": "your sister’s being in town. I knew it myself, as it was known to Miss"}
{"doc_id": "gutenberg_1342", "para_id": 5157, "text": "Bingley; but her brother is even yet ignorant of it. That they might"}
{"doc_id": "gutenberg_1342", "para_id": 5158, "text": "have met without ill consequence is, perhaps, probable; but his regard"}
{"doc_id": "gutenberg_1342", "para_id": 5159, "text": "did not appear to me enough extinguished for him to see her without some"}
{"doc_id": "gutenberg_1342", "para_id": 5160, "text": "danger. Perhaps this concealment, this disguise, was beneath me. It is"}
{"doc_id": "gutenberg_1342", "para_id": 5161, "text": "done, however, and it was done for the best. On this subject I have"}
{"doc_id": "gutenberg_1342", "para_id": 5162, "text": "nothing more to say, no other apology to offer. If I have wounded your"}
{"doc_id": "gutenberg_1342", "para_id": 5163, "text": "sister’s feelings, it was unknowingly done; and though the motives which"}
{"doc_id": "gutenberg_1342", "para_id": 5164, "text": "governed me may to you very naturally appear insufficient, I have not"}
{"doc_id": "gutenberg_1342", "para_id": 5165, "text": "yet learnt to condemn them.--With respect to that other, more weighty"}
{"doc_id": "gutenberg_1342", "para_id": 5166, "text": "accusation, of having injured Mr. Wickham, I can only refute it by"}
{"doc_id": "gutenberg_1342", "para_id": 5167, "text": "laying before you the whole of his connection with my family. Of what he"}
{"doc_id": "gutenberg_1342", "para_id": 5168, "text": "has _particularly_ accused me I am ignorant; but of the truth of what I"}
{"doc_id": "gutenberg_1342", "para_id": 5169, "text": "shall relate I can summon more than one witness of undoubted veracity."}
{"doc_id": "gutenberg_1342", "para_id": 5170, "text": "Mr. Wickham is the son of a very respectable man, who had for many years"}
{"doc_id": "gutenberg_1342", "para_id": 5171, "text": "the management of all the Pemberley estates, and whose good conduct in"}
{"doc_id": "gutenberg_1342", "para_id": 5172, "text": "the discharge of his trust naturally inclined my father to be of service"}
{"doc_id": "gutenberg_1342", "para_id": 5173, "text": "to him; and on George Wickham, who was his godson, his kindness was"}
{"doc_id": "gutenberg_1342", "para_id": 5174, "text": "therefore liberally bestowed. My father supported him at school, and"}
{"doc_id": "gutenberg_1342", "para_id": 5175, "text": "afterwards at Cambridge; most important assistance, as his own father,"}
{"doc_id": "gutenberg_1342", "para_id": 5176, "text": "always poor from the extravagance of his wife, would have been unable to"}
{"doc_id": "gutenberg_1342", "para_id": 5177, "text": "give him a gentleman’s education. My father was not only fond of this"}
{"doc_id": "gutenberg_1342", "para_id": 5178, "text": "young man’s society, whose manners were always engaging, he had also the"}
{"doc_id": "gutenberg_1342", "para_id": 5179, "text": "highest opinion of him, and hoping the church would be his profession,"}
{"doc_id": "gutenberg_1342", "para_id": 5180, "text": "intended to provide for him in it. As for myself, it is many, many years"}
{"doc_id": "gutenberg_1342", "para_id": 5181, "text": "since I first began to think of him in a very different manner. The"}
{"doc_id": "gutenberg_1342", "para_id": 5182, "text": "vicious propensities, the want of principle, which he was careful to"}
{"doc_id": "gutenberg_1342", "para_id": 5183, "text": "guard from the knowledge of his best friend, could not escape the"}
{"doc_id": "gutenberg_1342", "para_id": 5184, "text": "observation of a young man of nearly the same age with himself, and who"}
{"doc_id": "gutenberg_1342", "para_id": 5185, "text": "had opportunities of seeing him in unguarded moments, which Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 5186, "text": "could not have. Here again I shall give you pain--to what degree you"}
{"doc_id": "gutenberg_1342", "para_id": 5187, "text": "only can tell. But whatever may be the sentiments which Mr. Wickham has"}
{"doc_id": "gutenberg_1342", "para_id": 5188, "text": "created, a suspicion of their nature shall not prevent me from unfolding"}
{"doc_id": "gutenberg_1342", "para_id": 5189, "text": "his real character. It adds even another motive. My excellent father"}
{"doc_id": "gutenberg_1342", "para_id": 5190, "text": "died about five years ago; and his attachment to Mr. Wickham was to the"}
{"doc_id": "gutenberg_1342", "para_id": 5191, "text": "last so steady, that in his will he particularly recommended it to me to"}
{"doc_id": "gutenberg_1342", "para_id": 5192, "text": "promote his advancement in the best manner that his profession might"}
{"doc_id": "gutenberg_1342", "para_id": 5193, "text": "allow, and if he took orders, desired that a valuable family living"}
{"doc_id": "gutenberg_1342", "para_id": 5194, "text": "might be his as soon as it became vacant. There was also a legacy of"}
{"doc_id": "gutenberg_1342", "para_id": 5195, "text": "one thousand pounds. His own father did not long survive mine; and"}
{"doc_id": "gutenberg_1342", "para_id": 5196, "text": "within half a year from these events Mr. Wickham wrote to inform me"}
{"doc_id": "gutenberg_1342", "para_id": 5197, "text": "that, having finally resolved against taking orders, he hoped I should"}
{"doc_id": "gutenberg_1342", "para_id": 5198, "text": "not think it unreasonable for him to expect some more immediate"}
{"doc_id": "gutenberg_1342", "para_id": 5199, "text": "pecuniary advantage, in lieu of the preferment, by which he could not be"}
{"doc_id": "gutenberg_1342", "para_id": 5200, "text": "benefited. He had some intention, he added, of studying the law, and I"}
{"doc_id": "gutenberg_1342", "para_id": 5201, "text": "must be aware that the interest of one thousand pounds would be a very"}
{"doc_id": "gutenberg_1342", "para_id": 5202, "text": "insufficient support therein. I rather wished than believed him to be"}
{"doc_id": "gutenberg_1342", "para_id": 5203, "text": "sincere; but, at any rate, was perfectly ready to accede to his"}
{"doc_id": "gutenberg_1342", "para_id": 5204, "text": "proposal. I knew that Mr. Wickham ought not to be a clergyman. The"}
{"doc_id": "gutenberg_1342", "para_id": 5205, "text": "business was therefore soon settled. He resigned all claim to assistance"}
{"doc_id": "gutenberg_1342", "para_id": 5206, "text": "in the church, were it possible that he could ever be in a situation to"}
{"doc_id": "gutenberg_1342", "para_id": 5207, "text": "receive it, and accepted in return three thousand pounds. All connection"}
{"doc_id": "gutenberg_1342", "para_id": 5208, "text": "between us seemed now dissolved. I thought too ill of him to invite him"}
{"doc_id": "gutenberg_1342", "para_id": 5209, "text": "to Pemberley, or admit his society in town. In town, I believe, he"}
{"doc_id": "gutenberg_1342", "para_id": 5210, "text": "chiefly lived, but his studying the law was a mere pretence; and being"}
{"doc_id": "gutenberg_1342", "para_id": 5211, "text": "now free from all restraint, his life was a life of idleness and"}
{"doc_id": "gutenberg_1342", "para_id": 5212, "text": "dissipation. For about three years I heard little of him; but on the"}
{"doc_id": "gutenberg_1342", "para_id": 5213, "text": "decease of the incumbent of the living which had been designed for him,"}
{"doc_id": "gutenberg_1342", "para_id": 5214, "text": "he applied to me again by letter for the presentation. His"}
{"doc_id": "gutenberg_1342", "para_id": 5215, "text": "circumstances, he assured me, and I had no difficulty in believing it,"}
{"doc_id": "gutenberg_1342", "para_id": 5216, "text": "were exceedingly bad. He had found the law a most unprofitable study,"}
{"doc_id": "gutenberg_1342", "para_id": 5217, "text": "and was now absolutely resolved on being ordained, if I would present"}
{"doc_id": "gutenberg_1342", "para_id": 5218, "text": "him to the living in question--of which he trusted there could be little"}
{"doc_id": "gutenberg_1342", "para_id": 5219, "text": "doubt, as he was well assured that I had no other person to provide for,"}
{"doc_id": "gutenberg_1342", "para_id": 5220, "text": "and I could not have forgotten my revered father’s intentions. You will"}
{"doc_id": "gutenberg_1342", "para_id": 5221, "text": "hardly blame me for refusing to comply with this entreaty, or for"}
{"doc_id": "gutenberg_1342", "para_id": 5222, "text": "resisting every repetition of it. His resentment was in proportion to"}
{"doc_id": "gutenberg_1342", "para_id": 5223, "text": "the distress of his circumstances--and he was doubtless as violent in"}
{"doc_id": "gutenberg_1342", "para_id": 5224, "text": "his abuse of me to others as in his reproaches to myself. After this"}
{"doc_id": "gutenberg_1342", "para_id": 5225, "text": "period, every appearance of acquaintance was dropped. How he lived, I"}
{"doc_id": "gutenberg_1342", "para_id": 5226, "text": "know not. But last summer he was again most painfully obtruded on my"}
{"doc_id": "gutenberg_1342", "para_id": 5227, "text": "notice. I must now mention a circumstance which I would wish to forget"}
{"doc_id": "gutenberg_1342", "para_id": 5228, "text": "myself, and which no obligation less than the present should induce me"}
{"doc_id": "gutenberg_1342", "para_id": 5229, "text": "to unfold to any human being. Having said thus much, I feel no doubt of"}
{"doc_id": "gutenberg_1342", "para_id": 5230, "text": "your secrecy. My sister, who is more than ten years my junior, was left"}
{"doc_id": "gutenberg_1342", "para_id": 5231, "text": "to the guardianship of my mother’s nephew, Colonel Fitzwilliam, and"}
{"doc_id": "gutenberg_1342", "para_id": 5232, "text": "myself. About a year ago, she was taken from school, and an"}
{"doc_id": "gutenberg_1342", "para_id": 5233, "text": "establishment formed for her in London; and last summer she went with"}
{"doc_id": "gutenberg_1342", "para_id": 5234, "text": "the lady who presided over it to Ramsgate; and thither also went Mr."}
{"doc_id": "gutenberg_1342", "para_id": 5235, "text": "Wickham, undoubtedly by design; for there proved to have been a prior"}
{"doc_id": "gutenberg_1342", "para_id": 5236, "text": "acquaintance between him and Mrs. Younge, in whose character we were"}
{"doc_id": "gutenberg_1342", "para_id": 5237, "text": "most unhappily deceived; and by her connivance and aid he so far"}
{"doc_id": "gutenberg_1342", "para_id": 5238, "text": "recommended himself to Georgiana, whose affectionate heart retained a"}
{"doc_id": "gutenberg_1342", "para_id": 5239, "text": "strong impression of his kindness to her as a child, that she was"}
{"doc_id": "gutenberg_1342", "para_id": 5240, "text": "persuaded to believe herself in love and to consent to an elopement. She"}
{"doc_id": "gutenberg_1342", "para_id": 5241, "text": "was then but fifteen, which must be her excuse; and after stating her"}
{"doc_id": "gutenberg_1342", "para_id": 5242, "text": "imprudence, I am happy to add, that I owed the knowledge of it to"}
{"doc_id": "gutenberg_1342", "para_id": 5243, "text": "herself. I joined them unexpectedly a day or two before the intended"}
{"doc_id": "gutenberg_1342", "para_id": 5244, "text": "elopement; and then Georgiana, unable to support the idea of grieving"}
{"doc_id": "gutenberg_1342", "para_id": 5245, "text": "and offending a brother whom she almost looked up to as a father,"}
{"doc_id": "gutenberg_1342", "para_id": 5246, "text": "acknowledged the whole to me. You may imagine what I felt and how I"}
{"doc_id": "gutenberg_1342", "para_id": 5247, "text": "acted. Regard for my sister’s credit and feelings prevented any public"}
{"doc_id": "gutenberg_1342", "para_id": 5248, "text": "exposure; but I wrote to Mr. Wickham, who left the place immediately,"}
{"doc_id": "gutenberg_1342", "para_id": 5249, "text": "and Mrs. Younge was of course removed from her charge. Mr. Wickham’s"}
{"doc_id": "gutenberg_1342", "para_id": 5250, "text": "chief object was unquestionably my sister’s fortune, which is thirty"}
{"doc_id": "gutenberg_1342", "para_id": 5251, "text": "thousand pounds; but I cannot help supposing that the hope of revenging"}
{"doc_id": "gutenberg_1342", "para_id": 5252, "text": "himself on me was a strong inducement. His revenge would have been"}
{"doc_id": "gutenberg_1342", "para_id": 5253, "text": "complete indeed. This, madam, is a faithful narrative of every event in"}
{"doc_id": "gutenberg_1342", "para_id": 5254, "text": "which we have been concerned together; and if you do not absolutely"}
{"doc_id": "gutenberg_1342", "para_id": 5255, "text": "reject it as false, you will, I hope, acquit me henceforth of cruelty"}
{"doc_id": "gutenberg_1342", "para_id": 5256, "text": "towards Mr. Wickham. I know not in what manner, under what form of"}
{"doc_id": "gutenberg_1342", "para_id": 5257, "text": "falsehood, he has imposed on you; but his success is not perhaps to be"}
{"doc_id": "gutenberg_1342", "para_id": 5258, "text": "wondered at, ignorant as you previously were of everything concerning"}
{"doc_id": "gutenberg_1342", "para_id": 5259, "text": "either. Detection could not be in your power, and suspicion certainly"}
{"doc_id": "gutenberg_1342", "para_id": 5260, "text": "not in your inclination. You may possibly wonder why all this was not"}
{"doc_id": "gutenberg_1342", "para_id": 5261, "text": "told you last night. But I was not then master enough of myself to know"}
{"doc_id": "gutenberg_1342", "para_id": 5262, "text": "what could or ought to be revealed. For the truth of everything here"}
{"doc_id": "gutenberg_1342", "para_id": 5263, "text": "related, I can appeal more particularly to the testimony of Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 5264, "text": "Fitzwilliam, who, from our near relationship and constant intimacy, and"}
{"doc_id": "gutenberg_1342", "para_id": 5265, "text": "still more as one of the executors of my father’s will, has been"}
{"doc_id": "gutenberg_1342", "para_id": 5266, "text": "unavoidably acquainted with every particular of these transactions. If"}
{"doc_id": "gutenberg_1342", "para_id": 5267, "text": "your abhorrence of _me_ should make _my_ assertions valueless, you"}
{"doc_id": "gutenberg_1342", "para_id": 5268, "text": "cannot be prevented by the same cause from confiding in my cousin; and"}
{"doc_id": "gutenberg_1342", "para_id": 5269, "text": "that there may be the possibility of consulting him, I shall endeavour"}
{"doc_id": "gutenberg_1342", "para_id": 5270, "text": "to find some opportunity of putting this letter in your hands in the"}
{"doc_id": "gutenberg_1342", "para_id": 5271, "text": "course of the morning. I will only add, God bless you."}
{"doc_id": "gutenberg_1342", "para_id": 5272, "text": "Elizabeth, when Mr. Darcy gave her the letter, did not expect it to"}
{"doc_id": "gutenberg_1342", "para_id": 5273, "text": "contain a renewal of his offers, she had formed no expectation at all of"}
{"doc_id": "gutenberg_1342", "para_id": 5274, "text": "its contents. But such as they were, it may be well supposed how eagerly"}
{"doc_id": "gutenberg_1342", "para_id": 5275, "text": "she went through them, and what a contrariety of emotion they excited."}
{"doc_id": "gutenberg_1342", "para_id": 5276, "text": "Her feelings as she read were scarcely to be defined. With amazement did"}
{"doc_id": "gutenberg_1342", "para_id": 5277, "text": "she first understand that he believed any apology to be in his power;"}
{"doc_id": "gutenberg_1342", "para_id": 5278, "text": "and steadfastly was she persuaded, that he could have no explanation to"}
{"doc_id": "gutenberg_1342", "para_id": 5279, "text": "give, which a just sense of shame would not conceal. With a strong"}
{"doc_id": "gutenberg_1342", "para_id": 5280, "text": "prejudice against everything he might say, she began his account of"}
{"doc_id": "gutenberg_1342", "para_id": 5281, "text": "what had happened at Netherfield. She read with an eagerness which"}
{"doc_id": "gutenberg_1342", "para_id": 5282, "text": "hardly left her power of comprehension; and from impatience of knowing"}
{"doc_id": "gutenberg_1342", "para_id": 5283, "text": "what the next sentence might bring, was incapable of attending to the"}
{"doc_id": "gutenberg_1342", "para_id": 5284, "text": "sense of the one before her eyes. His belief of her sister’s"}
{"doc_id": "gutenberg_1342", "para_id": 5285, "text": "insensibility she instantly resolved to be false; and his account of the"}
{"doc_id": "gutenberg_1342", "para_id": 5286, "text": "real, the worst objections to the match, made her too angry to have any"}
{"doc_id": "gutenberg_1342", "para_id": 5287, "text": "wish of doing him justice. He expressed no regret for what he had done"}
{"doc_id": "gutenberg_1342", "para_id": 5288, "text": "which satisfied her; his style was not penitent, but haughty. It was all"}
{"doc_id": "gutenberg_1342", "para_id": 5289, "text": "But when this subject was succeeded by his account of Mr. Wickham--when"}
{"doc_id": "gutenberg_1342", "para_id": 5290, "text": "she read, with somewhat clearer attention, a relation of events which,"}
{"doc_id": "gutenberg_1342", "para_id": 5291, "text": "if true, must overthrow every cherished opinion of his worth, and which"}
{"doc_id": "gutenberg_1342", "para_id": 5292, "text": "bore so alarming an affinity to his own history of himself--her feelings"}
{"doc_id": "gutenberg_1342", "para_id": 5293, "text": "were yet more acutely painful and more difficult of definition."}
{"doc_id": "gutenberg_1342", "para_id": 5294, "text": "Astonishment, apprehension, and even horror, oppressed her. She wished"}
{"doc_id": "gutenberg_1342", "para_id": 5295, "text": "to discredit it entirely, repeatedly exclaiming, “This must be false!"}
{"doc_id": "gutenberg_1342", "para_id": 5296, "text": "This cannot be! This must be the grossest falsehood!”--and when she had"}
{"doc_id": "gutenberg_1342", "para_id": 5297, "text": "gone through the whole letter, though scarcely knowing anything of the"}
{"doc_id": "gutenberg_1342", "para_id": 5298, "text": "last page or two, put it hastily away, protesting that she would not"}
{"doc_id": "gutenberg_1342", "para_id": 5299, "text": "In this perturbed state of mind, with thoughts that could rest on"}
{"doc_id": "gutenberg_1342", "para_id": 5300, "text": "nothing, she walked on; but it would not do: in half a minute the letter"}
{"doc_id": "gutenberg_1342", "para_id": 5301, "text": "was unfolded again; and collecting herself as well as she could, she"}
{"doc_id": "gutenberg_1342", "para_id": 5302, "text": "again began the mortifying perusal of all that related to Wickham, and"}
{"doc_id": "gutenberg_1342", "para_id": 5303, "text": "commanded herself so far as to examine the meaning of every sentence."}
{"doc_id": "gutenberg_1342", "para_id": 5304, "text": "The account of his connection with the Pemberley family was exactly"}
{"doc_id": "gutenberg_1342", "para_id": 5305, "text": "what he had related himself; and the kindness of the late Mr. Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 5306, "text": "though she had not before known its extent, agreed equally well with his"}
{"doc_id": "gutenberg_1342", "para_id": 5307, "text": "own words. So far each recital confirmed the other; but when she came to"}
{"doc_id": "gutenberg_1342", "para_id": 5308, "text": "the will, the difference was great. What Wickham had said of the living"}
{"doc_id": "gutenberg_1342", "para_id": 5309, "text": "was fresh in her memory; and as she recalled his very words, it was"}
{"doc_id": "gutenberg_1342", "para_id": 5310, "text": "impossible not to feel that there was gross duplicity on one side or the"}
{"doc_id": "gutenberg_1342", "para_id": 5311, "text": "other, and, for a few moments, she flattered herself that her wishes did"}
{"doc_id": "gutenberg_1342", "para_id": 5312, "text": "not err. But when she read and re-read, with the closest attention, the"}
{"doc_id": "gutenberg_1342", "para_id": 5313, "text": "particulars immediately following of Wickham’s resigning all pretensions"}
{"doc_id": "gutenberg_1342", "para_id": 5314, "text": "to the living, of his receiving in lieu so considerable a sum as three"}
{"doc_id": "gutenberg_1342", "para_id": 5315, "text": "thousand pounds, again was she forced to hesitate. She put down the"}
{"doc_id": "gutenberg_1342", "para_id": 5316, "text": "letter, weighed every circumstance with what she meant to be"}
{"doc_id": "gutenberg_1342", "para_id": 5317, "text": "impartiality--deliberated on the probability of each statement--but with"}
{"doc_id": "gutenberg_1342", "para_id": 5318, "text": "little success. On both sides it was only assertion. Again she read on."}
{"doc_id": "gutenberg_1342", "para_id": 5319, "text": "But every line proved more clearly that the affair, which she had"}
{"doc_id": "gutenberg_1342", "para_id": 5320, "text": "believed it impossible that any contrivance could so represent as to"}
{"doc_id": "gutenberg_1342", "para_id": 5321, "text": "render Mr. Darcy’s conduct in it less than infamous, was capable of a"}
{"doc_id": "gutenberg_1342", "para_id": 5322, "text": "turn which must make him entirely blameless throughout the whole."}
{"doc_id": "gutenberg_1342", "para_id": 5323, "text": "The extravagance and general profligacy which he scrupled not to lay to"}
{"doc_id": "gutenberg_1342", "para_id": 5324, "text": "Mr. Wickham’s charge exceedingly shocked her; the more so, as she could"}
{"doc_id": "gutenberg_1342", "para_id": 5325, "text": "bring no proof of its injustice. She had never heard of him before his"}
{"doc_id": "gutenberg_1342", "para_id": 5326, "text": "entrance into the ----shire militia, in which he had engaged at the"}
{"doc_id": "gutenberg_1342", "para_id": 5327, "text": "persuasion of the young man, who, on meeting him accidentally in town,"}
{"doc_id": "gutenberg_1342", "para_id": 5328, "text": "had there renewed a slight acquaintance. Of his former way of life,"}
{"doc_id": "gutenberg_1342", "para_id": 5329, "text": "nothing had been known in Hertfordshire but what he told"}
{"doc_id": "gutenberg_1342", "para_id": 5330, "text": "himself. As to his real character, had information been in her power,"}
{"doc_id": "gutenberg_1342", "para_id": 5331, "text": "she had never felt a wish of inquiring. His countenance, voice, and"}
{"doc_id": "gutenberg_1342", "para_id": 5332, "text": "manner, had established him at once in the possession of every virtue."}
{"doc_id": "gutenberg_1342", "para_id": 5333, "text": "She tried to recollect some instance of goodness, some distinguished"}
{"doc_id": "gutenberg_1342", "para_id": 5334, "text": "trait of integrity or benevolence, that might rescue him from the"}
{"doc_id": "gutenberg_1342", "para_id": 5335, "text": "attacks of Mr. Darcy; or at least, by the predominance of virtue, atone"}
{"doc_id": "gutenberg_1342", "para_id": 5336, "text": "for those casual errors, under which she would endeavour to class what"}
{"doc_id": "gutenberg_1342", "para_id": 5337, "text": "Mr. Darcy had described as the idleness and vice of many years’"}
{"doc_id": "gutenberg_1342", "para_id": 5338, "text": "continuance. But no such recollection befriended her. She could see him"}
{"doc_id": "gutenberg_1342", "para_id": 5339, "text": "instantly before her, in every charm of air and address, but she could"}
{"doc_id": "gutenberg_1342", "para_id": 5340, "text": "remember no more substantial good than the general approbation of the"}
{"doc_id": "gutenberg_1342", "para_id": 5341, "text": "neighbourhood, and the regard which his social powers had gained him in"}
{"doc_id": "gutenberg_1342", "para_id": 5342, "text": "the mess. After pausing on this point a considerable while, she once"}
{"doc_id": "gutenberg_1342", "para_id": 5343, "text": "more continued to read. But, alas! the story which followed, of his"}
{"doc_id": "gutenberg_1342", "para_id": 5344, "text": "designs on Miss Darcy, received some confirmation from what had passed"}
{"doc_id": "gutenberg_1342", "para_id": 5345, "text": "between Colonel Fitzwilliam and herself only the morning before; and at"}
{"doc_id": "gutenberg_1342", "para_id": 5346, "text": "last she was referred for the truth of every particular to Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 5347, "text": "Fitzwilliam himself--from whom she had previously received the"}
{"doc_id": "gutenberg_1342", "para_id": 5348, "text": "information of his near concern in all his cousin’s affairs and whose"}
{"doc_id": "gutenberg_1342", "para_id": 5349, "text": "character she had no reason to question. At one time she had almost"}
{"doc_id": "gutenberg_1342", "para_id": 5350, "text": "resolved on applying to him, but the idea was checked by the awkwardness"}
{"doc_id": "gutenberg_1342", "para_id": 5351, "text": "of the application, and at length wholly banished by the conviction that"}
{"doc_id": "gutenberg_1342", "para_id": 5352, "text": "Mr. Darcy would never have hazarded such a proposal, if he had not been"}
{"doc_id": "gutenberg_1342", "para_id": 5353, "text": "She perfectly remembered everything that had passed in conversation"}
{"doc_id": "gutenberg_1342", "para_id": 5354, "text": "between Wickham and herself in their first evening at Mr. Philips’s."}
{"doc_id": "gutenberg_1342", "para_id": 5355, "text": "Many of his expressions were still fresh in her memory. She was _now_"}
{"doc_id": "gutenberg_1342", "para_id": 5356, "text": "struck with the impropriety of such communications to a stranger, and"}
{"doc_id": "gutenberg_1342", "para_id": 5357, "text": "wondered it had escaped her before. She saw the indelicacy of putting"}
{"doc_id": "gutenberg_1342", "para_id": 5358, "text": "himself forward as he had done, and the inconsistency of his professions"}
{"doc_id": "gutenberg_1342", "para_id": 5359, "text": "with his conduct. She remembered that he had boasted of having no fear"}
{"doc_id": "gutenberg_1342", "para_id": 5360, "text": "of seeing Mr. Darcy--that Mr. Darcy might leave the country, but that"}
{"doc_id": "gutenberg_1342", "para_id": 5361, "text": "_he_ should stand his ground; yet he had avoided the Netherfield ball"}
{"doc_id": "gutenberg_1342", "para_id": 5362, "text": "the very next week. She remembered, also, that till the Netherfield"}
{"doc_id": "gutenberg_1342", "para_id": 5363, "text": "family had quitted the country, he had told his story to no one but"}
{"doc_id": "gutenberg_1342", "para_id": 5364, "text": "herself; but that after their removal, it had been everywhere discussed;"}
{"doc_id": "gutenberg_1342", "para_id": 5365, "text": "that he had then no reserves, no scruples in sinking Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 5366, "text": "character, though he had assured her that respect for the father would"}
{"doc_id": "gutenberg_1342", "para_id": 5367, "text": "How differently did everything now appear in which he was concerned! His"}
{"doc_id": "gutenberg_1342", "para_id": 5368, "text": "attentions to Miss King were now the consequence of views solely and"}
{"doc_id": "gutenberg_1342", "para_id": 5369, "text": "hatefully mercenary; and the mediocrity of her fortune proved no longer"}
{"doc_id": "gutenberg_1342", "para_id": 5370, "text": "the moderation of his wishes, but his eagerness to grasp at anything."}
{"doc_id": "gutenberg_1342", "para_id": 5371, "text": "His behaviour to herself could now have had no tolerable motive: he had"}
{"doc_id": "gutenberg_1342", "para_id": 5372, "text": "either been deceived with regard to her fortune, or had been gratifying"}
{"doc_id": "gutenberg_1342", "para_id": 5373, "text": "his vanity by encouraging the preference which she believed she had most"}
{"doc_id": "gutenberg_1342", "para_id": 5374, "text": "incautiously shown. Every lingering struggle in his favour grew fainter"}
{"doc_id": "gutenberg_1342", "para_id": 5375, "text": "and fainter; and in further justification of Mr. Darcy, she could not"}
{"doc_id": "gutenberg_1342", "para_id": 5376, "text": "but allow that Mr. Bingley, when questioned by Jane, had long ago"}
{"doc_id": "gutenberg_1342", "para_id": 5377, "text": "asserted his blamelessness in the affair;--that, proud and repulsive as"}
{"doc_id": "gutenberg_1342", "para_id": 5378, "text": "were his manners, she had never, in the whole course of their"}
{"doc_id": "gutenberg_1342", "para_id": 5379, "text": "acquaintance--an acquaintance which had latterly brought them much"}
{"doc_id": "gutenberg_1342", "para_id": 5380, "text": "together, and given her a sort of intimacy with his ways--seen anything"}
{"doc_id": "gutenberg_1342", "para_id": 5381, "text": "that betrayed him to be unprincipled or unjust--anything that spoke him"}
{"doc_id": "gutenberg_1342", "para_id": 5382, "text": "of irreligious or immoral habits;--that among his own connections he was"}
{"doc_id": "gutenberg_1342", "para_id": 5383, "text": "esteemed and valued;--that even Wickham had allowed him merit as a"}
{"doc_id": "gutenberg_1342", "para_id": 5384, "text": "brother, and that she had often heard him speak so affectionately of his"}
{"doc_id": "gutenberg_1342", "para_id": 5385, "text": "sister as to prove him capable of some amiable feeling;--that had his"}
{"doc_id": "gutenberg_1342", "para_id": 5386, "text": "actions been what Wickham represented them, so gross a violation of"}
{"doc_id": "gutenberg_1342", "para_id": 5387, "text": "everything right could hardly have been concealed from the world; and"}
{"doc_id": "gutenberg_1342", "para_id": 5388, "text": "that friendship between a person capable of it and such an amiable man"}
{"doc_id": "gutenberg_1342", "para_id": 5389, "text": "She grew absolutely ashamed of herself. Of neither Darcy nor Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 5390, "text": "could she think, without feeling that she had been blind, partial,"}
{"doc_id": "gutenberg_1342", "para_id": 5391, "text": "“How despicably have I acted!” she cried. “I, who have prided myself on"}
{"doc_id": "gutenberg_1342", "para_id": 5392, "text": "my discernment! I, who have valued myself on my abilities! who have"}
{"doc_id": "gutenberg_1342", "para_id": 5393, "text": "often disdained the generous candour of my sister, and gratified my"}
{"doc_id": "gutenberg_1342", "para_id": 5394, "text": "vanity in useless or blameless distrust. How humiliating is this"}
{"doc_id": "gutenberg_1342", "para_id": 5395, "text": "discovery! Yet, how just a humiliation! Had I been in love, I could not"}
{"doc_id": "gutenberg_1342", "para_id": 5396, "text": "have been more wretchedly blind. But vanity, not love, has been my"}
{"doc_id": "gutenberg_1342", "para_id": 5397, "text": "folly. Pleased with the preference of one, and offended by the neglect"}
{"doc_id": "gutenberg_1342", "para_id": 5398, "text": "of the other, on the very beginning of our acquaintance, I have courted"}
{"doc_id": "gutenberg_1342", "para_id": 5399, "text": "prepossession and ignorance, and driven reason away where either were"}
{"doc_id": "gutenberg_1342", "para_id": 5400, "text": "From herself to Jane, from Jane to Bingley, her thoughts were in a line"}
{"doc_id": "gutenberg_1342", "para_id": 5401, "text": "which soon brought to her recollection that Mr. Darcy’s explanation"}
{"doc_id": "gutenberg_1342", "para_id": 5402, "text": "_there_ had appeared very insufficient; and she read it again. Widely"}
{"doc_id": "gutenberg_1342", "para_id": 5403, "text": "different was the effect of a second perusal. How could she deny that"}
{"doc_id": "gutenberg_1342", "para_id": 5404, "text": "credit to his assertions, in one instance, which she had been obliged to"}
{"doc_id": "gutenberg_1342", "para_id": 5405, "text": "give in the other? He declared himself to have been totally unsuspicious"}
{"doc_id": "gutenberg_1342", "para_id": 5406, "text": "of her sister’s attachment; and she could not help remembering what"}
{"doc_id": "gutenberg_1342", "para_id": 5407, "text": "Charlotte’s opinion had always been. Neither could she deny the justice"}
{"doc_id": "gutenberg_1342", "para_id": 5408, "text": "of his description of Jane. She felt that Jane’s feelings, though"}
{"doc_id": "gutenberg_1342", "para_id": 5409, "text": "fervent, were little displayed, and that there was a constant"}
{"doc_id": "gutenberg_1342", "para_id": 5410, "text": "complacency in her air and manner, not often united with great"}
{"doc_id": "gutenberg_1342", "para_id": 5411, "text": "When she came to that part of the letter in which her family were"}
{"doc_id": "gutenberg_1342", "para_id": 5412, "text": "mentioned, in tones of such mortifying, yet merited, reproach, her sense"}
{"doc_id": "gutenberg_1342", "para_id": 5413, "text": "of shame was severe. The justice of the charge struck her too forcibly"}
{"doc_id": "gutenberg_1342", "para_id": 5414, "text": "for denial; and the circumstances to which he particularly alluded, as"}
{"doc_id": "gutenberg_1342", "para_id": 5415, "text": "having passed at the Netherfield ball, and as confirming all his first"}
{"doc_id": "gutenberg_1342", "para_id": 5416, "text": "disapprobation, could not have made a stronger impression on his mind"}
{"doc_id": "gutenberg_1342", "para_id": 5417, "text": "The compliment to herself and her sister was not unfelt. It soothed, but"}
{"doc_id": "gutenberg_1342", "para_id": 5418, "text": "it could not console her for the contempt which had been thus"}
{"doc_id": "gutenberg_1342", "para_id": 5419, "text": "self-attracted by the rest of her family; and as she considered that"}
{"doc_id": "gutenberg_1342", "para_id": 5420, "text": "Jane’s disappointment had, in fact, been the work of her nearest"}
{"doc_id": "gutenberg_1342", "para_id": 5421, "text": "relations, and reflected how materially the credit of both must be hurt"}
{"doc_id": "gutenberg_1342", "para_id": 5422, "text": "by such impropriety of conduct, she felt depressed beyond anything she"}
{"doc_id": "gutenberg_1342", "para_id": 5423, "text": "After wandering along the lane for two hours, giving way to every"}
{"doc_id": "gutenberg_1342", "para_id": 5424, "text": "variety of thought, reconsidering events, determining probabilities, and"}
{"doc_id": "gutenberg_1342", "para_id": 5425, "text": "reconciling herself, as well as she could, to a change so sudden and so"}
{"doc_id": "gutenberg_1342", "para_id": 5426, "text": "important, fatigue, and a recollection of her long absence, made her at"}
{"doc_id": "gutenberg_1342", "para_id": 5427, "text": "length return home; and she entered the house with the wish of appearing"}
{"doc_id": "gutenberg_1342", "para_id": 5428, "text": "cheerful as usual, and the resolution of repressing such reflections as"}
{"doc_id": "gutenberg_1342", "para_id": 5429, "text": "She was immediately told, that the two gentlemen from Rosings had each"}
{"doc_id": "gutenberg_1342", "para_id": 5430, "text": "called during her absence; Mr. Darcy, only for a few minutes, to take"}
{"doc_id": "gutenberg_1342", "para_id": 5431, "text": "leave, but that Colonel Fitzwilliam had been sitting with them at least"}
{"doc_id": "gutenberg_1342", "para_id": 5432, "text": "an hour, hoping for her return, and almost resolving to walk after her"}
{"doc_id": "gutenberg_1342", "para_id": 5433, "text": "till she could be found. Elizabeth could but just _affect_ concern in"}
{"doc_id": "gutenberg_1342", "para_id": 5434, "text": "missing him; she really rejoiced at it. Colonel Fitzwilliam was no"}
{"doc_id": "gutenberg_1342", "para_id": 5435, "text": "longer an object. She could think only of her letter."}
{"doc_id": "gutenberg_1342", "para_id": 5436, "text": "The two gentlemen left Rosings the next morning; and Mr. Collins having"}
{"doc_id": "gutenberg_1342", "para_id": 5437, "text": "been in waiting near the lodges, to make them his parting obeisance, was"}
{"doc_id": "gutenberg_1342", "para_id": 5438, "text": "able to bring home the pleasing intelligence of their appearing in very"}
{"doc_id": "gutenberg_1342", "para_id": 5439, "text": "good health, and in as tolerable spirits as could be expected, after the"}
{"doc_id": "gutenberg_1342", "para_id": 5440, "text": "melancholy scene so lately gone through at Rosings. To Rosings he then"}
{"doc_id": "gutenberg_1342", "para_id": 5441, "text": "hastened to console Lady Catherine and her daughter; and on his return"}
{"doc_id": "gutenberg_1342", "para_id": 5442, "text": "brought back, with great satisfaction, a message from her Ladyship,"}
{"doc_id": "gutenberg_1342", "para_id": 5443, "text": "importing that she felt herself so dull as to make her very desirous of"}
{"doc_id": "gutenberg_1342", "para_id": 5444, "text": "Elizabeth could not see Lady Catherine without recollecting that, had"}
{"doc_id": "gutenberg_1342", "para_id": 5445, "text": "she chosen it, she might by this time have been presented to her as her"}
{"doc_id": "gutenberg_1342", "para_id": 5446, "text": "future niece; nor could she think, without a smile, of what her"}
{"doc_id": "gutenberg_1342", "para_id": 5447, "text": "Ladyship’s indignation would have been. “What would she have said? how"}
{"doc_id": "gutenberg_1342", "para_id": 5448, "text": "would she have behaved?” were the questions with which she amused"}
{"doc_id": "gutenberg_1342", "para_id": 5449, "text": "Their first subject was the diminution of the Rosings’ party. “I assure"}
{"doc_id": "gutenberg_1342", "para_id": 5450, "text": "you, I feel it exceedingly,” said Lady Catherine; “I believe nobody"}
{"doc_id": "gutenberg_1342", "para_id": 5451, "text": "feels the loss of friends so much as I do. But I am particularly"}
{"doc_id": "gutenberg_1342", "para_id": 5452, "text": "attached to these young men; and know them to be so much attached to me!"}
{"doc_id": "gutenberg_1342", "para_id": 5453, "text": "They were excessively sorry to go! But so they always are. The dear"}
{"doc_id": "gutenberg_1342", "para_id": 5454, "text": "Colonel rallied his spirits tolerably till just at last; but Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 5455, "text": "seemed to feel it most acutely--more, I think, than last year. His"}
{"doc_id": "gutenberg_1342", "para_id": 5456, "text": "Mr. Collins had a compliment and an allusion to throw in here, which"}
{"doc_id": "gutenberg_1342", "para_id": 5457, "text": "Lady Catherine observed, after dinner, that Miss Bennet seemed out of"}
{"doc_id": "gutenberg_1342", "para_id": 5458, "text": "spirits; and immediately accounting for it herself, by supposing that"}
{"doc_id": "gutenberg_1342", "para_id": 5459, "text": "she did not like to go home again so soon, she added,--"}
{"doc_id": "gutenberg_1342", "para_id": 5460, "text": "“But if that is the case, you must write to your mother to beg that you"}
{"doc_id": "gutenberg_1342", "para_id": 5461, "text": "may stay a little longer. Mrs. Collins will be very glad of your"}
{"doc_id": "gutenberg_1342", "para_id": 5462, "text": "“I am much obliged to your Ladyship for your kind invitation,” replied"}
{"doc_id": "gutenberg_1342", "para_id": 5463, "text": "Elizabeth; “but it is not in my power to accept it. I must be in town"}
{"doc_id": "gutenberg_1342", "para_id": 5464, "text": "“Why, at that rate, you will have been here only six weeks. I expected"}
{"doc_id": "gutenberg_1342", "para_id": 5465, "text": "you to stay two months. I told Mrs. Collins so before you came. There"}
{"doc_id": "gutenberg_1342", "para_id": 5466, "text": "can be no occasion for your going so soon. Mrs. Bennet could certainly"}
{"doc_id": "gutenberg_1342", "para_id": 5467, "text": "“But my father cannot. He wrote last week to hurry my return.”"}
{"doc_id": "gutenberg_1342", "para_id": 5468, "text": "“Oh, your father, of course, may spare you, if your mother can."}
{"doc_id": "gutenberg_1342", "para_id": 5469, "text": "Daughters are never of so much consequence to a father. And if you will"}
{"doc_id": "gutenberg_1342", "para_id": 5470, "text": "stay another _month_ complete, it will be in my power to take one of you"}
{"doc_id": "gutenberg_1342", "para_id": 5471, "text": "as far as London, for I am going there early in June, for a week; and"}
{"doc_id": "gutenberg_1342", "para_id": 5472, "text": "as Dawson does not object to the barouche-box, there will be very good"}
{"doc_id": "gutenberg_1342", "para_id": 5473, "text": "room for one of you--and, indeed, if the weather should happen to be"}
{"doc_id": "gutenberg_1342", "para_id": 5474, "text": "cool, I should not object to taking you both, as you are neither of you"}
{"doc_id": "gutenberg_1342", "para_id": 5475, "text": "“You are all kindness, madam; but I believe we must abide by our"}
{"doc_id": "gutenberg_1342", "para_id": 5476, "text": "Lady Catherine seemed resigned. “Mrs. Collins, you must send a servant"}
{"doc_id": "gutenberg_1342", "para_id": 5477, "text": "with them. You know I always speak my mind, and I cannot bear the idea"}
{"doc_id": "gutenberg_1342", "para_id": 5478, "text": "of two young women travelling post by themselves. It is highly improper."}
{"doc_id": "gutenberg_1342", "para_id": 5479, "text": "You must contrive to send somebody. I have the greatest dislike in the"}
{"doc_id": "gutenberg_1342", "para_id": 5480, "text": "world to that sort of thing. Young women should always be properly"}
{"doc_id": "gutenberg_1342", "para_id": 5481, "text": "guarded and attended, according to their situation in life. When my"}
{"doc_id": "gutenberg_1342", "para_id": 5482, "text": "niece Georgiana went to Ramsgate last summer, I made a point of her"}
{"doc_id": "gutenberg_1342", "para_id": 5483, "text": "having two men-servants go with her. Miss Darcy, the daughter of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 5484, "text": "Darcy of Pemberley, and Lady Anne, could not have appeared with"}
{"doc_id": "gutenberg_1342", "para_id": 5485, "text": "propriety in a different manner. I am excessively attentive to all those"}
{"doc_id": "gutenberg_1342", "para_id": 5486, "text": "things. You must send John with the young ladies, Mrs. Collins. I am"}
{"doc_id": "gutenberg_1342", "para_id": 5487, "text": "glad it occurred to me to mention it; for it would really be"}
{"doc_id": "gutenberg_1342", "para_id": 5488, "text": "“Oh! Your uncle! He keeps a man-servant, does he? I am very glad you"}
{"doc_id": "gutenberg_1342", "para_id": 5489, "text": "have somebody who thinks of those things. Where shall you change horses?"}
{"doc_id": "gutenberg_1342", "para_id": 5490, "text": "Oh, Bromley, of course. If you mention my name at the Bell, you will be"}
{"doc_id": "gutenberg_1342", "para_id": 5491, "text": "Lady Catherine had many other questions to ask respecting their journey;"}
{"doc_id": "gutenberg_1342", "para_id": 5492, "text": "and as she did not answer them all herself attention was"}
{"doc_id": "gutenberg_1342", "para_id": 5493, "text": "necessary--which Elizabeth believed to be lucky for her; or, with a"}
{"doc_id": "gutenberg_1342", "para_id": 5494, "text": "mind so occupied, she might have forgotten where she was. Reflection"}
{"doc_id": "gutenberg_1342", "para_id": 5495, "text": "must be reserved for solitary hours: whenever she was alone, she gave"}
{"doc_id": "gutenberg_1342", "para_id": 5496, "text": "way to it as the greatest relief; and not a day went by without a"}
{"doc_id": "gutenberg_1342", "para_id": 5497, "text": "solitary walk, in which she might indulge in all the delight of"}
{"doc_id": "gutenberg_1342", "para_id": 5498, "text": "Mr. Darcy’s letter she was in a fair way of soon knowing by heart. She"}
{"doc_id": "gutenberg_1342", "para_id": 5499, "text": "studied every sentence; and her feelings towards its writer were at"}
{"doc_id": "gutenberg_1342", "para_id": 5500, "text": "times widely different. When she remembered the style of his address,"}
{"doc_id": "gutenberg_1342", "para_id": 5501, "text": "she was still full of indignation: but when she considered how unjustly"}
{"doc_id": "gutenberg_1342", "para_id": 5502, "text": "she had condemned and upbraided him, her anger was turned against"}
{"doc_id": "gutenberg_1342", "para_id": 5503, "text": "herself; and his disappointed feelings became the object of compassion."}
{"doc_id": "gutenberg_1342", "para_id": 5504, "text": "His attachment excited gratitude, his general character respect: but she"}
{"doc_id": "gutenberg_1342", "para_id": 5505, "text": "could not approve him; nor could she for a moment repent her refusal, or"}
{"doc_id": "gutenberg_1342", "para_id": 5506, "text": "feel the slightest inclination ever to see him again. In her own past"}
{"doc_id": "gutenberg_1342", "para_id": 5507, "text": "behaviour, there was a constant source of vexation and regret: and in"}
{"doc_id": "gutenberg_1342", "para_id": 5508, "text": "the unhappy defects of her family, a subject of yet heavier chagrin."}
{"doc_id": "gutenberg_1342", "para_id": 5509, "text": "They were hopeless of remedy. Her father, contented with laughing at"}
{"doc_id": "gutenberg_1342", "para_id": 5510, "text": "them, would never exert himself to restrain the wild giddiness of his"}
{"doc_id": "gutenberg_1342", "para_id": 5511, "text": "youngest daughters; and her mother, with manners so far from right"}
{"doc_id": "gutenberg_1342", "para_id": 5512, "text": "herself, was entirely insensible of the evil. Elizabeth had frequently"}
{"doc_id": "gutenberg_1342", "para_id": 5513, "text": "united with Jane in an endeavour to check the imprudence of Catherine"}
{"doc_id": "gutenberg_1342", "para_id": 5514, "text": "and Lydia; but while they were supported by their mother’s indulgence,"}
{"doc_id": "gutenberg_1342", "para_id": 5515, "text": "what chance could there be of improvement? Catherine, weak-spirited,"}
{"doc_id": "gutenberg_1342", "para_id": 5516, "text": "irritable, and completely under Lydia’s guidance, had been always"}
{"doc_id": "gutenberg_1342", "para_id": 5517, "text": "affronted by their advice; and Lydia, self-willed and careless, would"}
{"doc_id": "gutenberg_1342", "para_id": 5518, "text": "scarcely give them a hearing. They were ignorant, idle, and vain. While"}
{"doc_id": "gutenberg_1342", "para_id": 5519, "text": "there was an officer in Meryton, they would flirt with him; and while"}
{"doc_id": "gutenberg_1342", "para_id": 5520, "text": "Meryton was within a walk of Longbourn, they would be going there for"}
{"doc_id": "gutenberg_1342", "para_id": 5521, "text": "Anxiety on Jane’s behalf was another prevailing concern; and Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 5522, "text": "explanation, by restoring Bingley to all her former good opinion,"}
{"doc_id": "gutenberg_1342", "para_id": 5523, "text": "heightened the sense of what Jane had lost. His affection was proved to"}
{"doc_id": "gutenberg_1342", "para_id": 5524, "text": "have been sincere, and his conduct cleared of all blame, unless any"}
{"doc_id": "gutenberg_1342", "para_id": 5525, "text": "could attach to the implicitness of his confidence in his friend. How"}
{"doc_id": "gutenberg_1342", "para_id": 5526, "text": "grievous then was the thought that, of a situation so desirable in every"}
{"doc_id": "gutenberg_1342", "para_id": 5527, "text": "respect, so replete with advantage, so promising for happiness, Jane had"}
{"doc_id": "gutenberg_1342", "para_id": 5528, "text": "been deprived, by the folly and indecorum of her own family!"}
{"doc_id": "gutenberg_1342", "para_id": 5529, "text": "When to these recollections was added the development of Wickham’s"}
{"doc_id": "gutenberg_1342", "para_id": 5530, "text": "character, it may be easily believed that the happy spirits which had"}
{"doc_id": "gutenberg_1342", "para_id": 5531, "text": "seldom been depressed before were now so much affected as to make it"}
{"doc_id": "gutenberg_1342", "para_id": 5532, "text": "almost impossible for her to appear tolerably cheerful."}
{"doc_id": "gutenberg_1342", "para_id": 5533, "text": "Their engagements at Rosings were as frequent during the last week of"}
{"doc_id": "gutenberg_1342", "para_id": 5534, "text": "her stay as they had been at first. The very last evening was spent"}
{"doc_id": "gutenberg_1342", "para_id": 5535, "text": "there; and her Ladyship again inquired minutely into the particulars of"}
{"doc_id": "gutenberg_1342", "para_id": 5536, "text": "their journey, gave them directions as to the best method of packing,"}
{"doc_id": "gutenberg_1342", "para_id": 5537, "text": "and was so urgent on the necessity of placing gowns in the only right"}
{"doc_id": "gutenberg_1342", "para_id": 5538, "text": "way, that Maria thought herself obliged, on her return, to undo all the"}
{"doc_id": "gutenberg_1342", "para_id": 5539, "text": "When they parted, Lady Catherine, with great condescension, wished them"}
{"doc_id": "gutenberg_1342", "para_id": 5540, "text": "a good journey, and invited them to come to Hunsford again next year;"}
{"doc_id": "gutenberg_1342", "para_id": 5541, "text": "and Miss de Bourgh exerted herself so far as to courtesy and hold out"}
{"doc_id": "gutenberg_1342", "para_id": 5542, "text": "On Saturday morning Elizabeth and Mr. Collins met for breakfast a few"}
{"doc_id": "gutenberg_1342", "para_id": 5543, "text": "minutes before the others appeared; and he took the opportunity of"}
{"doc_id": "gutenberg_1342", "para_id": 5544, "text": "paying the parting civilities which he deemed indispensably necessary."}
{"doc_id": "gutenberg_1342", "para_id": 5545, "text": "“I know not, Miss Elizabeth,” said he, “whether Mrs. Collins has yet"}
{"doc_id": "gutenberg_1342", "para_id": 5546, "text": "expressed her sense of your kindness in coming to us; but I am very"}
{"doc_id": "gutenberg_1342", "para_id": 5547, "text": "certain you will not leave the house without receiving her thanks for"}
{"doc_id": "gutenberg_1342", "para_id": 5548, "text": "it. The favour of your company has been much felt, I assure you. We know"}
{"doc_id": "gutenberg_1342", "para_id": 5549, "text": "how little there is to tempt anyone to our humble abode. Our plain"}
{"doc_id": "gutenberg_1342", "para_id": 5550, "text": "manner of living, our small rooms, and few domestics, and the little we"}
{"doc_id": "gutenberg_1342", "para_id": 5551, "text": "see of the world, must make Hunsford extremely dull to a young lady like"}
{"doc_id": "gutenberg_1342", "para_id": 5552, "text": "yourself; but I hope you will believe us grateful for the condescension,"}
{"doc_id": "gutenberg_1342", "para_id": 5553, "text": "and that we have done everything in our power to prevent you spending"}
{"doc_id": "gutenberg_1342", "para_id": 5554, "text": "Elizabeth was eager with her thanks and assurances of happiness. She had"}
{"doc_id": "gutenberg_1342", "para_id": 5555, "text": "spent six weeks with great enjoyment; and the pleasure of being with"}
{"doc_id": "gutenberg_1342", "para_id": 5556, "text": "Charlotte, and the kind attention she had received, must make _her_ feel"}
{"doc_id": "gutenberg_1342", "para_id": 5557, "text": "the obliged. Mr. Collins was gratified; and with a more smiling"}
{"doc_id": "gutenberg_1342", "para_id": 5558, "text": "“It gives me the greatest pleasure to hear that you have passed your"}
{"doc_id": "gutenberg_1342", "para_id": 5559, "text": "time not disagreeably. We have certainly done our best; and most"}
{"doc_id": "gutenberg_1342", "para_id": 5560, "text": "fortunately having it in our power to introduce you to very superior"}
{"doc_id": "gutenberg_1342", "para_id": 5561, "text": "society, and from our connection with Rosings, the frequent means of"}
{"doc_id": "gutenberg_1342", "para_id": 5562, "text": "varying the humble home scene, I think we may flatter ourselves that"}
{"doc_id": "gutenberg_1342", "para_id": 5563, "text": "your Hunsford visit cannot have been entirely irksome. Our situation"}
{"doc_id": "gutenberg_1342", "para_id": 5564, "text": "with regard to Lady Catherine’s family is, indeed, the sort of"}
{"doc_id": "gutenberg_1342", "para_id": 5565, "text": "extraordinary advantage and blessing which few can boast. You see on"}
{"doc_id": "gutenberg_1342", "para_id": 5566, "text": "what a footing we are. You see how continually we are engaged there. In"}
{"doc_id": "gutenberg_1342", "para_id": 5567, "text": "truth, I must acknowledge, that, with all the disadvantages of this"}
{"doc_id": "gutenberg_1342", "para_id": 5568, "text": "humble parsonage, I should not think anyone abiding in it an object of"}
{"doc_id": "gutenberg_1342", "para_id": 5569, "text": "compassion, while they are sharers of our intimacy at Rosings.”"}
{"doc_id": "gutenberg_1342", "para_id": 5570, "text": "Words were insufficient for the elevation of his feelings; and he was"}
{"doc_id": "gutenberg_1342", "para_id": 5571, "text": "obliged to walk about the room, while Elizabeth tried to unite civility"}
{"doc_id": "gutenberg_1342", "para_id": 5572, "text": "“You may, in fact, carry a very favourable report of us into"}
{"doc_id": "gutenberg_1342", "para_id": 5573, "text": "Hertfordshire, my dear cousin. I flatter myself, at least, that you will"}
{"doc_id": "gutenberg_1342", "para_id": 5574, "text": "be able to do so. Lady Catherine’s great attentions to Mrs. Collins you"}
{"doc_id": "gutenberg_1342", "para_id": 5575, "text": "have been a daily witness of; and altogether I trust it does not appear"}
{"doc_id": "gutenberg_1342", "para_id": 5576, "text": "that your friend has drawn an unfortunate--but on this point it will be"}
{"doc_id": "gutenberg_1342", "para_id": 5577, "text": "as well to be silent. Only let me assure you, my dear Miss Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 5578, "text": "that I can from my heart most cordially wish you equal felicity in"}
{"doc_id": "gutenberg_1342", "para_id": 5579, "text": "marriage. My dear Charlotte and I have but one mind and one way of"}
{"doc_id": "gutenberg_1342", "para_id": 5580, "text": "thinking. There is in everything a most remarkable resemblance of"}
{"doc_id": "gutenberg_1342", "para_id": 5581, "text": "character and ideas between us. We seem to have been designed for each"}
{"doc_id": "gutenberg_1342", "para_id": 5582, "text": "Elizabeth could safely say that it was a great happiness where that was"}
{"doc_id": "gutenberg_1342", "para_id": 5583, "text": "the case, and with equal sincerity could add, that she firmly believed"}
{"doc_id": "gutenberg_1342", "para_id": 5584, "text": "and rejoiced in his domestic comforts. She was not sorry, however, to"}
{"doc_id": "gutenberg_1342", "para_id": 5585, "text": "have the recital of them interrupted by the entrance of the lady from"}
{"doc_id": "gutenberg_1342", "para_id": 5586, "text": "whom they sprang. Poor Charlotte! it was melancholy to leave her to such"}
{"doc_id": "gutenberg_1342", "para_id": 5587, "text": "society! But she had chosen it with her eyes open; and though evidently"}
{"doc_id": "gutenberg_1342", "para_id": 5588, "text": "regretting that her visitors were to go, she did not seem to ask for"}
{"doc_id": "gutenberg_1342", "para_id": 5589, "text": "compassion. Her home and her housekeeping, her parish and her poultry,"}
{"doc_id": "gutenberg_1342", "para_id": 5590, "text": "and all their dependent concerns, had not yet lost their charms."}
{"doc_id": "gutenberg_1342", "para_id": 5591, "text": "At length the chaise arrived, the trunks were fastened on, the parcels"}
{"doc_id": "gutenberg_1342", "para_id": 5592, "text": "placed within, and it was pronounced to be ready. After an affectionate"}
{"doc_id": "gutenberg_1342", "para_id": 5593, "text": "parting between the friends, Elizabeth was attended to the carriage by"}
{"doc_id": "gutenberg_1342", "para_id": 5594, "text": "Mr. Collins; and as they walked down the garden, he was commissioning"}
{"doc_id": "gutenberg_1342", "para_id": 5595, "text": "her with his best respects to all her family, not forgetting his thanks"}
{"doc_id": "gutenberg_1342", "para_id": 5596, "text": "for the kindness he had received at Longbourn in the winter, and his"}
{"doc_id": "gutenberg_1342", "para_id": 5597, "text": "compliments to Mr. and Mrs. Gardiner, though unknown. He then handed"}
{"doc_id": "gutenberg_1342", "para_id": 5598, "text": "her in, Maria followed, and the door was on the point of being closed,"}
{"doc_id": "gutenberg_1342", "para_id": 5599, "text": "when he suddenly reminded them, with some consternation, that they had"}
{"doc_id": "gutenberg_1342", "para_id": 5600, "text": "hitherto forgotten to leave any message for the ladies of Rosings."}
{"doc_id": "gutenberg_1342", "para_id": 5601, "text": "“But,” he added, “you will of course wish to have your humble respects"}
{"doc_id": "gutenberg_1342", "para_id": 5602, "text": "delivered to them, with your grateful thanks for their kindness to you"}
{"doc_id": "gutenberg_1342", "para_id": 5603, "text": "Elizabeth made no objection: the door was then allowed to be shut, and"}
{"doc_id": "gutenberg_1342", "para_id": 5604, "text": "“Good gracious!” cried Maria, after a few minutes’ silence, “it seems"}
{"doc_id": "gutenberg_1342", "para_id": 5605, "text": "but a day or two since we first came! and yet how many things have"}
{"doc_id": "gutenberg_1342", "para_id": 5606, "text": "“A great many indeed,” said her companion, with a sigh."}
{"doc_id": "gutenberg_1342", "para_id": 5607, "text": "“We have dined nine times at Rosings, besides drinking tea there twice!"}
{"doc_id": "gutenberg_1342", "para_id": 5608, "text": "Elizabeth privately added, “And how much I shall have to conceal!”"}
{"doc_id": "gutenberg_1342", "para_id": 5609, "text": "Their journey was performed without much conversation, or any alarm; and"}
{"doc_id": "gutenberg_1342", "para_id": 5610, "text": "within four hours of their leaving Hunsford they reached Mr. Gardiner’s"}
{"doc_id": "gutenberg_1342", "para_id": 5611, "text": "Jane looked well, and Elizabeth had little opportunity of studying her"}
{"doc_id": "gutenberg_1342", "para_id": 5612, "text": "spirits, amidst the various engagements which the kindness of her aunt"}
{"doc_id": "gutenberg_1342", "para_id": 5613, "text": "had reserved for them. But Jane was to go home with her, and at"}
{"doc_id": "gutenberg_1342", "para_id": 5614, "text": "Longbourn there would be leisure enough for observation."}
{"doc_id": "gutenberg_1342", "para_id": 5615, "text": "It was not without an effort, meanwhile, that she could wait even for"}
{"doc_id": "gutenberg_1342", "para_id": 5616, "text": "Longbourn, before she told her sister of Mr. Darcy’s proposals. To know"}
{"doc_id": "gutenberg_1342", "para_id": 5617, "text": "that she had the power of revealing what would so exceedingly astonish"}
{"doc_id": "gutenberg_1342", "para_id": 5618, "text": "Jane, and must, at the same time, so highly gratify whatever of her own"}
{"doc_id": "gutenberg_1342", "para_id": 5619, "text": "vanity she had not yet been able to reason away, was such a temptation"}
{"doc_id": "gutenberg_1342", "para_id": 5620, "text": "to openness as nothing could have conquered, but the state of indecision"}
{"doc_id": "gutenberg_1342", "para_id": 5621, "text": "in which she remained as to the extent of what she should communicate,"}
{"doc_id": "gutenberg_1342", "para_id": 5622, "text": "and her fear, if she once entered on the subject, of being hurried into"}
{"doc_id": "gutenberg_1342", "para_id": 5623, "text": "repeating something of Bingley, which might only grieve her sister"}
{"doc_id": "gutenberg_1342", "para_id": 5624, "text": "It was the second week in May, in which the three young ladies set out"}
{"doc_id": "gutenberg_1342", "para_id": 5625, "text": "together from Gracechurch Street for the town of ----, in Hertfordshire;"}
{"doc_id": "gutenberg_1342", "para_id": 5626, "text": "and, as they drew near the appointed inn where Mr. Bennet’s carriage was"}
{"doc_id": "gutenberg_1342", "para_id": 5627, "text": "to meet them, they quickly perceived, in token of the coachman’s"}
{"doc_id": "gutenberg_1342", "para_id": 5628, "text": "punctuality, both Kitty and Lydia looking out of a dining-room upstairs."}
{"doc_id": "gutenberg_1342", "para_id": 5629, "text": "These two girls had been above an hour in the place, happily employed"}
{"doc_id": "gutenberg_1342", "para_id": 5630, "text": "in visiting an opposite milliner, watching the sentinel on guard, and"}
{"doc_id": "gutenberg_1342", "para_id": 5631, "text": "After welcoming their sisters, they triumphantly displayed a table set"}
{"doc_id": "gutenberg_1342", "para_id": 5632, "text": "out with such cold meat as an inn larder usually affords, exclaiming,"}
{"doc_id": "gutenberg_1342", "para_id": 5633, "text": "“Is not this nice? is not this an agreeable surprise?”"}
{"doc_id": "gutenberg_1342", "para_id": 5634, "text": "“And we mean to treat you all,” added Lydia; “but you must lend us the"}
{"doc_id": "gutenberg_1342", "para_id": 5635, "text": "money, for we have just spent ours at the shop out there.” Then showing"}
{"doc_id": "gutenberg_1342", "para_id": 5636, "text": "her purchases,--“Look here, I have bought this bonnet. I do not think it"}
{"doc_id": "gutenberg_1342", "para_id": 5637, "text": "is very pretty; but I thought I might as well buy it as not. I shall"}
{"doc_id": "gutenberg_1342", "para_id": 5638, "text": "pull it to pieces as soon as I get home, and see if I can make it up any"}
{"doc_id": "gutenberg_1342", "para_id": 5639, "text": "And when her sisters abused it as ugly, she added, with perfect"}
{"doc_id": "gutenberg_1342", "para_id": 5640, "text": "unconcern, “Oh, but there were two or three much uglier in the shop; and"}
{"doc_id": "gutenberg_1342", "para_id": 5641, "text": "when I have bought some prettier-coloured satin to trim it with fresh, I"}
{"doc_id": "gutenberg_1342", "para_id": 5642, "text": "think it will be very tolerable. Besides, it will not much signify what"}
{"doc_id": "gutenberg_1342", "para_id": 5643, "text": "one wears this summer, after the ----shire have left Meryton, and they"}
{"doc_id": "gutenberg_1342", "para_id": 5644, "text": "“Are they, indeed?” cried Elizabeth, with the greatest satisfaction."}
{"doc_id": "gutenberg_1342", "para_id": 5645, "text": "“They are going to be encamped near Brighton; and I do so want papa to"}
{"doc_id": "gutenberg_1342", "para_id": 5646, "text": "take us all there for the summer! It would be such a delicious scheme,"}
{"doc_id": "gutenberg_1342", "para_id": 5647, "text": "and I dare say would hardly cost anything at all. Mamma would like to"}
{"doc_id": "gutenberg_1342", "para_id": 5648, "text": "go, too, of all things! Only think what a miserable summer else we shall"}
{"doc_id": "gutenberg_1342", "para_id": 5649, "text": "“Yes,” thought Elizabeth; “_that_ would be a delightful scheme, indeed,"}
{"doc_id": "gutenberg_1342", "para_id": 5650, "text": "and completely do for us at once. Good Heaven! Brighton and a whole"}
{"doc_id": "gutenberg_1342", "para_id": 5651, "text": "campful of soldiers, to us, who have been overset already by one poor"}
{"doc_id": "gutenberg_1342", "para_id": 5652, "text": "regiment of militia, and the monthly balls of Meryton!”"}
{"doc_id": "gutenberg_1342", "para_id": 5653, "text": "“Now I have got some news for you,” said Lydia, as they sat down to"}
{"doc_id": "gutenberg_1342", "para_id": 5654, "text": "table. “What do you think? It is excellent news, capital news, and about"}
{"doc_id": "gutenberg_1342", "para_id": 5655, "text": "Jane and Elizabeth looked at each other, and the waiter was told that he"}
{"doc_id": "gutenberg_1342", "para_id": 5656, "text": "“Ay, that is just like your formality and discretion. You thought the"}
{"doc_id": "gutenberg_1342", "para_id": 5657, "text": "waiter must not hear, as if he cared! I dare say he often hears worse"}
{"doc_id": "gutenberg_1342", "para_id": 5658, "text": "things said than I am going to say. But he is an ugly fellow! I am glad"}
{"doc_id": "gutenberg_1342", "para_id": 5659, "text": "he is gone. I never saw such a long chin in my life. Well, but now for"}
{"doc_id": "gutenberg_1342", "para_id": 5660, "text": "my news: it is about dear Wickham; too good for the waiter, is not it?"}
{"doc_id": "gutenberg_1342", "para_id": 5661, "text": "There is no danger of Wickham’s marrying Mary King--there’s for you! She"}
{"doc_id": "gutenberg_1342", "para_id": 5662, "text": "is gone down to her uncle at Liverpool; gone to stay. Wickham is safe.”"}
{"doc_id": "gutenberg_1342", "para_id": 5663, "text": "“And Mary King is safe!” added Elizabeth; “safe from a connection"}
{"doc_id": "gutenberg_1342", "para_id": 5664, "text": "“She is a great fool for going away, if she liked him.”"}
{"doc_id": "gutenberg_1342", "para_id": 5665, "text": "“But I hope there is no strong attachment on either side,” said Jane."}
{"doc_id": "gutenberg_1342", "para_id": 5666, "text": "“I am sure there is not on _his_. I will answer for it, he never cared"}
{"doc_id": "gutenberg_1342", "para_id": 5667, "text": "three straws about her. Who _could_ about such a nasty little freckled"}
{"doc_id": "gutenberg_1342", "para_id": 5668, "text": "Elizabeth was shocked to think that, however incapable of such"}
{"doc_id": "gutenberg_1342", "para_id": 5669, "text": "coarseness of _expression_ herself, the coarseness of the _sentiment_"}
{"doc_id": "gutenberg_1342", "para_id": 5670, "text": "was little other than her own breast had formerly harboured and fancied"}
{"doc_id": "gutenberg_1342", "para_id": 5671, "text": "As soon as all had ate, and the elder ones paid, the carriage was"}
{"doc_id": "gutenberg_1342", "para_id": 5672, "text": "ordered; and, after some contrivance, the whole party, with all their"}
{"doc_id": "gutenberg_1342", "para_id": 5673, "text": "boxes, workbags, and parcels, and the unwelcome addition of Kitty’s and"}
{"doc_id": "gutenberg_1342", "para_id": 5674, "text": "“How nicely we are crammed in!” cried Lydia. “I am glad I brought my"}
{"doc_id": "gutenberg_1342", "para_id": 5675, "text": "bonnet, if it is only for the fun of having another band-box! Well, now"}
{"doc_id": "gutenberg_1342", "para_id": 5676, "text": "let us be quite comfortable and snug, and talk and laugh all the way"}
{"doc_id": "gutenberg_1342", "para_id": 5677, "text": "home. And in the first place, let us hear what has happened to you all"}
{"doc_id": "gutenberg_1342", "para_id": 5678, "text": "since you went away. Have you seen any pleasant men? Have you had any"}
{"doc_id": "gutenberg_1342", "para_id": 5679, "text": "flirting? I was in great hopes that one of you would have got a husband"}
{"doc_id": "gutenberg_1342", "para_id": 5680, "text": "before you came back. Jane will be quite an old maid soon, I declare."}
{"doc_id": "gutenberg_1342", "para_id": 5681, "text": "She is almost three-and-twenty! Lord! how ashamed I should be of not"}
{"doc_id": "gutenberg_1342", "para_id": 5682, "text": "being married before three-and-twenty! My aunt Philips wants you so to"}
{"doc_id": "gutenberg_1342", "para_id": 5683, "text": "get husbands you can’t think. She says Lizzy had better have taken Mr."}
{"doc_id": "gutenberg_1342", "para_id": 5684, "text": "Collins; but _I_ do not think there would have been any fun in it. Lord!"}
{"doc_id": "gutenberg_1342", "para_id": 5685, "text": "how I should like to be married before any of you! and then I would"}
{"doc_id": "gutenberg_1342", "para_id": 5686, "text": "_chaperon_ you about to all the balls. Dear me! we had such a good piece"}
{"doc_id": "gutenberg_1342", "para_id": 5687, "text": "of fun the other day at Colonel Forster’s! Kitty and me were to spend"}
{"doc_id": "gutenberg_1342", "para_id": 5688, "text": "the day there, and Mrs. Forster promised to have a little dance in the"}
{"doc_id": "gutenberg_1342", "para_id": 5689, "text": "evening; (by-the-bye, Mrs. Forster and me are _such_ friends!) and so"}
{"doc_id": "gutenberg_1342", "para_id": 5690, "text": "she asked the two Harringtons to come: but Harriet was ill, and so Pen"}
{"doc_id": "gutenberg_1342", "para_id": 5691, "text": "was forced to come by herself; and then, what do you think we did? We"}
{"doc_id": "gutenberg_1342", "para_id": 5692, "text": "dressed up Chamberlayne in woman’s clothes, on purpose to pass for a"}
{"doc_id": "gutenberg_1342", "para_id": 5693, "text": "lady,--only think what fun! Not a soul knew of it, but Colonel and Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 5694, "text": "Forster, and Kitty and me, except my aunt, for we were forced to borrow"}
{"doc_id": "gutenberg_1342", "para_id": 5695, "text": "one of her gowns; and you cannot imagine how well he looked! When Denny,"}
{"doc_id": "gutenberg_1342", "para_id": 5696, "text": "and Wickham, and Pratt, and two or three more of the men came in, they"}
{"doc_id": "gutenberg_1342", "para_id": 5697, "text": "did not know him in the least. Lord! how I laughed! and so did Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 5698, "text": "Forster. I thought I should have died. And _that_ made the men suspect"}
{"doc_id": "gutenberg_1342", "para_id": 5699, "text": "something, and then they soon found out what was the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 5700, "text": "With such kind of histories of their parties and good jokes did Lydia,"}
{"doc_id": "gutenberg_1342", "para_id": 5701, "text": "assisted by Kitty’s hints and additions, endeavour to amuse her"}
{"doc_id": "gutenberg_1342", "para_id": 5702, "text": "companions all the way to Longbourn. Elizabeth listened as little as she"}
{"doc_id": "gutenberg_1342", "para_id": 5703, "text": "could, but there was no escaping the frequent mention of Wickham’s name."}
{"doc_id": "gutenberg_1342", "para_id": 5704, "text": "Their reception at home was most kind. Mrs. Bennet rejoiced to see Jane"}
{"doc_id": "gutenberg_1342", "para_id": 5705, "text": "in undiminished beauty; and more than once during dinner did Mr. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 5706, "text": "Their party in the dining-room was large, for almost all the Lucases"}
{"doc_id": "gutenberg_1342", "para_id": 5707, "text": "came to meet Maria and hear the news; and various were the subjects"}
{"doc_id": "gutenberg_1342", "para_id": 5708, "text": "which occupied them: Lady Lucas was inquiring of Maria, across the"}
{"doc_id": "gutenberg_1342", "para_id": 5709, "text": "table, after the welfare and poultry of her eldest daughter; Mrs. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 5710, "text": "was doubly engaged, on one hand collecting an account of the present"}
{"doc_id": "gutenberg_1342", "para_id": 5711, "text": "fashions from Jane, who sat some way below her, and on the other,"}
{"doc_id": "gutenberg_1342", "para_id": 5712, "text": "retailing them all to the younger Miss Lucases; and Lydia, in a voice"}
{"doc_id": "gutenberg_1342", "para_id": 5713, "text": "rather louder than any other person’s, was enumerating the various"}
{"doc_id": "gutenberg_1342", "para_id": 5714, "text": "pleasures of the morning to anybody who would hear her."}
{"doc_id": "gutenberg_1342", "para_id": 5715, "text": "“Oh, Mary,” said she, “I wish you had gone with us, for we had such fun!"}
{"doc_id": "gutenberg_1342", "para_id": 5716, "text": "as we went along Kitty and me drew up all the blinds, and pretended"}
{"doc_id": "gutenberg_1342", "para_id": 5717, "text": "there was nobody in the coach; and I should have gone so all the way, if"}
{"doc_id": "gutenberg_1342", "para_id": 5718, "text": "Kitty had not been sick; and when we got to the George, I do think we"}
{"doc_id": "gutenberg_1342", "para_id": 5719, "text": "behaved very handsomely, for we treated the other three with the nicest"}
{"doc_id": "gutenberg_1342", "para_id": 5720, "text": "cold luncheon in the world, and if you would have gone, we would have"}
{"doc_id": "gutenberg_1342", "para_id": 5721, "text": "treated you too. And then when we came away it was such fun! I thought"}
{"doc_id": "gutenberg_1342", "para_id": 5722, "text": "we never should have got into the coach. I was ready to die of laughter."}
{"doc_id": "gutenberg_1342", "para_id": 5723, "text": "And then we were so merry all the way home! we talked and laughed so"}
{"doc_id": "gutenberg_1342", "para_id": 5724, "text": "loud, that anybody might have heard us ten miles off!”"}
{"doc_id": "gutenberg_1342", "para_id": 5725, "text": "To this, Mary very gravely replied, “Far be it from me, my dear sister,"}
{"doc_id": "gutenberg_1342", "para_id": 5726, "text": "to depreciate such pleasures. They would doubtless be congenial with the"}
{"doc_id": "gutenberg_1342", "para_id": 5727, "text": "generality of female minds. But I confess they would have no charms for"}
{"doc_id": "gutenberg_1342", "para_id": 5728, "text": "But of this answer Lydia heard not a word. She seldom listened to"}
{"doc_id": "gutenberg_1342", "para_id": 5729, "text": "anybody for more than half a minute, and never attended to Mary at all."}
{"doc_id": "gutenberg_1342", "para_id": 5730, "text": "In the afternoon Lydia was urgent with the rest of the girls to walk to"}
{"doc_id": "gutenberg_1342", "para_id": 5731, "text": "Meryton, and see how everybody went on; but Elizabeth steadily opposed"}
{"doc_id": "gutenberg_1342", "para_id": 5732, "text": "the scheme. It should not be said, that the Miss Bennets could not be at"}
{"doc_id": "gutenberg_1342", "para_id": 5733, "text": "home half a day before they were in pursuit of the officers. There was"}
{"doc_id": "gutenberg_1342", "para_id": 5734, "text": "another reason, too, for her opposition. She dreaded seeing Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 5735, "text": "again, and was resolved to avoid it as long as possible. The comfort to"}
{"doc_id": "gutenberg_1342", "para_id": 5736, "text": "_her_, of the regiment’s approaching removal, was indeed beyond"}
{"doc_id": "gutenberg_1342", "para_id": 5737, "text": "expression. In a fortnight they were to go, and once gone, she hoped"}
{"doc_id": "gutenberg_1342", "para_id": 5738, "text": "there could be nothing more to plague her on his account."}
{"doc_id": "gutenberg_1342", "para_id": 5739, "text": "She had not been many hours at home, before she found that the Brighton"}
{"doc_id": "gutenberg_1342", "para_id": 5740, "text": "scheme, of which Lydia had given them a hint at the inn, was under"}
{"doc_id": "gutenberg_1342", "para_id": 5741, "text": "frequent discussion between her parents. Elizabeth saw directly that her"}
{"doc_id": "gutenberg_1342", "para_id": 5742, "text": "father had not the smallest intention of yielding; but his answers were"}
{"doc_id": "gutenberg_1342", "para_id": 5743, "text": "at the same time so vague and equivocal, that her mother, though often"}
{"doc_id": "gutenberg_1342", "para_id": 5744, "text": "disheartened, had never yet despaired of succeeding at last."}
{"doc_id": "gutenberg_1342", "para_id": 5745, "text": "Elizabeth’s impatience to acquaint Jane with what had happened could no"}
{"doc_id": "gutenberg_1342", "para_id": 5746, "text": "longer be overcome; and at length resolving to suppress every particular"}
{"doc_id": "gutenberg_1342", "para_id": 5747, "text": "in which her sister was concerned, and preparing her to be surprised,"}
{"doc_id": "gutenberg_1342", "para_id": 5748, "text": "she related to her the next morning the chief of the scene between Mr."}
{"doc_id": "gutenberg_1342", "para_id": 5749, "text": "Miss Bennet’s astonishment was soon lessened by the strong sisterly"}
{"doc_id": "gutenberg_1342", "para_id": 5750, "text": "partiality which made any admiration of Elizabeth appear perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 5751, "text": "natural; and all surprise was shortly lost in other feelings. She was"}
{"doc_id": "gutenberg_1342", "para_id": 5752, "text": "sorry that Mr. Darcy should have delivered his sentiments in a manner so"}
{"doc_id": "gutenberg_1342", "para_id": 5753, "text": "little suited to recommend them; but still more was she grieved for the"}
{"doc_id": "gutenberg_1342", "para_id": 5754, "text": "unhappiness which her sister’s refusal must have given him."}
{"doc_id": "gutenberg_1342", "para_id": 5755, "text": "“His being so sure of succeeding was wrong,” said she, “and certainly"}
{"doc_id": "gutenberg_1342", "para_id": 5756, "text": "ought not to have appeared; but consider how much it must increase his"}
{"doc_id": "gutenberg_1342", "para_id": 5757, "text": "“Indeed,” replied Elizabeth, “I am heartily sorry for him; but he has"}
{"doc_id": "gutenberg_1342", "para_id": 5758, "text": "other feelings which will probably soon drive away his regard for me."}
{"doc_id": "gutenberg_1342", "para_id": 5759, "text": "“But you blame me for having spoken so warmly of Wickham?”"}
{"doc_id": "gutenberg_1342", "para_id": 5760, "text": "“No--I do not know that you were wrong in saying what you did.”"}
{"doc_id": "gutenberg_1342", "para_id": 5761, "text": "“But you _will_ know it, when I have told you what happened the very"}
{"doc_id": "gutenberg_1342", "para_id": 5762, "text": "She then spoke of the letter, repeating the whole of its contents as far"}
{"doc_id": "gutenberg_1342", "para_id": 5763, "text": "as they concerned George Wickham. What a stroke was this for poor Jane,"}
{"doc_id": "gutenberg_1342", "para_id": 5764, "text": "who would willingly have gone through the world without believing that"}
{"doc_id": "gutenberg_1342", "para_id": 5765, "text": "so much wickedness existed in the whole race of mankind as was here"}
{"doc_id": "gutenberg_1342", "para_id": 5766, "text": "collected in one individual! Nor was Darcy’s vindication, though"}
{"doc_id": "gutenberg_1342", "para_id": 5767, "text": "grateful to her feelings, capable of consoling her for such discovery."}
{"doc_id": "gutenberg_1342", "para_id": 5768, "text": "Most earnestly did she labour to prove the probability of error, and"}
{"doc_id": "gutenberg_1342", "para_id": 5769, "text": "“This will not do,” said Elizabeth; “you never will be able to make both"}
{"doc_id": "gutenberg_1342", "para_id": 5770, "text": "of them good for anything. Take your choice, but you must be satisfied"}
{"doc_id": "gutenberg_1342", "para_id": 5771, "text": "with only one. There is but such a quantity of merit between them; just"}
{"doc_id": "gutenberg_1342", "para_id": 5772, "text": "enough to make one good sort of man; and of late it has been shifting"}
{"doc_id": "gutenberg_1342", "para_id": 5773, "text": "about pretty much. For my part, I am inclined to believe it all Mr."}
{"doc_id": "gutenberg_1342", "para_id": 5774, "text": "It was some time, however, before a smile could be extorted from Jane."}
{"doc_id": "gutenberg_1342", "para_id": 5775, "text": "“I do not know when I have been more shocked,” said she. “Wickham so"}
{"doc_id": "gutenberg_1342", "para_id": 5776, "text": "very bad! It is almost past belief. And poor Mr. Darcy! dear Lizzy,"}
{"doc_id": "gutenberg_1342", "para_id": 5777, "text": "only consider what he must have suffered. Such a disappointment! and"}
{"doc_id": "gutenberg_1342", "para_id": 5778, "text": "with the knowledge of your ill opinion too! and having to relate such a"}
{"doc_id": "gutenberg_1342", "para_id": 5779, "text": "thing of his sister! It is really too distressing, I am sure you must"}
{"doc_id": "gutenberg_1342", "para_id": 5780, "text": "“Oh no, my regret and compassion are all done away by seeing you so full"}
{"doc_id": "gutenberg_1342", "para_id": 5781, "text": "of both. I know you will do him such ample justice, that I am growing"}
{"doc_id": "gutenberg_1342", "para_id": 5782, "text": "every moment more unconcerned and indifferent. Your profusion makes me"}
{"doc_id": "gutenberg_1342", "para_id": 5783, "text": "saving; and if you lament over him much longer, my heart will be as"}
{"doc_id": "gutenberg_1342", "para_id": 5784, "text": "“Poor Wickham! there is such an expression of goodness in his"}
{"doc_id": "gutenberg_1342", "para_id": 5785, "text": "countenance! such an openness and gentleness in his manner.”"}
{"doc_id": "gutenberg_1342", "para_id": 5786, "text": "“There certainly was some great mismanagement in the education of those"}
{"doc_id": "gutenberg_1342", "para_id": 5787, "text": "two young men. One has got all the goodness, and the other all the"}
{"doc_id": "gutenberg_1342", "para_id": 5788, "text": "“I never thought Mr. Darcy so deficient in the _appearance_ of it as you"}
{"doc_id": "gutenberg_1342", "para_id": 5789, "text": "“And yet I meant to be uncommonly clever in taking so decided a dislike"}
{"doc_id": "gutenberg_1342", "para_id": 5790, "text": "to him, without any reason. It is such a spur to one’s genius, such an"}
{"doc_id": "gutenberg_1342", "para_id": 5791, "text": "opening for wit, to have a dislike of that kind. One may be continually"}
{"doc_id": "gutenberg_1342", "para_id": 5792, "text": "abusive without saying anything just; but one cannot be always laughing"}
{"doc_id": "gutenberg_1342", "para_id": 5793, "text": "at a man without now and then stumbling on something witty.”"}
{"doc_id": "gutenberg_1342", "para_id": 5794, "text": "“Lizzy, when you first read that letter, I am sure you could not treat"}
{"doc_id": "gutenberg_1342", "para_id": 5795, "text": "“Indeed, I could not. I was uncomfortable enough, I was very"}
{"doc_id": "gutenberg_1342", "para_id": 5796, "text": "uncomfortable--I may say unhappy. And with no one to speak to of what I"}
{"doc_id": "gutenberg_1342", "para_id": 5797, "text": "felt, no Jane to comfort me, and say that I had not been so very weak,"}
{"doc_id": "gutenberg_1342", "para_id": 5798, "text": "and vain, and nonsensical, as I knew I had! Oh, how I wanted you!”"}
{"doc_id": "gutenberg_1342", "para_id": 5799, "text": "“How unfortunate that you should have used such very strong expressions"}
{"doc_id": "gutenberg_1342", "para_id": 5800, "text": "in speaking of Wickham to Mr. Darcy, for now they _do_ appear wholly"}
{"doc_id": "gutenberg_1342", "para_id": 5801, "text": "“Certainly. But the misfortune of speaking with bitterness is a most"}
{"doc_id": "gutenberg_1342", "para_id": 5802, "text": "natural consequence of the prejudices I had been encouraging. There is"}
{"doc_id": "gutenberg_1342", "para_id": 5803, "text": "one point on which I want your advice. I want to be told whether I"}
{"doc_id": "gutenberg_1342", "para_id": 5804, "text": "ought, or ought not, to make our acquaintance in general understand"}
{"doc_id": "gutenberg_1342", "para_id": 5805, "text": "Miss Bennet paused a little, and then replied, “Surely there can be no"}
{"doc_id": "gutenberg_1342", "para_id": 5806, "text": "occasion for exposing him so dreadfully. What is your own opinion?”"}
{"doc_id": "gutenberg_1342", "para_id": 5807, "text": "“That it ought not to be attempted. Mr. Darcy has not authorized me to"}
{"doc_id": "gutenberg_1342", "para_id": 5808, "text": "make his communication public. On the contrary, every particular"}
{"doc_id": "gutenberg_1342", "para_id": 5809, "text": "relative to his sister was meant to be kept as much as possible to"}
{"doc_id": "gutenberg_1342", "para_id": 5810, "text": "myself; and if I endeavour to undeceive people as to the rest of his"}
{"doc_id": "gutenberg_1342", "para_id": 5811, "text": "conduct, who will believe me? The general prejudice against Mr. Darcy is"}
{"doc_id": "gutenberg_1342", "para_id": 5812, "text": "so violent, that it would be the death of half the good people in"}
{"doc_id": "gutenberg_1342", "para_id": 5813, "text": "Meryton, to attempt to place him in an amiable light. I am not equal to"}
{"doc_id": "gutenberg_1342", "para_id": 5814, "text": "it. Wickham will soon be gone; and, therefore, it will not signify to"}
{"doc_id": "gutenberg_1342", "para_id": 5815, "text": "anybody here what he really is. Some time hence it will be all found"}
{"doc_id": "gutenberg_1342", "para_id": 5816, "text": "out, and then we may laugh at their stupidity in not knowing it before."}
{"doc_id": "gutenberg_1342", "para_id": 5817, "text": "“You are quite right. To have his errors made public might ruin him for"}
{"doc_id": "gutenberg_1342", "para_id": 5818, "text": "ever. He is now, perhaps, sorry for what he has done, and anxious to"}
{"doc_id": "gutenberg_1342", "para_id": 5819, "text": "re-establish a character. We must not make him desperate.”"}
{"doc_id": "gutenberg_1342", "para_id": 5820, "text": "The tumult of Elizabeth’s mind was allayed by this conversation. She"}
{"doc_id": "gutenberg_1342", "para_id": 5821, "text": "had got rid of two of the secrets which had weighed on her for a"}
{"doc_id": "gutenberg_1342", "para_id": 5822, "text": "fortnight, and was certain of a willing listener in Jane, whenever she"}
{"doc_id": "gutenberg_1342", "para_id": 5823, "text": "might wish to talk again of either. But there was still something"}
{"doc_id": "gutenberg_1342", "para_id": 5824, "text": "lurking behind, of which prudence forbade the disclosure. She dared not"}
{"doc_id": "gutenberg_1342", "para_id": 5825, "text": "relate the other half of Mr. Darcy’s letter, nor explain to her sister"}
{"doc_id": "gutenberg_1342", "para_id": 5826, "text": "how sincerely she had been valued by his friend. Here was knowledge in"}
{"doc_id": "gutenberg_1342", "para_id": 5827, "text": "which no one could partake; and she was sensible that nothing less than"}
{"doc_id": "gutenberg_1342", "para_id": 5828, "text": "a perfect understanding between the parties could justify her in"}
{"doc_id": "gutenberg_1342", "para_id": 5829, "text": "throwing off this last encumbrance of mystery. “And then,” said she, “if"}
{"doc_id": "gutenberg_1342", "para_id": 5830, "text": "that very improbable event should ever take place, I shall merely be"}
{"doc_id": "gutenberg_1342", "para_id": 5831, "text": "able to tell what Bingley may tell in a much more agreeable manner"}
{"doc_id": "gutenberg_1342", "para_id": 5832, "text": "himself. The liberty of communication cannot be mine till it has lost"}
{"doc_id": "gutenberg_1342", "para_id": 5833, "text": "She was now, on being settled at home, at leisure to observe the real"}
{"doc_id": "gutenberg_1342", "para_id": 5834, "text": "state of her sister’s spirits. Jane was not happy. She still cherished a"}
{"doc_id": "gutenberg_1342", "para_id": 5835, "text": "very tender affection for Bingley. Having never even fancied herself in"}
{"doc_id": "gutenberg_1342", "para_id": 5836, "text": "love before, her regard had all the warmth of first attachment, and from"}
{"doc_id": "gutenberg_1342", "para_id": 5837, "text": "her age and disposition, greater steadiness than first attachments often"}
{"doc_id": "gutenberg_1342", "para_id": 5838, "text": "boast; and so fervently did she value his remembrance, and prefer him to"}
{"doc_id": "gutenberg_1342", "para_id": 5839, "text": "every other man, that all her good sense, and all her attention to the"}
{"doc_id": "gutenberg_1342", "para_id": 5840, "text": "feelings of her friends, were requisite to check the indulgence of those"}
{"doc_id": "gutenberg_1342", "para_id": 5841, "text": "regrets which must have been injurious to her own health and their"}
{"doc_id": "gutenberg_1342", "para_id": 5842, "text": "“Well, Lizzy,” said Mrs. Bennet, one day, “what is your opinion _now_ of"}
{"doc_id": "gutenberg_1342", "para_id": 5843, "text": "this sad business of Jane’s? For my part, I am determined never to speak"}
{"doc_id": "gutenberg_1342", "para_id": 5844, "text": "of it again to anybody. I told my sister Philips so the other day. But I"}
{"doc_id": "gutenberg_1342", "para_id": 5845, "text": "cannot find out that Jane saw anything of him in London. Well, he is a"}
{"doc_id": "gutenberg_1342", "para_id": 5846, "text": "very undeserving young man--and I do not suppose there is the least"}
{"doc_id": "gutenberg_1342", "para_id": 5847, "text": "chance in the world of her ever getting him now. There is no talk of his"}
{"doc_id": "gutenberg_1342", "para_id": 5848, "text": "coming to Netherfield again in the summer; and I have inquired of"}
{"doc_id": "gutenberg_1342", "para_id": 5849, "text": "“I do not believe that he will ever live at Netherfield any more.”"}
{"doc_id": "gutenberg_1342", "para_id": 5850, "text": "“Oh, well! it is just as he chooses. Nobody wants him to come; though I"}
{"doc_id": "gutenberg_1342", "para_id": 5851, "text": "shall always say that he used my daughter extremely ill; and, if I was"}
{"doc_id": "gutenberg_1342", "para_id": 5852, "text": "her, I would not have put up with it. Well, my comfort is, I am sure"}
{"doc_id": "gutenberg_1342", "para_id": 5853, "text": "Jane will die of a broken heart, and then he will be sorry for what he"}
{"doc_id": "gutenberg_1342", "para_id": 5854, "text": "But as Elizabeth could not receive comfort from any such expectation she"}
{"doc_id": "gutenberg_1342", "para_id": 5855, "text": "“Well, Lizzy,” continued her mother, soon afterwards, “and so the"}
{"doc_id": "gutenberg_1342", "para_id": 5856, "text": "Collinses live very comfortable, do they? Well, well, I only hope it"}
{"doc_id": "gutenberg_1342", "para_id": 5857, "text": "will last. And what sort of table do they keep? Charlotte is an"}
{"doc_id": "gutenberg_1342", "para_id": 5858, "text": "excellent manager, I dare say. If she is half as sharp as her mother,"}
{"doc_id": "gutenberg_1342", "para_id": 5859, "text": "she is saving enough. There is nothing extravagant in _their_"}
{"doc_id": "gutenberg_1342", "para_id": 5860, "text": "“A great deal of good management, depend upon it. Yes, yes. _They_ will"}
{"doc_id": "gutenberg_1342", "para_id": 5861, "text": "take care not to outrun their income. _They_ will never be distressed"}
{"doc_id": "gutenberg_1342", "para_id": 5862, "text": "for money. Well, much good may it do them! And so, I suppose, they often"}
{"doc_id": "gutenberg_1342", "para_id": 5863, "text": "talk of having Longbourn when your father is dead. They look upon it"}
{"doc_id": "gutenberg_1342", "para_id": 5864, "text": "quite as their own, I dare say, whenever that happens.”"}
{"doc_id": "gutenberg_1342", "para_id": 5865, "text": "“It was a subject which they could not mention before me.”"}
{"doc_id": "gutenberg_1342", "para_id": 5866, "text": "“No; it would have been strange if they had. But I make no doubt they"}
{"doc_id": "gutenberg_1342", "para_id": 5867, "text": "often talk of it between themselves. Well, if they can be easy with an"}
{"doc_id": "gutenberg_1342", "para_id": 5868, "text": "estate that is not lawfully their own, so much the better. _I_ should be"}
{"doc_id": "gutenberg_1342", "para_id": 5869, "text": "ashamed of having one that was only entailed on me.”"}
{"doc_id": "gutenberg_1342", "para_id": 5870, "text": "The first week of their return was soon gone. The second began. It was"}
{"doc_id": "gutenberg_1342", "para_id": 5871, "text": "the last of the regiment’s stay in Meryton, and all the young ladies in"}
{"doc_id": "gutenberg_1342", "para_id": 5872, "text": "the neighbourhood were drooping apace. The dejection was almost"}
{"doc_id": "gutenberg_1342", "para_id": 5873, "text": "universal. The elder Miss Bennets alone were still able to eat, drink,"}
{"doc_id": "gutenberg_1342", "para_id": 5874, "text": "and sleep, and pursue the usual course of their employments. Very"}
{"doc_id": "gutenberg_1342", "para_id": 5875, "text": "frequently were they reproached for this insensibility by Kitty and"}
{"doc_id": "gutenberg_1342", "para_id": 5876, "text": "Lydia, whose own misery was extreme, and who could not comprehend such"}
{"doc_id": "gutenberg_1342", "para_id": 5877, "text": "“Good Heaven! What is to become of us? What are we to do?” would they"}
{"doc_id": "gutenberg_1342", "para_id": 5878, "text": "often exclaim in the bitterness of woe. “How can you be smiling so,"}
{"doc_id": "gutenberg_1342", "para_id": 5879, "text": "Their affectionate mother shared all their grief; she remembered what"}
{"doc_id": "gutenberg_1342", "para_id": 5880, "text": "she had herself endured on a similar occasion five-and-twenty years ago."}
{"doc_id": "gutenberg_1342", "para_id": 5881, "text": "“I am sure,” said she, “I cried for two days together when Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 5882, "text": "Miller’s regiment went away. I thought I should have broke my heart.”"}
{"doc_id": "gutenberg_1342", "para_id": 5883, "text": "“If one could but go to Brighton!” observed Mrs. Bennet."}
{"doc_id": "gutenberg_1342", "para_id": 5884, "text": "“Oh yes!--if one could but go to Brighton! But papa is so disagreeable.”"}
{"doc_id": "gutenberg_1342", "para_id": 5885, "text": "“And my aunt Philips is sure it would do _me_ a great deal of good,”"}
{"doc_id": "gutenberg_1342", "para_id": 5886, "text": "Such were the kind of lamentations resounding perpetually through"}
{"doc_id": "gutenberg_1342", "para_id": 5887, "text": "Longbourn House. Elizabeth tried to be diverted by them; but all sense"}
{"doc_id": "gutenberg_1342", "para_id": 5888, "text": "of pleasure was lost in shame. She felt anew the justice of Mr. Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 5889, "text": "objections; and never had she before been so much disposed to pardon his"}
{"doc_id": "gutenberg_1342", "para_id": 5890, "text": "But the gloom of Lydia’s prospect was shortly cleared away; for she"}
{"doc_id": "gutenberg_1342", "para_id": 5891, "text": "received an invitation from Mrs. Forster, the wife of the colonel of the"}
{"doc_id": "gutenberg_1342", "para_id": 5892, "text": "regiment, to accompany her to Brighton. This invaluable friend was a"}
{"doc_id": "gutenberg_1342", "para_id": 5893, "text": "very young woman, and very lately married. A resemblance in good-humour"}
{"doc_id": "gutenberg_1342", "para_id": 5894, "text": "and good spirits had recommended her and Lydia to each other, and out of"}
{"doc_id": "gutenberg_1342", "para_id": 5895, "text": "their _three_ months’ acquaintance they had been intimate _two_."}
{"doc_id": "gutenberg_1342", "para_id": 5896, "text": "The rapture of Lydia on this occasion, her adoration of Mrs. Forster,"}
{"doc_id": "gutenberg_1342", "para_id": 5897, "text": "the delight of Mrs. Bennet, and the mortification of Kitty, are scarcely"}
{"doc_id": "gutenberg_1342", "para_id": 5898, "text": "to be described. Wholly inattentive to her sister’s feelings, Lydia flew"}
{"doc_id": "gutenberg_1342", "para_id": 5899, "text": "about the house in restless ecstasy, calling for everyone’s"}
{"doc_id": "gutenberg_1342", "para_id": 5900, "text": "congratulations, and laughing and talking with more violence than ever;"}
{"doc_id": "gutenberg_1342", "para_id": 5901, "text": "whilst the luckless Kitty continued in the parlour repining at her fate"}
{"doc_id": "gutenberg_1342", "para_id": 5902, "text": "in terms as unreasonable as her accent was peevish."}
{"doc_id": "gutenberg_1342", "para_id": 5903, "text": "“I cannot see why Mrs. Forster should not ask _me_ as well as Lydia,”"}
{"doc_id": "gutenberg_1342", "para_id": 5904, "text": "said she, “though I am _not_ her particular friend. I have just as much"}
{"doc_id": "gutenberg_1342", "para_id": 5905, "text": "right to be asked as she has, and more too, for I am two years older.”"}
{"doc_id": "gutenberg_1342", "para_id": 5906, "text": "In vain did Elizabeth attempt to make her reasonable, and Jane to make"}
{"doc_id": "gutenberg_1342", "para_id": 5907, "text": "her resigned. As for Elizabeth herself, this invitation was so far from"}
{"doc_id": "gutenberg_1342", "para_id": 5908, "text": "exciting in her the same feelings as in her mother and Lydia, that she"}
{"doc_id": "gutenberg_1342", "para_id": 5909, "text": "considered it as the death-warrant of all possibility of common sense"}
{"doc_id": "gutenberg_1342", "para_id": 5910, "text": "for the latter; and detestable as such a step must make her, were it"}
{"doc_id": "gutenberg_1342", "para_id": 5911, "text": "known, she could not help secretly advising her father not to let her"}
{"doc_id": "gutenberg_1342", "para_id": 5912, "text": "go. She represented to him all the improprieties of Lydia’s general"}
{"doc_id": "gutenberg_1342", "para_id": 5913, "text": "behaviour, the little advantage she could derive from the friendship of"}
{"doc_id": "gutenberg_1342", "para_id": 5914, "text": "such a woman as Mrs. Forster, and the probability of her being yet more"}
{"doc_id": "gutenberg_1342", "para_id": 5915, "text": "imprudent with such a companion at Brighton, where the temptations must"}
{"doc_id": "gutenberg_1342", "para_id": 5916, "text": "be greater than at home. He heard her attentively, and then said,--"}
{"doc_id": "gutenberg_1342", "para_id": 5917, "text": "“Lydia will never be easy till she has exposed herself in some public"}
{"doc_id": "gutenberg_1342", "para_id": 5918, "text": "place or other, and we can never expect her to do it with so little"}
{"doc_id": "gutenberg_1342", "para_id": 5919, "text": "expense or inconvenience to her family as under the present"}
{"doc_id": "gutenberg_1342", "para_id": 5920, "text": "“If you were aware,” said Elizabeth, “of the very great disadvantage to"}
{"doc_id": "gutenberg_1342", "para_id": 5921, "text": "us all, which must arise from the public notice of Lydia’s unguarded and"}
{"doc_id": "gutenberg_1342", "para_id": 5922, "text": "imprudent manner, nay, which has already arisen from it, I am sure you"}
{"doc_id": "gutenberg_1342", "para_id": 5923, "text": "“Already arisen!” repeated Mr. Bennet. “What! has she frightened away"}
{"doc_id": "gutenberg_1342", "para_id": 5924, "text": "some of your lovers? Poor little Lizzy! But do not be cast down. Such"}
{"doc_id": "gutenberg_1342", "para_id": 5925, "text": "squeamish youths as cannot bear to be connected with a little absurdity"}
{"doc_id": "gutenberg_1342", "para_id": 5926, "text": "are not worth a regret. Come, let me see the list of the pitiful fellows"}
{"doc_id": "gutenberg_1342", "para_id": 5927, "text": "“Indeed, you are mistaken. I have no such injuries to resent. It is not"}
{"doc_id": "gutenberg_1342", "para_id": 5928, "text": "of peculiar, but of general evils, which I am now complaining. Our"}
{"doc_id": "gutenberg_1342", "para_id": 5929, "text": "importance, our respectability in the world, must be affected by the"}
{"doc_id": "gutenberg_1342", "para_id": 5930, "text": "wild volatility, the assurance and disdain of all restraint which mark"}
{"doc_id": "gutenberg_1342", "para_id": 5931, "text": "Lydia’s character. Excuse me,--for I must speak plainly. If you, my dear"}
{"doc_id": "gutenberg_1342", "para_id": 5932, "text": "father, will not take the trouble of checking her exuberant spirits, and"}
{"doc_id": "gutenberg_1342", "para_id": 5933, "text": "of teaching her that her present pursuits are not to be the business of"}
{"doc_id": "gutenberg_1342", "para_id": 5934, "text": "her life, she will soon be beyond the reach of amendment. Her character"}
{"doc_id": "gutenberg_1342", "para_id": 5935, "text": "will be fixed; and she will, at sixteen, be the most determined flirt"}
{"doc_id": "gutenberg_1342", "para_id": 5936, "text": "that ever made herself and her family ridiculous;--a flirt, too, in the"}
{"doc_id": "gutenberg_1342", "para_id": 5937, "text": "worst and meanest degree of flirtation; without any attraction beyond"}
{"doc_id": "gutenberg_1342", "para_id": 5938, "text": "youth and a tolerable person; and, from the ignorance and emptiness of"}
{"doc_id": "gutenberg_1342", "para_id": 5939, "text": "her mind, wholly unable to ward off any portion of that universal"}
{"doc_id": "gutenberg_1342", "para_id": 5940, "text": "contempt which her rage for admiration will excite. In this danger Kitty"}
{"doc_id": "gutenberg_1342", "para_id": 5941, "text": "is also comprehended. She will follow wherever Lydia leads. Vain,"}
{"doc_id": "gutenberg_1342", "para_id": 5942, "text": "ignorant, idle, and absolutely uncontrolled! Oh, my dear father, can you"}
{"doc_id": "gutenberg_1342", "para_id": 5943, "text": "suppose it possible that they will not be censured and despised wherever"}
{"doc_id": "gutenberg_1342", "para_id": 5944, "text": "they are known, and that their sisters will not be often involved in the"}
{"doc_id": "gutenberg_1342", "para_id": 5945, "text": "Mr. Bennet saw that her whole heart was in the subject; and,"}
{"doc_id": "gutenberg_1342", "para_id": 5946, "text": "“Do not make yourself uneasy, my love. Wherever you and Jane are known,"}
{"doc_id": "gutenberg_1342", "para_id": 5947, "text": "you must be respected and valued; and you will not appear to less"}
{"doc_id": "gutenberg_1342", "para_id": 5948, "text": "advantage for having a couple of--or I may say, three--very silly"}
{"doc_id": "gutenberg_1342", "para_id": 5949, "text": "sisters. We shall have no peace at Longbourn if Lydia does not go to"}
{"doc_id": "gutenberg_1342", "para_id": 5950, "text": "Brighton. Let her go, then. Colonel Forster is a sensible man, and will"}
{"doc_id": "gutenberg_1342", "para_id": 5951, "text": "keep her out of any real mischief; and she is luckily too poor to be an"}
{"doc_id": "gutenberg_1342", "para_id": 5952, "text": "object of prey to anybody. At Brighton she will be of less importance"}
{"doc_id": "gutenberg_1342", "para_id": 5953, "text": "even as a common flirt than she has been here. The officers will find"}
{"doc_id": "gutenberg_1342", "para_id": 5954, "text": "women better worth their notice. Let us hope, therefore, that her being"}
{"doc_id": "gutenberg_1342", "para_id": 5955, "text": "there may teach her her own insignificance. At any rate, she cannot grow"}
{"doc_id": "gutenberg_1342", "para_id": 5956, "text": "many degrees worse, without authorizing us to lock her up for the rest"}
{"doc_id": "gutenberg_1342", "para_id": 5957, "text": "With this answer Elizabeth was forced to be content; but her own opinion"}
{"doc_id": "gutenberg_1342", "para_id": 5958, "text": "continued the same, and she left him disappointed and sorry. It was not"}
{"doc_id": "gutenberg_1342", "para_id": 5959, "text": "in her nature, however, to increase her vexations by dwelling on them."}
{"doc_id": "gutenberg_1342", "para_id": 5960, "text": "She was confident of having performed her duty; and to fret over"}
{"doc_id": "gutenberg_1342", "para_id": 5961, "text": "unavoidable evils, or augment them by anxiety, was no part of her"}
{"doc_id": "gutenberg_1342", "para_id": 5962, "text": "Had Lydia and her mother known the substance of her conference with her"}
{"doc_id": "gutenberg_1342", "para_id": 5963, "text": "father, their indignation would hardly have found expression in their"}
{"doc_id": "gutenberg_1342", "para_id": 5964, "text": "united volubility. In Lydia’s imagination, a visit to Brighton comprised"}
{"doc_id": "gutenberg_1342", "para_id": 5965, "text": "every possibility of earthly happiness. She saw, with the creative eye"}
{"doc_id": "gutenberg_1342", "para_id": 5966, "text": "of fancy, the streets of that gay bathing-place covered with officers."}
{"doc_id": "gutenberg_1342", "para_id": 5967, "text": "She saw herself the object of attention to tens and to scores of them at"}
{"doc_id": "gutenberg_1342", "para_id": 5968, "text": "present unknown. She saw all the glories of the camp: its tents"}
{"doc_id": "gutenberg_1342", "para_id": 5969, "text": "stretched forth in beauteous uniformity of lines, crowded with the young"}
{"doc_id": "gutenberg_1342", "para_id": 5970, "text": "and the gay, and dazzling with scarlet; and, to complete the view, she"}
{"doc_id": "gutenberg_1342", "para_id": 5971, "text": "saw herself seated beneath a tent, tenderly flirting with at least six"}
{"doc_id": "gutenberg_1342", "para_id": 5972, "text": "Had she known that her sister sought to tear her from such prospects and"}
{"doc_id": "gutenberg_1342", "para_id": 5973, "text": "such realities as these, what would have been her sensations? They could"}
{"doc_id": "gutenberg_1342", "para_id": 5974, "text": "have been understood only by her mother, who might have felt nearly the"}
{"doc_id": "gutenberg_1342", "para_id": 5975, "text": "same. Lydia’s going to Brighton was all that consoled her for the"}
{"doc_id": "gutenberg_1342", "para_id": 5976, "text": "melancholy conviction of her husband’s never intending to go there"}
{"doc_id": "gutenberg_1342", "para_id": 5977, "text": "But they were entirely ignorant of what had passed; and their raptures"}
{"doc_id": "gutenberg_1342", "para_id": 5978, "text": "continued, with little intermission, to the very day of Lydia’s leaving"}
{"doc_id": "gutenberg_1342", "para_id": 5979, "text": "Elizabeth was now to see Mr. Wickham for the last time. Having been"}
{"doc_id": "gutenberg_1342", "para_id": 5980, "text": "frequently in company with him since her return, agitation was pretty"}
{"doc_id": "gutenberg_1342", "para_id": 5981, "text": "well over; the agitations of former partiality entirely so. She had even"}
{"doc_id": "gutenberg_1342", "para_id": 5982, "text": "learnt to detect, in the very gentleness which had first delighted her,"}
{"doc_id": "gutenberg_1342", "para_id": 5983, "text": "an affectation and a sameness to disgust and weary. In his present"}
{"doc_id": "gutenberg_1342", "para_id": 5984, "text": "behaviour to herself, moreover, she had a fresh source of displeasure;"}
{"doc_id": "gutenberg_1342", "para_id": 5985, "text": "for the inclination he soon testified of renewing those attentions which"}
{"doc_id": "gutenberg_1342", "para_id": 5986, "text": "had marked the early part of their acquaintance could only serve, after"}
{"doc_id": "gutenberg_1342", "para_id": 5987, "text": "what had since passed, to provoke her. She lost all concern for him in"}
{"doc_id": "gutenberg_1342", "para_id": 5988, "text": "finding herself thus selected as the object of such idle and frivolous"}
{"doc_id": "gutenberg_1342", "para_id": 5989, "text": "gallantry; and while she steadily repressed it, could not but feel the"}
{"doc_id": "gutenberg_1342", "para_id": 5990, "text": "reproof contained in his believing, that however long, and for whatever"}
{"doc_id": "gutenberg_1342", "para_id": 5991, "text": "cause, his attentions had been withdrawn, her vanity would be gratified,"}
{"doc_id": "gutenberg_1342", "para_id": 5992, "text": "and her preference secured, at any time, by their renewal."}
{"doc_id": "gutenberg_1342", "para_id": 5993, "text": "On the very last day of the regiment’s remaining in Meryton, he dined,"}
{"doc_id": "gutenberg_1342", "para_id": 5994, "text": "with others of the officers, at Longbourn; and so little was Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 5995, "text": "disposed to part from him in good-humour, that, on his making some"}
{"doc_id": "gutenberg_1342", "para_id": 5996, "text": "inquiry as to the manner in which her time had passed at Hunsford, she"}
{"doc_id": "gutenberg_1342", "para_id": 5997, "text": "mentioned Colonel Fitzwilliam’s and Mr. Darcy’s having both spent three"}
{"doc_id": "gutenberg_1342", "para_id": 5998, "text": "weeks at Rosings, and asked him if he were acquainted with the former."}
{"doc_id": "gutenberg_1342", "para_id": 5999, "text": "He looked surprised, displeased, alarmed; but, with a moment’s"}
{"doc_id": "gutenberg_1342", "para_id": 6000, "text": "recollection, and a returning smile, replied, that he had formerly seen"}
{"doc_id": "gutenberg_1342", "para_id": 6001, "text": "him often; and, after observing that he was a very gentlemanlike man,"}
{"doc_id": "gutenberg_1342", "para_id": 6002, "text": "asked her how she had liked him. Her answer was warmly in his favour."}
{"doc_id": "gutenberg_1342", "para_id": 6003, "text": "With an air of indifference, he soon afterwards added, “How long did you"}
{"doc_id": "gutenberg_1342", "para_id": 6004, "text": "“His manners are very different from his cousin’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 6005, "text": "“Yes, very different; but I think Mr. Darcy improves on acquaintance.”"}
{"doc_id": "gutenberg_1342", "para_id": 6006, "text": "“Indeed!” cried Wickham, with a look which did not escape her. “And pray"}
{"doc_id": "gutenberg_1342", "para_id": 6007, "text": "may I ask--” but checking himself, he added, in a gayer tone, “Is it in"}
{"doc_id": "gutenberg_1342", "para_id": 6008, "text": "address that he improves? Has he deigned to add aught of civility to his"}
{"doc_id": "gutenberg_1342", "para_id": 6009, "text": "ordinary style? for I dare not hope,” he continued, in a lower and more"}
{"doc_id": "gutenberg_1342", "para_id": 6010, "text": "“Oh, no!” said Elizabeth. “In essentials, I believe, he is very much"}
{"doc_id": "gutenberg_1342", "para_id": 6011, "text": "While she spoke, Wickham looked as if scarcely knowing whether to"}
{"doc_id": "gutenberg_1342", "para_id": 6012, "text": "rejoice over her words or to distrust their meaning. There was a"}
{"doc_id": "gutenberg_1342", "para_id": 6013, "text": "something in her countenance which made him listen with an apprehensive"}
{"doc_id": "gutenberg_1342", "para_id": 6014, "text": "“When I said that he improved on acquaintance, I did not mean that"}
{"doc_id": "gutenberg_1342", "para_id": 6015, "text": "either his mind or manners were in a state of improvement; but that,"}
{"doc_id": "gutenberg_1342", "para_id": 6016, "text": "from knowing him better, his disposition was better understood.”"}
{"doc_id": "gutenberg_1342", "para_id": 6017, "text": "Wickham’s alarm now appeared in a heightened complexion and agitated"}
{"doc_id": "gutenberg_1342", "para_id": 6018, "text": "look; for a few minutes he was silent; till, shaking off his"}
{"doc_id": "gutenberg_1342", "para_id": 6019, "text": "embarrassment, he turned to her again, and said in the gentlest of"}
{"doc_id": "gutenberg_1342", "para_id": 6020, "text": "“You, who so well know my feelings towards Mr. Darcy, will readily"}
{"doc_id": "gutenberg_1342", "para_id": 6021, "text": "comprehend how sincerely I must rejoice that he is wise enough to assume"}
{"doc_id": "gutenberg_1342", "para_id": 6022, "text": "even the _appearance_ of what is right. His pride, in that direction,"}
{"doc_id": "gutenberg_1342", "para_id": 6023, "text": "may be of service, if not to himself, to many others, for it must deter"}
{"doc_id": "gutenberg_1342", "para_id": 6024, "text": "him from such foul misconduct as I have suffered by. I only fear that"}
{"doc_id": "gutenberg_1342", "para_id": 6025, "text": "the sort of cautiousness to which you, I imagine, have been alluding, is"}
{"doc_id": "gutenberg_1342", "para_id": 6026, "text": "merely adopted on his visits to his aunt, of whose good opinion and"}
{"doc_id": "gutenberg_1342", "para_id": 6027, "text": "judgment he stands much in awe. His fear of her has always operated, I"}
{"doc_id": "gutenberg_1342", "para_id": 6028, "text": "know, when they were together; and a good deal is to be imputed to his"}
{"doc_id": "gutenberg_1342", "para_id": 6029, "text": "wish of forwarding the match with Miss de Bourgh, which I am certain he"}
{"doc_id": "gutenberg_1342", "para_id": 6030, "text": "Elizabeth could not repress a smile at this, but she answered only by a"}
{"doc_id": "gutenberg_1342", "para_id": 6031, "text": "slight inclination of the head. She saw that he wanted to engage her on"}
{"doc_id": "gutenberg_1342", "para_id": 6032, "text": "the old subject of his grievances, and she was in no humour to indulge"}
{"doc_id": "gutenberg_1342", "para_id": 6033, "text": "him. The rest of the evening passed with the _appearance_, on his side,"}
{"doc_id": "gutenberg_1342", "para_id": 6034, "text": "of usual cheerfulness, but with no further attempt to distinguish"}
{"doc_id": "gutenberg_1342", "para_id": 6035, "text": "Elizabeth; and they parted at last with mutual civility, and possibly a"}
{"doc_id": "gutenberg_1342", "para_id": 6036, "text": "When the party broke up, Lydia returned with Mrs. Forster to Meryton,"}
{"doc_id": "gutenberg_1342", "para_id": 6037, "text": "from whence they were to set out early the next morning. The separation"}
{"doc_id": "gutenberg_1342", "para_id": 6038, "text": "between her and her family was rather noisy than pathetic. Kitty was the"}
{"doc_id": "gutenberg_1342", "para_id": 6039, "text": "only one who shed tears; but she did weep from vexation and envy. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 6040, "text": "Bennet was diffuse in her good wishes for the felicity of her daughter,"}
{"doc_id": "gutenberg_1342", "para_id": 6041, "text": "and impressive in her injunctions that she would not miss the"}
{"doc_id": "gutenberg_1342", "para_id": 6042, "text": "opportunity of enjoying herself as much as possible,--advice which there"}
{"doc_id": "gutenberg_1342", "para_id": 6043, "text": "was every reason to believe would be attended to; and, in the clamorous"}
{"doc_id": "gutenberg_1342", "para_id": 6044, "text": "happiness of Lydia herself in bidding farewell, the more gentle adieus"}
{"doc_id": "gutenberg_1342", "para_id": 6045, "text": "Had Elizabeth’s opinion been all drawn from her own family, she could"}
{"doc_id": "gutenberg_1342", "para_id": 6046, "text": "not have formed a very pleasing picture of conjugal felicity or domestic"}
{"doc_id": "gutenberg_1342", "para_id": 6047, "text": "comfort. Her father, captivated by youth and beauty, and that appearance"}
{"doc_id": "gutenberg_1342", "para_id": 6048, "text": "of good-humour which youth and beauty generally give, had married a"}
{"doc_id": "gutenberg_1342", "para_id": 6049, "text": "woman whose weak understanding and illiberal mind had very early in"}
{"doc_id": "gutenberg_1342", "para_id": 6050, "text": "their marriage put an end to all real affection for her. Respect,"}
{"doc_id": "gutenberg_1342", "para_id": 6051, "text": "esteem, and confidence had vanished for ever; and all his views of"}
{"doc_id": "gutenberg_1342", "para_id": 6052, "text": "domestic happiness were overthrown. But Mr. Bennet was not of a"}
{"doc_id": "gutenberg_1342", "para_id": 6053, "text": "disposition to seek comfort for the disappointment which his own"}
{"doc_id": "gutenberg_1342", "para_id": 6054, "text": "imprudence had brought on in any of those pleasures which too often"}
{"doc_id": "gutenberg_1342", "para_id": 6055, "text": "console the unfortunate for their folly or their vice. He was fond of"}
{"doc_id": "gutenberg_1342", "para_id": 6056, "text": "the country and of books; and from these tastes had arisen his principal"}
{"doc_id": "gutenberg_1342", "para_id": 6057, "text": "enjoyments. To his wife he was very little otherwise indebted than as"}
{"doc_id": "gutenberg_1342", "para_id": 6058, "text": "her ignorance and folly had contributed to his amusement. This is not"}
{"doc_id": "gutenberg_1342", "para_id": 6059, "text": "the sort of happiness which a man would in general wish to owe to his"}
{"doc_id": "gutenberg_1342", "para_id": 6060, "text": "wife; but where other powers of entertainment are wanting, the true"}
{"doc_id": "gutenberg_1342", "para_id": 6061, "text": "philosopher will derive benefit from such as are given."}
{"doc_id": "gutenberg_1342", "para_id": 6062, "text": "Elizabeth, however, had never been blind to the impropriety of her"}
{"doc_id": "gutenberg_1342", "para_id": 6063, "text": "father’s behaviour as a husband. She had always seen it with pain; but"}
{"doc_id": "gutenberg_1342", "para_id": 6064, "text": "respecting his abilities, and grateful for his affectionate treatment of"}
{"doc_id": "gutenberg_1342", "para_id": 6065, "text": "herself, she endeavoured to forget what she could not overlook, and to"}
{"doc_id": "gutenberg_1342", "para_id": 6066, "text": "banish from her thoughts that continual breach of conjugal obligation"}
{"doc_id": "gutenberg_1342", "para_id": 6067, "text": "and decorum which, in exposing his wife to the contempt of her own"}
{"doc_id": "gutenberg_1342", "para_id": 6068, "text": "children, was so highly reprehensible. But she had never felt so"}
{"doc_id": "gutenberg_1342", "para_id": 6069, "text": "strongly as now the disadvantages which must attend the children of so"}
{"doc_id": "gutenberg_1342", "para_id": 6070, "text": "unsuitable a marriage, nor ever been so fully aware of the evils arising"}
{"doc_id": "gutenberg_1342", "para_id": 6071, "text": "from so ill-judged a direction of talents--talents which, rightly used,"}
{"doc_id": "gutenberg_1342", "para_id": 6072, "text": "might at least have preserved the respectability of his daughters, even"}
{"doc_id": "gutenberg_1342", "para_id": 6073, "text": "When Elizabeth had rejoiced over Wickham’s departure, she found little"}
{"doc_id": "gutenberg_1342", "para_id": 6074, "text": "other cause for satisfaction in the loss of the regiment. Their parties"}
{"doc_id": "gutenberg_1342", "para_id": 6075, "text": "abroad were less varied than before; and at home she had a mother and"}
{"doc_id": "gutenberg_1342", "para_id": 6076, "text": "sister, whose constant repinings at the dulness of everything around"}
{"doc_id": "gutenberg_1342", "para_id": 6077, "text": "them threw a real gloom over their domestic circle; and, though Kitty"}
{"doc_id": "gutenberg_1342", "para_id": 6078, "text": "might in time regain her natural degree of sense, since the disturbers"}
{"doc_id": "gutenberg_1342", "para_id": 6079, "text": "of her brain were removed, her other sister, from whose disposition"}
{"doc_id": "gutenberg_1342", "para_id": 6080, "text": "greater evil might be apprehended, was likely to be hardened in all her"}
{"doc_id": "gutenberg_1342", "para_id": 6081, "text": "folly and assurance, by a situation of such double danger as a"}
{"doc_id": "gutenberg_1342", "para_id": 6082, "text": "watering-place and a camp. Upon the whole, therefore, she found, what"}
{"doc_id": "gutenberg_1342", "para_id": 6083, "text": "has been sometimes found before, that an event to which she had looked"}
{"doc_id": "gutenberg_1342", "para_id": 6084, "text": "forward with impatient desire, did not, in taking place, bring all the"}
{"doc_id": "gutenberg_1342", "para_id": 6085, "text": "satisfaction she had promised herself. It was consequently necessary to"}
{"doc_id": "gutenberg_1342", "para_id": 6086, "text": "name some other period for the commencement of actual felicity; to have"}
{"doc_id": "gutenberg_1342", "para_id": 6087, "text": "some other point on which her wishes and hopes might be fixed, and by"}
{"doc_id": "gutenberg_1342", "para_id": 6088, "text": "again enjoying the pleasure of anticipation, console herself for the"}
{"doc_id": "gutenberg_1342", "para_id": 6089, "text": "present, and prepare for another disappointment. Her tour to the Lakes"}
{"doc_id": "gutenberg_1342", "para_id": 6090, "text": "was now the object of her happiest thoughts: it was her best consolation"}
{"doc_id": "gutenberg_1342", "para_id": 6091, "text": "for all the uncomfortable hours which the discontentedness of her mother"}
{"doc_id": "gutenberg_1342", "para_id": 6092, "text": "and Kitty made inevitable; and could she have included Jane in the"}
{"doc_id": "gutenberg_1342", "para_id": 6093, "text": "“But it is fortunate,” thought she, “that I have something to wish for."}
{"doc_id": "gutenberg_1342", "para_id": 6094, "text": "Were the whole arrangement complete, my disappointment would be certain."}
{"doc_id": "gutenberg_1342", "para_id": 6095, "text": "But here, by carrying with me one ceaseless source of regret in my"}
{"doc_id": "gutenberg_1342", "para_id": 6096, "text": "sister’s absence, I may reasonably hope to have all my expectations of"}
{"doc_id": "gutenberg_1342", "para_id": 6097, "text": "pleasure realized. A scheme of which every part promises delight can"}
{"doc_id": "gutenberg_1342", "para_id": 6098, "text": "never be successful; and general disappointment is only warded off by"}
{"doc_id": "gutenberg_1342", "para_id": 6099, "text": "When Lydia went away she promised to write very often and very minutely"}
{"doc_id": "gutenberg_1342", "para_id": 6100, "text": "to her mother and Kitty; but her letters were always long expected, and"}
{"doc_id": "gutenberg_1342", "para_id": 6101, "text": "always very short. Those to her mother contained little else than that"}
{"doc_id": "gutenberg_1342", "para_id": 6102, "text": "they were just returned from the library, where such and such officers"}
{"doc_id": "gutenberg_1342", "para_id": 6103, "text": "had attended them, and where she had seen such beautiful ornaments as"}
{"doc_id": "gutenberg_1342", "para_id": 6104, "text": "made her quite wild; that she had a new gown, or a new parasol, which"}
{"doc_id": "gutenberg_1342", "para_id": 6105, "text": "she would have described more fully, but was obliged to leave off in a"}
{"doc_id": "gutenberg_1342", "para_id": 6106, "text": "violent hurry, as Mrs. Forster called her, and they were going to the"}
{"doc_id": "gutenberg_1342", "para_id": 6107, "text": "camp; and from her correspondence with her sister there was still less"}
{"doc_id": "gutenberg_1342", "para_id": 6108, "text": "to be learnt, for her letters to Kitty, though rather longer, were much"}
{"doc_id": "gutenberg_1342", "para_id": 6109, "text": "too full of lines under the words to be made public."}
{"doc_id": "gutenberg_1342", "para_id": 6110, "text": "After the first fortnight or three weeks of her absence, health,"}
{"doc_id": "gutenberg_1342", "para_id": 6111, "text": "good-humour, and cheerfulness began to reappear at Longbourn. Everything"}
{"doc_id": "gutenberg_1342", "para_id": 6112, "text": "wore a happier aspect. The families who had been in town for the winter"}
{"doc_id": "gutenberg_1342", "para_id": 6113, "text": "came back again, and summer finery and summer engagements arose. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 6114, "text": "Bennet was restored to her usual querulous serenity; and by the middle"}
{"doc_id": "gutenberg_1342", "para_id": 6115, "text": "of June Kitty was so much recovered as to be able to enter Meryton"}
{"doc_id": "gutenberg_1342", "para_id": 6116, "text": "without tears,--an event of such happy promise as to make Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 6117, "text": "hope, that by the following Christmas she might be so tolerably"}
{"doc_id": "gutenberg_1342", "para_id": 6118, "text": "reasonable as not to mention an officer above once a day, unless, by"}
{"doc_id": "gutenberg_1342", "para_id": 6119, "text": "some cruel and malicious arrangement at the War Office, another regiment"}
{"doc_id": "gutenberg_1342", "para_id": 6120, "text": "The time fixed for the beginning of their northern tour was now fast"}
{"doc_id": "gutenberg_1342", "para_id": 6121, "text": "approaching; and a fortnight only was wanting of it, when a letter"}
{"doc_id": "gutenberg_1342", "para_id": 6122, "text": "arrived from Mrs. Gardiner, which at once delayed its commencement and"}
{"doc_id": "gutenberg_1342", "para_id": 6123, "text": "curtailed its extent. Mr. Gardiner would be prevented by business from"}
{"doc_id": "gutenberg_1342", "para_id": 6124, "text": "setting out till a fortnight later in July, and must be in London again"}
{"doc_id": "gutenberg_1342", "para_id": 6125, "text": "within a month; and as that left too short a period for them to go so"}
{"doc_id": "gutenberg_1342", "para_id": 6126, "text": "far, and see so much as they had proposed, or at least to see it with"}
{"doc_id": "gutenberg_1342", "para_id": 6127, "text": "the leisure and comfort they had built on, they were obliged to give up"}
{"doc_id": "gutenberg_1342", "para_id": 6128, "text": "the Lakes, and substitute a more contracted tour; and, according to the"}
{"doc_id": "gutenberg_1342", "para_id": 6129, "text": "present plan, were to go no farther northward than Derbyshire. In that"}
{"doc_id": "gutenberg_1342", "para_id": 6130, "text": "county there was enough to be seen to occupy the chief of their three"}
{"doc_id": "gutenberg_1342", "para_id": 6131, "text": "weeks; and to Mrs. Gardiner it had a peculiarly strong attraction. The"}
{"doc_id": "gutenberg_1342", "para_id": 6132, "text": "town where she had formerly passed some years of her life, and where"}
{"doc_id": "gutenberg_1342", "para_id": 6133, "text": "they were now to spend a few days, was probably as great an object of"}
{"doc_id": "gutenberg_1342", "para_id": 6134, "text": "her curiosity as all the celebrated beauties of Matlock, Chatsworth,"}
{"doc_id": "gutenberg_1342", "para_id": 6135, "text": "Elizabeth was excessively disappointed: she had set her heart on seeing"}
{"doc_id": "gutenberg_1342", "para_id": 6136, "text": "the Lakes; and still thought there might have been time enough. But it"}
{"doc_id": "gutenberg_1342", "para_id": 6137, "text": "was her business to be satisfied--and certainly her temper to be happy;"}
{"doc_id": "gutenberg_1342", "para_id": 6138, "text": "With the mention of Derbyshire, there were many ideas connected. It was"}
{"doc_id": "gutenberg_1342", "para_id": 6139, "text": "impossible for her to see the word without thinking of Pemberley and its"}
{"doc_id": "gutenberg_1342", "para_id": 6140, "text": "owner. “But surely,” said she, “I may enter his county with impunity,"}
{"doc_id": "gutenberg_1342", "para_id": 6141, "text": "and rob it of a few petrified spars, without his perceiving me.”"}
{"doc_id": "gutenberg_1342", "para_id": 6142, "text": "The period of expectation was now doubled. Four weeks were to pass away"}
{"doc_id": "gutenberg_1342", "para_id": 6143, "text": "before her uncle and aunt’s arrival. But they did pass away, and Mr. and"}
{"doc_id": "gutenberg_1342", "para_id": 6144, "text": "Mrs. Gardiner, with their four children, did at length appear at"}
{"doc_id": "gutenberg_1342", "para_id": 6145, "text": "Longbourn. The children, two girls of six and eight years old, and two"}
{"doc_id": "gutenberg_1342", "para_id": 6146, "text": "younger boys, were to be left under the particular care of their cousin"}
{"doc_id": "gutenberg_1342", "para_id": 6147, "text": "Jane, who was the general favourite, and whose steady sense and"}
{"doc_id": "gutenberg_1342", "para_id": 6148, "text": "sweetness of temper exactly adapted her for attending to them in every"}
{"doc_id": "gutenberg_1342", "para_id": 6149, "text": "way--teaching them, playing with them, and loving them."}
{"doc_id": "gutenberg_1342", "para_id": 6150, "text": "The Gardiners stayed only one night at Longbourn, and set off the next"}
{"doc_id": "gutenberg_1342", "para_id": 6151, "text": "morning with Elizabeth in pursuit of novelty and amusement. One"}
{"doc_id": "gutenberg_1342", "para_id": 6152, "text": "enjoyment was certain--that of suitableness as companions; a"}
{"doc_id": "gutenberg_1342", "para_id": 6153, "text": "suitableness which comprehended health and temper to bear"}
{"doc_id": "gutenberg_1342", "para_id": 6154, "text": "inconveniences--cheerfulness to enhance every pleasure--and affection"}
{"doc_id": "gutenberg_1342", "para_id": 6155, "text": "and intelligence, which might supply it among themselves if there were"}
{"doc_id": "gutenberg_1342", "para_id": 6156, "text": "It is not the object of this work to give a description of Derbyshire,"}
{"doc_id": "gutenberg_1342", "para_id": 6157, "text": "nor of any of the remarkable places through which their route thither"}
{"doc_id": "gutenberg_1342", "para_id": 6158, "text": "lay--Oxford, Blenheim, Warwick, Kenilworth, Birmingham, etc., are"}
{"doc_id": "gutenberg_1342", "para_id": 6159, "text": "sufficiently known. A small part of Derbyshire is all the present"}
{"doc_id": "gutenberg_1342", "para_id": 6160, "text": "concern. To the little town of Lambton, the scene of Mrs. Gardiner’s"}
{"doc_id": "gutenberg_1342", "para_id": 6161, "text": "former residence, and where she had lately learned that some"}
{"doc_id": "gutenberg_1342", "para_id": 6162, "text": "acquaintance still remained, they bent their steps, after having seen"}
{"doc_id": "gutenberg_1342", "para_id": 6163, "text": "all the principal wonders of the country; and within five miles of"}
{"doc_id": "gutenberg_1342", "para_id": 6164, "text": "Lambton, Elizabeth found, from her aunt, that Pemberley was situated. It"}
{"doc_id": "gutenberg_1342", "para_id": 6165, "text": "was not in their direct road; nor more than a mile or two out of it. In"}
{"doc_id": "gutenberg_1342", "para_id": 6166, "text": "talking over their route the evening before, Mrs. Gardiner expressed an"}
{"doc_id": "gutenberg_1342", "para_id": 6167, "text": "inclination to see the place again. Mr. Gardiner declared his"}
{"doc_id": "gutenberg_1342", "para_id": 6168, "text": "willingness, and Elizabeth was applied to for her approbation."}
{"doc_id": "gutenberg_1342", "para_id": 6169, "text": "“My love, should not you like to see a place of which you have heard so"}
{"doc_id": "gutenberg_1342", "para_id": 6170, "text": "much?” said her aunt. “A place, too, with which so many of your"}
{"doc_id": "gutenberg_1342", "para_id": 6171, "text": "acquaintance are connected. Wickham passed all his youth there, you"}
{"doc_id": "gutenberg_1342", "para_id": 6172, "text": "Elizabeth was distressed. She felt that she had no business at"}
{"doc_id": "gutenberg_1342", "para_id": 6173, "text": "Pemberley, and was obliged to assume a disinclination for seeing it. She"}
{"doc_id": "gutenberg_1342", "para_id": 6174, "text": "must own that she was tired of great houses: after going over so many,"}
{"doc_id": "gutenberg_1342", "para_id": 6175, "text": "she really had no pleasure in fine carpets or satin curtains."}
{"doc_id": "gutenberg_1342", "para_id": 6176, "text": "Mrs. Gardiner abused her stupidity. “If it were merely a fine house"}
{"doc_id": "gutenberg_1342", "para_id": 6177, "text": "richly furnished,” said she, “I should not care about it myself; but the"}
{"doc_id": "gutenberg_1342", "para_id": 6178, "text": "grounds are delightful. They have some of the finest woods in the"}
{"doc_id": "gutenberg_1342", "para_id": 6179, "text": "Elizabeth said no more; but her mind could not acquiesce. The"}
{"doc_id": "gutenberg_1342", "para_id": 6180, "text": "possibility of meeting Mr. Darcy, while viewing the place, instantly"}
{"doc_id": "gutenberg_1342", "para_id": 6181, "text": "occurred. It would be dreadful! She blushed at the very idea; and"}
{"doc_id": "gutenberg_1342", "para_id": 6182, "text": "thought it would be better to speak openly to her aunt, than to run such"}
{"doc_id": "gutenberg_1342", "para_id": 6183, "text": "a risk. But against this there were objections; and she finally resolved"}
{"doc_id": "gutenberg_1342", "para_id": 6184, "text": "that it could be the last resource, if her private inquiries as to the"}
{"doc_id": "gutenberg_1342", "para_id": 6185, "text": "Accordingly, when she retired at night, she asked the chambermaid"}
{"doc_id": "gutenberg_1342", "para_id": 6186, "text": "whether Pemberley were not a very fine place, what was the name of its"}
{"doc_id": "gutenberg_1342", "para_id": 6187, "text": "proprietor, and, with no little alarm, whether the family were down for"}
{"doc_id": "gutenberg_1342", "para_id": 6188, "text": "the summer? A most welcome negative followed the last question; and her"}
{"doc_id": "gutenberg_1342", "para_id": 6189, "text": "alarms being now removed, she was at leisure to feel a great deal of"}
{"doc_id": "gutenberg_1342", "para_id": 6190, "text": "curiosity to see the house herself; and when the subject was revived the"}
{"doc_id": "gutenberg_1342", "para_id": 6191, "text": "next morning, and she was again applied to, could readily answer, and"}
{"doc_id": "gutenberg_1342", "para_id": 6192, "text": "with a proper air of indifference, that she had not really any dislike"}
{"doc_id": "gutenberg_1342", "para_id": 6193, "text": "Elizabeth, as they drove along, watched for the first appearance of"}
{"doc_id": "gutenberg_1342", "para_id": 6194, "text": "Pemberley Woods with some perturbation; and when at length they turned"}
{"doc_id": "gutenberg_1342", "para_id": 6195, "text": "in at the lodge, her spirits were in a high flutter."}
{"doc_id": "gutenberg_1342", "para_id": 6196, "text": "The park was very large, and contained great variety of ground. They"}
{"doc_id": "gutenberg_1342", "para_id": 6197, "text": "entered it in one of its lowest points, and drove for some time through"}
{"doc_id": "gutenberg_1342", "para_id": 6198, "text": "Elizabeth’s mind was too full for conversation, but she saw and admired"}
{"doc_id": "gutenberg_1342", "para_id": 6199, "text": "every remarkable spot and point of view. They gradually ascended for"}
{"doc_id": "gutenberg_1342", "para_id": 6200, "text": "half a mile, and then found themselves at the top of a considerable"}
{"doc_id": "gutenberg_1342", "para_id": 6201, "text": "eminence, where the wood ceased, and the eye was instantly caught by"}
{"doc_id": "gutenberg_1342", "para_id": 6202, "text": "Pemberley House, situated on the opposite side of the valley, into which"}
{"doc_id": "gutenberg_1342", "para_id": 6203, "text": "the road with some abruptness wound. It was a large, handsome stone"}
{"doc_id": "gutenberg_1342", "para_id": 6204, "text": "building, standing well on rising ground, and backed by a ridge of high"}
{"doc_id": "gutenberg_1342", "para_id": 6205, "text": "woody hills; and in front a stream of some natural importance was"}
{"doc_id": "gutenberg_1342", "para_id": 6206, "text": "swelled into greater, but without any artificial appearance. Its banks"}
{"doc_id": "gutenberg_1342", "para_id": 6207, "text": "were neither formal nor falsely adorned. Elizabeth was delighted. She"}
{"doc_id": "gutenberg_1342", "para_id": 6208, "text": "had never seen a place for which nature had done more, or where natural"}
{"doc_id": "gutenberg_1342", "para_id": 6209, "text": "beauty had been so little counteracted by an awkward taste. They were"}
{"doc_id": "gutenberg_1342", "para_id": 6210, "text": "all of them warm in their admiration; and at that moment she felt that"}
{"doc_id": "gutenberg_1342", "para_id": 6211, "text": "They descended the hill, crossed the bridge, and drove to the door; and,"}
{"doc_id": "gutenberg_1342", "para_id": 6212, "text": "while examining the nearer aspect of the house, all her apprehension of"}
{"doc_id": "gutenberg_1342", "para_id": 6213, "text": "meeting its owner returned. She dreaded lest the chambermaid had been"}
{"doc_id": "gutenberg_1342", "para_id": 6214, "text": "mistaken. On applying to see the place, they were admitted into the"}
{"doc_id": "gutenberg_1342", "para_id": 6215, "text": "hall; and Elizabeth, as they waited for the housekeeper, had leisure to"}
{"doc_id": "gutenberg_1342", "para_id": 6216, "text": "The housekeeper came; a respectable looking elderly woman, much less"}
{"doc_id": "gutenberg_1342", "para_id": 6217, "text": "fine, and more civil, than she had any notion of finding her. They"}
{"doc_id": "gutenberg_1342", "para_id": 6218, "text": "followed her into the dining-parlour. It was a large, well-proportioned"}
{"doc_id": "gutenberg_1342", "para_id": 6219, "text": "room, handsomely fitted up. Elizabeth, after slightly surveying it, went"}
{"doc_id": "gutenberg_1342", "para_id": 6220, "text": "to a window to enjoy its prospect. The hill, crowned with wood, from"}
{"doc_id": "gutenberg_1342", "para_id": 6221, "text": "which they had descended, receiving increased abruptness from the"}
{"doc_id": "gutenberg_1342", "para_id": 6222, "text": "distance, was a beautiful object. Every disposition of the ground was"}
{"doc_id": "gutenberg_1342", "para_id": 6223, "text": "good; and she looked on the whole scene, the river, the trees scattered"}
{"doc_id": "gutenberg_1342", "para_id": 6224, "text": "on its banks, and the winding of the valley, as far as she could trace"}
{"doc_id": "gutenberg_1342", "para_id": 6225, "text": "it, with delight. As they passed into other rooms, these objects were"}
{"doc_id": "gutenberg_1342", "para_id": 6226, "text": "taking different positions; but from every window there were beauties"}
{"doc_id": "gutenberg_1342", "para_id": 6227, "text": "to be seen. The rooms were lofty and handsome, and their furniture"}
{"doc_id": "gutenberg_1342", "para_id": 6228, "text": "suitable to the fortune of their proprietor; but Elizabeth saw, with"}
{"doc_id": "gutenberg_1342", "para_id": 6229, "text": "admiration of his taste, that it was neither gaudy nor uselessly"}
{"doc_id": "gutenberg_1342", "para_id": 6230, "text": "fine,--with less of splendour, and more real elegance, than the"}
{"doc_id": "gutenberg_1342", "para_id": 6231, "text": "“And of this place,” thought she, “I might have been mistress! With"}
{"doc_id": "gutenberg_1342", "para_id": 6232, "text": "these rooms I might have now been familiarly acquainted! Instead of"}
{"doc_id": "gutenberg_1342", "para_id": 6233, "text": "viewing them as a stranger, I might have rejoiced in them as my own, and"}
{"doc_id": "gutenberg_1342", "para_id": 6234, "text": "welcomed to them as visitors my uncle and aunt. But, no,” recollecting"}
{"doc_id": "gutenberg_1342", "para_id": 6235, "text": "herself, “that could never be; my uncle and aunt would have been lost to"}
{"doc_id": "gutenberg_1342", "para_id": 6236, "text": "me; I should not have been allowed to invite them.”"}
{"doc_id": "gutenberg_1342", "para_id": 6237, "text": "This was a lucky recollection--it saved her from something like regret."}
{"doc_id": "gutenberg_1342", "para_id": 6238, "text": "She longed to inquire of the housekeeper whether her master were really"}
{"doc_id": "gutenberg_1342", "para_id": 6239, "text": "absent, but had not courage for it. At length, however, the question was"}
{"doc_id": "gutenberg_1342", "para_id": 6240, "text": "asked by her uncle; and she turned away with alarm, while Mrs. Reynolds"}
{"doc_id": "gutenberg_1342", "para_id": 6241, "text": "replied, that he was; adding, “But we expect him to-morrow, with a large"}
{"doc_id": "gutenberg_1342", "para_id": 6242, "text": "party of friends.” How rejoiced was Elizabeth that their own journey had"}
{"doc_id": "gutenberg_1342", "para_id": 6243, "text": "Her aunt now called her to look at a picture. She approached, and saw"}
{"doc_id": "gutenberg_1342", "para_id": 6244, "text": "the likeness of Mr. Wickham, suspended, amongst several other"}
{"doc_id": "gutenberg_1342", "para_id": 6245, "text": "miniatures, over the mantel-piece. Her aunt asked her, smilingly, how"}
{"doc_id": "gutenberg_1342", "para_id": 6246, "text": "she liked it. The housekeeper came forward, and told them it was the"}
{"doc_id": "gutenberg_1342", "para_id": 6247, "text": "picture of a young gentleman, the son of her late master’s steward, who"}
{"doc_id": "gutenberg_1342", "para_id": 6248, "text": "had been brought up by him at his own expense. “He is now gone into the"}
{"doc_id": "gutenberg_1342", "para_id": 6249, "text": "army,” she added; “but I am afraid he has turned out very wild.”"}
{"doc_id": "gutenberg_1342", "para_id": 6250, "text": "Mrs. Gardiner looked at her niece with a smile, but Elizabeth could not"}
{"doc_id": "gutenberg_1342", "para_id": 6251, "text": "“And that,” said Mrs. Reynolds, pointing to another of the miniatures,"}
{"doc_id": "gutenberg_1342", "para_id": 6252, "text": "“is my master--and very like him. It was drawn at the same time as the"}
{"doc_id": "gutenberg_1342", "para_id": 6253, "text": "“I have heard much of your master’s fine person,” said Mrs. Gardiner,"}
{"doc_id": "gutenberg_1342", "para_id": 6254, "text": "looking at the picture; “it is a handsome face. But, Lizzy, you can tell"}
{"doc_id": "gutenberg_1342", "para_id": 6255, "text": "Mrs. Reynolds’ respect for Elizabeth seemed to increase on this"}
{"doc_id": "gutenberg_1342", "para_id": 6256, "text": "“And do not you think him a very handsome gentleman, ma’am?”"}
{"doc_id": "gutenberg_1342", "para_id": 6257, "text": "“I am sure _I_ know none so handsome; but in the gallery upstairs you"}
{"doc_id": "gutenberg_1342", "para_id": 6258, "text": "will see a finer, larger picture of him than this. This room was my late"}
{"doc_id": "gutenberg_1342", "para_id": 6259, "text": "master’s favourite room, and these miniatures are just as they used to"}
{"doc_id": "gutenberg_1342", "para_id": 6260, "text": "This accounted to Elizabeth for Mr. Wickham’s being among them."}
{"doc_id": "gutenberg_1342", "para_id": 6261, "text": "Mrs. Reynolds then directed their attention to one of Miss Darcy, drawn"}
{"doc_id": "gutenberg_1342", "para_id": 6262, "text": "“And is Miss Darcy as handsome as her brother?” said Mr. Gardiner."}
{"doc_id": "gutenberg_1342", "para_id": 6263, "text": "“Oh, yes--the handsomest young lady that ever was seen; and so"}
{"doc_id": "gutenberg_1342", "para_id": 6264, "text": "accomplished! She plays and sings all day long. In the next room is a"}
{"doc_id": "gutenberg_1342", "para_id": 6265, "text": "new instrument just come down for her--a present from my master: she"}
{"doc_id": "gutenberg_1342", "para_id": 6266, "text": "Mr. Gardiner, whose manners were easy and pleasant, encouraged her"}
{"doc_id": "gutenberg_1342", "para_id": 6267, "text": "communicativeness by his questions and remarks: Mrs. Reynolds, either"}
{"doc_id": "gutenberg_1342", "para_id": 6268, "text": "from pride or attachment, had evidently great pleasure in talking of her"}
{"doc_id": "gutenberg_1342", "para_id": 6269, "text": "“Is your master much at Pemberley in the course of the year?”"}
{"doc_id": "gutenberg_1342", "para_id": 6270, "text": "“Not so much as I could wish, sir: but I dare say he may spend half his"}
{"doc_id": "gutenberg_1342", "para_id": 6271, "text": "time here; and Miss Darcy is always down for the summer months.”"}
{"doc_id": "gutenberg_1342", "para_id": 6272, "text": "“Except,” thought Elizabeth, “when she goes to Ramsgate.”"}
{"doc_id": "gutenberg_1342", "para_id": 6273, "text": "“If your master would marry, you might see more of him.”"}
{"doc_id": "gutenberg_1342", "para_id": 6274, "text": "“Yes, sir; but I do not know when _that_ will be. I do not know who is"}
{"doc_id": "gutenberg_1342", "para_id": 6275, "text": "Mr. and Mrs. Gardiner smiled. Elizabeth could not help saying, “It is"}
{"doc_id": "gutenberg_1342", "para_id": 6276, "text": "very much to his credit, I am sure, that you should think so.”"}
{"doc_id": "gutenberg_1342", "para_id": 6277, "text": "“I say no more than the truth, and what everybody will say that knows"}
{"doc_id": "gutenberg_1342", "para_id": 6278, "text": "him,” replied the other. Elizabeth thought this was going pretty far;"}
{"doc_id": "gutenberg_1342", "para_id": 6279, "text": "and she listened with increasing astonishment as the housekeeper added,"}
{"doc_id": "gutenberg_1342", "para_id": 6280, "text": "“I have never had a cross word from him in my life, and I have known him"}
{"doc_id": "gutenberg_1342", "para_id": 6281, "text": "This was praise of all others most extraordinary, most opposite to her"}
{"doc_id": "gutenberg_1342", "para_id": 6282, "text": "ideas. That he was not a good-tempered man had been her firmest opinion."}
{"doc_id": "gutenberg_1342", "para_id": 6283, "text": "Her keenest attention was awakened: she longed to hear more; and was"}
{"doc_id": "gutenberg_1342", "para_id": 6284, "text": "“There are very few people of whom so much can be said. You are lucky in"}
{"doc_id": "gutenberg_1342", "para_id": 6285, "text": "“Yes, sir, I know I am. If I were to go through the world, I could not"}
{"doc_id": "gutenberg_1342", "para_id": 6286, "text": "meet with a better. But I have always observed, that they who are"}
{"doc_id": "gutenberg_1342", "para_id": 6287, "text": "good-natured when children, are good-natured when they grow up; and he"}
{"doc_id": "gutenberg_1342", "para_id": 6288, "text": "was always the sweetest tempered, most generous-hearted boy in the"}
{"doc_id": "gutenberg_1342", "para_id": 6289, "text": "Elizabeth almost stared at her. “Can this be Mr. Darcy?” thought she."}
{"doc_id": "gutenberg_1342", "para_id": 6290, "text": "“His father was an excellent man,” said Mrs. Gardiner."}
{"doc_id": "gutenberg_1342", "para_id": 6291, "text": "“Yes, ma’am, that he was indeed; and his son will be just like him--just"}
{"doc_id": "gutenberg_1342", "para_id": 6292, "text": "Elizabeth listened, wondered, doubted, and was impatient for more. Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 6293, "text": "Reynolds could interest her on no other point. She related the subjects"}
{"doc_id": "gutenberg_1342", "para_id": 6294, "text": "of the pictures, the dimensions of the rooms, and the price of the"}
{"doc_id": "gutenberg_1342", "para_id": 6295, "text": "furniture in vain. Mr. Gardiner, highly amused by the kind of family"}
{"doc_id": "gutenberg_1342", "para_id": 6296, "text": "prejudice, to which he attributed her excessive commendation of her"}
{"doc_id": "gutenberg_1342", "para_id": 6297, "text": "master, soon led again to the subject; and she dwelt with energy on his"}
{"doc_id": "gutenberg_1342", "para_id": 6298, "text": "many merits, as they proceeded together up the great staircase."}
{"doc_id": "gutenberg_1342", "para_id": 6299, "text": "“He is the best landlord, and the best master,” said she, “that ever"}
{"doc_id": "gutenberg_1342", "para_id": 6300, "text": "lived. Not like the wild young men now-a-days, who think of nothing but"}
{"doc_id": "gutenberg_1342", "para_id": 6301, "text": "themselves. There is not one of his tenants or servants but what will"}
{"doc_id": "gutenberg_1342", "para_id": 6302, "text": "give him a good name. Some people call him proud; but I am sure I never"}
{"doc_id": "gutenberg_1342", "para_id": 6303, "text": "saw anything of it. To my fancy, it is only because he does not rattle"}
{"doc_id": "gutenberg_1342", "para_id": 6304, "text": "“In what an amiable light does this place him!” thought Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 6305, "text": "“This fine account of him,” whispered her aunt as they walked, “is not"}
{"doc_id": "gutenberg_1342", "para_id": 6306, "text": "quite consistent with his behaviour to our poor friend.”"}
{"doc_id": "gutenberg_1342", "para_id": 6307, "text": "“That is not very likely; our authority was too good.”"}
{"doc_id": "gutenberg_1342", "para_id": 6308, "text": "On reaching the spacious lobby above, they were shown into a very pretty"}
{"doc_id": "gutenberg_1342", "para_id": 6309, "text": "sitting-room, lately fitted up with greater elegance and lightness than"}
{"doc_id": "gutenberg_1342", "para_id": 6310, "text": "the apartments below; and were informed that it was but just done to"}
{"doc_id": "gutenberg_1342", "para_id": 6311, "text": "give pleasure to Miss Darcy, who had taken a liking to the room, when"}
{"doc_id": "gutenberg_1342", "para_id": 6312, "text": "“He is certainly a good brother,” said Elizabeth, as she walked towards"}
{"doc_id": "gutenberg_1342", "para_id": 6313, "text": "Mrs. Reynolds anticipated Miss Darcy’s delight, when she should enter"}
{"doc_id": "gutenberg_1342", "para_id": 6314, "text": "the room. “And this is always the way with him,” she added. “Whatever"}
{"doc_id": "gutenberg_1342", "para_id": 6315, "text": "can give his sister any pleasure, is sure to be done in a moment. There"}
{"doc_id": "gutenberg_1342", "para_id": 6316, "text": "The picture gallery, and two or three of the principal bed-rooms, were"}
{"doc_id": "gutenberg_1342", "para_id": 6317, "text": "all that remained to be shown. In the former were many good paintings:"}
{"doc_id": "gutenberg_1342", "para_id": 6318, "text": "but Elizabeth knew nothing of the art; and from such as had been already"}
{"doc_id": "gutenberg_1342", "para_id": 6319, "text": "visible below, she had willingly turned to look at some drawings of Miss"}
{"doc_id": "gutenberg_1342", "para_id": 6320, "text": "Darcy’s, in crayons, whose subjects were usually more interesting, and"}
{"doc_id": "gutenberg_1342", "para_id": 6321, "text": "In the gallery there were many family portraits, but they could have"}
{"doc_id": "gutenberg_1342", "para_id": 6322, "text": "little to fix the attention of a stranger. Elizabeth walked on in quest"}
{"doc_id": "gutenberg_1342", "para_id": 6323, "text": "of the only face whose features would be known to her. At last it"}
{"doc_id": "gutenberg_1342", "para_id": 6324, "text": "arrested her--and she beheld a striking resemblance of Mr. Darcy, with"}
{"doc_id": "gutenberg_1342", "para_id": 6325, "text": "such a smile over the face, as she remembered to have sometimes seen,"}
{"doc_id": "gutenberg_1342", "para_id": 6326, "text": "when he looked at her. She stood several minutes before the picture, in"}
{"doc_id": "gutenberg_1342", "para_id": 6327, "text": "earnest contemplation, and returned to it again before they quitted the"}
{"doc_id": "gutenberg_1342", "para_id": 6328, "text": "gallery. Mrs. Reynolds informed them, that it had been taken in his"}
{"doc_id": "gutenberg_1342", "para_id": 6329, "text": "There was certainly at this moment, in Elizabeth’s mind, a more gentle"}
{"doc_id": "gutenberg_1342", "para_id": 6330, "text": "sensation towards the original than she had ever felt in the height of"}
{"doc_id": "gutenberg_1342", "para_id": 6331, "text": "their acquaintance. The commendation bestowed on him by Mrs. Reynolds"}
{"doc_id": "gutenberg_1342", "para_id": 6332, "text": "was of no trifling nature. What praise is more valuable than the praise"}
{"doc_id": "gutenberg_1342", "para_id": 6333, "text": "of an intelligent servant? As a brother, a landlord, a master, she"}
{"doc_id": "gutenberg_1342", "para_id": 6334, "text": "considered how many people’s happiness were in his guardianship! How"}
{"doc_id": "gutenberg_1342", "para_id": 6335, "text": "much of pleasure or pain it was in his power to bestow! How much of good"}
{"doc_id": "gutenberg_1342", "para_id": 6336, "text": "or evil must be done by him! Every idea that had been brought forward by"}
{"doc_id": "gutenberg_1342", "para_id": 6337, "text": "the housekeeper was favourable to his character; and as she stood before"}
{"doc_id": "gutenberg_1342", "para_id": 6338, "text": "the canvas, on which he was represented, and fixed his eyes upon"}
{"doc_id": "gutenberg_1342", "para_id": 6339, "text": "herself, she thought of his regard with a deeper sentiment of gratitude"}
{"doc_id": "gutenberg_1342", "para_id": 6340, "text": "than it had ever raised before: she remembered its warmth, and softened"}
{"doc_id": "gutenberg_1342", "para_id": 6341, "text": "When all of the house that was open to general inspection had been seen,"}
{"doc_id": "gutenberg_1342", "para_id": 6342, "text": "they returned down stairs; and, taking leave of the housekeeper, were"}
{"doc_id": "gutenberg_1342", "para_id": 6343, "text": "consigned over to the gardener, who met them at the hall door."}
{"doc_id": "gutenberg_1342", "para_id": 6344, "text": "As they walked across the lawn towards the river, Elizabeth turned back"}
{"doc_id": "gutenberg_1342", "para_id": 6345, "text": "to look again; her uncle and aunt stopped also; and while the former was"}
{"doc_id": "gutenberg_1342", "para_id": 6346, "text": "conjecturing as to the date of the building, the owner of it himself"}
{"doc_id": "gutenberg_1342", "para_id": 6347, "text": "suddenly came forward from the road which led behind it to the stables."}
{"doc_id": "gutenberg_1342", "para_id": 6348, "text": "They were within twenty yards of each other; and so abrupt was his"}
{"doc_id": "gutenberg_1342", "para_id": 6349, "text": "appearance, that it was impossible to avoid his sight. Their eyes"}
{"doc_id": "gutenberg_1342", "para_id": 6350, "text": "instantly met, and the cheeks of each were overspread with the deepest"}
{"doc_id": "gutenberg_1342", "para_id": 6351, "text": "blush. He absolutely started, and for a moment seemed immovable from"}
{"doc_id": "gutenberg_1342", "para_id": 6352, "text": "surprise; but shortly recovering himself, advanced towards the party,"}
{"doc_id": "gutenberg_1342", "para_id": 6353, "text": "and spoke to Elizabeth, if not in terms of perfect composure, at least"}
{"doc_id": "gutenberg_1342", "para_id": 6354, "text": "She had instinctively turned away; but stopping on his approach,"}
{"doc_id": "gutenberg_1342", "para_id": 6355, "text": "received his compliments with an embarrassment impossible to be"}
{"doc_id": "gutenberg_1342", "para_id": 6356, "text": "overcome. Had his first appearance, or his resemblance to the picture"}
{"doc_id": "gutenberg_1342", "para_id": 6357, "text": "they had just been examining, been insufficient to assure the other two"}
{"doc_id": "gutenberg_1342", "para_id": 6358, "text": "that they now saw Mr. Darcy, the gardener’s expression of surprise, on"}
{"doc_id": "gutenberg_1342", "para_id": 6359, "text": "beholding his master, must immediately have told it. They stood a little"}
{"doc_id": "gutenberg_1342", "para_id": 6360, "text": "aloof while he was talking to their niece, who, astonished and confused,"}
{"doc_id": "gutenberg_1342", "para_id": 6361, "text": "scarcely dared lift her eyes to his face, and knew not what answer she"}
{"doc_id": "gutenberg_1342", "para_id": 6362, "text": "returned to his civil inquiries after her family. Amazed at the"}
{"doc_id": "gutenberg_1342", "para_id": 6363, "text": "alteration of his manner since they last parted, every sentence that he"}
{"doc_id": "gutenberg_1342", "para_id": 6364, "text": "uttered was increasing her embarrassment; and every idea of the"}
{"doc_id": "gutenberg_1342", "para_id": 6365, "text": "impropriety of her being found there recurring to her mind, the few"}
{"doc_id": "gutenberg_1342", "para_id": 6366, "text": "minutes in which they continued together were some of the most"}
{"doc_id": "gutenberg_1342", "para_id": 6367, "text": "uncomfortable of her life. Nor did he seem much more at ease; when he"}
{"doc_id": "gutenberg_1342", "para_id": 6368, "text": "spoke, his accent had none of its usual sedateness; and he repeated his"}
{"doc_id": "gutenberg_1342", "para_id": 6369, "text": "inquiries as to the time of her having left Longbourn, and of her stay"}
{"doc_id": "gutenberg_1342", "para_id": 6370, "text": "in Derbyshire, so often, and in so hurried a way, as plainly spoke the"}
{"doc_id": "gutenberg_1342", "para_id": 6371, "text": "At length, every idea seemed to fail him; and after standing a few"}
{"doc_id": "gutenberg_1342", "para_id": 6372, "text": "moments without saying a word, he suddenly recollected himself, and took"}
{"doc_id": "gutenberg_1342", "para_id": 6373, "text": "The others then joined her, and expressed their admiration of his"}
{"doc_id": "gutenberg_1342", "para_id": 6374, "text": "figure; but Elizabeth heard not a word, and, wholly engrossed by her own"}
{"doc_id": "gutenberg_1342", "para_id": 6375, "text": "feelings, followed them in silence. She was overpowered by shame and"}
{"doc_id": "gutenberg_1342", "para_id": 6376, "text": "vexation. Her coming there was the most unfortunate, the most ill-judged"}
{"doc_id": "gutenberg_1342", "para_id": 6377, "text": "thing in the world! How strange must it appear to him! In what a"}
{"doc_id": "gutenberg_1342", "para_id": 6378, "text": "disgraceful light might it not strike so vain a man! It might seem as if"}
{"doc_id": "gutenberg_1342", "para_id": 6379, "text": "she had purposely thrown herself in his way again! Oh! why did she come?"}
{"doc_id": "gutenberg_1342", "para_id": 6380, "text": "or, why did he thus come a day before he was expected? Had they been"}
{"doc_id": "gutenberg_1342", "para_id": 6381, "text": "only ten minutes sooner, they should have been beyond the reach of his"}
{"doc_id": "gutenberg_1342", "para_id": 6382, "text": "discrimination; for it was plain that he was that moment arrived, that"}
{"doc_id": "gutenberg_1342", "para_id": 6383, "text": "moment alighted from his horse or his carriage. She blushed again and"}
{"doc_id": "gutenberg_1342", "para_id": 6384, "text": "again over the perverseness of the meeting. And his behaviour, so"}
{"doc_id": "gutenberg_1342", "para_id": 6385, "text": "strikingly altered,--what could it mean? That he should even speak to"}
{"doc_id": "gutenberg_1342", "para_id": 6386, "text": "her was amazing!--but to speak with such civility, to inquire after her"}
{"doc_id": "gutenberg_1342", "para_id": 6387, "text": "family! Never in her life had she seen his manners so little dignified,"}
{"doc_id": "gutenberg_1342", "para_id": 6388, "text": "never had he spoken with such gentleness as on this unexpected meeting."}
{"doc_id": "gutenberg_1342", "para_id": 6389, "text": "What a contrast did it offer to his last address in Rosings Park, when"}
{"doc_id": "gutenberg_1342", "para_id": 6390, "text": "he put his letter into her hand! She knew not what to think, or how to"}
{"doc_id": "gutenberg_1342", "para_id": 6391, "text": "They had now entered a beautiful walk by the side of the water, and"}
{"doc_id": "gutenberg_1342", "para_id": 6392, "text": "every step was bringing forward a nobler fall of ground, or a finer"}
{"doc_id": "gutenberg_1342", "para_id": 6393, "text": "reach of the woods to which they were approaching: but it was some time"}
{"doc_id": "gutenberg_1342", "para_id": 6394, "text": "before Elizabeth was sensible of any of it; and, though she answered"}
{"doc_id": "gutenberg_1342", "para_id": 6395, "text": "mechanically to the repeated appeals of her uncle and aunt, and seemed"}
{"doc_id": "gutenberg_1342", "para_id": 6396, "text": "to direct her eyes to such objects as they pointed out, she"}
{"doc_id": "gutenberg_1342", "para_id": 6397, "text": "distinguished no part of the scene. Her thoughts were all fixed on that"}
{"doc_id": "gutenberg_1342", "para_id": 6398, "text": "one spot of Pemberley House, whichever it might be, where Mr. Darcy then"}
{"doc_id": "gutenberg_1342", "para_id": 6399, "text": "was. She longed to know what at that moment was passing in his mind; in"}
{"doc_id": "gutenberg_1342", "para_id": 6400, "text": "what manner he thought of her, and whether, in defiance of everything,"}
{"doc_id": "gutenberg_1342", "para_id": 6401, "text": "she was still dear to him. Perhaps he had been civil only because he"}
{"doc_id": "gutenberg_1342", "para_id": 6402, "text": "felt himself at ease; yet there had been _that_ in his voice, which was"}
{"doc_id": "gutenberg_1342", "para_id": 6403, "text": "not like ease. Whether he had felt more of pain or of pleasure in seeing"}
{"doc_id": "gutenberg_1342", "para_id": 6404, "text": "her, she could not tell, but he certainly had not seen her with"}
{"doc_id": "gutenberg_1342", "para_id": 6405, "text": "At length, however, the remarks of her companions on her absence of mind"}
{"doc_id": "gutenberg_1342", "para_id": 6406, "text": "roused her, and she felt the necessity of appearing more like herself."}
{"doc_id": "gutenberg_1342", "para_id": 6407, "text": "They entered the woods, and, bidding adieu to the river for a while,"}
{"doc_id": "gutenberg_1342", "para_id": 6408, "text": "ascended some of the higher grounds; whence, in spots where the opening"}
{"doc_id": "gutenberg_1342", "para_id": 6409, "text": "of the trees gave the eye power to wander, were many charming views of"}
{"doc_id": "gutenberg_1342", "para_id": 6410, "text": "the valley, the opposite hills, with the long range of woods"}
{"doc_id": "gutenberg_1342", "para_id": 6411, "text": "overspreading many, and occasionally part of the stream. Mr. Gardiner"}
{"doc_id": "gutenberg_1342", "para_id": 6412, "text": "expressed a wish of going round the whole park, but feared it might be"}
{"doc_id": "gutenberg_1342", "para_id": 6413, "text": "beyond a walk. With a triumphant smile, they were told, that it was ten"}
{"doc_id": "gutenberg_1342", "para_id": 6414, "text": "miles round. It settled the matter; and they pursued the accustomed"}
{"doc_id": "gutenberg_1342", "para_id": 6415, "text": "circuit; which brought them again, after some time, in a descent among"}
{"doc_id": "gutenberg_1342", "para_id": 6416, "text": "hanging woods, to the edge of the water, and one of its narrowest parts."}
{"doc_id": "gutenberg_1342", "para_id": 6417, "text": "They crossed it by a simple bridge, in character with the general air of"}
{"doc_id": "gutenberg_1342", "para_id": 6418, "text": "the scene: it was a spot less adorned than any they had yet visited; and"}
{"doc_id": "gutenberg_1342", "para_id": 6419, "text": "the valley, here contracted into a glen, allowed room only for the"}
{"doc_id": "gutenberg_1342", "para_id": 6420, "text": "stream, and a narrow walk amidst the rough coppice-wood which bordered"}
{"doc_id": "gutenberg_1342", "para_id": 6421, "text": "it. Elizabeth longed to explore its windings; but when they had crossed"}
{"doc_id": "gutenberg_1342", "para_id": 6422, "text": "the bridge, and perceived their distance from the house, Mrs. Gardiner,"}
{"doc_id": "gutenberg_1342", "para_id": 6423, "text": "who was not a great walker, could go no farther, and thought only of"}
{"doc_id": "gutenberg_1342", "para_id": 6424, "text": "returning to the carriage as quickly as possible. Her niece was,"}
{"doc_id": "gutenberg_1342", "para_id": 6425, "text": "therefore, obliged to submit, and they took their way towards the house"}
{"doc_id": "gutenberg_1342", "para_id": 6426, "text": "on the opposite side of the river, in the nearest direction; but their"}
{"doc_id": "gutenberg_1342", "para_id": 6427, "text": "progress was slow, for Mr. Gardiner, though seldom able to indulge the"}
{"doc_id": "gutenberg_1342", "para_id": 6428, "text": "taste, was very fond of fishing, and was so much engaged in watching the"}
{"doc_id": "gutenberg_1342", "para_id": 6429, "text": "occasional appearance of some trout in the water, and talking to the man"}
{"doc_id": "gutenberg_1342", "para_id": 6430, "text": "about them, that he advanced but little. Whilst wandering on in this"}
{"doc_id": "gutenberg_1342", "para_id": 6431, "text": "slow manner, they were again surprised, and Elizabeth’s astonishment was"}
{"doc_id": "gutenberg_1342", "para_id": 6432, "text": "quite equal to what it had been at first, by the sight of Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 6433, "text": "approaching them, and at no great distance. The walk being here less"}
{"doc_id": "gutenberg_1342", "para_id": 6434, "text": "sheltered than on the other side, allowed them to see him before they"}
{"doc_id": "gutenberg_1342", "para_id": 6435, "text": "met. Elizabeth, however astonished, was at least more prepared for an"}
{"doc_id": "gutenberg_1342", "para_id": 6436, "text": "interview than before, and resolved to appear and to speak with"}
{"doc_id": "gutenberg_1342", "para_id": 6437, "text": "calmness, if he really intended to meet them. For a few moments, indeed,"}
{"doc_id": "gutenberg_1342", "para_id": 6438, "text": "she felt that he would probably strike into some other path. The idea"}
{"doc_id": "gutenberg_1342", "para_id": 6439, "text": "lasted while a turning in the walk concealed him from their view; the"}
{"doc_id": "gutenberg_1342", "para_id": 6440, "text": "turning past, he was immediately before them. With a glance she saw that"}
{"doc_id": "gutenberg_1342", "para_id": 6441, "text": "he had lost none of his recent civility; and, to imitate his politeness,"}
{"doc_id": "gutenberg_1342", "para_id": 6442, "text": "she began as they met to admire the beauty of the place; but she had not"}
{"doc_id": "gutenberg_1342", "para_id": 6443, "text": "got beyond the words “delightful,” and “charming,” when some unlucky"}
{"doc_id": "gutenberg_1342", "para_id": 6444, "text": "recollections obtruded, and she fancied that praise of Pemberley from"}
{"doc_id": "gutenberg_1342", "para_id": 6445, "text": "her might be mischievously construed. Her colour changed, and she said"}
{"doc_id": "gutenberg_1342", "para_id": 6446, "text": "Mrs. Gardiner was standing a little behind; and on her pausing, he asked"}
{"doc_id": "gutenberg_1342", "para_id": 6447, "text": "her if she would do him the honour of introducing him to her friends."}
{"doc_id": "gutenberg_1342", "para_id": 6448, "text": "This was a stroke of civility for which she was quite unprepared; and"}
{"doc_id": "gutenberg_1342", "para_id": 6449, "text": "she could hardly suppress a smile at his being now seeking the"}
{"doc_id": "gutenberg_1342", "para_id": 6450, "text": "acquaintance of some of those very people, against whom his pride had"}
{"doc_id": "gutenberg_1342", "para_id": 6451, "text": "revolted, in his offer to herself. “What will be his surprise,” thought"}
{"doc_id": "gutenberg_1342", "para_id": 6452, "text": "she, “when he knows who they are! He takes them now for people of"}
{"doc_id": "gutenberg_1342", "para_id": 6453, "text": "The introduction, however, was immediately made; and as she named their"}
{"doc_id": "gutenberg_1342", "para_id": 6454, "text": "relationship to herself, she stole a sly look at him, to see how he bore"}
{"doc_id": "gutenberg_1342", "para_id": 6455, "text": "it; and was not without the expectation of his decamping as fast as he"}
{"doc_id": "gutenberg_1342", "para_id": 6456, "text": "could from such disgraceful companions. That he was _surprised_ by the"}
{"doc_id": "gutenberg_1342", "para_id": 6457, "text": "connection was evident: he sustained it, however, with fortitude: and,"}
{"doc_id": "gutenberg_1342", "para_id": 6458, "text": "so far from going away, turned back with them, and entered into"}
{"doc_id": "gutenberg_1342", "para_id": 6459, "text": "conversation with Mr. Gardiner. Elizabeth could not but be pleased,"}
{"doc_id": "gutenberg_1342", "para_id": 6460, "text": "could not but triumph. It was consoling that he should know she had some"}
{"doc_id": "gutenberg_1342", "para_id": 6461, "text": "relations for whom there was no need to blush. She listened most"}
{"doc_id": "gutenberg_1342", "para_id": 6462, "text": "attentively to all that passed between them, and gloried in every"}
{"doc_id": "gutenberg_1342", "para_id": 6463, "text": "expression, every sentence of her uncle, which marked his intelligence,"}
{"doc_id": "gutenberg_1342", "para_id": 6464, "text": "The conversation soon turned upon fishing; and she heard Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 6465, "text": "invite him, with the greatest civility, to fish there as often as he"}
{"doc_id": "gutenberg_1342", "para_id": 6466, "text": "chose, while he continued in the neighbourhood, offering at the same"}
{"doc_id": "gutenberg_1342", "para_id": 6467, "text": "time to supply him with fishing tackle, and pointing out those parts of"}
{"doc_id": "gutenberg_1342", "para_id": 6468, "text": "the stream where there was usually most sport. Mrs. Gardiner, who was"}
{"doc_id": "gutenberg_1342", "para_id": 6469, "text": "walking arm in arm with Elizabeth, gave her a look expressive of her"}
{"doc_id": "gutenberg_1342", "para_id": 6470, "text": "wonder. Elizabeth said nothing, but it gratified her exceedingly; the"}
{"doc_id": "gutenberg_1342", "para_id": 6471, "text": "compliment must be all for herself. Her astonishment, however, was"}
{"doc_id": "gutenberg_1342", "para_id": 6472, "text": "extreme; and continually was she repeating, “Why is he so altered? From"}
{"doc_id": "gutenberg_1342", "para_id": 6473, "text": "what can it proceed? It cannot be for _me_, it cannot be for _my_ sake"}
{"doc_id": "gutenberg_1342", "para_id": 6474, "text": "that his manners are thus softened. My reproofs at Hunsford could not"}
{"doc_id": "gutenberg_1342", "para_id": 6475, "text": "work such a change as this. It is impossible that he should still love"}
{"doc_id": "gutenberg_1342", "para_id": 6476, "text": "After walking some time in this way, the two ladies in front, the two"}
{"doc_id": "gutenberg_1342", "para_id": 6477, "text": "gentlemen behind, on resuming their places, after descending to the"}
{"doc_id": "gutenberg_1342", "para_id": 6478, "text": "brink of the river for the better inspection of some curious"}
{"doc_id": "gutenberg_1342", "para_id": 6479, "text": "water-plant, there chanced to be a little alteration. It originated in"}
{"doc_id": "gutenberg_1342", "para_id": 6480, "text": "Mrs. Gardiner, who, fatigued by the exercise of the morning, found"}
{"doc_id": "gutenberg_1342", "para_id": 6481, "text": "Elizabeth’s arm inadequate to her support, and consequently preferred"}
{"doc_id": "gutenberg_1342", "para_id": 6482, "text": "her husband’s. Mr. Darcy took her place by her niece, and they walked on"}
{"doc_id": "gutenberg_1342", "para_id": 6483, "text": "together. After a short silence the lady first spoke. She wished him to"}
{"doc_id": "gutenberg_1342", "para_id": 6484, "text": "know that she had been assured of his absence before she came to the"}
{"doc_id": "gutenberg_1342", "para_id": 6485, "text": "place, and accordingly began by observing, that his arrival had been"}
{"doc_id": "gutenberg_1342", "para_id": 6486, "text": "very unexpected--“for your housekeeper,” she added, “informed us that"}
{"doc_id": "gutenberg_1342", "para_id": 6487, "text": "you would certainly not be here till to-morrow; and, indeed, before we"}
{"doc_id": "gutenberg_1342", "para_id": 6488, "text": "left Bakewell, we understood that you were not immediately expected in"}
{"doc_id": "gutenberg_1342", "para_id": 6489, "text": "the country.” He acknowledged the truth of it all; and said that"}
{"doc_id": "gutenberg_1342", "para_id": 6490, "text": "business with his steward had occasioned his coming forward a few hours"}
{"doc_id": "gutenberg_1342", "para_id": 6491, "text": "before the rest of the party with whom he had been travelling. “They"}
{"doc_id": "gutenberg_1342", "para_id": 6492, "text": "will join me early to-morrow,” he continued, “and among them are some"}
{"doc_id": "gutenberg_1342", "para_id": 6493, "text": "who will claim an acquaintance with you,--Mr. Bingley and his sisters.”"}
{"doc_id": "gutenberg_1342", "para_id": 6494, "text": "Elizabeth answered only by a slight bow. Her thoughts were instantly"}
{"doc_id": "gutenberg_1342", "para_id": 6495, "text": "driven back to the time when Mr. Bingley’s name had been last mentioned"}
{"doc_id": "gutenberg_1342", "para_id": 6496, "text": "between them; and if she might judge from his complexion, _his_ mind was"}
{"doc_id": "gutenberg_1342", "para_id": 6497, "text": "“There is also one other person in the party,” he continued after a"}
{"doc_id": "gutenberg_1342", "para_id": 6498, "text": "pause, “who more particularly wishes to be known to you. Will you allow"}
{"doc_id": "gutenberg_1342", "para_id": 6499, "text": "me, or do I ask too much, to introduce my sister to your acquaintance"}
{"doc_id": "gutenberg_1342", "para_id": 6500, "text": "The surprise of such an application was great indeed; it was too great"}
{"doc_id": "gutenberg_1342", "para_id": 6501, "text": "for her to know in what manner she acceded to it. She immediately felt"}
{"doc_id": "gutenberg_1342", "para_id": 6502, "text": "that whatever desire Miss Darcy might have of being acquainted with her,"}
{"doc_id": "gutenberg_1342", "para_id": 6503, "text": "must be the work of her brother, and without looking farther, it was"}
{"doc_id": "gutenberg_1342", "para_id": 6504, "text": "satisfactory; it was gratifying to know that his resentment had not made"}
{"doc_id": "gutenberg_1342", "para_id": 6505, "text": "They now walked on in silence; each of them deep in thought. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 6506, "text": "was not comfortable; that was impossible; but she was flattered and"}
{"doc_id": "gutenberg_1342", "para_id": 6507, "text": "pleased. His wish of introducing his sister to her was a compliment of"}
{"doc_id": "gutenberg_1342", "para_id": 6508, "text": "the highest kind. They soon outstripped the others; and when they had"}
{"doc_id": "gutenberg_1342", "para_id": 6509, "text": "reached the carriage, Mr. and Mrs. Gardiner were half a quarter of a"}
{"doc_id": "gutenberg_1342", "para_id": 6510, "text": "He then asked her to walk into the house--but she declared herself not"}
{"doc_id": "gutenberg_1342", "para_id": 6511, "text": "tired, and they stood together on the lawn. At such a time much might"}
{"doc_id": "gutenberg_1342", "para_id": 6512, "text": "have been said, and silence was very awkward. She wanted to talk, but"}
{"doc_id": "gutenberg_1342", "para_id": 6513, "text": "there seemed an embargo on every subject. At last she recollected that"}
{"doc_id": "gutenberg_1342", "para_id": 6514, "text": "she had been travelling, and they talked of Matlock and Dovedale with"}
{"doc_id": "gutenberg_1342", "para_id": 6515, "text": "great perseverance. Yet time and her aunt moved slowly--and her patience"}
{"doc_id": "gutenberg_1342", "para_id": 6516, "text": "and her ideas were nearly worn out before the _tête-à-tête_ was over."}
{"doc_id": "gutenberg_1342", "para_id": 6517, "text": "On Mr. and Mrs. Gardiner’s coming up they were all pressed to go into"}
{"doc_id": "gutenberg_1342", "para_id": 6518, "text": "the house and take some refreshment; but this was declined, and they"}
{"doc_id": "gutenberg_1342", "para_id": 6519, "text": "parted on each side with the utmost politeness. Mr. Darcy handed the"}
{"doc_id": "gutenberg_1342", "para_id": 6520, "text": "ladies into the carriage; and when it drove off, Elizabeth saw him"}
{"doc_id": "gutenberg_1342", "para_id": 6521, "text": "The observations of her uncle and aunt now began; and each of them"}
{"doc_id": "gutenberg_1342", "para_id": 6522, "text": "pronounced him to be infinitely superior to anything they had expected."}
{"doc_id": "gutenberg_1342", "para_id": 6523, "text": "“He is perfectly well-behaved, polite, and unassuming,” said her uncle."}
{"doc_id": "gutenberg_1342", "para_id": 6524, "text": "“There _is_ something a little stately in him, to be sure,” replied her"}
{"doc_id": "gutenberg_1342", "para_id": 6525, "text": "aunt; “but it is confined to his air, and is not unbecoming. I can now"}
{"doc_id": "gutenberg_1342", "para_id": 6526, "text": "say with the housekeeper, that though some people may call him proud,"}
{"doc_id": "gutenberg_1342", "para_id": 6527, "text": "“I was never more surprised than by his behaviour to us. It was more"}
{"doc_id": "gutenberg_1342", "para_id": 6528, "text": "than civil; it was really attentive; and there was no necessity for such"}
{"doc_id": "gutenberg_1342", "para_id": 6529, "text": "attention. His acquaintance with Elizabeth was very trifling.”"}
{"doc_id": "gutenberg_1342", "para_id": 6530, "text": "“To be sure, Lizzy,” said her aunt, “he is not so handsome as Wickham;"}
{"doc_id": "gutenberg_1342", "para_id": 6531, "text": "or rather he has not Wickham’s countenance, for his features are"}
{"doc_id": "gutenberg_1342", "para_id": 6532, "text": "perfectly good. But how came you to tell us that he was so"}
{"doc_id": "gutenberg_1342", "para_id": 6533, "text": "Elizabeth excused herself as well as she could: said that she had liked"}
{"doc_id": "gutenberg_1342", "para_id": 6534, "text": "him better when they met in Kent than before, and that she had never"}
{"doc_id": "gutenberg_1342", "para_id": 6535, "text": "“But perhaps he may be a little whimsical in his civilities,” replied"}
{"doc_id": "gutenberg_1342", "para_id": 6536, "text": "her uncle. “Your great men often are; and therefore I shall not take him"}
{"doc_id": "gutenberg_1342", "para_id": 6537, "text": "at his word about fishing, as he might change his mind another day, and"}
{"doc_id": "gutenberg_1342", "para_id": 6538, "text": "Elizabeth felt that they had entirely mistaken his character, but said"}
{"doc_id": "gutenberg_1342", "para_id": 6539, "text": "“From what we have seen of him,” continued Mrs. Gardiner, “I really"}
{"doc_id": "gutenberg_1342", "para_id": 6540, "text": "should not have thought that he could have behaved in so cruel a way by"}
{"doc_id": "gutenberg_1342", "para_id": 6541, "text": "anybody as he has done by poor Wickham. He has not an ill-natured look."}
{"doc_id": "gutenberg_1342", "para_id": 6542, "text": "On the contrary, there is something pleasing about his mouth when he"}
{"doc_id": "gutenberg_1342", "para_id": 6543, "text": "speaks. And there is something of dignity in his countenance, that would"}
{"doc_id": "gutenberg_1342", "para_id": 6544, "text": "not give one an unfavourable idea of his heart. But, to be sure, the"}
{"doc_id": "gutenberg_1342", "para_id": 6545, "text": "good lady who showed us the house did give him a most flaming character!"}
{"doc_id": "gutenberg_1342", "para_id": 6546, "text": "I could hardly help laughing aloud sometimes. But he is a liberal"}
{"doc_id": "gutenberg_1342", "para_id": 6547, "text": "master, I suppose, and _that_, in the eye of a servant, comprehends"}
{"doc_id": "gutenberg_1342", "para_id": 6548, "text": "Elizabeth here felt herself called on to say something in vindication of"}
{"doc_id": "gutenberg_1342", "para_id": 6549, "text": "his behaviour to Wickham; and, therefore, gave them to understand, in as"}
{"doc_id": "gutenberg_1342", "para_id": 6550, "text": "guarded a manner as she could, that by what she had heard from his"}
{"doc_id": "gutenberg_1342", "para_id": 6551, "text": "relations in Kent, his actions were capable of a very different"}
{"doc_id": "gutenberg_1342", "para_id": 6552, "text": "construction; and that his character was by no means so faulty, nor"}
{"doc_id": "gutenberg_1342", "para_id": 6553, "text": "Wickham’s so amiable, as they had been considered in Hertfordshire. In"}
{"doc_id": "gutenberg_1342", "para_id": 6554, "text": "confirmation of this, she related the particulars of all the pecuniary"}
{"doc_id": "gutenberg_1342", "para_id": 6555, "text": "transactions in which they had been connected, without actually naming"}
{"doc_id": "gutenberg_1342", "para_id": 6556, "text": "her authority, but stating it to be such as might be relied on."}
{"doc_id": "gutenberg_1342", "para_id": 6557, "text": "Mrs. Gardiner was surprised and concerned: but as they were now"}
{"doc_id": "gutenberg_1342", "para_id": 6558, "text": "approaching the scene of her former pleasures, every idea gave way to"}
{"doc_id": "gutenberg_1342", "para_id": 6559, "text": "the charm of recollection; and she was too much engaged in pointing out"}
{"doc_id": "gutenberg_1342", "para_id": 6560, "text": "to her husband all the interesting spots in its environs, to think of"}
{"doc_id": "gutenberg_1342", "para_id": 6561, "text": "anything else. Fatigued as she had been by the morning’s walk, they had"}
{"doc_id": "gutenberg_1342", "para_id": 6562, "text": "no sooner dined than she set off again in quest of her former"}
{"doc_id": "gutenberg_1342", "para_id": 6563, "text": "acquaintance, and the evening was spent in the satisfactions of an"}
{"doc_id": "gutenberg_1342", "para_id": 6564, "text": "intercourse renewed after many years’ discontinuance."}
{"doc_id": "gutenberg_1342", "para_id": 6565, "text": "The occurrences of the day were too full of interest to leave Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 6566, "text": "much attention for any of these new friends; and she could do nothing"}
{"doc_id": "gutenberg_1342", "para_id": 6567, "text": "but think, and think with wonder, of Mr. Darcy’s civility, and, above"}
{"doc_id": "gutenberg_1342", "para_id": 6568, "text": "all, of his wishing her to be acquainted with his sister."}
{"doc_id": "gutenberg_1342", "para_id": 6569, "text": "Elizabeth had settled it that Mr. Darcy would bring his sister to visit"}
{"doc_id": "gutenberg_1342", "para_id": 6570, "text": "her the very day after her reaching Pemberley; and was, consequently,"}
{"doc_id": "gutenberg_1342", "para_id": 6571, "text": "resolved not to be out of sight of the inn the whole of that morning."}
{"doc_id": "gutenberg_1342", "para_id": 6572, "text": "But her conclusion was false; for on the very morning after their own"}
{"doc_id": "gutenberg_1342", "para_id": 6573, "text": "arrival at Lambton these visitors came. They had been walking about the"}
{"doc_id": "gutenberg_1342", "para_id": 6574, "text": "place with some of their new friends, and were just returned to the inn"}
{"doc_id": "gutenberg_1342", "para_id": 6575, "text": "to dress themselves for dining with the same family, when the sound of a"}
{"doc_id": "gutenberg_1342", "para_id": 6576, "text": "carriage drew them to a window, and they saw a gentleman and lady in a"}
{"doc_id": "gutenberg_1342", "para_id": 6577, "text": "curricle driving up the street. Elizabeth, immediately recognizing the"}
{"doc_id": "gutenberg_1342", "para_id": 6578, "text": "livery, guessed what it meant, and imparted no small degree of surprise"}
{"doc_id": "gutenberg_1342", "para_id": 6579, "text": "to her relations, by acquainting them with the honour which she"}
{"doc_id": "gutenberg_1342", "para_id": 6580, "text": "expected. Her uncle and aunt were all amazement; and the embarrassment"}
{"doc_id": "gutenberg_1342", "para_id": 6581, "text": "of her manner as she spoke, joined to the circumstance itself, and many"}
{"doc_id": "gutenberg_1342", "para_id": 6582, "text": "of the circumstances of the preceding day, opened to them a new idea on"}
{"doc_id": "gutenberg_1342", "para_id": 6583, "text": "the business. Nothing had ever suggested it before, but they now felt"}
{"doc_id": "gutenberg_1342", "para_id": 6584, "text": "that there was no other way of accounting for such attentions from such"}
{"doc_id": "gutenberg_1342", "para_id": 6585, "text": "a quarter than by supposing a partiality for their niece. While these"}
{"doc_id": "gutenberg_1342", "para_id": 6586, "text": "newly-born notions were passing in their heads, the perturbation of"}
{"doc_id": "gutenberg_1342", "para_id": 6587, "text": "Elizabeth’s feelings was every moment increasing. She was quite amazed"}
{"doc_id": "gutenberg_1342", "para_id": 6588, "text": "at her own discomposure; but, amongst other causes of disquiet, she"}
{"doc_id": "gutenberg_1342", "para_id": 6589, "text": "dreaded lest the partiality of the brother should have said too much in"}
{"doc_id": "gutenberg_1342", "para_id": 6590, "text": "her favour; and, more than commonly anxious to please, she naturally"}
{"doc_id": "gutenberg_1342", "para_id": 6591, "text": "suspected that every power of pleasing would fail her."}
{"doc_id": "gutenberg_1342", "para_id": 6592, "text": "She retreated from the window, fearful of being seen; and as she walked"}
{"doc_id": "gutenberg_1342", "para_id": 6593, "text": "up and down the room, endeavouring to compose herself, saw such looks of"}
{"doc_id": "gutenberg_1342", "para_id": 6594, "text": "inquiring surprise in her uncle and aunt as made everything worse."}
{"doc_id": "gutenberg_1342", "para_id": 6595, "text": "Miss Darcy and her brother appeared, and this formidable introduction"}
{"doc_id": "gutenberg_1342", "para_id": 6596, "text": "took place. With astonishment did Elizabeth see that her new"}
{"doc_id": "gutenberg_1342", "para_id": 6597, "text": "acquaintance was at least as much embarrassed as herself. Since her"}
{"doc_id": "gutenberg_1342", "para_id": 6598, "text": "being at Lambton, she had heard that Miss Darcy was exceedingly proud;"}
{"doc_id": "gutenberg_1342", "para_id": 6599, "text": "but the observation of a very few minutes convinced her that she was"}
{"doc_id": "gutenberg_1342", "para_id": 6600, "text": "only exceedingly shy. She found it difficult to obtain even a word from"}
{"doc_id": "gutenberg_1342", "para_id": 6601, "text": "Miss Darcy was tall, and on a larger scale than Elizabeth; and, though"}
{"doc_id": "gutenberg_1342", "para_id": 6602, "text": "little more than sixteen, her figure was formed, and her appearance"}
{"doc_id": "gutenberg_1342", "para_id": 6603, "text": "womanly and graceful. She was less handsome than her brother, but there"}
{"doc_id": "gutenberg_1342", "para_id": 6604, "text": "was sense and good-humour in her face, and her manners were perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 6605, "text": "unassuming and gentle. Elizabeth, who had expected to find in her as"}
{"doc_id": "gutenberg_1342", "para_id": 6606, "text": "acute and unembarrassed an observer as ever Mr. Darcy had been, was much"}
{"doc_id": "gutenberg_1342", "para_id": 6607, "text": "They had not been long together before Darcy told her that Bingley was"}
{"doc_id": "gutenberg_1342", "para_id": 6608, "text": "also coming to wait on her; and she had barely time to express her"}
{"doc_id": "gutenberg_1342", "para_id": 6609, "text": "satisfaction, and prepare for such a visitor, when Bingley’s quick step"}
{"doc_id": "gutenberg_1342", "para_id": 6610, "text": "was heard on the stairs, and in a moment he entered the room. All"}
{"doc_id": "gutenberg_1342", "para_id": 6611, "text": "Elizabeth’s anger against him had been long done away; but had she still"}
{"doc_id": "gutenberg_1342", "para_id": 6612, "text": "felt any, it could hardly have stood its ground against the unaffected"}
{"doc_id": "gutenberg_1342", "para_id": 6613, "text": "cordiality with which he expressed himself on seeing her again. He"}
{"doc_id": "gutenberg_1342", "para_id": 6614, "text": "inquired in a friendly, though general, way, after her family, and"}
{"doc_id": "gutenberg_1342", "para_id": 6615, "text": "looked and spoke with the same good-humoured ease that he had ever done."}
{"doc_id": "gutenberg_1342", "para_id": 6616, "text": "To Mr. and Mrs. Gardiner he was scarcely a less interesting personage"}
{"doc_id": "gutenberg_1342", "para_id": 6617, "text": "than to herself. They had long wished to see him. The whole party before"}
{"doc_id": "gutenberg_1342", "para_id": 6618, "text": "them, indeed, excited a lively attention. The suspicions which had just"}
{"doc_id": "gutenberg_1342", "para_id": 6619, "text": "arisen of Mr. Darcy and their niece, directed their observation towards"}
{"doc_id": "gutenberg_1342", "para_id": 6620, "text": "each with an earnest, though guarded, inquiry; and they soon drew from"}
{"doc_id": "gutenberg_1342", "para_id": 6621, "text": "those inquiries the full conviction that one of them at least knew what"}
{"doc_id": "gutenberg_1342", "para_id": 6622, "text": "it was to love. Of the lady’s sensations they remained a little in"}
{"doc_id": "gutenberg_1342", "para_id": 6623, "text": "doubt; but that the gentleman was overflowing with admiration was"}
{"doc_id": "gutenberg_1342", "para_id": 6624, "text": "Elizabeth, on her side, had much to do. She wanted to ascertain the"}
{"doc_id": "gutenberg_1342", "para_id": 6625, "text": "feelings of each of her visitors, she wanted to compose her own, and to"}
{"doc_id": "gutenberg_1342", "para_id": 6626, "text": "make herself agreeable to all; and in the latter object, where she"}
{"doc_id": "gutenberg_1342", "para_id": 6627, "text": "feared most to fail, she was most sure of success, for those to whom"}
{"doc_id": "gutenberg_1342", "para_id": 6628, "text": "she endeavoured to give pleasure were pre-possessed in her favour."}
{"doc_id": "gutenberg_1342", "para_id": 6629, "text": "Bingley was ready, Georgiana was eager, and Darcy determined, to be"}
{"doc_id": "gutenberg_1342", "para_id": 6630, "text": "In seeing Bingley, her thoughts naturally flew to her sister; and oh!"}
{"doc_id": "gutenberg_1342", "para_id": 6631, "text": "how ardently did she long to know whether any of his were directed in a"}
{"doc_id": "gutenberg_1342", "para_id": 6632, "text": "like manner. Sometimes she could fancy that he talked less than on"}
{"doc_id": "gutenberg_1342", "para_id": 6633, "text": "former occasions, and once or twice pleased herself with the notion"}
{"doc_id": "gutenberg_1342", "para_id": 6634, "text": "that, as he looked at her, he was trying to trace a resemblance. But,"}
{"doc_id": "gutenberg_1342", "para_id": 6635, "text": "though this might be imaginary, she could not be deceived as to his"}
{"doc_id": "gutenberg_1342", "para_id": 6636, "text": "behaviour to Miss Darcy, who had been set up as a rival to Jane. No"}
{"doc_id": "gutenberg_1342", "para_id": 6637, "text": "look appeared on either side that spoke particular regard. Nothing"}
{"doc_id": "gutenberg_1342", "para_id": 6638, "text": "occurred between them that could justify the hopes of his sister. On"}
{"doc_id": "gutenberg_1342", "para_id": 6639, "text": "this point she was soon satisfied; and two or three little circumstances"}
{"doc_id": "gutenberg_1342", "para_id": 6640, "text": "occurred ere they parted, which, in her anxious interpretation, denoted"}
{"doc_id": "gutenberg_1342", "para_id": 6641, "text": "a recollection of Jane, not untinctured by tenderness, and a wish of"}
{"doc_id": "gutenberg_1342", "para_id": 6642, "text": "saying more that might lead to the mention of her, had he dared. He"}
{"doc_id": "gutenberg_1342", "para_id": 6643, "text": "observed to her, at a moment when the others were talking together, and"}
{"doc_id": "gutenberg_1342", "para_id": 6644, "text": "in a tone which had something of real regret, that it “was a very long"}
{"doc_id": "gutenberg_1342", "para_id": 6645, "text": "time since he had had the pleasure of seeing her;” and, before she could"}
{"doc_id": "gutenberg_1342", "para_id": 6646, "text": "reply, he added, “It is above eight months. We have not met since the"}
{"doc_id": "gutenberg_1342", "para_id": 6647, "text": "26th of November, when we were all dancing together at Netherfield.”"}
{"doc_id": "gutenberg_1342", "para_id": 6648, "text": "Elizabeth was pleased to find his memory so exact; and he afterwards"}
{"doc_id": "gutenberg_1342", "para_id": 6649, "text": "took occasion to ask her, when unattended to by any of the rest, whether"}
{"doc_id": "gutenberg_1342", "para_id": 6650, "text": "_all_ her sisters were at Longbourn. There was not much in the question,"}
{"doc_id": "gutenberg_1342", "para_id": 6651, "text": "nor in the preceding remark; but there was a look and a manner which"}
{"doc_id": "gutenberg_1342", "para_id": 6652, "text": "It was not often that she could turn her eyes on Mr. Darcy himself; but"}
{"doc_id": "gutenberg_1342", "para_id": 6653, "text": "whenever she did catch a glimpse she saw an expression of general"}
{"doc_id": "gutenberg_1342", "para_id": 6654, "text": "complaisance, and in all that he said, she heard an accent so far"}
{"doc_id": "gutenberg_1342", "para_id": 6655, "text": "removed from _hauteur_ or disdain of his companions, as convinced her"}
{"doc_id": "gutenberg_1342", "para_id": 6656, "text": "that the improvement of manners which she had yesterday witnessed,"}
{"doc_id": "gutenberg_1342", "para_id": 6657, "text": "however temporary its existence might prove, had at least outlived one"}
{"doc_id": "gutenberg_1342", "para_id": 6658, "text": "day. When she saw him thus seeking the acquaintance, and courting the"}
{"doc_id": "gutenberg_1342", "para_id": 6659, "text": "good opinion of people with whom any intercourse a few months ago would"}
{"doc_id": "gutenberg_1342", "para_id": 6660, "text": "have been a disgrace; when she saw him thus civil, not only to herself,"}
{"doc_id": "gutenberg_1342", "para_id": 6661, "text": "but to the very relations whom he had openly disdained, and recollected"}
{"doc_id": "gutenberg_1342", "para_id": 6662, "text": "their last lively scene in Hunsford Parsonage, the difference, the"}
{"doc_id": "gutenberg_1342", "para_id": 6663, "text": "change was so great, and struck so forcibly on her mind, that she could"}
{"doc_id": "gutenberg_1342", "para_id": 6664, "text": "hardly restrain her astonishment from being visible. Never, even in the"}
{"doc_id": "gutenberg_1342", "para_id": 6665, "text": "company of his dear friends at Netherfield, or his dignified relations"}
{"doc_id": "gutenberg_1342", "para_id": 6666, "text": "at Rosings, had she seen him so desirous to please, so free from"}
{"doc_id": "gutenberg_1342", "para_id": 6667, "text": "self-consequence or unbending reserve, as now, when no importance could"}
{"doc_id": "gutenberg_1342", "para_id": 6668, "text": "result from the success of his endeavours, and when even the"}
{"doc_id": "gutenberg_1342", "para_id": 6669, "text": "acquaintance of those to whom his attentions were addressed, would draw"}
{"doc_id": "gutenberg_1342", "para_id": 6670, "text": "down the ridicule and censure of the ladies both of Netherfield and"}
{"doc_id": "gutenberg_1342", "para_id": 6671, "text": "Their visitors stayed with them above half an hour; and when they arose"}
{"doc_id": "gutenberg_1342", "para_id": 6672, "text": "to depart, Mr. Darcy called on his sister to join him in expressing"}
{"doc_id": "gutenberg_1342", "para_id": 6673, "text": "their wish of seeing Mr. and Mrs. Gardiner, and Miss Bennet, to dinner"}
{"doc_id": "gutenberg_1342", "para_id": 6674, "text": "at Pemberley, before they left the country. Miss Darcy, though with a"}
{"doc_id": "gutenberg_1342", "para_id": 6675, "text": "diffidence which marked her little in the habit of giving invitations,"}
{"doc_id": "gutenberg_1342", "para_id": 6676, "text": "readily obeyed. Mrs. Gardiner looked at her niece, desirous of knowing"}
{"doc_id": "gutenberg_1342", "para_id": 6677, "text": "how _she_, whom the invitation most concerned, felt disposed as to its"}
{"doc_id": "gutenberg_1342", "para_id": 6678, "text": "acceptance, but Elizabeth had turned away her head. Presuming, however,"}
{"doc_id": "gutenberg_1342", "para_id": 6679, "text": "that this studied avoidance spoke rather a momentary embarrassment than"}
{"doc_id": "gutenberg_1342", "para_id": 6680, "text": "any dislike of the proposal, and seeing in her husband, who was fond of"}
{"doc_id": "gutenberg_1342", "para_id": 6681, "text": "society, a perfect willingness to accept it, she ventured to engage for"}
{"doc_id": "gutenberg_1342", "para_id": 6682, "text": "her attendance, and the day after the next was fixed on."}
{"doc_id": "gutenberg_1342", "para_id": 6683, "text": "Bingley expressed great pleasure in the certainty of seeing Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 6684, "text": "again, having still a great deal to say to her, and many inquiries to"}
{"doc_id": "gutenberg_1342", "para_id": 6685, "text": "make after all their Hertfordshire friends. Elizabeth, construing all"}
{"doc_id": "gutenberg_1342", "para_id": 6686, "text": "this into a wish of hearing her speak of her sister, was pleased; and"}
{"doc_id": "gutenberg_1342", "para_id": 6687, "text": "on this account, as well as some others, found herself, when their"}
{"doc_id": "gutenberg_1342", "para_id": 6688, "text": "visitors left them, capable of considering the last half hour with some"}
{"doc_id": "gutenberg_1342", "para_id": 6689, "text": "satisfaction, though while it was passing the enjoyment of it had been"}
{"doc_id": "gutenberg_1342", "para_id": 6690, "text": "little. Eager to be alone, and fearful of inquiries or hints from her"}
{"doc_id": "gutenberg_1342", "para_id": 6691, "text": "uncle and aunt, she stayed with them only long enough to hear their"}
{"doc_id": "gutenberg_1342", "para_id": 6692, "text": "favourable opinion of Bingley, and then hurried away to dress."}
{"doc_id": "gutenberg_1342", "para_id": 6693, "text": "But she had no reason to fear Mr. and Mrs. Gardiner’s curiosity; it was"}
{"doc_id": "gutenberg_1342", "para_id": 6694, "text": "not their wish to force her communication. It was evident that she was"}
{"doc_id": "gutenberg_1342", "para_id": 6695, "text": "much better acquainted with Mr. Darcy than they had before any idea of;"}
{"doc_id": "gutenberg_1342", "para_id": 6696, "text": "it was evident that he was very much in love with her. They saw much to"}
{"doc_id": "gutenberg_1342", "para_id": 6697, "text": "Of Mr. Darcy it was now a matter of anxiety to think well; and, as far"}
{"doc_id": "gutenberg_1342", "para_id": 6698, "text": "as their acquaintance reached, there was no fault to find. They could"}
{"doc_id": "gutenberg_1342", "para_id": 6699, "text": "not be untouched by his politeness; and had they drawn his character"}
{"doc_id": "gutenberg_1342", "para_id": 6700, "text": "from their own feelings and his servant’s report, without any reference"}
{"doc_id": "gutenberg_1342", "para_id": 6701, "text": "to any other account, the circle in Hertfordshire to which he was known"}
{"doc_id": "gutenberg_1342", "para_id": 6702, "text": "would not have recognized it for Mr. Darcy. There was now an interest,"}
{"doc_id": "gutenberg_1342", "para_id": 6703, "text": "however, in believing the housekeeper; and they soon became sensible"}
{"doc_id": "gutenberg_1342", "para_id": 6704, "text": "that the authority of a servant, who had known him since he was four"}
{"doc_id": "gutenberg_1342", "para_id": 6705, "text": "years old, and whose own manners indicated respectability, was not to be"}
{"doc_id": "gutenberg_1342", "para_id": 6706, "text": "hastily rejected. Neither had anything occurred in the intelligence of"}
{"doc_id": "gutenberg_1342", "para_id": 6707, "text": "their Lambton friends that could materially lessen its weight. They had"}
{"doc_id": "gutenberg_1342", "para_id": 6708, "text": "nothing to accuse him of but pride; pride he probably had, and if not,"}
{"doc_id": "gutenberg_1342", "para_id": 6709, "text": "it would certainly be imputed by the inhabitants of a small market town"}
{"doc_id": "gutenberg_1342", "para_id": 6710, "text": "where the family did not visit. It was acknowledged, however, that he"}
{"doc_id": "gutenberg_1342", "para_id": 6711, "text": "was a liberal man, and did much good among the poor."}
{"doc_id": "gutenberg_1342", "para_id": 6712, "text": "With respect to Wickham, the travellers soon found that he was not held"}
{"doc_id": "gutenberg_1342", "para_id": 6713, "text": "there in much estimation; for though the chief of his concerns with the"}
{"doc_id": "gutenberg_1342", "para_id": 6714, "text": "son of his patron were imperfectly understood, it was yet a well-known"}
{"doc_id": "gutenberg_1342", "para_id": 6715, "text": "fact that, on his quitting Derbyshire, he had left many debts behind"}
{"doc_id": "gutenberg_1342", "para_id": 6716, "text": "As for Elizabeth, her thoughts were at Pemberley this evening more than"}
{"doc_id": "gutenberg_1342", "para_id": 6717, "text": "the last; and the evening, though as it passed it seemed long, was not"}
{"doc_id": "gutenberg_1342", "para_id": 6718, "text": "long enough to determine her feelings towards _one_ in that mansion; and"}
{"doc_id": "gutenberg_1342", "para_id": 6719, "text": "she lay awake two whole hours, endeavouring to make them out. She"}
{"doc_id": "gutenberg_1342", "para_id": 6720, "text": "certainly did not hate him. No; hatred had vanished long ago, and she"}
{"doc_id": "gutenberg_1342", "para_id": 6721, "text": "had almost as long been ashamed of ever feeling a dislike against him,"}
{"doc_id": "gutenberg_1342", "para_id": 6722, "text": "that could be so called. The respect created by the conviction of his"}
{"doc_id": "gutenberg_1342", "para_id": 6723, "text": "valuable qualities, though at first unwillingly admitted, had for some"}
{"doc_id": "gutenberg_1342", "para_id": 6724, "text": "time ceased to be repugnant to her feelings; and it was now heightened"}
{"doc_id": "gutenberg_1342", "para_id": 6725, "text": "into somewhat of a friendlier nature by the testimony so highly in his"}
{"doc_id": "gutenberg_1342", "para_id": 6726, "text": "favour, and bringing forward his disposition in so amiable a light,"}
{"doc_id": "gutenberg_1342", "para_id": 6727, "text": "which yesterday had produced. But above all, above respect and esteem,"}
{"doc_id": "gutenberg_1342", "para_id": 6728, "text": "there was a motive within her of good-will which could not be"}
{"doc_id": "gutenberg_1342", "para_id": 6729, "text": "overlooked. It was gratitude;--gratitude, not merely for having once"}
{"doc_id": "gutenberg_1342", "para_id": 6730, "text": "loved her, but for loving her still well enough to forgive all the"}
{"doc_id": "gutenberg_1342", "para_id": 6731, "text": "petulance and acrimony of her manner in rejecting him, and all the"}
{"doc_id": "gutenberg_1342", "para_id": 6732, "text": "unjust accusations accompanying her rejection. He who, she had been"}
{"doc_id": "gutenberg_1342", "para_id": 6733, "text": "persuaded, would avoid her as his greatest enemy, seemed, on this"}
{"doc_id": "gutenberg_1342", "para_id": 6734, "text": "accidental meeting, most eager to preserve the acquaintance; and"}
{"doc_id": "gutenberg_1342", "para_id": 6735, "text": "without any indelicate display of regard, or any peculiarity of manner,"}
{"doc_id": "gutenberg_1342", "para_id": 6736, "text": "where their two selves only were concerned, was soliciting the good"}
{"doc_id": "gutenberg_1342", "para_id": 6737, "text": "opinion of her friends, and bent on making her known to his sister. Such"}
{"doc_id": "gutenberg_1342", "para_id": 6738, "text": "a change in a man of so much pride excited not only astonishment but"}
{"doc_id": "gutenberg_1342", "para_id": 6739, "text": "gratitude--for to love, ardent love, it must be attributed; and, as"}
{"doc_id": "gutenberg_1342", "para_id": 6740, "text": "such, its impression on her was of a sort to be encouraged, as by no"}
{"doc_id": "gutenberg_1342", "para_id": 6741, "text": "means unpleasing, though it could not be exactly defined. She respected,"}
{"doc_id": "gutenberg_1342", "para_id": 6742, "text": "she esteemed, she was grateful to him, she felt a real interest in his"}
{"doc_id": "gutenberg_1342", "para_id": 6743, "text": "welfare; and she only wanted to know how far she wished that welfare to"}
{"doc_id": "gutenberg_1342", "para_id": 6744, "text": "depend upon herself, and how far it would be for the happiness of both"}
{"doc_id": "gutenberg_1342", "para_id": 6745, "text": "that she should employ the power, which her fancy told her she still"}
{"doc_id": "gutenberg_1342", "para_id": 6746, "text": "possessed, of bringing on the renewal of his addresses."}
{"doc_id": "gutenberg_1342", "para_id": 6747, "text": "It had been settled in the evening, between the aunt and niece, that"}
{"doc_id": "gutenberg_1342", "para_id": 6748, "text": "such a striking civility as Miss Darcy’s, in coming to them on the very"}
{"doc_id": "gutenberg_1342", "para_id": 6749, "text": "day of her arrival at Pemberley--for she had reached it only to a late"}
{"doc_id": "gutenberg_1342", "para_id": 6750, "text": "breakfast--ought to be imitated, though it could not be equalled, by"}
{"doc_id": "gutenberg_1342", "para_id": 6751, "text": "some exertion of politeness on their side; and, consequently, that it"}
{"doc_id": "gutenberg_1342", "para_id": 6752, "text": "would be highly expedient to wait on her at Pemberley the following"}
{"doc_id": "gutenberg_1342", "para_id": 6753, "text": "morning. They were, therefore, to go. Elizabeth was pleased; though when"}
{"doc_id": "gutenberg_1342", "para_id": 6754, "text": "she asked herself the reason, she had very little to say in reply."}
{"doc_id": "gutenberg_1342", "para_id": 6755, "text": "Mr. Gardiner left them soon after breakfast. The fishing scheme had been"}
{"doc_id": "gutenberg_1342", "para_id": 6756, "text": "renewed the day before, and a positive engagement made of his meeting"}
{"doc_id": "gutenberg_1342", "para_id": 6757, "text": "Convinced as Elizabeth now was that Miss Bingley’s dislike of her had"}
{"doc_id": "gutenberg_1342", "para_id": 6758, "text": "originated in jealousy, she could not help feeling how very unwelcome"}
{"doc_id": "gutenberg_1342", "para_id": 6759, "text": "her appearance at Pemberley must be to her, and was curious to know"}
{"doc_id": "gutenberg_1342", "para_id": 6760, "text": "with how much civility on that lady’s side the acquaintance would now"}
{"doc_id": "gutenberg_1342", "para_id": 6761, "text": "On reaching the house, they were shown through the hall into the saloon,"}
{"doc_id": "gutenberg_1342", "para_id": 6762, "text": "whose northern aspect rendered it delightful for summer. Its windows,"}
{"doc_id": "gutenberg_1342", "para_id": 6763, "text": "opening to the ground, admitted a most refreshing view of the high woody"}
{"doc_id": "gutenberg_1342", "para_id": 6764, "text": "hills behind the house, and of the beautiful oaks and Spanish chestnuts"}
{"doc_id": "gutenberg_1342", "para_id": 6765, "text": "In this room they were received by Miss Darcy, who was sitting there"}
{"doc_id": "gutenberg_1342", "para_id": 6766, "text": "with Mrs. Hurst and Miss Bingley, and the lady with whom she lived in"}
{"doc_id": "gutenberg_1342", "para_id": 6767, "text": "London. Georgiana’s reception of them was very civil, but attended with"}
{"doc_id": "gutenberg_1342", "para_id": 6768, "text": "all that embarrassment which, though proceeding from shyness and the"}
{"doc_id": "gutenberg_1342", "para_id": 6769, "text": "fear of doing wrong, would easily give to those who felt themselves"}
{"doc_id": "gutenberg_1342", "para_id": 6770, "text": "inferior the belief of her being proud and reserved. Mrs. Gardiner and"}
{"doc_id": "gutenberg_1342", "para_id": 6771, "text": "her niece, however, did her justice, and pitied her."}
{"doc_id": "gutenberg_1342", "para_id": 6772, "text": "By Mrs. Hurst and Miss Bingley they were noticed only by a courtesy; and"}
{"doc_id": "gutenberg_1342", "para_id": 6773, "text": "on their being seated, a pause, awkward as such pauses must always be,"}
{"doc_id": "gutenberg_1342", "para_id": 6774, "text": "succeeded for a few moments. It was first broken by Mrs. Annesley, a"}
{"doc_id": "gutenberg_1342", "para_id": 6775, "text": "genteel, agreeable-looking woman, whose endeavour to introduce some kind"}
{"doc_id": "gutenberg_1342", "para_id": 6776, "text": "of discourse proved her to be more truly well-bred than either of the"}
{"doc_id": "gutenberg_1342", "para_id": 6777, "text": "others; and between her and Mrs. Gardiner, with occasional help from"}
{"doc_id": "gutenberg_1342", "para_id": 6778, "text": "Elizabeth, the conversation was carried on. Miss Darcy looked as if she"}
{"doc_id": "gutenberg_1342", "para_id": 6779, "text": "wished for courage enough to join in it; and sometimes did venture a"}
{"doc_id": "gutenberg_1342", "para_id": 6780, "text": "short sentence, when there was least danger of its being heard."}
{"doc_id": "gutenberg_1342", "para_id": 6781, "text": "Elizabeth soon saw that she was herself closely watched by Miss Bingley,"}
{"doc_id": "gutenberg_1342", "para_id": 6782, "text": "and that she could not speak a word, especially to Miss Darcy, without"}
{"doc_id": "gutenberg_1342", "para_id": 6783, "text": "calling her attention. This observation would not have prevented her"}
{"doc_id": "gutenberg_1342", "para_id": 6784, "text": "from trying to talk to the latter, had they not been seated at an"}
{"doc_id": "gutenberg_1342", "para_id": 6785, "text": "inconvenient distance; but she was not sorry to be spared the necessity"}
{"doc_id": "gutenberg_1342", "para_id": 6786, "text": "of saying much: her own thoughts were employing her. She expected every"}
{"doc_id": "gutenberg_1342", "para_id": 6787, "text": "moment that some of the gentlemen would enter the room: she wished, she"}
{"doc_id": "gutenberg_1342", "para_id": 6788, "text": "feared, that the master of the house might be amongst them; and whether"}
{"doc_id": "gutenberg_1342", "para_id": 6789, "text": "she wished or feared it most, she could scarcely determine. After"}
{"doc_id": "gutenberg_1342", "para_id": 6790, "text": "sitting in this manner a quarter of an hour, without hearing Miss"}
{"doc_id": "gutenberg_1342", "para_id": 6791, "text": "Bingley’s voice, Elizabeth was roused by receiving from her a cold"}
{"doc_id": "gutenberg_1342", "para_id": 6792, "text": "inquiry after the health of her family. She answered with equal"}
{"doc_id": "gutenberg_1342", "para_id": 6793, "text": "indifference and brevity, and the other said no more."}
{"doc_id": "gutenberg_1342", "para_id": 6794, "text": "The next variation which their visit afforded was produced by the"}
{"doc_id": "gutenberg_1342", "para_id": 6795, "text": "entrance of servants with cold meat, cake, and a variety of all the"}
{"doc_id": "gutenberg_1342", "para_id": 6796, "text": "finest fruits in season; but this did not take place till after many a"}
{"doc_id": "gutenberg_1342", "para_id": 6797, "text": "significant look and smile from Mrs. Annesley to Miss Darcy had been"}
{"doc_id": "gutenberg_1342", "para_id": 6798, "text": "given, to remind her of her post. There was now employment for the whole"}
{"doc_id": "gutenberg_1342", "para_id": 6799, "text": "party; for though they could not all talk, they could all eat; and the"}
{"doc_id": "gutenberg_1342", "para_id": 6800, "text": "beautiful pyramids of grapes, nectarines, and peaches, soon collected"}
{"doc_id": "gutenberg_1342", "para_id": 6801, "text": "While thus engaged, Elizabeth had a fair opportunity of deciding whether"}
{"doc_id": "gutenberg_1342", "para_id": 6802, "text": "she most feared or wished for the appearance of Mr. Darcy, by the"}
{"doc_id": "gutenberg_1342", "para_id": 6803, "text": "feelings which prevailed on his entering the room; and then, though but"}
{"doc_id": "gutenberg_1342", "para_id": 6804, "text": "a moment before she had believed her wishes to predominate, she began to"}
{"doc_id": "gutenberg_1342", "para_id": 6805, "text": "He had been some time with Mr. Gardiner, who, with two or three other"}
{"doc_id": "gutenberg_1342", "para_id": 6806, "text": "gentlemen from the house, was engaged by the river; and had left him"}
{"doc_id": "gutenberg_1342", "para_id": 6807, "text": "only on learning that the ladies of the family intended a visit to"}
{"doc_id": "gutenberg_1342", "para_id": 6808, "text": "Georgiana that morning. No sooner did he appear, than Elizabeth wisely"}
{"doc_id": "gutenberg_1342", "para_id": 6809, "text": "resolved to be perfectly easy and unembarrassed;--a resolution the more"}
{"doc_id": "gutenberg_1342", "para_id": 6810, "text": "necessary to be made, but perhaps not the more easily kept, because she"}
{"doc_id": "gutenberg_1342", "para_id": 6811, "text": "saw that the suspicions of the whole party were awakened against them,"}
{"doc_id": "gutenberg_1342", "para_id": 6812, "text": "and that there was scarcely an eye which did not watch his behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 6813, "text": "when he first came into the room. In no countenance was attentive"}
{"doc_id": "gutenberg_1342", "para_id": 6814, "text": "curiosity so strongly marked as in Miss Bingley’s, in spite of the"}
{"doc_id": "gutenberg_1342", "para_id": 6815, "text": "smiles which overspread her face whenever she spoke to one of its"}
{"doc_id": "gutenberg_1342", "para_id": 6816, "text": "objects; for jealousy had not yet made her desperate, and her attentions"}
{"doc_id": "gutenberg_1342", "para_id": 6817, "text": "to Mr. Darcy were by no means over. Miss Darcy, on her brother’s"}
{"doc_id": "gutenberg_1342", "para_id": 6818, "text": "entrance, exerted herself much more to talk; and Elizabeth saw that he"}
{"doc_id": "gutenberg_1342", "para_id": 6819, "text": "was anxious for his sister and herself to get acquainted, and forwarded,"}
{"doc_id": "gutenberg_1342", "para_id": 6820, "text": "as much as possible, every attempt at conversation on either side. Miss"}
{"doc_id": "gutenberg_1342", "para_id": 6821, "text": "Bingley saw all this likewise; and, in the imprudence of anger, took the"}
{"doc_id": "gutenberg_1342", "para_id": 6822, "text": "first opportunity of saying, with sneering civility,--"}
{"doc_id": "gutenberg_1342", "para_id": 6823, "text": "“Pray, Miss Eliza, are not the ----shire militia removed from Meryton?"}
{"doc_id": "gutenberg_1342", "para_id": 6824, "text": "In Darcy’s presence she dared not mention Wickham’s name: but Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 6825, "text": "instantly comprehended that he was uppermost in her thoughts; and the"}
{"doc_id": "gutenberg_1342", "para_id": 6826, "text": "various recollections connected with him gave her a moment’s distress;"}
{"doc_id": "gutenberg_1342", "para_id": 6827, "text": "but, exerting herself vigorously to repel the ill-natured attack, she"}
{"doc_id": "gutenberg_1342", "para_id": 6828, "text": "presently answered the question in a tolerably disengaged tone. While"}
{"doc_id": "gutenberg_1342", "para_id": 6829, "text": "she spoke, an involuntary glance showed her Darcy with a heightened"}
{"doc_id": "gutenberg_1342", "para_id": 6830, "text": "complexion, earnestly looking at her, and his sister overcome with"}
{"doc_id": "gutenberg_1342", "para_id": 6831, "text": "confusion, and unable to lift up her eyes. Had Miss Bingley known what"}
{"doc_id": "gutenberg_1342", "para_id": 6832, "text": "pain she was then giving her beloved friend, she undoubtedly would have"}
{"doc_id": "gutenberg_1342", "para_id": 6833, "text": "refrained from the hint; but she had merely intended to discompose"}
{"doc_id": "gutenberg_1342", "para_id": 6834, "text": "Elizabeth, by bringing forward the idea of a man to whom she believed"}
{"doc_id": "gutenberg_1342", "para_id": 6835, "text": "her partial, to make her betray a sensibility which might injure her in"}
{"doc_id": "gutenberg_1342", "para_id": 6836, "text": "Darcy’s opinion, and, perhaps, to remind the latter of all the follies"}
{"doc_id": "gutenberg_1342", "para_id": 6837, "text": "and absurdities by which some part of her family were connected with"}
{"doc_id": "gutenberg_1342", "para_id": 6838, "text": "that corps. Not a syllable had ever reached her of Miss Darcy’s"}
{"doc_id": "gutenberg_1342", "para_id": 6839, "text": "meditated elopement. To no creature had it been revealed, where secrecy"}
{"doc_id": "gutenberg_1342", "para_id": 6840, "text": "was possible, except to Elizabeth; and from all Bingley’s connections"}
{"doc_id": "gutenberg_1342", "para_id": 6841, "text": "her brother was particularly anxious to conceal it, from that very wish"}
{"doc_id": "gutenberg_1342", "para_id": 6842, "text": "which Elizabeth had long ago attributed to him, of their becoming"}
{"doc_id": "gutenberg_1342", "para_id": 6843, "text": "hereafter her own. He had certainly formed such a plan; and without"}
{"doc_id": "gutenberg_1342", "para_id": 6844, "text": "meaning that it should affect his endeavour to separate him from Miss"}
{"doc_id": "gutenberg_1342", "para_id": 6845, "text": "Bennet, it is probable that it might add something to his lively concern"}
{"doc_id": "gutenberg_1342", "para_id": 6846, "text": "Elizabeth’s collected behaviour, however, soon quieted his emotion; and"}
{"doc_id": "gutenberg_1342", "para_id": 6847, "text": "as Miss Bingley, vexed and disappointed, dared not approach nearer to"}
{"doc_id": "gutenberg_1342", "para_id": 6848, "text": "Wickham, Georgiana also recovered in time, though not enough to be able"}
{"doc_id": "gutenberg_1342", "para_id": 6849, "text": "to speak any more. Her brother, whose eye she feared to meet, scarcely"}
{"doc_id": "gutenberg_1342", "para_id": 6850, "text": "recollected her interest in the affair; and the very circumstance which"}
{"doc_id": "gutenberg_1342", "para_id": 6851, "text": "had been designed to turn his thoughts from Elizabeth, seemed to have"}
{"doc_id": "gutenberg_1342", "para_id": 6852, "text": "Their visit did not continue long after the question and answer above"}
{"doc_id": "gutenberg_1342", "para_id": 6853, "text": "mentioned; and while Mr. Darcy was attending them to their carriage,"}
{"doc_id": "gutenberg_1342", "para_id": 6854, "text": "Miss Bingley was venting her feelings in criticisms on Elizabeth’s"}
{"doc_id": "gutenberg_1342", "para_id": 6855, "text": "person, behaviour, and dress. But Georgiana would not join her. Her"}
{"doc_id": "gutenberg_1342", "para_id": 6856, "text": "brother’s recommendation was enough to insure her favour: his judgment"}
{"doc_id": "gutenberg_1342", "para_id": 6857, "text": "could not err; and he had spoken in such terms of Elizabeth, as to leave"}
{"doc_id": "gutenberg_1342", "para_id": 6858, "text": "Georgiana without the power of finding her otherwise than lovely and"}
{"doc_id": "gutenberg_1342", "para_id": 6859, "text": "amiable. When Darcy returned to the saloon, Miss Bingley could not help"}
{"doc_id": "gutenberg_1342", "para_id": 6860, "text": "repeating to him some part of what she had been saying to his sister."}
{"doc_id": "gutenberg_1342", "para_id": 6861, "text": "“How very ill Eliza Bennet looks this morning, Mr. Darcy,” she cried: “I"}
{"doc_id": "gutenberg_1342", "para_id": 6862, "text": "never in my life saw anyone so much altered as she is since the winter."}
{"doc_id": "gutenberg_1342", "para_id": 6863, "text": "She is grown so brown and coarse! Louisa and I were agreeing that we"}
{"doc_id": "gutenberg_1342", "para_id": 6864, "text": "However little Mr. Darcy might have liked such an address, he contented"}
{"doc_id": "gutenberg_1342", "para_id": 6865, "text": "himself with coolly replying, that he perceived no other alteration than"}
{"doc_id": "gutenberg_1342", "para_id": 6866, "text": "her being rather tanned,--no miraculous consequence of travelling in the"}
{"doc_id": "gutenberg_1342", "para_id": 6867, "text": "“For my own part,” she rejoined, “I must confess that I never could see"}
{"doc_id": "gutenberg_1342", "para_id": 6868, "text": "any beauty in her. Her face is too thin; her complexion has no"}
{"doc_id": "gutenberg_1342", "para_id": 6869, "text": "brilliancy; and her features are not at all handsome. Her nose wants"}
{"doc_id": "gutenberg_1342", "para_id": 6870, "text": "character; there is nothing marked in its lines. Her teeth are"}
{"doc_id": "gutenberg_1342", "para_id": 6871, "text": "tolerable, but not out of the common way; and as for her eyes, which"}
{"doc_id": "gutenberg_1342", "para_id": 6872, "text": "have sometimes been called so fine, I never could perceive anything"}
{"doc_id": "gutenberg_1342", "para_id": 6873, "text": "extraordinary in them. They have a sharp, shrewish look, which I do not"}
{"doc_id": "gutenberg_1342", "para_id": 6874, "text": "like at all; and in her air altogether, there is a self-sufficiency"}
{"doc_id": "gutenberg_1342", "para_id": 6875, "text": "Persuaded as Miss Bingley was that Darcy admired Elizabeth, this was not"}
{"doc_id": "gutenberg_1342", "para_id": 6876, "text": "the best method of recommending herself; but angry people are not always"}
{"doc_id": "gutenberg_1342", "para_id": 6877, "text": "wise; and in seeing him at last look somewhat nettled, she had all the"}
{"doc_id": "gutenberg_1342", "para_id": 6878, "text": "success she expected. He was resolutely silent, however; and, from a"}
{"doc_id": "gutenberg_1342", "para_id": 6879, "text": "determination of making him speak, she continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 6880, "text": "“I remember, when we first knew her in Hertfordshire, how amazed we all"}
{"doc_id": "gutenberg_1342", "para_id": 6881, "text": "were to find that she was a reputed beauty; and I particularly recollect"}
{"doc_id": "gutenberg_1342", "para_id": 6882, "text": "your saying one night, after they had been dining at Netherfield, ‘_She_"}
{"doc_id": "gutenberg_1342", "para_id": 6883, "text": "a beauty! I should as soon call her mother a wit.’ But afterwards she"}
{"doc_id": "gutenberg_1342", "para_id": 6884, "text": "seemed to improve on you, and I believe you thought her rather pretty at"}
{"doc_id": "gutenberg_1342", "para_id": 6885, "text": "“Yes,” replied Darcy, who could contain himself no longer, “but _that_"}
{"doc_id": "gutenberg_1342", "para_id": 6886, "text": "was only when I first knew her; for it is many months since I have"}
{"doc_id": "gutenberg_1342", "para_id": 6887, "text": "considered her as one of the handsomest women of my acquaintance.”"}
{"doc_id": "gutenberg_1342", "para_id": 6888, "text": "He then went away, and Miss Bingley was left to all the satisfaction of"}
{"doc_id": "gutenberg_1342", "para_id": 6889, "text": "having forced him to say what gave no one any pain but herself."}
{"doc_id": "gutenberg_1342", "para_id": 6890, "text": "Mrs. Gardiner and Elizabeth talked of all that had occurred during their"}
{"doc_id": "gutenberg_1342", "para_id": 6891, "text": "visit, as they returned, except what had particularly interested them"}
{"doc_id": "gutenberg_1342", "para_id": 6892, "text": "both. The looks and behaviour of everybody they had seen were discussed,"}
{"doc_id": "gutenberg_1342", "para_id": 6893, "text": "except of the person who had mostly engaged their attention. They talked"}
{"doc_id": "gutenberg_1342", "para_id": 6894, "text": "of his sister, his friends, his house, his fruit, of everything but"}
{"doc_id": "gutenberg_1342", "para_id": 6895, "text": "himself; yet Elizabeth was longing to know what Mrs. Gardiner thought of"}
{"doc_id": "gutenberg_1342", "para_id": 6896, "text": "him, and Mrs. Gardiner would have been highly gratified by her niece’s"}
{"doc_id": "gutenberg_1342", "para_id": 6897, "text": "Elizabeth had been a good deal disappointed in not finding a letter from"}
{"doc_id": "gutenberg_1342", "para_id": 6898, "text": "Jane on their first arrival at Lambton; and this disappointment had been"}
{"doc_id": "gutenberg_1342", "para_id": 6899, "text": "renewed on each of the mornings that had now been spent there; but on"}
{"doc_id": "gutenberg_1342", "para_id": 6900, "text": "the third her repining was over, and her sister justified, by the"}
{"doc_id": "gutenberg_1342", "para_id": 6901, "text": "receipt of two letters from her at once, on one of which was marked that"}
{"doc_id": "gutenberg_1342", "para_id": 6902, "text": "it had been mis-sent elsewhere. Elizabeth was not surprised at it, as"}
{"doc_id": "gutenberg_1342", "para_id": 6903, "text": "They had just been preparing to walk as the letters came in; and her"}
{"doc_id": "gutenberg_1342", "para_id": 6904, "text": "uncle and aunt, leaving her to enjoy them in quiet, set off by"}
{"doc_id": "gutenberg_1342", "para_id": 6905, "text": "themselves. The one mis-sent must be first attended to; it had been"}
{"doc_id": "gutenberg_1342", "para_id": 6906, "text": "written five days ago. The beginning contained an account of all their"}
{"doc_id": "gutenberg_1342", "para_id": 6907, "text": "little parties and engagements, with such news as the country afforded;"}
{"doc_id": "gutenberg_1342", "para_id": 6908, "text": "but the latter half, which was dated a day later, and written in evident"}
{"doc_id": "gutenberg_1342", "para_id": 6909, "text": "agitation, gave more important intelligence. It was to this effect:--"}
{"doc_id": "gutenberg_1342", "para_id": 6910, "text": "“Since writing the above, dearest Lizzy, something has occurred of a"}
{"doc_id": "gutenberg_1342", "para_id": 6911, "text": "most unexpected and serious nature; but I am afraid of alarming you--be"}
{"doc_id": "gutenberg_1342", "para_id": 6912, "text": "assured that we are all well. What I have to say relates to poor Lydia."}
{"doc_id": "gutenberg_1342", "para_id": 6913, "text": "An express came at twelve last night, just as we were all gone to bed,"}
{"doc_id": "gutenberg_1342", "para_id": 6914, "text": "from Colonel Forster, to inform us that she was gone off to Scotland"}
{"doc_id": "gutenberg_1342", "para_id": 6915, "text": "with one of his officers; to own the truth, with Wickham! Imagine our"}
{"doc_id": "gutenberg_1342", "para_id": 6916, "text": "surprise. To Kitty, however, it does not seem so wholly unexpected. I am"}
{"doc_id": "gutenberg_1342", "para_id": 6917, "text": "very, very sorry. So imprudent a match on both sides! But I am willing"}
{"doc_id": "gutenberg_1342", "para_id": 6918, "text": "to hope the best, and that his character has been misunderstood."}
{"doc_id": "gutenberg_1342", "para_id": 6919, "text": "Thoughtless and indiscreet I can easily believe him, but this step (and"}
{"doc_id": "gutenberg_1342", "para_id": 6920, "text": "let us rejoice over it) marks nothing bad at heart. His choice is"}
{"doc_id": "gutenberg_1342", "para_id": 6921, "text": "disinterested at least, for he must know my father can give her nothing."}
{"doc_id": "gutenberg_1342", "para_id": 6922, "text": "Our poor mother is sadly grieved. My father bears it better. How"}
{"doc_id": "gutenberg_1342", "para_id": 6923, "text": "thankful am I, that we never let them know what has been said against"}
{"doc_id": "gutenberg_1342", "para_id": 6924, "text": "him; we must forget it ourselves. They were off Saturday night about"}
{"doc_id": "gutenberg_1342", "para_id": 6925, "text": "twelve, as is conjectured, but were not missed till yesterday morning at"}
{"doc_id": "gutenberg_1342", "para_id": 6926, "text": "eight. The express was sent off directly. My dear Lizzy, they must have"}
{"doc_id": "gutenberg_1342", "para_id": 6927, "text": "passed within ten miles of us. Colonel Forster gives us reason to expect"}
{"doc_id": "gutenberg_1342", "para_id": 6928, "text": "him here soon. Lydia left a few lines for his wife, informing her of"}
{"doc_id": "gutenberg_1342", "para_id": 6929, "text": "their intention. I must conclude, for I cannot be long from my poor"}
{"doc_id": "gutenberg_1342", "para_id": 6930, "text": "mother. I am afraid you will not be able to make it out, but I hardly"}
{"doc_id": "gutenberg_1342", "para_id": 6931, "text": "Without allowing herself time for consideration, and scarcely knowing"}
{"doc_id": "gutenberg_1342", "para_id": 6932, "text": "what she felt, Elizabeth, on finishing this letter, instantly seized the"}
{"doc_id": "gutenberg_1342", "para_id": 6933, "text": "other, and opening it with the utmost impatience, read as follows: it"}
{"doc_id": "gutenberg_1342", "para_id": 6934, "text": "had been written a day later than the conclusion of the first."}
{"doc_id": "gutenberg_1342", "para_id": 6935, "text": "“By this time, my dearest sister, you have received my hurried letter; I"}
{"doc_id": "gutenberg_1342", "para_id": 6936, "text": "wish this may be more intelligible, but though not confined for time, my"}
{"doc_id": "gutenberg_1342", "para_id": 6937, "text": "head is so bewildered that I cannot answer for being coherent. Dearest"}
{"doc_id": "gutenberg_1342", "para_id": 6938, "text": "Lizzy, I hardly know what I would write, but I have bad news for you,"}
{"doc_id": "gutenberg_1342", "para_id": 6939, "text": "and it cannot be delayed. Imprudent as a marriage between Mr. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 6940, "text": "and our poor Lydia would be, we are now anxious to be assured it has"}
{"doc_id": "gutenberg_1342", "para_id": 6941, "text": "taken place, for there is but too much reason to fear they are not gone"}
{"doc_id": "gutenberg_1342", "para_id": 6942, "text": "to Scotland. Colonel Forster came yesterday, having left Brighton the"}
{"doc_id": "gutenberg_1342", "para_id": 6943, "text": "day before, not many hours after the express. Though Lydia’s short"}
{"doc_id": "gutenberg_1342", "para_id": 6944, "text": "letter to Mrs. F. gave them to understand that they were going to Gretna"}
{"doc_id": "gutenberg_1342", "para_id": 6945, "text": "Green, something was dropped by Denny expressing his belief that W."}
{"doc_id": "gutenberg_1342", "para_id": 6946, "text": "never intended to go there, or to marry Lydia at all, which was repeated"}
{"doc_id": "gutenberg_1342", "para_id": 6947, "text": "to Colonel F., who, instantly taking the alarm, set off from B.,"}
{"doc_id": "gutenberg_1342", "para_id": 6948, "text": "intending to trace their route. He did trace them easily to Clapham, but"}
{"doc_id": "gutenberg_1342", "para_id": 6949, "text": "no farther; for on entering that place, they removed into a"}
{"doc_id": "gutenberg_1342", "para_id": 6950, "text": "hackney-coach, and dismissed the chaise that brought them from Epsom."}
{"doc_id": "gutenberg_1342", "para_id": 6951, "text": "All that is known after this is, that they were seen to continue the"}
{"doc_id": "gutenberg_1342", "para_id": 6952, "text": "London road. I know not what to think. After making every possible"}
{"doc_id": "gutenberg_1342", "para_id": 6953, "text": "inquiry on that side of London, Colonel F. came on into Hertfordshire,"}
{"doc_id": "gutenberg_1342", "para_id": 6954, "text": "anxiously renewing them at all the turnpikes, and at the inns in Barnet"}
{"doc_id": "gutenberg_1342", "para_id": 6955, "text": "and Hatfield, but without any success,--no such people had been seen to"}
{"doc_id": "gutenberg_1342", "para_id": 6956, "text": "pass through. With the kindest concern he came on to Longbourn, and"}
{"doc_id": "gutenberg_1342", "para_id": 6957, "text": "broke his apprehensions to us in a manner most creditable to his heart."}
{"doc_id": "gutenberg_1342", "para_id": 6958, "text": "I am sincerely grieved for him and Mrs. F.; but no one can throw any"}
{"doc_id": "gutenberg_1342", "para_id": 6959, "text": "blame on them. Our distress, my dear Lizzy, is very great. My father and"}
{"doc_id": "gutenberg_1342", "para_id": 6960, "text": "mother believe the worst, but I cannot think so ill of him. Many"}
{"doc_id": "gutenberg_1342", "para_id": 6961, "text": "circumstances might make it more eligible for them to be married"}
{"doc_id": "gutenberg_1342", "para_id": 6962, "text": "privately in town than to pursue their first plan; and even if _he_"}
{"doc_id": "gutenberg_1342", "para_id": 6963, "text": "could form such a design against a young woman of Lydia’s connections,"}
{"doc_id": "gutenberg_1342", "para_id": 6964, "text": "which is not likely, can I suppose her so lost to everything?"}
{"doc_id": "gutenberg_1342", "para_id": 6965, "text": "Impossible! I grieve to find, however, that Colonel F. is not disposed"}
{"doc_id": "gutenberg_1342", "para_id": 6966, "text": "to depend upon their marriage: he shook his head when I expressed my"}
{"doc_id": "gutenberg_1342", "para_id": 6967, "text": "hopes, and said he feared W. was not a man to be trusted. My poor mother"}
{"doc_id": "gutenberg_1342", "para_id": 6968, "text": "is really ill, and keeps her room. Could she exert herself, it would be"}
{"doc_id": "gutenberg_1342", "para_id": 6969, "text": "better, but this is not to be expected; and as to my father, I never in"}
{"doc_id": "gutenberg_1342", "para_id": 6970, "text": "my life saw him so affected. Poor Kitty has anger for having concealed"}
{"doc_id": "gutenberg_1342", "para_id": 6971, "text": "their attachment; but as it was a matter of confidence, one cannot"}
{"doc_id": "gutenberg_1342", "para_id": 6972, "text": "wonder. I am truly glad, dearest Lizzy, that you have been spared"}
{"doc_id": "gutenberg_1342", "para_id": 6973, "text": "something of these distressing scenes; but now, as the first shock is"}
{"doc_id": "gutenberg_1342", "para_id": 6974, "text": "over, shall I own that I long for your return? I am not so selfish,"}
{"doc_id": "gutenberg_1342", "para_id": 6975, "text": "however, as to press for it, if inconvenient. Adieu! I take up my pen"}
{"doc_id": "gutenberg_1342", "para_id": 6976, "text": "again to do, what I have just told you I would not; but circumstances"}
{"doc_id": "gutenberg_1342", "para_id": 6977, "text": "are such, that I cannot help earnestly begging you all to come here as"}
{"doc_id": "gutenberg_1342", "para_id": 6978, "text": "soon as possible. I know my dear uncle and aunt so well, that I am not"}
{"doc_id": "gutenberg_1342", "para_id": 6979, "text": "afraid of requesting it, though I have still something more to ask of"}
{"doc_id": "gutenberg_1342", "para_id": 6980, "text": "the former. My father is going to London with Colonel Forster instantly,"}
{"doc_id": "gutenberg_1342", "para_id": 6981, "text": "to try to discover her. What he means to do, I am sure I know not; but"}
{"doc_id": "gutenberg_1342", "para_id": 6982, "text": "his excessive distress will not allow him to pursue any measure in the"}
{"doc_id": "gutenberg_1342", "para_id": 6983, "text": "best and safest way, and Colonel Forster is obliged to be at Brighton"}
{"doc_id": "gutenberg_1342", "para_id": 6984, "text": "again to-morrow evening. In such an exigence my uncle’s advice and"}
{"doc_id": "gutenberg_1342", "para_id": 6985, "text": "assistance would be everything in the world; he will immediately"}
{"doc_id": "gutenberg_1342", "para_id": 6986, "text": "comprehend what I must feel, and I rely upon his goodness.”"}
{"doc_id": "gutenberg_1342", "para_id": 6987, "text": "“Oh! where, where is my uncle?” cried Elizabeth, darting from her seat"}
{"doc_id": "gutenberg_1342", "para_id": 6988, "text": "as she finished the letter, in eagerness to follow him, without losing a"}
{"doc_id": "gutenberg_1342", "para_id": 6989, "text": "moment of the time so precious; but as she reached the door, it was"}
{"doc_id": "gutenberg_1342", "para_id": 6990, "text": "opened by a servant, and Mr. Darcy appeared. Her pale face and"}
{"doc_id": "gutenberg_1342", "para_id": 6991, "text": "impetuous manner made him start, and before he could recover himself"}
{"doc_id": "gutenberg_1342", "para_id": 6992, "text": "enough to speak, she, in whose mind every idea was superseded by Lydia’s"}
{"doc_id": "gutenberg_1342", "para_id": 6993, "text": "situation, hastily exclaimed, “I beg your pardon, but I must leave you."}
{"doc_id": "gutenberg_1342", "para_id": 6994, "text": "I must find Mr. Gardiner this moment on business that cannot be delayed;"}
{"doc_id": "gutenberg_1342", "para_id": 6995, "text": "“Good God! what is the matter?” cried he, with more feeling than"}
{"doc_id": "gutenberg_1342", "para_id": 6996, "text": "politeness; then recollecting himself, “I will not detain you a minute;"}
{"doc_id": "gutenberg_1342", "para_id": 6997, "text": "but let me, or let the servant, go after Mr. and Mrs. Gardiner. You are"}
{"doc_id": "gutenberg_1342", "para_id": 6998, "text": "Elizabeth hesitated; but her knees trembled under her, and she felt how"}
{"doc_id": "gutenberg_1342", "para_id": 6999, "text": "little would be gained by her attempting to pursue them. Calling back"}
{"doc_id": "gutenberg_1342", "para_id": 7000, "text": "the servant, therefore, she commissioned him, though in so breathless an"}
{"doc_id": "gutenberg_1342", "para_id": 7001, "text": "accent as made her almost unintelligible, to fetch his master and"}
{"doc_id": "gutenberg_1342", "para_id": 7002, "text": "On his quitting the room, she sat down, unable to support herself, and"}
{"doc_id": "gutenberg_1342", "para_id": 7003, "text": "looking so miserably ill, that it was impossible for Darcy to leave her,"}
{"doc_id": "gutenberg_1342", "para_id": 7004, "text": "or to refrain from saying, in a tone of gentleness and commiseration,"}
{"doc_id": "gutenberg_1342", "para_id": 7005, "text": "“Let me call your maid. Is there nothing you could take to give you"}
{"doc_id": "gutenberg_1342", "para_id": 7006, "text": "present relief? A glass of wine; shall I get you one? You are very ill.”"}
{"doc_id": "gutenberg_1342", "para_id": 7007, "text": "“No, I thank you,” she replied, endeavouring to recover herself. “There"}
{"doc_id": "gutenberg_1342", "para_id": 7008, "text": "is nothing the matter with me. I am quite well, I am only distressed by"}
{"doc_id": "gutenberg_1342", "para_id": 7009, "text": "some dreadful news which I have just received from Longbourn.”"}
{"doc_id": "gutenberg_1342", "para_id": 7010, "text": "She burst into tears as she alluded to it, and for a few minutes could"}
{"doc_id": "gutenberg_1342", "para_id": 7011, "text": "not speak another word. Darcy, in wretched suspense, could only say"}
{"doc_id": "gutenberg_1342", "para_id": 7012, "text": "concern, and observe her in compassionate silence. At length she spoke"}
{"doc_id": "gutenberg_1342", "para_id": 7013, "text": "again. “I have just had a letter from Jane, with such dreadful news. It"}
{"doc_id": "gutenberg_1342", "para_id": 7014, "text": "cannot be concealed from anyone. My youngest sister has left all her"}
{"doc_id": "gutenberg_1342", "para_id": 7015, "text": "friends--has eloped; has thrown herself into the power of--of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7016, "text": "Wickham. They are gone off together from Brighton. _You_ know him too"}
{"doc_id": "gutenberg_1342", "para_id": 7017, "text": "well to doubt the rest. She has no money, no connections, nothing that"}
{"doc_id": "gutenberg_1342", "para_id": 7018, "text": "“When I consider,” she added, in a yet more agitated voice, “that _I_"}
{"doc_id": "gutenberg_1342", "para_id": 7019, "text": "might have prevented it! _I_ who knew what he was. Had I but explained"}
{"doc_id": "gutenberg_1342", "para_id": 7020, "text": "some part of it only--some part of what I learnt, to my own family! Had"}
{"doc_id": "gutenberg_1342", "para_id": 7021, "text": "his character been known, this could not have happened. But it is all,"}
{"doc_id": "gutenberg_1342", "para_id": 7022, "text": "“I am grieved, indeed,” cried Darcy: “grieved--shocked. But is it"}
{"doc_id": "gutenberg_1342", "para_id": 7023, "text": "“Oh, yes! They left Brighton together on Sunday night, and were traced"}
{"doc_id": "gutenberg_1342", "para_id": 7024, "text": "almost to London, but not beyond: they are certainly not gone to"}
{"doc_id": "gutenberg_1342", "para_id": 7025, "text": "“And what has been done, what has been attempted, to recover her?”"}
{"doc_id": "gutenberg_1342", "para_id": 7026, "text": "“My father has gone to London, and Jane has written to beg my uncle’s"}
{"doc_id": "gutenberg_1342", "para_id": 7027, "text": "immediate assistance, and we shall be off, I hope, in half an hour. But"}
{"doc_id": "gutenberg_1342", "para_id": 7028, "text": "nothing can be done; I know very well that nothing can be done. How is"}
{"doc_id": "gutenberg_1342", "para_id": 7029, "text": "such a man to be worked on? How are they even to be discovered? I have"}
{"doc_id": "gutenberg_1342", "para_id": 7030, "text": "“When _my_ eyes were opened to his real character, oh! had I known what"}
{"doc_id": "gutenberg_1342", "para_id": 7031, "text": "I ought, what I dared to do! But I knew not--I was afraid of doing too"}
{"doc_id": "gutenberg_1342", "para_id": 7032, "text": "Darcy made no answer. He seemed scarcely to hear her, and was walking up"}
{"doc_id": "gutenberg_1342", "para_id": 7033, "text": "and down the room in earnest meditation; his brow contracted, his air"}
{"doc_id": "gutenberg_1342", "para_id": 7034, "text": "gloomy. Elizabeth soon observed, and instantly understood it. Her power"}
{"doc_id": "gutenberg_1342", "para_id": 7035, "text": "was sinking; everything _must_ sink under such a proof of family"}
{"doc_id": "gutenberg_1342", "para_id": 7036, "text": "weakness, such an assurance of the deepest disgrace. She could neither"}
{"doc_id": "gutenberg_1342", "para_id": 7037, "text": "wonder nor condemn; but the belief of his self-conquest brought nothing"}
{"doc_id": "gutenberg_1342", "para_id": 7038, "text": "consolatory to her bosom, afforded no palliation of her distress. It"}
{"doc_id": "gutenberg_1342", "para_id": 7039, "text": "was, on the contrary, exactly calculated to make her understand her own"}
{"doc_id": "gutenberg_1342", "para_id": 7040, "text": "wishes; and never had she so honestly felt that she could have loved"}
{"doc_id": "gutenberg_1342", "para_id": 7041, "text": "But self, though it would intrude, could not engross her. Lydia--the"}
{"doc_id": "gutenberg_1342", "para_id": 7042, "text": "humiliation, the misery she was bringing on them all--soon swallowed up"}
{"doc_id": "gutenberg_1342", "para_id": 7043, "text": "every private care; and covering her face with her handkerchief,"}
{"doc_id": "gutenberg_1342", "para_id": 7044, "text": "Elizabeth was soon lost to everything else; and, after a pause of"}
{"doc_id": "gutenberg_1342", "para_id": 7045, "text": "several minutes, was only recalled to a sense of her situation by the"}
{"doc_id": "gutenberg_1342", "para_id": 7046, "text": "voice of her companion, who, in a manner which, though it spoke"}
{"doc_id": "gutenberg_1342", "para_id": 7047, "text": "“I am afraid you have been long desiring my absence, nor have I anything"}
{"doc_id": "gutenberg_1342", "para_id": 7048, "text": "to plead in excuse of my stay, but real, though unavailing concern."}
{"doc_id": "gutenberg_1342", "para_id": 7049, "text": "Would to Heaven that anything could be either said or done on my part,"}
{"doc_id": "gutenberg_1342", "para_id": 7050, "text": "that might offer consolation to such distress! But I will not torment"}
{"doc_id": "gutenberg_1342", "para_id": 7051, "text": "you with vain wishes, which may seem purposely to ask for your thanks."}
{"doc_id": "gutenberg_1342", "para_id": 7052, "text": "This unfortunate affair will, I fear, prevent my sister’s having the"}
{"doc_id": "gutenberg_1342", "para_id": 7053, "text": "“Oh, yes! Be so kind as to apologize for us to Miss Darcy. Say that"}
{"doc_id": "gutenberg_1342", "para_id": 7054, "text": "urgent business calls us home immediately. Conceal the unhappy truth as"}
{"doc_id": "gutenberg_1342", "para_id": 7055, "text": "He readily assured her of his secrecy, again expressed his sorrow for"}
{"doc_id": "gutenberg_1342", "para_id": 7056, "text": "her distress, wished it a happier conclusion than there was at present"}
{"doc_id": "gutenberg_1342", "para_id": 7057, "text": "reason to hope, and, leaving his compliments for her relations, with"}
{"doc_id": "gutenberg_1342", "para_id": 7058, "text": "As he quitted the room, Elizabeth felt how improbable it was that they"}
{"doc_id": "gutenberg_1342", "para_id": 7059, "text": "should ever see each other again on such terms of cordiality as had"}
{"doc_id": "gutenberg_1342", "para_id": 7060, "text": "marked their several meetings in Derbyshire; and as she threw a"}
{"doc_id": "gutenberg_1342", "para_id": 7061, "text": "retrospective glance over the whole of their acquaintance, so full of"}
{"doc_id": "gutenberg_1342", "para_id": 7062, "text": "contradictions and varieties, sighed at the perverseness of those"}
{"doc_id": "gutenberg_1342", "para_id": 7063, "text": "feelings which would now have promoted its continuance, and would"}
{"doc_id": "gutenberg_1342", "para_id": 7064, "text": "If gratitude and esteem are good foundations of affection, Elizabeth’s"}
{"doc_id": "gutenberg_1342", "para_id": 7065, "text": "change of sentiment will be neither improbable nor faulty. But if"}
{"doc_id": "gutenberg_1342", "para_id": 7066, "text": "otherwise, if the regard springing from such sources is unreasonable or"}
{"doc_id": "gutenberg_1342", "para_id": 7067, "text": "unnatural, in comparison of what is so often described as arising on a"}
{"doc_id": "gutenberg_1342", "para_id": 7068, "text": "first interview with its object, and even before two words have been"}
{"doc_id": "gutenberg_1342", "para_id": 7069, "text": "exchanged, nothing can be said in her defence, except that she had given"}
{"doc_id": "gutenberg_1342", "para_id": 7070, "text": "somewhat of a trial to the latter method, in her partiality for Wickham,"}
{"doc_id": "gutenberg_1342", "para_id": 7071, "text": "and that its ill success might, perhaps, authorize her to seek the other"}
{"doc_id": "gutenberg_1342", "para_id": 7072, "text": "less interesting mode of attachment. Be that as it may, she saw him go"}
{"doc_id": "gutenberg_1342", "para_id": 7073, "text": "with regret; and in this early example of what Lydia’s infamy must"}
{"doc_id": "gutenberg_1342", "para_id": 7074, "text": "produce, found additional anguish as she reflected on that wretched"}
{"doc_id": "gutenberg_1342", "para_id": 7075, "text": "business. Never since reading Jane’s second letter had she entertained a"}
{"doc_id": "gutenberg_1342", "para_id": 7076, "text": "hope of Wickham’s meaning to marry her. No one but Jane, she thought,"}
{"doc_id": "gutenberg_1342", "para_id": 7077, "text": "could flatter herself with such an expectation. Surprise was the least"}
{"doc_id": "gutenberg_1342", "para_id": 7078, "text": "of all her feelings on this development. While the contents of the first"}
{"doc_id": "gutenberg_1342", "para_id": 7079, "text": "letter remained on her mind, she was all surprise, all astonishment,"}
{"doc_id": "gutenberg_1342", "para_id": 7080, "text": "that Wickham should marry a girl whom it was impossible he could marry"}
{"doc_id": "gutenberg_1342", "para_id": 7081, "text": "for money; and how Lydia could ever have attached him had appeared"}
{"doc_id": "gutenberg_1342", "para_id": 7082, "text": "incomprehensible. But now it was all too natural. For such an attachment"}
{"doc_id": "gutenberg_1342", "para_id": 7083, "text": "as this, she might have sufficient charms; and though she did not"}
{"doc_id": "gutenberg_1342", "para_id": 7084, "text": "suppose Lydia to be deliberately engaging in an elopement, without the"}
{"doc_id": "gutenberg_1342", "para_id": 7085, "text": "intention of marriage, she had no difficulty in believing that neither"}
{"doc_id": "gutenberg_1342", "para_id": 7086, "text": "her virtue nor her understanding would preserve her from falling an easy"}
{"doc_id": "gutenberg_1342", "para_id": 7087, "text": "She had never perceived, while the regiment was in Hertfordshire, that"}
{"doc_id": "gutenberg_1342", "para_id": 7088, "text": "Lydia had any partiality for him; but she was convinced that Lydia had"}
{"doc_id": "gutenberg_1342", "para_id": 7089, "text": "wanted only encouragement to attach herself to anybody. Sometimes one"}
{"doc_id": "gutenberg_1342", "para_id": 7090, "text": "officer, sometimes another, had been her favourite, as their attentions"}
{"doc_id": "gutenberg_1342", "para_id": 7091, "text": "raised them in her opinion. Her affections had been continually"}
{"doc_id": "gutenberg_1342", "para_id": 7092, "text": "fluctuating, but never without an object. The mischief of neglect and"}
{"doc_id": "gutenberg_1342", "para_id": 7093, "text": "mistaken indulgence towards such a girl--oh! how acutely did she now"}
{"doc_id": "gutenberg_1342", "para_id": 7094, "text": "She was wild to be at home--to hear, to see, to be upon the spot to"}
{"doc_id": "gutenberg_1342", "para_id": 7095, "text": "share with Jane in the cares that must now fall wholly upon her, in a"}
{"doc_id": "gutenberg_1342", "para_id": 7096, "text": "family so deranged; a father absent, a mother incapable of exertion, and"}
{"doc_id": "gutenberg_1342", "para_id": 7097, "text": "requiring constant attendance; and though almost persuaded that nothing"}
{"doc_id": "gutenberg_1342", "para_id": 7098, "text": "could be done for Lydia, her uncle’s interference seemed of the utmost"}
{"doc_id": "gutenberg_1342", "para_id": 7099, "text": "importance, and till he entered the room the misery of her impatience"}
{"doc_id": "gutenberg_1342", "para_id": 7100, "text": "was severe. Mr. and Mrs. Gardiner had hurried back in alarm, supposing,"}
{"doc_id": "gutenberg_1342", "para_id": 7101, "text": "by the servant’s account, that their niece was taken suddenly ill; but"}
{"doc_id": "gutenberg_1342", "para_id": 7102, "text": "satisfying them instantly on that head, she eagerly communicated the"}
{"doc_id": "gutenberg_1342", "para_id": 7103, "text": "cause of their summons, reading the two letters aloud, and dwelling on"}
{"doc_id": "gutenberg_1342", "para_id": 7104, "text": "the postscript of the last with trembling energy. Though Lydia had never"}
{"doc_id": "gutenberg_1342", "para_id": 7105, "text": "been a favourite with them, Mr. and Mrs. Gardiner could not but be"}
{"doc_id": "gutenberg_1342", "para_id": 7106, "text": "deeply affected. Not Lydia only, but all were concerned in it; and after"}
{"doc_id": "gutenberg_1342", "para_id": 7107, "text": "the first exclamations of surprise and horror, Mr. Gardiner readily"}
{"doc_id": "gutenberg_1342", "para_id": 7108, "text": "promised every assistance in his power. Elizabeth, though expecting no"}
{"doc_id": "gutenberg_1342", "para_id": 7109, "text": "less, thanked him with tears of gratitude; and all three being actuated"}
{"doc_id": "gutenberg_1342", "para_id": 7110, "text": "by one spirit, everything relating to their journey was speedily"}
{"doc_id": "gutenberg_1342", "para_id": 7111, "text": "settled. They were to be off as soon as possible. “But what is to be"}
{"doc_id": "gutenberg_1342", "para_id": 7112, "text": "done about Pemberley?” cried Mrs. Gardiner. “John told us Mr. Darcy was"}
{"doc_id": "gutenberg_1342", "para_id": 7113, "text": "“Yes; and I told him we should not be able to keep our engagement."}
{"doc_id": "gutenberg_1342", "para_id": 7114, "text": "“What is all settled?” repeated the other, as she ran into her room to"}
{"doc_id": "gutenberg_1342", "para_id": 7115, "text": "prepare. “And are they upon such terms as for her to disclose the real"}
{"doc_id": "gutenberg_1342", "para_id": 7116, "text": "But wishes were vain; or, at best, could serve only to amuse her in the"}
{"doc_id": "gutenberg_1342", "para_id": 7117, "text": "hurry and confusion of the following hour. Had Elizabeth been at leisure"}
{"doc_id": "gutenberg_1342", "para_id": 7118, "text": "to be idle, she would have remained certain that all employment was"}
{"doc_id": "gutenberg_1342", "para_id": 7119, "text": "impossible to one so wretched as herself; but she had her share of"}
{"doc_id": "gutenberg_1342", "para_id": 7120, "text": "business as well as her aunt, and amongst the rest there were notes to"}
{"doc_id": "gutenberg_1342", "para_id": 7121, "text": "be written to all their friends at Lambton, with false excuses for their"}
{"doc_id": "gutenberg_1342", "para_id": 7122, "text": "sudden departure. An hour, however, saw the whole completed; and Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7123, "text": "Gardiner, meanwhile, having settled his account at the inn, nothing"}
{"doc_id": "gutenberg_1342", "para_id": 7124, "text": "remained to be done but to go; and Elizabeth, after all the misery of"}
{"doc_id": "gutenberg_1342", "para_id": 7125, "text": "the morning, found herself, in a shorter space of time than she could"}
{"doc_id": "gutenberg_1342", "para_id": 7126, "text": "have supposed, seated in the carriage, and on the road to Longbourn."}
{"doc_id": "gutenberg_1342", "para_id": 7127, "text": "“I have been thinking it over again, Elizabeth,” said her uncle, as they"}
{"doc_id": "gutenberg_1342", "para_id": 7128, "text": "drove from the town; “and really, upon serious consideration, I am much"}
{"doc_id": "gutenberg_1342", "para_id": 7129, "text": "more inclined than I was to judge as your eldest sister does of the"}
{"doc_id": "gutenberg_1342", "para_id": 7130, "text": "matter. It appears to me so very unlikely that any young man should form"}
{"doc_id": "gutenberg_1342", "para_id": 7131, "text": "such a design against a girl who is by no means unprotected or"}
{"doc_id": "gutenberg_1342", "para_id": 7132, "text": "friendless, and who was actually staying in his Colonel’s family, that I"}
{"doc_id": "gutenberg_1342", "para_id": 7133, "text": "am strongly inclined to hope the best. Could he expect that her friends"}
{"doc_id": "gutenberg_1342", "para_id": 7134, "text": "would not step forward? Could he expect to be noticed again by the"}
{"doc_id": "gutenberg_1342", "para_id": 7135, "text": "regiment, after such an affront to Colonel Forster? His temptation is"}
{"doc_id": "gutenberg_1342", "para_id": 7136, "text": "“Do you really think so?” cried Elizabeth, brightening up for a moment."}
{"doc_id": "gutenberg_1342", "para_id": 7137, "text": "“Upon my word,” said Mrs. Gardiner, “I begin to be of your uncle’s"}
{"doc_id": "gutenberg_1342", "para_id": 7138, "text": "opinion. It is really too great a violation of decency, honour, and"}
{"doc_id": "gutenberg_1342", "para_id": 7139, "text": "interest, for him to be guilty of it. I cannot think so very ill of"}
{"doc_id": "gutenberg_1342", "para_id": 7140, "text": "Wickham. Can you, yourself, Lizzie, so wholly give him up, as to believe"}
{"doc_id": "gutenberg_1342", "para_id": 7141, "text": "“Not perhaps of neglecting his own interest. But of every other neglect"}
{"doc_id": "gutenberg_1342", "para_id": 7142, "text": "I can believe him capable. If, indeed, it should be so! But I dare not"}
{"doc_id": "gutenberg_1342", "para_id": 7143, "text": "hope it. Why should they not go on to Scotland, if that had been the"}
{"doc_id": "gutenberg_1342", "para_id": 7144, "text": "“In the first place,” replied Mr. Gardiner, “there is no absolute proof"}
{"doc_id": "gutenberg_1342", "para_id": 7145, "text": "“Oh, but their removing from the chaise into a hackney coach is such a"}
{"doc_id": "gutenberg_1342", "para_id": 7146, "text": "presumption! And, besides, no traces of them were to be found on the"}
{"doc_id": "gutenberg_1342", "para_id": 7147, "text": "“Well, then,--supposing them to be in London--they may be there, though"}
{"doc_id": "gutenberg_1342", "para_id": 7148, "text": "for the purpose of concealment, for no more exceptionable purpose. It is"}
{"doc_id": "gutenberg_1342", "para_id": 7149, "text": "not likely that money should be very abundant on either side; and it"}
{"doc_id": "gutenberg_1342", "para_id": 7150, "text": "might strike them that they could be more economically, though less"}
{"doc_id": "gutenberg_1342", "para_id": 7151, "text": "expeditiously, married in London, than in Scotland.”"}
{"doc_id": "gutenberg_1342", "para_id": 7152, "text": "“But why all this secrecy? Why any fear of detection? Why must their"}
{"doc_id": "gutenberg_1342", "para_id": 7153, "text": "marriage be private? Oh, no, no--this is not likely. His most particular"}
{"doc_id": "gutenberg_1342", "para_id": 7154, "text": "friend, you see by Jane’s account, was persuaded of his never intending"}
{"doc_id": "gutenberg_1342", "para_id": 7155, "text": "to marry her. Wickham will never marry a woman without some money. He"}
{"doc_id": "gutenberg_1342", "para_id": 7156, "text": "cannot afford it. And what claims has Lydia, what attractions has she"}
{"doc_id": "gutenberg_1342", "para_id": 7157, "text": "beyond youth, health, and good humour, that could make him for her sake"}
{"doc_id": "gutenberg_1342", "para_id": 7158, "text": "forego every chance of benefiting himself by marrying well? As to what"}
{"doc_id": "gutenberg_1342", "para_id": 7159, "text": "restraint the apprehensions of disgrace in the corps might throw on a"}
{"doc_id": "gutenberg_1342", "para_id": 7160, "text": "dishonourable elopement with her, I am not able to judge; for I know"}
{"doc_id": "gutenberg_1342", "para_id": 7161, "text": "nothing of the effects that such a step might produce. But as to your"}
{"doc_id": "gutenberg_1342", "para_id": 7162, "text": "other objection, I am afraid it will hardly hold good. Lydia has no"}
{"doc_id": "gutenberg_1342", "para_id": 7163, "text": "brothers to step forward; and he might imagine, from my father’s"}
{"doc_id": "gutenberg_1342", "para_id": 7164, "text": "behaviour, from his indolence and the little attention he has ever"}
{"doc_id": "gutenberg_1342", "para_id": 7165, "text": "seemed to give to what was going forward in his family, that _he_ would"}
{"doc_id": "gutenberg_1342", "para_id": 7166, "text": "do as little and think as little about it, as any father could do, in"}
{"doc_id": "gutenberg_1342", "para_id": 7167, "text": "“But can you think that Lydia is so lost to everything but love of him,"}
{"doc_id": "gutenberg_1342", "para_id": 7168, "text": "as to consent to live with him on any other terms than marriage?”"}
{"doc_id": "gutenberg_1342", "para_id": 7169, "text": "“It does seem, and it is most shocking, indeed,” replied Elizabeth, with"}
{"doc_id": "gutenberg_1342", "para_id": 7170, "text": "tears in her eyes, “that a sister’s sense of decency and virtue in such"}
{"doc_id": "gutenberg_1342", "para_id": 7171, "text": "a point should admit of doubt. But, really, I know not what to say."}
{"doc_id": "gutenberg_1342", "para_id": 7172, "text": "Perhaps I am not doing her justice. But she is very young: she has never"}
{"doc_id": "gutenberg_1342", "para_id": 7173, "text": "been taught to think on serious subjects; and for the last half year,"}
{"doc_id": "gutenberg_1342", "para_id": 7174, "text": "nay, for a twelvemonth, she has been given up to nothing but amusement"}
{"doc_id": "gutenberg_1342", "para_id": 7175, "text": "and vanity. She has been allowed to dispose of her time in the most idle"}
{"doc_id": "gutenberg_1342", "para_id": 7176, "text": "and frivolous manner, and to adopt any opinions that came in her way."}
{"doc_id": "gutenberg_1342", "para_id": 7177, "text": "Since the ----shire were first quartered in Meryton, nothing but love,"}
{"doc_id": "gutenberg_1342", "para_id": 7178, "text": "flirtation, and officers, have been in her head. She has been doing"}
{"doc_id": "gutenberg_1342", "para_id": 7179, "text": "everything in her power, by thinking and talking on the subject, to give"}
{"doc_id": "gutenberg_1342", "para_id": 7180, "text": "greater--what shall I call it?--susceptibility to her feelings; which"}
{"doc_id": "gutenberg_1342", "para_id": 7181, "text": "are naturally lively enough. And we all know that Wickham has every"}
{"doc_id": "gutenberg_1342", "para_id": 7182, "text": "charm of person and address that can captivate a woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 7183, "text": "“But you see that Jane,” said her aunt, “does not think so ill of"}
{"doc_id": "gutenberg_1342", "para_id": 7184, "text": "Wickham, as to believe him capable of the attempt.”"}
{"doc_id": "gutenberg_1342", "para_id": 7185, "text": "“Of whom does Jane ever think ill? And who is there, whatever might be"}
{"doc_id": "gutenberg_1342", "para_id": 7186, "text": "their former conduct, that she would believe capable of such an attempt,"}
{"doc_id": "gutenberg_1342", "para_id": 7187, "text": "till it were proved against them? But Jane knows, as well as I do, what"}
{"doc_id": "gutenberg_1342", "para_id": 7188, "text": "Wickham really is. We both know that he has been profligate in every"}
{"doc_id": "gutenberg_1342", "para_id": 7189, "text": "sense of the word; that he has neither integrity nor honour; that he is"}
{"doc_id": "gutenberg_1342", "para_id": 7190, "text": "“And do you really know all this?” cried Mrs. Gardiner, whose curiosity"}
{"doc_id": "gutenberg_1342", "para_id": 7191, "text": "“I do, indeed,” replied Elizabeth, colouring. “I told you the other day"}
{"doc_id": "gutenberg_1342", "para_id": 7192, "text": "of his infamous behaviour to Mr. Darcy; and you, yourself, when last at"}
{"doc_id": "gutenberg_1342", "para_id": 7193, "text": "Longbourn, heard in what manner he spoke of the man who had behaved with"}
{"doc_id": "gutenberg_1342", "para_id": 7194, "text": "such forbearance and liberality towards him. And there are other"}
{"doc_id": "gutenberg_1342", "para_id": 7195, "text": "circumstances which I am not at liberty--which it is not worth while to"}
{"doc_id": "gutenberg_1342", "para_id": 7196, "text": "relate; but his lies about the whole Pemberley family are endless. From"}
{"doc_id": "gutenberg_1342", "para_id": 7197, "text": "what he said of Miss Darcy, I was thoroughly prepared to see a proud,"}
{"doc_id": "gutenberg_1342", "para_id": 7198, "text": "reserved, disagreeable girl. Yet he knew to the contrary himself. He"}
{"doc_id": "gutenberg_1342", "para_id": 7199, "text": "must know that she was as amiable and unpretending as we have found"}
{"doc_id": "gutenberg_1342", "para_id": 7200, "text": "“But does Lydia know nothing of this? can she be ignorant of what you"}
{"doc_id": "gutenberg_1342", "para_id": 7201, "text": "“Oh, yes!--that, that is the worst of all. Till I was in Kent, and saw"}
{"doc_id": "gutenberg_1342", "para_id": 7202, "text": "so much both of Mr. Darcy and his relation Colonel Fitzwilliam, I was"}
{"doc_id": "gutenberg_1342", "para_id": 7203, "text": "ignorant of the truth myself. And when I returned home the ----shire"}
{"doc_id": "gutenberg_1342", "para_id": 7204, "text": "was to leave Meryton in a week or fortnight’s time. As that was the"}
{"doc_id": "gutenberg_1342", "para_id": 7205, "text": "case, neither Jane, to whom I related the whole, nor I, thought it"}
{"doc_id": "gutenberg_1342", "para_id": 7206, "text": "necessary to make our knowledge public; for of what use could it"}
{"doc_id": "gutenberg_1342", "para_id": 7207, "text": "apparently be to anyone, that the good opinion, which all the"}
{"doc_id": "gutenberg_1342", "para_id": 7208, "text": "neighbourhood had of him, should then be overthrown? And even when it"}
{"doc_id": "gutenberg_1342", "para_id": 7209, "text": "was settled that Lydia should go with Mrs. Forster, the necessity of"}
{"doc_id": "gutenberg_1342", "para_id": 7210, "text": "opening her eyes to his character never occurred to me. That _she_ could"}
{"doc_id": "gutenberg_1342", "para_id": 7211, "text": "be in any danger from the deception never entered my head. That such a"}
{"doc_id": "gutenberg_1342", "para_id": 7212, "text": "consequence as _this_ should ensue, you may easily believe was far"}
{"doc_id": "gutenberg_1342", "para_id": 7213, "text": "“When they all removed to Brighton, therefore, you had no reason, I"}
{"doc_id": "gutenberg_1342", "para_id": 7214, "text": "“Not the slightest. I can remember no symptom of affection on either"}
{"doc_id": "gutenberg_1342", "para_id": 7215, "text": "side; and had anything of the kind been perceptible, you must be aware"}
{"doc_id": "gutenberg_1342", "para_id": 7216, "text": "that ours is not a family on which it could be thrown away. When first"}
{"doc_id": "gutenberg_1342", "para_id": 7217, "text": "he entered the corps, she was ready enough to admire him; but so we all"}
{"doc_id": "gutenberg_1342", "para_id": 7218, "text": "were. Every girl in or near Meryton was out of her senses about him for"}
{"doc_id": "gutenberg_1342", "para_id": 7219, "text": "the first two months: but he never distinguished _her_ by any particular"}
{"doc_id": "gutenberg_1342", "para_id": 7220, "text": "attention; and, consequently, after a moderate period of extravagant and"}
{"doc_id": "gutenberg_1342", "para_id": 7221, "text": "wild admiration, her fancy for him gave way, and others of the regiment,"}
{"doc_id": "gutenberg_1342", "para_id": 7222, "text": "who treated her with more distinction, again became her favourites.”"}
{"doc_id": "gutenberg_1342", "para_id": 7223, "text": "It may be easily believed, that however little of novelty could be added"}
{"doc_id": "gutenberg_1342", "para_id": 7224, "text": "to their fears, hopes, and conjectures, on this interesting subject by"}
{"doc_id": "gutenberg_1342", "para_id": 7225, "text": "its repeated discussion, no other could detain them from it long, during"}
{"doc_id": "gutenberg_1342", "para_id": 7226, "text": "the whole of the journey. From Elizabeth’s thoughts it was never absent."}
{"doc_id": "gutenberg_1342", "para_id": 7227, "text": "Fixed there by the keenest of all anguish, self-reproach, she could"}
{"doc_id": "gutenberg_1342", "para_id": 7228, "text": "They travelled as expeditiously as possible; and sleeping one night on"}
{"doc_id": "gutenberg_1342", "para_id": 7229, "text": "the road, reached Longbourn by dinnertime the next day. It was a comfort"}
{"doc_id": "gutenberg_1342", "para_id": 7230, "text": "to Elizabeth to consider that Jane could not have been wearied by long"}
{"doc_id": "gutenberg_1342", "para_id": 7231, "text": "The little Gardiners, attracted by the sight of a chaise, were standing"}
{"doc_id": "gutenberg_1342", "para_id": 7232, "text": "on the steps of the house, as they entered the paddock; and when the"}
{"doc_id": "gutenberg_1342", "para_id": 7233, "text": "carriage drove up to the door, the joyful surprise that lighted up their"}
{"doc_id": "gutenberg_1342", "para_id": 7234, "text": "faces and displayed itself over their whole bodies, in a variety of"}
{"doc_id": "gutenberg_1342", "para_id": 7235, "text": "capers and frisks, was the first pleasing earnest of their welcome."}
{"doc_id": "gutenberg_1342", "para_id": 7236, "text": "Elizabeth jumped out; and after giving each of them a hasty kiss,"}
{"doc_id": "gutenberg_1342", "para_id": 7237, "text": "hurried into the vestibule, where Jane, who came running downstairs from"}
{"doc_id": "gutenberg_1342", "para_id": 7238, "text": "Elizabeth, as she affectionately embraced her, whilst tears filled the"}
{"doc_id": "gutenberg_1342", "para_id": 7239, "text": "eyes of both, lost not a moment in asking whether anything had been"}
{"doc_id": "gutenberg_1342", "para_id": 7240, "text": "“Not yet,” replied Jane. “But now that my dear uncle is come, I hope"}
{"doc_id": "gutenberg_1342", "para_id": 7241, "text": "“We have heard only once. He wrote me a few lines on Wednesday, to say"}
{"doc_id": "gutenberg_1342", "para_id": 7242, "text": "that he had arrived in safety, and to give me his directions, which I"}
{"doc_id": "gutenberg_1342", "para_id": 7243, "text": "particularly begged him to do. He merely added, that he should not write"}
{"doc_id": "gutenberg_1342", "para_id": 7244, "text": "again, till he had something of importance to mention.”"}
{"doc_id": "gutenberg_1342", "para_id": 7245, "text": "“My mother is tolerably well, I trust; though her spirits are greatly"}
{"doc_id": "gutenberg_1342", "para_id": 7246, "text": "shaken. She is upstairs, and will have great satisfaction in seeing you"}
{"doc_id": "gutenberg_1342", "para_id": 7247, "text": "all. She does not yet leave her dressing-room. Mary and Kitty, thank"}
{"doc_id": "gutenberg_1342", "para_id": 7248, "text": "“But you--how are you?” cried Elizabeth. “You look pale. How much you"}
{"doc_id": "gutenberg_1342", "para_id": 7249, "text": "Her sister, however, assured her of her being perfectly well; and their"}
{"doc_id": "gutenberg_1342", "para_id": 7250, "text": "conversation, which had been passing while Mr. and Mrs. Gardiner were"}
{"doc_id": "gutenberg_1342", "para_id": 7251, "text": "engaged with their children, was now put an end to by the approach of"}
{"doc_id": "gutenberg_1342", "para_id": 7252, "text": "the whole party. Jane ran to her uncle and aunt, and welcomed and"}
{"doc_id": "gutenberg_1342", "para_id": 7253, "text": "thanked them both, with alternate smiles and tears."}
{"doc_id": "gutenberg_1342", "para_id": 7254, "text": "When they were all in the drawing-room, the questions which Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 7255, "text": "had already asked were of course repeated by the others, and they soon"}
{"doc_id": "gutenberg_1342", "para_id": 7256, "text": "found that Jane had no intelligence to give. The sanguine hope of good,"}
{"doc_id": "gutenberg_1342", "para_id": 7257, "text": "however, which the benevolence of her heart suggested, had not yet"}
{"doc_id": "gutenberg_1342", "para_id": 7258, "text": "deserted her; she still expected that it would all end well, and that"}
{"doc_id": "gutenberg_1342", "para_id": 7259, "text": "every morning would bring some letter, either from Lydia or her father,"}
{"doc_id": "gutenberg_1342", "para_id": 7260, "text": "to explain their proceedings, and, perhaps, announce the marriage."}
{"doc_id": "gutenberg_1342", "para_id": 7261, "text": "Mrs. Bennet, to whose apartment they all repaired, after a few minutes’"}
{"doc_id": "gutenberg_1342", "para_id": 7262, "text": "conversation together, received them exactly as might be expected; with"}
{"doc_id": "gutenberg_1342", "para_id": 7263, "text": "tears and lamentations of regret, invectives against the villainous"}
{"doc_id": "gutenberg_1342", "para_id": 7264, "text": "conduct of Wickham, and complaints of her own sufferings and ill-usage;"}
{"doc_id": "gutenberg_1342", "para_id": 7265, "text": "blaming everybody but the person to whose ill-judging indulgence the"}
{"doc_id": "gutenberg_1342", "para_id": 7266, "text": "“If I had been able,” said she, “to carry my point in going to Brighton"}
{"doc_id": "gutenberg_1342", "para_id": 7267, "text": "with all my family, _this_ would not have happened: but poor dear Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 7268, "text": "had nobody to take care of her. Why did the Forsters ever let her go out"}
{"doc_id": "gutenberg_1342", "para_id": 7269, "text": "of their sight? I am sure there was some great neglect or other on their"}
{"doc_id": "gutenberg_1342", "para_id": 7270, "text": "side, for she is not the kind of girl to do such a thing, if she had"}
{"doc_id": "gutenberg_1342", "para_id": 7271, "text": "been well looked after. I always thought they were very unfit to have"}
{"doc_id": "gutenberg_1342", "para_id": 7272, "text": "the charge of her; but I was over-ruled, as I always am. Poor, dear"}
{"doc_id": "gutenberg_1342", "para_id": 7273, "text": "child! And now here’s Mr. Bennet gone away, and I know he will fight"}
{"doc_id": "gutenberg_1342", "para_id": 7274, "text": "Wickham, wherever he meets him, and then he will be killed, and what is"}
{"doc_id": "gutenberg_1342", "para_id": 7275, "text": "to become of us all? The Collinses will turn us out, before he is cold"}
{"doc_id": "gutenberg_1342", "para_id": 7276, "text": "in his grave; and if you are not kind to us, brother, I do not know what"}
{"doc_id": "gutenberg_1342", "para_id": 7277, "text": "They all exclaimed against such terrific ideas; and Mr. Gardiner, after"}
{"doc_id": "gutenberg_1342", "para_id": 7278, "text": "general assurances of his affection for her and all her family, told her"}
{"doc_id": "gutenberg_1342", "para_id": 7279, "text": "that he meant to be in London the very next day, and would assist Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7280, "text": "“Do not give way to useless alarm,” added he: “though it is right to be"}
{"doc_id": "gutenberg_1342", "para_id": 7281, "text": "prepared for the worst, there is no occasion to look on it as certain."}
{"doc_id": "gutenberg_1342", "para_id": 7282, "text": "It is not quite a week since they left Brighton. In a few days more, we"}
{"doc_id": "gutenberg_1342", "para_id": 7283, "text": "may gain some news of them; and till we know that they are not married,"}
{"doc_id": "gutenberg_1342", "para_id": 7284, "text": "and have no design of marrying, do not let us give the matter over as"}
{"doc_id": "gutenberg_1342", "para_id": 7285, "text": "lost. As soon as I get to town, I shall go to my brother, and make him"}
{"doc_id": "gutenberg_1342", "para_id": 7286, "text": "come home with me to Gracechurch Street, and then we may consult"}
{"doc_id": "gutenberg_1342", "para_id": 7287, "text": "“Oh, my dear brother,” replied Mrs. Bennet, “that is exactly what I"}
{"doc_id": "gutenberg_1342", "para_id": 7288, "text": "could most wish for. And now do, when you get to town, find them out,"}
{"doc_id": "gutenberg_1342", "para_id": 7289, "text": "wherever they may be; and if they are not married already, _make_ them"}
{"doc_id": "gutenberg_1342", "para_id": 7290, "text": "marry. And as for wedding clothes, do not let them wait for that, but"}
{"doc_id": "gutenberg_1342", "para_id": 7291, "text": "tell Lydia she shall have as much money as she chooses to buy them,"}
{"doc_id": "gutenberg_1342", "para_id": 7292, "text": "after they are married. And, above all things, keep Mr. Bennet from"}
{"doc_id": "gutenberg_1342", "para_id": 7293, "text": "fighting. Tell him what a dreadful state I am in--that I am frightened"}
{"doc_id": "gutenberg_1342", "para_id": 7294, "text": "out of my wits; and have such tremblings, such flutterings all over me,"}
{"doc_id": "gutenberg_1342", "para_id": 7295, "text": "such spasms in my side, and pains in my head, and such beatings at my"}
{"doc_id": "gutenberg_1342", "para_id": 7296, "text": "heart, that I can get no rest by night nor by day. And tell my dear"}
{"doc_id": "gutenberg_1342", "para_id": 7297, "text": "Lydia not to give any directions about her clothes till she has seen me,"}
{"doc_id": "gutenberg_1342", "para_id": 7298, "text": "for she does not know which are the best warehouses. Oh, brother, how"}
{"doc_id": "gutenberg_1342", "para_id": 7299, "text": "But Mr. Gardiner, though he assured her again of his earnest endeavours"}
{"doc_id": "gutenberg_1342", "para_id": 7300, "text": "in the cause, could not avoid recommending moderation to her, as well in"}
{"doc_id": "gutenberg_1342", "para_id": 7301, "text": "her hopes as her fears; and after talking with her in this manner till"}
{"doc_id": "gutenberg_1342", "para_id": 7302, "text": "dinner was on table, they left her to vent all her feelings on the"}
{"doc_id": "gutenberg_1342", "para_id": 7303, "text": "housekeeper, who attended in the absence of her daughters."}
{"doc_id": "gutenberg_1342", "para_id": 7304, "text": "Though her brother and sister were persuaded that there was no real"}
{"doc_id": "gutenberg_1342", "para_id": 7305, "text": "occasion for such a seclusion from the family, they did not attempt to"}
{"doc_id": "gutenberg_1342", "para_id": 7306, "text": "oppose it; for they knew that she had not prudence enough to hold her"}
{"doc_id": "gutenberg_1342", "para_id": 7307, "text": "tongue before the servants, while they waited at table, and judged it"}
{"doc_id": "gutenberg_1342", "para_id": 7308, "text": "better that _one_ only of the household, and the one whom they could"}
{"doc_id": "gutenberg_1342", "para_id": 7309, "text": "most trust, should comprehend all her fears and solicitude on the"}
{"doc_id": "gutenberg_1342", "para_id": 7310, "text": "In the dining-room they were soon joined by Mary and Kitty, who had been"}
{"doc_id": "gutenberg_1342", "para_id": 7311, "text": "too busily engaged in their separate apartments to make their appearance"}
{"doc_id": "gutenberg_1342", "para_id": 7312, "text": "before. One came from her books, and the other from her toilette. The"}
{"doc_id": "gutenberg_1342", "para_id": 7313, "text": "faces of both, however, were tolerably calm; and no change was visible"}
{"doc_id": "gutenberg_1342", "para_id": 7314, "text": "in either, except that the loss of her favourite sister, or the anger"}
{"doc_id": "gutenberg_1342", "para_id": 7315, "text": "which she had herself incurred in the business, had given something more"}
{"doc_id": "gutenberg_1342", "para_id": 7316, "text": "of fretfulness than usual to the accents of Kitty. As for Mary, she was"}
{"doc_id": "gutenberg_1342", "para_id": 7317, "text": "mistress enough of herself to whisper to Elizabeth, with a countenance"}
{"doc_id": "gutenberg_1342", "para_id": 7318, "text": "of grave reflection, soon after they were seated at table,--"}
{"doc_id": "gutenberg_1342", "para_id": 7319, "text": "“This is a most unfortunate affair, and will probably be much talked of."}
{"doc_id": "gutenberg_1342", "para_id": 7320, "text": "But we must stem the tide of malice, and pour into the wounded bosoms of"}
{"doc_id": "gutenberg_1342", "para_id": 7321, "text": "Then perceiving in Elizabeth no inclination of replying, she added,"}
{"doc_id": "gutenberg_1342", "para_id": 7322, "text": "“Unhappy as the event must be for Lydia, we may draw from it this useful"}
{"doc_id": "gutenberg_1342", "para_id": 7323, "text": "lesson:--that loss of virtue in a female is irretrievable, that one"}
{"doc_id": "gutenberg_1342", "para_id": 7324, "text": "false step involves her in endless ruin, that her reputation is no less"}
{"doc_id": "gutenberg_1342", "para_id": 7325, "text": "brittle than it is beautiful, and that she cannot be too much guarded in"}
{"doc_id": "gutenberg_1342", "para_id": 7326, "text": "her behaviour towards the undeserving of the other sex.”"}
{"doc_id": "gutenberg_1342", "para_id": 7327, "text": "Elizabeth lifted up her eyes in amazement, but was too much oppressed to"}
{"doc_id": "gutenberg_1342", "para_id": 7328, "text": "make any reply. Mary, however, continued to console herself with such"}
{"doc_id": "gutenberg_1342", "para_id": 7329, "text": "kind of moral extractions from the evil before them."}
{"doc_id": "gutenberg_1342", "para_id": 7330, "text": "In the afternoon, the two elder Miss Bennets were able to be for half an"}
{"doc_id": "gutenberg_1342", "para_id": 7331, "text": "hour by themselves; and Elizabeth instantly availed herself of the"}
{"doc_id": "gutenberg_1342", "para_id": 7332, "text": "opportunity of making any inquiries which Jane was equally eager to"}
{"doc_id": "gutenberg_1342", "para_id": 7333, "text": "satisfy. After joining in general lamentations over the dreadful sequel"}
{"doc_id": "gutenberg_1342", "para_id": 7334, "text": "of this event, which Elizabeth considered as all but certain, and Miss"}
{"doc_id": "gutenberg_1342", "para_id": 7335, "text": "Bennet could not assert to be wholly impossible, the former continued"}
{"doc_id": "gutenberg_1342", "para_id": 7336, "text": "the subject by saying, “But tell me all and everything about it which I"}
{"doc_id": "gutenberg_1342", "para_id": 7337, "text": "have not already heard. Give me further particulars. What did Colonel"}
{"doc_id": "gutenberg_1342", "para_id": 7338, "text": "Forster say? Had they no apprehension of anything before the elopement"}
{"doc_id": "gutenberg_1342", "para_id": 7339, "text": "took place? They must have seen them together for ever.”"}
{"doc_id": "gutenberg_1342", "para_id": 7340, "text": "“Colonel Forster did own that he had often suspected some partiality,"}
{"doc_id": "gutenberg_1342", "para_id": 7341, "text": "especially on Lydia’s side, but nothing to give him any alarm. I am so"}
{"doc_id": "gutenberg_1342", "para_id": 7342, "text": "grieved for him. His behaviour was attentive and kind to the utmost. He"}
{"doc_id": "gutenberg_1342", "para_id": 7343, "text": "_was_ coming to us, in order to assure us of his concern, before he had"}
{"doc_id": "gutenberg_1342", "para_id": 7344, "text": "any idea of their not being gone to Scotland: when that apprehension"}
{"doc_id": "gutenberg_1342", "para_id": 7345, "text": "“And was Denny convinced that Wickham would not marry? Did he know of"}
{"doc_id": "gutenberg_1342", "para_id": 7346, "text": "their intending to go off? Had Colonel Forster seen Denny himself?”"}
{"doc_id": "gutenberg_1342", "para_id": 7347, "text": "“Yes; but when questioned by _him_, Denny denied knowing anything of"}
{"doc_id": "gutenberg_1342", "para_id": 7348, "text": "their plan, and would not give his real opinion about it. He did not"}
{"doc_id": "gutenberg_1342", "para_id": 7349, "text": "repeat his persuasion of their not marrying, and from _that_ I am"}
{"doc_id": "gutenberg_1342", "para_id": 7350, "text": "inclined to hope he might have been misunderstood before.”"}
{"doc_id": "gutenberg_1342", "para_id": 7351, "text": "“And till Colonel Forster came himself, not one of you entertained a"}
{"doc_id": "gutenberg_1342", "para_id": 7352, "text": "“How was it possible that such an idea should enter our brains? I felt a"}
{"doc_id": "gutenberg_1342", "para_id": 7353, "text": "little uneasy--a little fearful of my sister’s happiness with him in"}
{"doc_id": "gutenberg_1342", "para_id": 7354, "text": "marriage, because I knew that his conduct had not been always quite"}
{"doc_id": "gutenberg_1342", "para_id": 7355, "text": "right. My father and mother knew nothing of that; they only felt how"}
{"doc_id": "gutenberg_1342", "para_id": 7356, "text": "imprudent a match it must be. Kitty then owned, with a very natural"}
{"doc_id": "gutenberg_1342", "para_id": 7357, "text": "triumph on knowing more than the rest of us, that in Lydia’s last letter"}
{"doc_id": "gutenberg_1342", "para_id": 7358, "text": "she had prepared her for such a step. She had known, it seems, of their"}
{"doc_id": "gutenberg_1342", "para_id": 7359, "text": "“And did Colonel Forster appear to think ill of Wickham himself? Does he"}
{"doc_id": "gutenberg_1342", "para_id": 7360, "text": "“I must confess that he did not speak so well of Wickham as he formerly"}
{"doc_id": "gutenberg_1342", "para_id": 7361, "text": "did. He believed him to be imprudent and extravagant; and since this sad"}
{"doc_id": "gutenberg_1342", "para_id": 7362, "text": "affair has taken place, it is said that he left Meryton greatly in debt:"}
{"doc_id": "gutenberg_1342", "para_id": 7363, "text": "“Oh, Jane, had we been less secret, had we told what we knew of him,"}
{"doc_id": "gutenberg_1342", "para_id": 7364, "text": "“Perhaps it would have been better,” replied her sister."}
{"doc_id": "gutenberg_1342", "para_id": 7365, "text": "“But to expose the former faults of any person, without knowing what"}
{"doc_id": "gutenberg_1342", "para_id": 7366, "text": "their present feelings were, seemed unjustifiable.”"}
{"doc_id": "gutenberg_1342", "para_id": 7367, "text": "“Could Colonel Forster repeat the particulars of Lydia’s note to his"}
{"doc_id": "gutenberg_1342", "para_id": 7368, "text": "Jane then took it from her pocket-book, and gave it to Elizabeth. These"}
{"doc_id": "gutenberg_1342", "para_id": 7369, "text": "“You will laugh when you know where I am gone, and I cannot help"}
{"doc_id": "gutenberg_1342", "para_id": 7370, "text": "laughing myself at your surprise to-morrow morning, as soon as I am"}
{"doc_id": "gutenberg_1342", "para_id": 7371, "text": "missed. I am going to Gretna Green, and if you cannot guess with"}
{"doc_id": "gutenberg_1342", "para_id": 7372, "text": "who, I shall think you a simpleton, for there is but one man in the"}
{"doc_id": "gutenberg_1342", "para_id": 7373, "text": "world I love, and he is an angel. I should never be happy without"}
{"doc_id": "gutenberg_1342", "para_id": 7374, "text": "him, so think it no harm to be off. You need not send them word at"}
{"doc_id": "gutenberg_1342", "para_id": 7375, "text": "Longbourn of my going, if you do not like it, for it will make the"}
{"doc_id": "gutenberg_1342", "para_id": 7376, "text": "surprise the greater when I write to them, and sign my name Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 7377, "text": "Wickham. What a good joke it will be! I can hardly write for"}
{"doc_id": "gutenberg_1342", "para_id": 7378, "text": "laughing. Pray make my excuses to Pratt for not keeping my"}
{"doc_id": "gutenberg_1342", "para_id": 7379, "text": "engagement, and dancing with him to-night. Tell him I hope he will"}
{"doc_id": "gutenberg_1342", "para_id": 7380, "text": "excuse me when he knows all, and tell him I will dance with him at"}
{"doc_id": "gutenberg_1342", "para_id": 7381, "text": "the next ball we meet with great pleasure. I shall send for my"}
{"doc_id": "gutenberg_1342", "para_id": 7382, "text": "clothes when I get to Longbourn; but I wish you would tell Sally to"}
{"doc_id": "gutenberg_1342", "para_id": 7383, "text": "mend a great slit in my worked muslin gown before they are packed"}
{"doc_id": "gutenberg_1342", "para_id": 7384, "text": "up. Good-bye. Give my love to Colonel Forster. I hope you will"}
{"doc_id": "gutenberg_1342", "para_id": 7385, "text": "“Oh, thoughtless, thoughtless Lydia!” cried Elizabeth when she had"}
{"doc_id": "gutenberg_1342", "para_id": 7386, "text": "finished it. “What a letter is this, to be written at such a moment! But"}
{"doc_id": "gutenberg_1342", "para_id": 7387, "text": "at least it shows that _she_ was serious in the object of her journey."}
{"doc_id": "gutenberg_1342", "para_id": 7388, "text": "Whatever he might afterwards persuade her to, it was not on her side a"}
{"doc_id": "gutenberg_1342", "para_id": 7389, "text": "_scheme_ of infamy. My poor father! how he must have felt it!”"}
{"doc_id": "gutenberg_1342", "para_id": 7390, "text": "“I never saw anyone so shocked. He could not speak a word for full ten"}
{"doc_id": "gutenberg_1342", "para_id": 7391, "text": "minutes. My mother was taken ill immediately, and the whole house in"}
{"doc_id": "gutenberg_1342", "para_id": 7392, "text": "“Oh, Jane,” cried Elizabeth, “was there a servant belonging to it who"}
{"doc_id": "gutenberg_1342", "para_id": 7393, "text": "did not know the whole story before the end of the day?”"}
{"doc_id": "gutenberg_1342", "para_id": 7394, "text": "“I do not know: I hope there was. But to be guarded at such a time is"}
{"doc_id": "gutenberg_1342", "para_id": 7395, "text": "very difficult. My mother was in hysterics; and though I endeavoured to"}
{"doc_id": "gutenberg_1342", "para_id": 7396, "text": "give her every assistance in my power, I am afraid I did not do so much"}
{"doc_id": "gutenberg_1342", "para_id": 7397, "text": "as I might have done. But the horror of what might possibly happen"}
{"doc_id": "gutenberg_1342", "para_id": 7398, "text": "“Your attendance upon her has been too much for you. You do not look"}
{"doc_id": "gutenberg_1342", "para_id": 7399, "text": "well. Oh that I had been with you! you have had every care and anxiety"}
{"doc_id": "gutenberg_1342", "para_id": 7400, "text": "“Mary and Kitty have been very kind, and would have shared in every"}
{"doc_id": "gutenberg_1342", "para_id": 7401, "text": "fatigue, I am sure, but I did not think it right for either of them."}
{"doc_id": "gutenberg_1342", "para_id": 7402, "text": "Kitty is slight and delicate, and Mary studies so much that her hours of"}
{"doc_id": "gutenberg_1342", "para_id": 7403, "text": "repose should not be broken in on. My aunt Philips came to Longbourn on"}
{"doc_id": "gutenberg_1342", "para_id": 7404, "text": "Tuesday, after my father went away; and was so good as to stay till"}
{"doc_id": "gutenberg_1342", "para_id": 7405, "text": "Thursday with me. She was of great use and comfort to us all, and Lady"}
{"doc_id": "gutenberg_1342", "para_id": 7406, "text": "Lucas has been very kind: she walked here on Wednesday morning to"}
{"doc_id": "gutenberg_1342", "para_id": 7407, "text": "condole with us, and offered her services, or any of her daughters, if"}
{"doc_id": "gutenberg_1342", "para_id": 7408, "text": "“She had better have stayed at home,” cried Elizabeth: “perhaps she"}
{"doc_id": "gutenberg_1342", "para_id": 7409, "text": "_meant_ well, but, under such a misfortune as this, one cannot see too"}
{"doc_id": "gutenberg_1342", "para_id": 7410, "text": "little of one’s neighbours. Assistance is impossible; condolence,"}
{"doc_id": "gutenberg_1342", "para_id": 7411, "text": "insufferable. Let them triumph over us at a distance, and be satisfied.”"}
{"doc_id": "gutenberg_1342", "para_id": 7412, "text": "She then proceeded to inquire into the measures which her father had"}
{"doc_id": "gutenberg_1342", "para_id": 7413, "text": "intended to pursue, while in town, for the recovery of his daughter."}
{"doc_id": "gutenberg_1342", "para_id": 7414, "text": "“He meant, I believe,” replied Jane, “to go to Epsom, the place where"}
{"doc_id": "gutenberg_1342", "para_id": 7415, "text": "they last changed horses, see the postilions, and try if anything could"}
{"doc_id": "gutenberg_1342", "para_id": 7416, "text": "be made out from them. His principal object must be to discover the"}
{"doc_id": "gutenberg_1342", "para_id": 7417, "text": "number of the hackney coach which took them from Clapham. It had come"}
{"doc_id": "gutenberg_1342", "para_id": 7418, "text": "with a fare from London; and as he thought the circumstance of a"}
{"doc_id": "gutenberg_1342", "para_id": 7419, "text": "gentleman and lady’s removing from one carriage into another might be"}
{"doc_id": "gutenberg_1342", "para_id": 7420, "text": "remarked, he meant to make inquiries at Clapham. If he could anyhow"}
{"doc_id": "gutenberg_1342", "para_id": 7421, "text": "discover at what house the coachman had before set down his fare, he"}
{"doc_id": "gutenberg_1342", "para_id": 7422, "text": "determined to make inquiries there, and hoped it might not be impossible"}
{"doc_id": "gutenberg_1342", "para_id": 7423, "text": "to find out the stand and number of the coach. I do not know of any"}
{"doc_id": "gutenberg_1342", "para_id": 7424, "text": "other designs that he had formed; but he was in such a hurry to be gone,"}
{"doc_id": "gutenberg_1342", "para_id": 7425, "text": "and his spirits so greatly discomposed, that I had difficulty in finding"}
{"doc_id": "gutenberg_1342", "para_id": 7426, "text": "The whole party were in hopes of a letter from Mr. Bennet the next"}
{"doc_id": "gutenberg_1342", "para_id": 7427, "text": "morning, but the post came in without bringing a single line from him."}
{"doc_id": "gutenberg_1342", "para_id": 7428, "text": "His family knew him to be, on all common occasions, a most negligent and"}
{"doc_id": "gutenberg_1342", "para_id": 7429, "text": "dilatory correspondent; but at such a time they had hoped for exertion."}
{"doc_id": "gutenberg_1342", "para_id": 7430, "text": "They were forced to conclude, that he had no pleasing intelligence to"}
{"doc_id": "gutenberg_1342", "para_id": 7431, "text": "send; but even of _that_ they would have been glad to be certain. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7432, "text": "Gardiner had waited only for the letters before he set off."}
{"doc_id": "gutenberg_1342", "para_id": 7433, "text": "When he was gone, they were certain at least of receiving constant"}
{"doc_id": "gutenberg_1342", "para_id": 7434, "text": "information of what was going on; and their uncle promised, at parting,"}
{"doc_id": "gutenberg_1342", "para_id": 7435, "text": "to prevail on Mr. Bennet to return to Longbourn as soon as he could, to"}
{"doc_id": "gutenberg_1342", "para_id": 7436, "text": "the great consolation of his sister, who considered it as the only"}
{"doc_id": "gutenberg_1342", "para_id": 7437, "text": "security for her husband’s not being killed in a duel."}
{"doc_id": "gutenberg_1342", "para_id": 7438, "text": "Mrs. Gardiner and the children were to remain in Hertfordshire a few"}
{"doc_id": "gutenberg_1342", "para_id": 7439, "text": "days longer, as the former thought her presence might be serviceable to"}
{"doc_id": "gutenberg_1342", "para_id": 7440, "text": "her nieces. She shared in their attendance on Mrs. Bennet, and was a"}
{"doc_id": "gutenberg_1342", "para_id": 7441, "text": "great comfort to them in their hours of freedom. Their other aunt also"}
{"doc_id": "gutenberg_1342", "para_id": 7442, "text": "visited them frequently, and always, as she said, with the design of"}
{"doc_id": "gutenberg_1342", "para_id": 7443, "text": "cheering and heartening them up--though, as she never came without"}
{"doc_id": "gutenberg_1342", "para_id": 7444, "text": "reporting some fresh instance of Wickham’s extravagance or irregularity,"}
{"doc_id": "gutenberg_1342", "para_id": 7445, "text": "she seldom went away without leaving them more dispirited than she found"}
{"doc_id": "gutenberg_1342", "para_id": 7446, "text": "All Meryton seemed striving to blacken the man who, but three months"}
{"doc_id": "gutenberg_1342", "para_id": 7447, "text": "before, had been almost an angel of light. He was declared to be in debt"}
{"doc_id": "gutenberg_1342", "para_id": 7448, "text": "to every tradesman in the place, and his intrigues, all honoured with"}
{"doc_id": "gutenberg_1342", "para_id": 7449, "text": "the title of seduction, had been extended into every tradesman’s family."}
{"doc_id": "gutenberg_1342", "para_id": 7450, "text": "Everybody declared that he was the wickedest young man in the world; and"}
{"doc_id": "gutenberg_1342", "para_id": 7451, "text": "everybody began to find out that they had always distrusted the"}
{"doc_id": "gutenberg_1342", "para_id": 7452, "text": "appearance of his goodness. Elizabeth, though she did not credit above"}
{"doc_id": "gutenberg_1342", "para_id": 7453, "text": "half of what was said, believed enough to make her former assurance of"}
{"doc_id": "gutenberg_1342", "para_id": 7454, "text": "her sister’s ruin still more certain; and even Jane, who believed still"}
{"doc_id": "gutenberg_1342", "para_id": 7455, "text": "less of it, became almost hopeless, more especially as the time was now"}
{"doc_id": "gutenberg_1342", "para_id": 7456, "text": "come, when, if they had gone to Scotland, which she had never before"}
{"doc_id": "gutenberg_1342", "para_id": 7457, "text": "entirely despaired of, they must in all probability have gained some"}
{"doc_id": "gutenberg_1342", "para_id": 7458, "text": "Mr. Gardiner left Longbourn on Sunday; on Tuesday, his wife received a"}
{"doc_id": "gutenberg_1342", "para_id": 7459, "text": "letter from him: it told them, that on his arrival he had immediately"}
{"doc_id": "gutenberg_1342", "para_id": 7460, "text": "found out his brother, and persuaded him to come to Gracechurch Street."}
{"doc_id": "gutenberg_1342", "para_id": 7461, "text": "That Mr. Bennet had been to Epsom and Clapham, before his arrival, but"}
{"doc_id": "gutenberg_1342", "para_id": 7462, "text": "without gaining any satisfactory information; and that he was now"}
{"doc_id": "gutenberg_1342", "para_id": 7463, "text": "determined to inquire at all the principal hotels in town, as Mr. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 7464, "text": "thought it possible they might have gone to one of them, on their first"}
{"doc_id": "gutenberg_1342", "para_id": 7465, "text": "coming to London, before they procured lodgings. Mr. Gardiner himself"}
{"doc_id": "gutenberg_1342", "para_id": 7466, "text": "did not expect any success from this measure; but as his brother was"}
{"doc_id": "gutenberg_1342", "para_id": 7467, "text": "eager in it, he meant to assist him in pursuing it. He added, that Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7468, "text": "Bennet seemed wholly disinclined at present to leave London, and"}
{"doc_id": "gutenberg_1342", "para_id": 7469, "text": "promised to write again very soon. There was also a postscript to this"}
{"doc_id": "gutenberg_1342", "para_id": 7470, "text": "“I have written to Colonel Forster to desire him to find out, if"}
{"doc_id": "gutenberg_1342", "para_id": 7471, "text": "possible, from some of the young man’s intimates in the regiment,"}
{"doc_id": "gutenberg_1342", "para_id": 7472, "text": "whether Wickham has any relations or connections who would be likely to"}
{"doc_id": "gutenberg_1342", "para_id": 7473, "text": "know in what part of the town he has now concealed himself. If there"}
{"doc_id": "gutenberg_1342", "para_id": 7474, "text": "were anyone that one could apply to, with a probability of gaining such"}
{"doc_id": "gutenberg_1342", "para_id": 7475, "text": "a clue as that, it might be of essential consequence. At present we have"}
{"doc_id": "gutenberg_1342", "para_id": 7476, "text": "nothing to guide us. Colonel Forster will, I dare say, do everything in"}
{"doc_id": "gutenberg_1342", "para_id": 7477, "text": "his power to satisfy us on this head. But, on second thoughts, perhaps"}
{"doc_id": "gutenberg_1342", "para_id": 7478, "text": "Lizzy could tell us what relations he has now living better than any"}
{"doc_id": "gutenberg_1342", "para_id": 7479, "text": "Elizabeth was at no loss to understand from whence this deference for"}
{"doc_id": "gutenberg_1342", "para_id": 7480, "text": "her authority proceeded; but it was not in her power to give any"}
{"doc_id": "gutenberg_1342", "para_id": 7481, "text": "information of so satisfactory a nature as the compliment deserved."}
{"doc_id": "gutenberg_1342", "para_id": 7482, "text": "She had never heard of his having had any relations, except a father"}
{"doc_id": "gutenberg_1342", "para_id": 7483, "text": "and mother, both of whom had been dead many years. It was possible,"}
{"doc_id": "gutenberg_1342", "para_id": 7484, "text": "however, that some of his companions in the ----shire might be able to"}
{"doc_id": "gutenberg_1342", "para_id": 7485, "text": "give more information; and though she was not very sanguine in expecting"}
{"doc_id": "gutenberg_1342", "para_id": 7486, "text": "it, the application was a something to look forward to."}
{"doc_id": "gutenberg_1342", "para_id": 7487, "text": "Every day at Longbourn was now a day of anxiety; but the most anxious"}
{"doc_id": "gutenberg_1342", "para_id": 7488, "text": "part of each was when the post was expected. The arrival of letters was"}
{"doc_id": "gutenberg_1342", "para_id": 7489, "text": "the first grand object of every morning’s impatience. Through letters,"}
{"doc_id": "gutenberg_1342", "para_id": 7490, "text": "whatever of good or bad was to be told would be communicated; and every"}
{"doc_id": "gutenberg_1342", "para_id": 7491, "text": "succeeding day was expected to bring some news of importance."}
{"doc_id": "gutenberg_1342", "para_id": 7492, "text": "But before they heard again from Mr. Gardiner, a letter arrived for"}
{"doc_id": "gutenberg_1342", "para_id": 7493, "text": "their father, from a different quarter, from Mr. Collins; which, as Jane"}
{"doc_id": "gutenberg_1342", "para_id": 7494, "text": "had received directions to open all that came for him in his absence,"}
{"doc_id": "gutenberg_1342", "para_id": 7495, "text": "she accordingly read; and Elizabeth, who knew what curiosities his"}
{"doc_id": "gutenberg_1342", "para_id": 7496, "text": "letters always were, looked over her, and read it likewise. It was as"}
{"doc_id": "gutenberg_1342", "para_id": 7497, "text": "“I feel myself called upon, by our relationship, and my situation"}
{"doc_id": "gutenberg_1342", "para_id": 7498, "text": "in life, to condole with you on the grievous affliction you are now"}
{"doc_id": "gutenberg_1342", "para_id": 7499, "text": "suffering under, of which we were yesterday informed by a letter"}
{"doc_id": "gutenberg_1342", "para_id": 7500, "text": "from Hertfordshire. Be assured, my dear sir, that Mrs. Collins and"}
{"doc_id": "gutenberg_1342", "para_id": 7501, "text": "myself sincerely sympathize with you, and all your respectable"}
{"doc_id": "gutenberg_1342", "para_id": 7502, "text": "family, in your present distress, which must be of the bitterest"}
{"doc_id": "gutenberg_1342", "para_id": 7503, "text": "kind, because proceeding from a cause which no time can remove. No"}
{"doc_id": "gutenberg_1342", "para_id": 7504, "text": "arguments shall be wanting on my part, that can alleviate so severe"}
{"doc_id": "gutenberg_1342", "para_id": 7505, "text": "a misfortune; or that may comfort you, under a circumstance that"}
{"doc_id": "gutenberg_1342", "para_id": 7506, "text": "must be, of all others, most afflicting to a parent’s mind. The"}
{"doc_id": "gutenberg_1342", "para_id": 7507, "text": "death of your daughter would have been a blessing in comparison of"}
{"doc_id": "gutenberg_1342", "para_id": 7508, "text": "this. And it is the more to be lamented, because there is reason to"}
{"doc_id": "gutenberg_1342", "para_id": 7509, "text": "suppose, as my dear Charlotte informs me, that this licentiousness"}
{"doc_id": "gutenberg_1342", "para_id": 7510, "text": "daughter has proceeded from a faulty degree of indulgence; though,"}
{"doc_id": "gutenberg_1342", "para_id": 7511, "text": "at the same time, for the consolation of yourself and Mrs. Bennet,"}
{"doc_id": "gutenberg_1342", "para_id": 7512, "text": "I am inclined to think that her own disposition must be naturally"}
{"doc_id": "gutenberg_1342", "para_id": 7513, "text": "bad, or she could not be guilty of such an enormity, at so early an"}
{"doc_id": "gutenberg_1342", "para_id": 7514, "text": "age. Howsoever that may be, you are grievously to be pitied; in"}
{"doc_id": "gutenberg_1342", "para_id": 7515, "text": "which opinion I am not only joined by Mrs. Collins, but likewise by"}
{"doc_id": "gutenberg_1342", "para_id": 7516, "text": "Lady Catherine and her daughter, to whom I have related the affair."}
{"doc_id": "gutenberg_1342", "para_id": 7517, "text": "They agree with me in apprehending that this false step in one"}
{"doc_id": "gutenberg_1342", "para_id": 7518, "text": "daughter will be injurious to the fortunes of all the others: for"}
{"doc_id": "gutenberg_1342", "para_id": 7519, "text": "who, as Lady Catherine herself condescendingly says, will connect"}
{"doc_id": "gutenberg_1342", "para_id": 7520, "text": "themselves with such a family? And this consideration leads me,"}
{"doc_id": "gutenberg_1342", "para_id": 7521, "text": "moreover, to reflect, with augmented satisfaction, on a certain"}
{"doc_id": "gutenberg_1342", "para_id": 7522, "text": "event of last November; for had it been otherwise, I must have been"}
{"doc_id": "gutenberg_1342", "para_id": 7523, "text": "involved in all your sorrow and disgrace. Let me advise you, then,"}
{"doc_id": "gutenberg_1342", "para_id": 7524, "text": "my dear sir, to console yourself as much as possible, to throw off"}
{"doc_id": "gutenberg_1342", "para_id": 7525, "text": "your unworthy child from your affection for ever, and leave her to"}
{"doc_id": "gutenberg_1342", "para_id": 7526, "text": "Mr. Gardiner did not write again, till he had received an answer from"}
{"doc_id": "gutenberg_1342", "para_id": 7527, "text": "Colonel Forster; and then he had nothing of a pleasant nature to send."}
{"doc_id": "gutenberg_1342", "para_id": 7528, "text": "It was not known that Wickham had a single relation with whom he kept up"}
{"doc_id": "gutenberg_1342", "para_id": 7529, "text": "any connection, and it was certain that he had no near one living. His"}
{"doc_id": "gutenberg_1342", "para_id": 7530, "text": "former acquaintance had been numerous; but since he had been in the"}
{"doc_id": "gutenberg_1342", "para_id": 7531, "text": "militia, it did not appear that he was on terms of particular friendship"}
{"doc_id": "gutenberg_1342", "para_id": 7532, "text": "with any of them. There was no one, therefore, who could be pointed out"}
{"doc_id": "gutenberg_1342", "para_id": 7533, "text": "as likely to give any news of him. And in the wretched state of his own"}
{"doc_id": "gutenberg_1342", "para_id": 7534, "text": "finances, there was a very powerful motive for secrecy, in addition to"}
{"doc_id": "gutenberg_1342", "para_id": 7535, "text": "his fear of discovery by Lydia’s relations; for it had just transpired"}
{"doc_id": "gutenberg_1342", "para_id": 7536, "text": "that he had left gaming debts behind him to a very considerable amount."}
{"doc_id": "gutenberg_1342", "para_id": 7537, "text": "Colonel Forster believed that more than a thousand pounds would be"}
{"doc_id": "gutenberg_1342", "para_id": 7538, "text": "necessary to clear his expenses at Brighton. He owed a good deal in the"}
{"doc_id": "gutenberg_1342", "para_id": 7539, "text": "town, but his debts of honour were still more formidable. Mr. Gardiner"}
{"doc_id": "gutenberg_1342", "para_id": 7540, "text": "did not attempt to conceal these particulars from the Longbourn family;"}
{"doc_id": "gutenberg_1342", "para_id": 7541, "text": "Jane heard them with horror. “A gamester!” she cried. “This is wholly"}
{"doc_id": "gutenberg_1342", "para_id": 7542, "text": "Mr. Gardiner added, in his letter, that they might expect to see their"}
{"doc_id": "gutenberg_1342", "para_id": 7543, "text": "father at home on the following day, which was Saturday. Rendered"}
{"doc_id": "gutenberg_1342", "para_id": 7544, "text": "spiritless by the ill success of all their endeavours, he had yielded to"}
{"doc_id": "gutenberg_1342", "para_id": 7545, "text": "his brother-in-law’s entreaty that he would return to his family and"}
{"doc_id": "gutenberg_1342", "para_id": 7546, "text": "leave it to him to do whatever occasion might suggest to be advisable"}
{"doc_id": "gutenberg_1342", "para_id": 7547, "text": "for continuing their pursuit. When Mrs. Bennet was told of this, she did"}
{"doc_id": "gutenberg_1342", "para_id": 7548, "text": "not express so much satisfaction as her children expected, considering"}
{"doc_id": "gutenberg_1342", "para_id": 7549, "text": "“What! is he coming home, and without poor Lydia?” she cried. “Sure he"}
{"doc_id": "gutenberg_1342", "para_id": 7550, "text": "will not leave London before he has found them. Who is to fight Wickham,"}
{"doc_id": "gutenberg_1342", "para_id": 7551, "text": "As Mrs. Gardiner began to wish to be at home, it was settled that she"}
{"doc_id": "gutenberg_1342", "para_id": 7552, "text": "and her children should go to London at the same time that Mr. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 7553, "text": "came from it. The coach, therefore, took them the first stage of their"}
{"doc_id": "gutenberg_1342", "para_id": 7554, "text": "Mrs. Gardiner went away in all the perplexity about Elizabeth and her"}
{"doc_id": "gutenberg_1342", "para_id": 7555, "text": "Derbyshire friend, that had attended her from that part of the world."}
{"doc_id": "gutenberg_1342", "para_id": 7556, "text": "His name had never been voluntarily mentioned before them by her niece;"}
{"doc_id": "gutenberg_1342", "para_id": 7557, "text": "and the kind of half-expectation which Mrs. Gardiner had formed, of"}
{"doc_id": "gutenberg_1342", "para_id": 7558, "text": "their being followed by a letter from him, had ended in nothing."}
{"doc_id": "gutenberg_1342", "para_id": 7559, "text": "Elizabeth had received none since her return, that could come from"}
{"doc_id": "gutenberg_1342", "para_id": 7560, "text": "The present unhappy state of the family rendered any other excuse for"}
{"doc_id": "gutenberg_1342", "para_id": 7561, "text": "the lowness of her spirits unnecessary; nothing, therefore, could be"}
{"doc_id": "gutenberg_1342", "para_id": 7562, "text": "fairly conjectured from _that_,--though Elizabeth, who was by this time"}
{"doc_id": "gutenberg_1342", "para_id": 7563, "text": "tolerably well acquainted with her own feelings, was perfectly aware"}
{"doc_id": "gutenberg_1342", "para_id": 7564, "text": "that, had she known nothing of Darcy, she could have borne the dread of"}
{"doc_id": "gutenberg_1342", "para_id": 7565, "text": "Lydia’s infamy somewhat better. It would have spared her, she thought,"}
{"doc_id": "gutenberg_1342", "para_id": 7566, "text": "When Mr. Bennet arrived, he had all the appearance of his usual"}
{"doc_id": "gutenberg_1342", "para_id": 7567, "text": "philosophic composure. He said as little as he had ever been in the"}
{"doc_id": "gutenberg_1342", "para_id": 7568, "text": "habit of saying; made no mention of the business that had taken him"}
{"doc_id": "gutenberg_1342", "para_id": 7569, "text": "away; and it was some time before his daughters had courage to speak of"}
{"doc_id": "gutenberg_1342", "para_id": 7570, "text": "It was not till the afternoon, when he joined them at tea, that"}
{"doc_id": "gutenberg_1342", "para_id": 7571, "text": "Elizabeth ventured to introduce the subject; and then, on her briefly"}
{"doc_id": "gutenberg_1342", "para_id": 7572, "text": "expressing her sorrow for what he must have endured, he replied, “Say"}
{"doc_id": "gutenberg_1342", "para_id": 7573, "text": "nothing of that. Who should suffer but myself? It has been my own doing,"}
{"doc_id": "gutenberg_1342", "para_id": 7574, "text": "“You must not be too severe upon yourself,” replied Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 7575, "text": "“You may well warn me against such an evil. Human nature is so prone to"}
{"doc_id": "gutenberg_1342", "para_id": 7576, "text": "fall into it! No, Lizzy, let me once in my life feel how much I have"}
{"doc_id": "gutenberg_1342", "para_id": 7577, "text": "been to blame. I am not afraid of being overpowered by the impression."}
{"doc_id": "gutenberg_1342", "para_id": 7578, "text": "“And Lydia used to want to go to London,” added Kitty."}
{"doc_id": "gutenberg_1342", "para_id": 7579, "text": "“She is happy, then,” said her father, drily; “and her residence there"}
{"doc_id": "gutenberg_1342", "para_id": 7580, "text": "Then, after a short silence, he continued, “Lizzy, I bear you no"}
{"doc_id": "gutenberg_1342", "para_id": 7581, "text": "ill-will for being justified in your advice to me last May, which,"}
{"doc_id": "gutenberg_1342", "para_id": 7582, "text": "considering the event, shows some greatness of mind.”"}
{"doc_id": "gutenberg_1342", "para_id": 7583, "text": "They were interrupted by Miss Bennet, who came to fetch her mother’s"}
{"doc_id": "gutenberg_1342", "para_id": 7584, "text": "“This is a parade,” cried he, “which does one good; it gives such an"}
{"doc_id": "gutenberg_1342", "para_id": 7585, "text": "elegance to misfortune! Another day I will do the same; I will sit in my"}
{"doc_id": "gutenberg_1342", "para_id": 7586, "text": "library, in my nightcap and powdering gown, and give as much trouble as"}
{"doc_id": "gutenberg_1342", "para_id": 7587, "text": "I can,--or perhaps I may defer it till Kitty runs away.”"}
{"doc_id": "gutenberg_1342", "para_id": 7588, "text": "“I am not going to run away, papa,” said Kitty, fretfully. “If _I_"}
{"doc_id": "gutenberg_1342", "para_id": 7589, "text": "should ever go to Brighton, I would behave better than Lydia.”"}
{"doc_id": "gutenberg_1342", "para_id": 7590, "text": "“_You_ go to Brighton! I would not trust you so near it as Eastbourne,"}
{"doc_id": "gutenberg_1342", "para_id": 7591, "text": "for fifty pounds! No, Kitty, I have at least learnt to be cautious, and"}
{"doc_id": "gutenberg_1342", "para_id": 7592, "text": "you will feel the effects of it. No officer is ever to enter my house"}
{"doc_id": "gutenberg_1342", "para_id": 7593, "text": "again, nor even to pass through the village. Balls will be absolutely"}
{"doc_id": "gutenberg_1342", "para_id": 7594, "text": "prohibited, unless you stand up with one of your sisters. And you are"}
{"doc_id": "gutenberg_1342", "para_id": 7595, "text": "never to stir out of doors, till you can prove that you have spent ten"}
{"doc_id": "gutenberg_1342", "para_id": 7596, "text": "Kitty, who took all these threats in a serious light, began to cry."}
{"doc_id": "gutenberg_1342", "para_id": 7597, "text": "“Well, well,” said he, “do not make yourself unhappy. If you are a good"}
{"doc_id": "gutenberg_1342", "para_id": 7598, "text": "girl for the next ten years, I will take you to a review at the end of"}
{"doc_id": "gutenberg_1342", "para_id": 7599, "text": "Two days after Mr. Bennet’s return, as Jane and Elizabeth were walking"}
{"doc_id": "gutenberg_1342", "para_id": 7600, "text": "together in the shrubbery behind the house, they saw the housekeeper"}
{"doc_id": "gutenberg_1342", "para_id": 7601, "text": "coming towards them, and concluding that she came to call them to their"}
{"doc_id": "gutenberg_1342", "para_id": 7602, "text": "mother, went forward to meet her; but instead of the expected summons,"}
{"doc_id": "gutenberg_1342", "para_id": 7603, "text": "when they approached her, she said to Miss Bennet, “I beg your pardon,"}
{"doc_id": "gutenberg_1342", "para_id": 7604, "text": "madam, for interrupting you, but I was in hopes you might have got some"}
{"doc_id": "gutenberg_1342", "para_id": 7605, "text": "good news from town, so I took the liberty of coming to ask.”"}
{"doc_id": "gutenberg_1342", "para_id": 7606, "text": "“What do you mean, Hill? We have heard nothing from town.”"}
{"doc_id": "gutenberg_1342", "para_id": 7607, "text": "“Dear madam,” cried Mrs. Hill, in great astonishment, “don’t you know"}
{"doc_id": "gutenberg_1342", "para_id": 7608, "text": "there is an express come for master from Mr. Gardiner? He has been here"}
{"doc_id": "gutenberg_1342", "para_id": 7609, "text": "Away ran the girls, too eager to get in to have time for speech. They"}
{"doc_id": "gutenberg_1342", "para_id": 7610, "text": "ran through the vestibule into the breakfast-room; from thence to the"}
{"doc_id": "gutenberg_1342", "para_id": 7611, "text": "library;--their father was in neither; and they were on the point of"}
{"doc_id": "gutenberg_1342", "para_id": 7612, "text": "seeking him upstairs with their mother, when they were met by the"}
{"doc_id": "gutenberg_1342", "para_id": 7613, "text": "“If you are looking for my master, ma’am, he is walking towards the"}
{"doc_id": "gutenberg_1342", "para_id": 7614, "text": "Upon this information, they instantly passed through the hall once more,"}
{"doc_id": "gutenberg_1342", "para_id": 7615, "text": "and ran across the lawn after their father, who was deliberately"}
{"doc_id": "gutenberg_1342", "para_id": 7616, "text": "pursuing his way towards a small wood on one side of the paddock."}
{"doc_id": "gutenberg_1342", "para_id": 7617, "text": "Jane, who was not so light, nor so much in the habit of running as"}
{"doc_id": "gutenberg_1342", "para_id": 7618, "text": "Elizabeth, soon lagged behind, while her sister, panting for breath,"}
{"doc_id": "gutenberg_1342", "para_id": 7619, "text": "“Oh, papa, what news? what news? have you heard from my uncle?”"}
{"doc_id": "gutenberg_1342", "para_id": 7620, "text": "“What is there of good to be expected?” said he, taking the letter from"}
{"doc_id": "gutenberg_1342", "para_id": 7621, "text": "his pocket; “but perhaps you would like to read it.”"}
{"doc_id": "gutenberg_1342", "para_id": 7622, "text": "Elizabeth impatiently caught it from his hand. Jane now came up."}
{"doc_id": "gutenberg_1342", "para_id": 7623, "text": "“Read it aloud,” said their father, “for I hardly know myself what it is"}
{"doc_id": "gutenberg_1342", "para_id": 7624, "text": "/* RIGHT “Gracechurch Street, _Monday, August 2_. */"}
{"doc_id": "gutenberg_1342", "para_id": 7625, "text": "“At last I am able to send you some tidings of my niece, and such"}
{"doc_id": "gutenberg_1342", "para_id": 7626, "text": "as, upon the whole, I hope will give you satisfaction. Soon after"}
{"doc_id": "gutenberg_1342", "para_id": 7627, "text": "you left me on Saturday, I was fortunate enough to find out in what"}
{"doc_id": "gutenberg_1342", "para_id": 7628, "text": "part of London they were. The particulars I reserve till we meet."}
{"doc_id": "gutenberg_1342", "para_id": 7629, "text": "It is enough to know they are discovered: I have seen them"}
{"doc_id": "gutenberg_1342", "para_id": 7630, "text": "“Then it is as I always hoped,” cried Jane: “they are married!”"}
{"doc_id": "gutenberg_1342", "para_id": 7631, "text": "Elizabeth read on: “I have seen them both. They are not married,"}
{"doc_id": "gutenberg_1342", "para_id": 7632, "text": "nor can I find there was any intention of being so; but if you are"}
{"doc_id": "gutenberg_1342", "para_id": 7633, "text": "willing to perform the engagements which I have ventured to make on"}
{"doc_id": "gutenberg_1342", "para_id": 7634, "text": "your side, I hope it will not be long before they are. All that is"}
{"doc_id": "gutenberg_1342", "para_id": 7635, "text": "required of you is, to assure to your daughter, by settlement, her"}
{"doc_id": "gutenberg_1342", "para_id": 7636, "text": "equal share of the five thousand pounds, secured among your"}
{"doc_id": "gutenberg_1342", "para_id": 7637, "text": "children after the decease of yourself and my sister; and,"}
{"doc_id": "gutenberg_1342", "para_id": 7638, "text": "moreover, to enter into an engagement of allowing her, during your"}
{"doc_id": "gutenberg_1342", "para_id": 7639, "text": "life, one hundred pounds per annum. These are conditions which,"}
{"doc_id": "gutenberg_1342", "para_id": 7640, "text": "considering everything, I had no hesitation in complying with, as"}
{"doc_id": "gutenberg_1342", "para_id": 7641, "text": "far as I thought myself privileged, for you. I shall send this by"}
{"doc_id": "gutenberg_1342", "para_id": 7642, "text": "express, that no time may be lost in bringing me your answer. You"}
{"doc_id": "gutenberg_1342", "para_id": 7643, "text": "will easily comprehend, from these particulars, that Mr. Wickham’s"}
{"doc_id": "gutenberg_1342", "para_id": 7644, "text": "circumstances are not so hopeless as they are generally believed to"}
{"doc_id": "gutenberg_1342", "para_id": 7645, "text": "be. The world has been deceived in that respect; and I am happy to"}
{"doc_id": "gutenberg_1342", "para_id": 7646, "text": "say, there will be some little money, even when all his debts are"}
{"doc_id": "gutenberg_1342", "para_id": 7647, "text": "discharged, to settle on my niece, in addition to her own fortune."}
{"doc_id": "gutenberg_1342", "para_id": 7648, "text": "If, as I conclude will be the case, you send me full powers to act"}
{"doc_id": "gutenberg_1342", "para_id": 7649, "text": "in your name throughout the whole of this business, I will"}
{"doc_id": "gutenberg_1342", "para_id": 7650, "text": "immediately give directions to Haggerston for preparing a proper"}
{"doc_id": "gutenberg_1342", "para_id": 7651, "text": "settlement. There will not be the smallest occasion for your coming"}
{"doc_id": "gutenberg_1342", "para_id": 7652, "text": "to town again; therefore stay quietly at Longbourn, and depend on"}
{"doc_id": "gutenberg_1342", "para_id": 7653, "text": "my diligence and care. Send back your answer as soon as you can,"}
{"doc_id": "gutenberg_1342", "para_id": 7654, "text": "and be careful to write explicitly. We have judged it best that my"}
{"doc_id": "gutenberg_1342", "para_id": 7655, "text": "niece should be married from this house, of which I hope you will"}
{"doc_id": "gutenberg_1342", "para_id": 7656, "text": "approve. She comes to us to-day. I shall write again as soon as"}
{"doc_id": "gutenberg_1342", "para_id": 7657, "text": "“Is it possible?” cried Elizabeth, when she had finished. “Can it be"}
{"doc_id": "gutenberg_1342", "para_id": 7658, "text": "“Wickham is not so undeserving, then, as we have thought him,” said her"}
{"doc_id": "gutenberg_1342", "para_id": 7659, "text": "“And have you answered the letter?” said Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 7660, "text": "Most earnestly did she then entreat him to lose no more time before he"}
{"doc_id": "gutenberg_1342", "para_id": 7661, "text": "“Oh! my dear father,” she cried, “come back and write immediately."}
{"doc_id": "gutenberg_1342", "para_id": 7662, "text": "Consider how important every moment is in such a case.”"}
{"doc_id": "gutenberg_1342", "para_id": 7663, "text": "“Let me write for you,” said Jane, “if you dislike the trouble"}
{"doc_id": "gutenberg_1342", "para_id": 7664, "text": "“I dislike it very much,” he replied; “but it must be done.”"}
{"doc_id": "gutenberg_1342", "para_id": 7665, "text": "And so saying, he turned back with them, and walked towards the house."}
{"doc_id": "gutenberg_1342", "para_id": 7666, "text": "“And--may I ask?” said Elizabeth; “but the terms, I suppose, must be"}
{"doc_id": "gutenberg_1342", "para_id": 7667, "text": "“Complied with! I am only ashamed of his asking so little.”"}
{"doc_id": "gutenberg_1342", "para_id": 7668, "text": "“Yes, yes, they must marry. There is nothing else to be done. But there"}
{"doc_id": "gutenberg_1342", "para_id": 7669, "text": "are two things that I want very much to know:--one is, how much money"}
{"doc_id": "gutenberg_1342", "para_id": 7670, "text": "your uncle has laid down to bring it about; and the other, how I am ever"}
{"doc_id": "gutenberg_1342", "para_id": 7671, "text": "“Money! my uncle!” cried Jane, “what do you mean, sir?”"}
{"doc_id": "gutenberg_1342", "para_id": 7672, "text": "“I mean that no man in his proper senses would marry Lydia on so slight"}
{"doc_id": "gutenberg_1342", "para_id": 7673, "text": "a temptation as one hundred a year during my life, and fifty after I am"}
{"doc_id": "gutenberg_1342", "para_id": 7674, "text": "“That is very true,” said Elizabeth; “though it had not occurred to me"}
{"doc_id": "gutenberg_1342", "para_id": 7675, "text": "before. His debts to be discharged, and something still to remain! Oh,"}
{"doc_id": "gutenberg_1342", "para_id": 7676, "text": "it must be my uncle’s doings! Generous, good man, I am afraid he has"}
{"doc_id": "gutenberg_1342", "para_id": 7677, "text": "distressed himself. A small sum could not do all this.”"}
{"doc_id": "gutenberg_1342", "para_id": 7678, "text": "“No,” said her father. “Wickham’s a fool if he takes her with a farthing"}
{"doc_id": "gutenberg_1342", "para_id": 7679, "text": "less than ten thousand pounds: I should be sorry to think so ill of him,"}
{"doc_id": "gutenberg_1342", "para_id": 7680, "text": "“Ten thousand pounds! Heaven forbid! How is half such a sum to be"}
{"doc_id": "gutenberg_1342", "para_id": 7681, "text": "Mr. Bennet made no answer; and each of them, deep in thought, continued"}
{"doc_id": "gutenberg_1342", "para_id": 7682, "text": "silent till they reached the house. Their father then went to the"}
{"doc_id": "gutenberg_1342", "para_id": 7683, "text": "library to write, and the girls walked into the breakfast-room."}
{"doc_id": "gutenberg_1342", "para_id": 7684, "text": "“And they are really to be married!” cried Elizabeth, as soon as they"}
{"doc_id": "gutenberg_1342", "para_id": 7685, "text": "were by themselves. “How strange this is! and for _this_ we are to be"}
{"doc_id": "gutenberg_1342", "para_id": 7686, "text": "thankful. That they should marry, small as is their chance of happiness,"}
{"doc_id": "gutenberg_1342", "para_id": 7687, "text": "and wretched as is his character, we are forced to rejoice! Oh, Lydia!”"}
{"doc_id": "gutenberg_1342", "para_id": 7688, "text": "“I comfort myself with thinking,” replied Jane, “that he certainly would"}
{"doc_id": "gutenberg_1342", "para_id": 7689, "text": "not marry Lydia, if he had not a real regard for her. Though our kind"}
{"doc_id": "gutenberg_1342", "para_id": 7690, "text": "uncle has done something towards clearing him, I cannot believe that ten"}
{"doc_id": "gutenberg_1342", "para_id": 7691, "text": "thousand pounds, or anything like it, has been advanced. He has children"}
{"doc_id": "gutenberg_1342", "para_id": 7692, "text": "of his own, and may have more. How could he spare half ten thousand"}
{"doc_id": "gutenberg_1342", "para_id": 7693, "text": "“If we are ever able to learn what Wickham’s debts have been,” said"}
{"doc_id": "gutenberg_1342", "para_id": 7694, "text": "Elizabeth, “and how much is settled on his side on our sister, we shall"}
{"doc_id": "gutenberg_1342", "para_id": 7695, "text": "exactly know what Mr. Gardiner has done for them, because Wickham has"}
{"doc_id": "gutenberg_1342", "para_id": 7696, "text": "not sixpence of his own. The kindness of my uncle and aunt can never be"}
{"doc_id": "gutenberg_1342", "para_id": 7697, "text": "requited. Their taking her home, and affording her their personal"}
{"doc_id": "gutenberg_1342", "para_id": 7698, "text": "protection and countenance, is such a sacrifice to her advantage as"}
{"doc_id": "gutenberg_1342", "para_id": 7699, "text": "years of gratitude cannot enough acknowledge. By this time she is"}
{"doc_id": "gutenberg_1342", "para_id": 7700, "text": "actually with them! If such goodness does not make her miserable now,"}
{"doc_id": "gutenberg_1342", "para_id": 7701, "text": "she will never deserve to be happy! What a meeting for her, when she"}
{"doc_id": "gutenberg_1342", "para_id": 7702, "text": "“We must endeavour to forget all that has passed on either side,” said"}
{"doc_id": "gutenberg_1342", "para_id": 7703, "text": "Jane: “I hope and trust they will yet be happy. His consenting to marry"}
{"doc_id": "gutenberg_1342", "para_id": 7704, "text": "her is a proof, I will believe, that he is come to a right way of"}
{"doc_id": "gutenberg_1342", "para_id": 7705, "text": "thinking. Their mutual affection will steady them; and I flatter myself"}
{"doc_id": "gutenberg_1342", "para_id": 7706, "text": "they will settle so quietly, and live in so rational a manner, as may in"}
{"doc_id": "gutenberg_1342", "para_id": 7707, "text": "“Their conduct has been such,” replied Elizabeth, “as neither you, nor"}
{"doc_id": "gutenberg_1342", "para_id": 7708, "text": "I, nor anybody, can ever forget. It is useless to talk of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 7709, "text": "It now occurred to the girls that their mother was in all likelihood"}
{"doc_id": "gutenberg_1342", "para_id": 7710, "text": "perfectly ignorant of what had happened. They went to the library,"}
{"doc_id": "gutenberg_1342", "para_id": 7711, "text": "therefore, and asked their father whether he would not wish them to make"}
{"doc_id": "gutenberg_1342", "para_id": 7712, "text": "it known to her. He was writing, and, without raising his head, coolly"}
{"doc_id": "gutenberg_1342", "para_id": 7713, "text": "Elizabeth took the letter from his writing-table, and they went upstairs"}
{"doc_id": "gutenberg_1342", "para_id": 7714, "text": "together. Mary and Kitty were both with Mrs. Bennet: one communication"}
{"doc_id": "gutenberg_1342", "para_id": 7715, "text": "would, therefore, do for all. After a slight preparation for good news,"}
{"doc_id": "gutenberg_1342", "para_id": 7716, "text": "the letter was read aloud. Mrs. Bennet could hardly contain herself. As"}
{"doc_id": "gutenberg_1342", "para_id": 7717, "text": "soon as Jane had read Mr. Gardiner’s hope of Lydia’s being soon married,"}
{"doc_id": "gutenberg_1342", "para_id": 7718, "text": "her joy burst forth, and every following sentence added to its"}
{"doc_id": "gutenberg_1342", "para_id": 7719, "text": "exuberance. She was now in an irritation as violent from delight as she"}
{"doc_id": "gutenberg_1342", "para_id": 7720, "text": "had ever been fidgety from alarm and vexation. To know that her daughter"}
{"doc_id": "gutenberg_1342", "para_id": 7721, "text": "would be married was enough. She was disturbed by no fear for her"}
{"doc_id": "gutenberg_1342", "para_id": 7722, "text": "felicity, nor humbled by any remembrance of her misconduct."}
{"doc_id": "gutenberg_1342", "para_id": 7723, "text": "“My dear, dear Lydia!” she cried: “this is delightful indeed! She will"}
{"doc_id": "gutenberg_1342", "para_id": 7724, "text": "be married! I shall see her again! She will be married at sixteen! My"}
{"doc_id": "gutenberg_1342", "para_id": 7725, "text": "good, kind brother! I knew how it would be--I knew he would manage"}
{"doc_id": "gutenberg_1342", "para_id": 7726, "text": "everything. How I long to see her! and to see dear Wickham too! But the"}
{"doc_id": "gutenberg_1342", "para_id": 7727, "text": "clothes, the wedding clothes! I will write to my sister Gardiner about"}
{"doc_id": "gutenberg_1342", "para_id": 7728, "text": "them directly. Lizzy, my dear, run down to your father, and ask him how"}
{"doc_id": "gutenberg_1342", "para_id": 7729, "text": "much he will give her. Stay, stay, I will go myself. Ring the bell,"}
{"doc_id": "gutenberg_1342", "para_id": 7730, "text": "Kitty, for Hill. I will put on my things in a moment. My dear, dear"}
{"doc_id": "gutenberg_1342", "para_id": 7731, "text": "Lydia! How merry we shall be together when we meet!”"}
{"doc_id": "gutenberg_1342", "para_id": 7732, "text": "Her eldest daughter endeavoured to give some relief to the violence of"}
{"doc_id": "gutenberg_1342", "para_id": 7733, "text": "these transports, by leading her thoughts to the obligations which Mr."}
{"doc_id": "gutenberg_1342", "para_id": 7734, "text": "“For we must attribute this happy conclusion,” she added, “in a great"}
{"doc_id": "gutenberg_1342", "para_id": 7735, "text": "measure to his kindness. We are persuaded that he has pledged himself to"}
{"doc_id": "gutenberg_1342", "para_id": 7736, "text": "“Well,” cried her mother, “it is all very right; who should do it but"}
{"doc_id": "gutenberg_1342", "para_id": 7737, "text": "her own uncle? If he had not had a family of his own, I and my children"}
{"doc_id": "gutenberg_1342", "para_id": 7738, "text": "must have had all his money, you know; and it is the first time we have"}
{"doc_id": "gutenberg_1342", "para_id": 7739, "text": "ever had anything from him except a few presents. Well! I am so happy."}
{"doc_id": "gutenberg_1342", "para_id": 7740, "text": "In a short time, I shall have a daughter married. Mrs. Wickham! How well"}
{"doc_id": "gutenberg_1342", "para_id": 7741, "text": "it sounds! And she was only sixteen last June. My dear Jane, I am in"}
{"doc_id": "gutenberg_1342", "para_id": 7742, "text": "such a flutter, that I am sure I can’t write; so I will dictate, and you"}
{"doc_id": "gutenberg_1342", "para_id": 7743, "text": "write for me. We will settle with your father about the money"}
{"doc_id": "gutenberg_1342", "para_id": 7744, "text": "afterwards; but the things should be ordered immediately.”"}
{"doc_id": "gutenberg_1342", "para_id": 7745, "text": "She was then proceeding to all the particulars of calico, muslin, and"}
{"doc_id": "gutenberg_1342", "para_id": 7746, "text": "cambric, and would shortly have dictated some very plentiful orders, had"}
{"doc_id": "gutenberg_1342", "para_id": 7747, "text": "not Jane, though with some difficulty, persuaded her to wait till her"}
{"doc_id": "gutenberg_1342", "para_id": 7748, "text": "father was at leisure to be consulted. One day’s delay, she observed,"}
{"doc_id": "gutenberg_1342", "para_id": 7749, "text": "would be of small importance; and her mother was too happy to be quite"}
{"doc_id": "gutenberg_1342", "para_id": 7750, "text": "so obstinate as usual. Other schemes, too, came into her head."}
{"doc_id": "gutenberg_1342", "para_id": 7751, "text": "“I will go to Meryton,” said she, “as soon as I am dressed, and tell the"}
{"doc_id": "gutenberg_1342", "para_id": 7752, "text": "good, good news to my sister Philips. And as I come back, I can call on"}
{"doc_id": "gutenberg_1342", "para_id": 7753, "text": "Lady Lucas and Mrs. Long. Kitty, run down and order the carriage. An"}
{"doc_id": "gutenberg_1342", "para_id": 7754, "text": "airing would do me a great deal of good, I am sure. Girls, can I do"}
{"doc_id": "gutenberg_1342", "para_id": 7755, "text": "anything for you in Meryton? Oh! here comes Hill. My dear Hill, have you"}
{"doc_id": "gutenberg_1342", "para_id": 7756, "text": "heard the good news? Miss Lydia is going to be married; and you shall"}
{"doc_id": "gutenberg_1342", "para_id": 7757, "text": "all have a bowl of punch to make merry at her wedding.”"}
{"doc_id": "gutenberg_1342", "para_id": 7758, "text": "Mrs. Hill began instantly to express her joy. Elizabeth received her"}
{"doc_id": "gutenberg_1342", "para_id": 7759, "text": "congratulations amongst the rest, and then, sick of this folly, took"}
{"doc_id": "gutenberg_1342", "para_id": 7760, "text": "refuge in her own room, that she might think with freedom. Poor Lydia’s"}
{"doc_id": "gutenberg_1342", "para_id": 7761, "text": "situation must, at best, be bad enough; but that it was no worse, she"}
{"doc_id": "gutenberg_1342", "para_id": 7762, "text": "had need to be thankful. She felt it so; and though, in looking forward,"}
{"doc_id": "gutenberg_1342", "para_id": 7763, "text": "neither rational happiness, nor worldly prosperity could be justly"}
{"doc_id": "gutenberg_1342", "para_id": 7764, "text": "expected for her sister, in looking back to what they had feared, only"}
{"doc_id": "gutenberg_1342", "para_id": 7765, "text": "two hours ago, she felt all the advantages of what they had gained."}
{"doc_id": "gutenberg_1342", "para_id": 7766, "text": "Mr. Bennet had very often wished, before this period of his life, that,"}
{"doc_id": "gutenberg_1342", "para_id": 7767, "text": "instead of spending his whole income, he had laid by an annual sum, for"}
{"doc_id": "gutenberg_1342", "para_id": 7768, "text": "the better provision of his children, and of his wife, if she survived"}
{"doc_id": "gutenberg_1342", "para_id": 7769, "text": "him. He now wished it more than ever. Had he done his duty in that"}
{"doc_id": "gutenberg_1342", "para_id": 7770, "text": "respect, Lydia need not have been indebted to her uncle for whatever of"}
{"doc_id": "gutenberg_1342", "para_id": 7771, "text": "honour or credit could now be purchased for her. The satisfaction of"}
{"doc_id": "gutenberg_1342", "para_id": 7772, "text": "prevailing on one of the most worthless young men in Great Britain to"}
{"doc_id": "gutenberg_1342", "para_id": 7773, "text": "be her husband might then have rested in its proper place."}
{"doc_id": "gutenberg_1342", "para_id": 7774, "text": "He was seriously concerned that a cause of so little advantage to anyone"}
{"doc_id": "gutenberg_1342", "para_id": 7775, "text": "should be forwarded at the sole expense of his brother-in-law; and he"}
{"doc_id": "gutenberg_1342", "para_id": 7776, "text": "was determined, if possible, to find out the extent of his assistance,"}
{"doc_id": "gutenberg_1342", "para_id": 7777, "text": "and to discharge the obligation as soon as he could."}
{"doc_id": "gutenberg_1342", "para_id": 7778, "text": "When first Mr. Bennet had married, economy was held to be perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 7779, "text": "useless; for, of course, they were to have a son. This son was to join"}
{"doc_id": "gutenberg_1342", "para_id": 7780, "text": "in cutting off the entail, as soon as he should be of age, and the widow"}
{"doc_id": "gutenberg_1342", "para_id": 7781, "text": "and younger children would by that means be provided for. Five daughters"}
{"doc_id": "gutenberg_1342", "para_id": 7782, "text": "successively entered the world, but yet the son was to come; and Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 7783, "text": "Bennet, for many years after Lydia’s birth, had been certain that he"}
{"doc_id": "gutenberg_1342", "para_id": 7784, "text": "would. This event had at last been despaired of, but it was then too"}
{"doc_id": "gutenberg_1342", "para_id": 7785, "text": "late to be saving. Mrs. Bennet had no turn for economy; and her"}
{"doc_id": "gutenberg_1342", "para_id": 7786, "text": "husband’s love of independence had alone prevented their exceeding their"}
{"doc_id": "gutenberg_1342", "para_id": 7787, "text": "Five thousand pounds was settled by marriage articles on Mrs. Bennet and"}
{"doc_id": "gutenberg_1342", "para_id": 7788, "text": "the children. But in what proportions it should be divided amongst the"}
{"doc_id": "gutenberg_1342", "para_id": 7789, "text": "latter depended on the will of the parents. This was one point, with"}
{"doc_id": "gutenberg_1342", "para_id": 7790, "text": "regard to Lydia at least, which was now to be settled, and Mr. Bennet"}
{"doc_id": "gutenberg_1342", "para_id": 7791, "text": "could have no hesitation in acceding to the proposal before him. In"}
{"doc_id": "gutenberg_1342", "para_id": 7792, "text": "terms of grateful acknowledgment for the kindness of his brother, though"}
{"doc_id": "gutenberg_1342", "para_id": 7793, "text": "expressed most concisely, he then delivered on paper his perfect"}
{"doc_id": "gutenberg_1342", "para_id": 7794, "text": "approbation of all that was done, and his willingness to fulfil the"}
{"doc_id": "gutenberg_1342", "para_id": 7795, "text": "engagements that had been made for him. He had never before supposed"}
{"doc_id": "gutenberg_1342", "para_id": 7796, "text": "that, could Wickham be prevailed on to marry his daughter, it would be"}
{"doc_id": "gutenberg_1342", "para_id": 7797, "text": "done with so little inconvenience to himself as by the present"}
{"doc_id": "gutenberg_1342", "para_id": 7798, "text": "arrangement. He would scarcely be ten pounds a year the loser, by the"}
{"doc_id": "gutenberg_1342", "para_id": 7799, "text": "hundred that was to be paid them; for, what with her board and pocket"}
{"doc_id": "gutenberg_1342", "para_id": 7800, "text": "allowance, and the continual presents in money which passed to her"}
{"doc_id": "gutenberg_1342", "para_id": 7801, "text": "through her mother’s hands, Lydia’s expenses had been very little within"}
{"doc_id": "gutenberg_1342", "para_id": 7802, "text": "That it would be done with such trifling exertion on his side, too, was"}
{"doc_id": "gutenberg_1342", "para_id": 7803, "text": "another very welcome surprise; for his chief wish at present was to have"}
{"doc_id": "gutenberg_1342", "para_id": 7804, "text": "as little trouble in the business as possible. When the first transports"}
{"doc_id": "gutenberg_1342", "para_id": 7805, "text": "of rage which had produced his activity in seeking her were over, he"}
{"doc_id": "gutenberg_1342", "para_id": 7806, "text": "naturally returned to all his former indolence. His letter was soon"}
{"doc_id": "gutenberg_1342", "para_id": 7807, "text": "despatched; for though dilatory in undertaking business, he was quick in"}
{"doc_id": "gutenberg_1342", "para_id": 7808, "text": "its execution. He begged to know further particulars of what he was"}
{"doc_id": "gutenberg_1342", "para_id": 7809, "text": "indebted to his brother; but was too angry with Lydia to send any"}
{"doc_id": "gutenberg_1342", "para_id": 7810, "text": "The good news quickly spread through the house; and with proportionate"}
{"doc_id": "gutenberg_1342", "para_id": 7811, "text": "speed through the neighbourhood. It was borne in the latter with decent"}
{"doc_id": "gutenberg_1342", "para_id": 7812, "text": "philosophy. To be sure, it would have been more for the advantage of"}
{"doc_id": "gutenberg_1342", "para_id": 7813, "text": "conversation, had Miss Lydia Bennet come upon the town; or, as the"}
{"doc_id": "gutenberg_1342", "para_id": 7814, "text": "happiest alternative, been secluded from the world in some distant"}
{"doc_id": "gutenberg_1342", "para_id": 7815, "text": "farm-house. But there was much to be talked of, in marrying her; and the"}
{"doc_id": "gutenberg_1342", "para_id": 7816, "text": "good-natured wishes for her well-doing, which had proceeded before from"}
{"doc_id": "gutenberg_1342", "para_id": 7817, "text": "all the spiteful old ladies in Meryton, lost but little of their spirit"}
{"doc_id": "gutenberg_1342", "para_id": 7818, "text": "in this change of circumstances, because with such a husband her misery"}
{"doc_id": "gutenberg_1342", "para_id": 7819, "text": "It was a fortnight since Mrs. Bennet had been down stairs, but on this"}
{"doc_id": "gutenberg_1342", "para_id": 7820, "text": "happy day she again took her seat at the head of her table, and in"}
{"doc_id": "gutenberg_1342", "para_id": 7821, "text": "spirits oppressively high. No sentiment of shame gave a damp to her"}
{"doc_id": "gutenberg_1342", "para_id": 7822, "text": "triumph. The marriage of a daughter, which had been the first object of"}
{"doc_id": "gutenberg_1342", "para_id": 7823, "text": "her wishes since Jane was sixteen, was now on the point of"}
{"doc_id": "gutenberg_1342", "para_id": 7824, "text": "accomplishment, and her thoughts and her words ran wholly on those"}
{"doc_id": "gutenberg_1342", "para_id": 7825, "text": "attendants of elegant nuptials, fine muslins, new carriages, and"}
{"doc_id": "gutenberg_1342", "para_id": 7826, "text": "servants. She was busily searching through the neighbourhood for a"}
{"doc_id": "gutenberg_1342", "para_id": 7827, "text": "proper situation for her daughter; and, without knowing or considering"}
{"doc_id": "gutenberg_1342", "para_id": 7828, "text": "what their income might be, rejected many as deficient in size and"}
{"doc_id": "gutenberg_1342", "para_id": 7829, "text": "“Haye Park might do,” said she, “if the Gouldings would quit it, or the"}
{"doc_id": "gutenberg_1342", "para_id": 7830, "text": "great house at Stoke, if the drawing-room were larger; but Ashworth is"}
{"doc_id": "gutenberg_1342", "para_id": 7831, "text": "too far off. I could not bear to have her ten miles from me; and as for"}
{"doc_id": "gutenberg_1342", "para_id": 7832, "text": "Her husband allowed her to talk on without interruption while the"}
{"doc_id": "gutenberg_1342", "para_id": 7833, "text": "servants remained. But when they had withdrawn, he said to her, “Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 7834, "text": "Bennet, before you take any, or all of these houses, for your son and"}
{"doc_id": "gutenberg_1342", "para_id": 7835, "text": "daughter, let us come to a right understanding. Into _one_ house in this"}
{"doc_id": "gutenberg_1342", "para_id": 7836, "text": "neighbourhood they shall never have admittance. I will not encourage the"}
{"doc_id": "gutenberg_1342", "para_id": 7837, "text": "imprudence of either, by receiving them at Longbourn.”"}
{"doc_id": "gutenberg_1342", "para_id": 7838, "text": "A long dispute followed this declaration; but Mr. Bennet was firm: it"}
{"doc_id": "gutenberg_1342", "para_id": 7839, "text": "soon led to another; and Mrs. Bennet found, with amazement and horror,"}
{"doc_id": "gutenberg_1342", "para_id": 7840, "text": "that her husband would not advance a guinea to buy clothes for his"}
{"doc_id": "gutenberg_1342", "para_id": 7841, "text": "daughter. He protested that she should receive from him no mark of"}
{"doc_id": "gutenberg_1342", "para_id": 7842, "text": "affection whatever on the occasion. Mrs. Bennet could hardly comprehend"}
{"doc_id": "gutenberg_1342", "para_id": 7843, "text": "it. That his anger could be carried to such a point of inconceivable"}
{"doc_id": "gutenberg_1342", "para_id": 7844, "text": "resentment as to refuse his daughter a privilege, without which her"}
{"doc_id": "gutenberg_1342", "para_id": 7845, "text": "marriage would scarcely seem valid, exceeded all that she could believe"}
{"doc_id": "gutenberg_1342", "para_id": 7846, "text": "possible. She was more alive to the disgrace, which her want of new"}
{"doc_id": "gutenberg_1342", "para_id": 7847, "text": "clothes must reflect on her daughter’s nuptials, than to any sense of"}
{"doc_id": "gutenberg_1342", "para_id": 7848, "text": "shame at her eloping and living with Wickham a fortnight before they"}
{"doc_id": "gutenberg_1342", "para_id": 7849, "text": "Elizabeth was now most heartily sorry that she had, from the distress of"}
{"doc_id": "gutenberg_1342", "para_id": 7850, "text": "the moment, been led to make Mr. Darcy acquainted with their fears for"}
{"doc_id": "gutenberg_1342", "para_id": 7851, "text": "her sister; for since her marriage would so shortly give the proper"}
{"doc_id": "gutenberg_1342", "para_id": 7852, "text": "termination to the elopement, they might hope to conceal its"}
{"doc_id": "gutenberg_1342", "para_id": 7853, "text": "unfavourable beginning from all those who were not immediately on the"}
{"doc_id": "gutenberg_1342", "para_id": 7854, "text": "She had no fear of its spreading farther, through his means. There were"}
{"doc_id": "gutenberg_1342", "para_id": 7855, "text": "few people on whose secrecy she would have more confidently depended;"}
{"doc_id": "gutenberg_1342", "para_id": 7856, "text": "but at the same time there was no one whose knowledge of a sister’s"}
{"doc_id": "gutenberg_1342", "para_id": 7857, "text": "frailty would have mortified her so much. Not, however, from any fear of"}
{"doc_id": "gutenberg_1342", "para_id": 7858, "text": "disadvantage from it individually to herself; for at any rate there"}
{"doc_id": "gutenberg_1342", "para_id": 7859, "text": "seemed a gulf impassable between them. Had Lydia’s marriage been"}
{"doc_id": "gutenberg_1342", "para_id": 7860, "text": "concluded on the most honourable terms, it was not to be supposed that"}
{"doc_id": "gutenberg_1342", "para_id": 7861, "text": "Mr. Darcy would connect himself with a family, where to every other"}
{"doc_id": "gutenberg_1342", "para_id": 7862, "text": "objection would now be added an alliance and relationship of the nearest"}
{"doc_id": "gutenberg_1342", "para_id": 7863, "text": "From such a connection she could not wonder that he should shrink. The"}
{"doc_id": "gutenberg_1342", "para_id": 7864, "text": "wish of procuring her regard, which she had assured herself of his"}
{"doc_id": "gutenberg_1342", "para_id": 7865, "text": "feeling in Derbyshire, could not in rational expectation survive such a"}
{"doc_id": "gutenberg_1342", "para_id": 7866, "text": "blow as this. She was humbled, she was grieved; she repented, though she"}
{"doc_id": "gutenberg_1342", "para_id": 7867, "text": "hardly knew of what. She became jealous of his esteem, when she could no"}
{"doc_id": "gutenberg_1342", "para_id": 7868, "text": "longer hope to be benefited by it. She wanted to hear of him, when there"}
{"doc_id": "gutenberg_1342", "para_id": 7869, "text": "seemed the least chance of gaining intelligence. She was convinced that"}
{"doc_id": "gutenberg_1342", "para_id": 7870, "text": "she could have been happy with him, when it was no longer likely they"}
{"doc_id": "gutenberg_1342", "para_id": 7871, "text": "What a triumph for him, as she often thought, could he know that the"}
{"doc_id": "gutenberg_1342", "para_id": 7872, "text": "proposals which she had proudly spurned only four months ago would now"}
{"doc_id": "gutenberg_1342", "para_id": 7873, "text": "have been gladly and gratefully received! He was as generous, she"}
{"doc_id": "gutenberg_1342", "para_id": 7874, "text": "doubted not, as the most generous of his sex. But while he was mortal,"}
{"doc_id": "gutenberg_1342", "para_id": 7875, "text": "She began now to comprehend that he was exactly the man who, in"}
{"doc_id": "gutenberg_1342", "para_id": 7876, "text": "disposition and talents, would most suit her. His understanding and"}
{"doc_id": "gutenberg_1342", "para_id": 7877, "text": "temper, though unlike her own, would have answered all her wishes. It"}
{"doc_id": "gutenberg_1342", "para_id": 7878, "text": "was an union that must have been to the advantage of both: by her ease"}
{"doc_id": "gutenberg_1342", "para_id": 7879, "text": "and liveliness, his mind might have been softened, his manners improved;"}
{"doc_id": "gutenberg_1342", "para_id": 7880, "text": "and from his judgment, information, and knowledge of the world, she must"}
{"doc_id": "gutenberg_1342", "para_id": 7881, "text": "But no such happy marriage could now teach the admiring multitude what"}
{"doc_id": "gutenberg_1342", "para_id": 7882, "text": "connubial felicity really was. An union of a different tendency, and"}
{"doc_id": "gutenberg_1342", "para_id": 7883, "text": "precluding the possibility of the other, was soon to be formed in their"}
{"doc_id": "gutenberg_1342", "para_id": 7884, "text": "How Wickham and Lydia were to be supported in tolerable independence she"}
{"doc_id": "gutenberg_1342", "para_id": 7885, "text": "could not imagine. But how little of permanent happiness could belong to"}
{"doc_id": "gutenberg_1342", "para_id": 7886, "text": "a couple who were only brought together because their passions were"}
{"doc_id": "gutenberg_1342", "para_id": 7887, "text": "stronger than their virtue, she could easily conjecture."}
{"doc_id": "gutenberg_1342", "para_id": 7888, "text": "Mr. Gardiner soon wrote again to his brother. To Mr. Bennet’s"}
{"doc_id": "gutenberg_1342", "para_id": 7889, "text": "acknowledgments he briefly replied, with assurances of his eagerness to"}
{"doc_id": "gutenberg_1342", "para_id": 7890, "text": "promote the welfare of any of his family; and concluded with entreaties"}
{"doc_id": "gutenberg_1342", "para_id": 7891, "text": "that the subject might never be mentioned to him again. The principal"}
{"doc_id": "gutenberg_1342", "para_id": 7892, "text": "purport of his letter was to inform them, that Mr. Wickham had resolved"}
{"doc_id": "gutenberg_1342", "para_id": 7893, "text": "“It was greatly my wish that he should do so,” he added, “as soon as his"}
{"doc_id": "gutenberg_1342", "para_id": 7894, "text": "marriage was fixed on. And I think you will agree with me, in"}
{"doc_id": "gutenberg_1342", "para_id": 7895, "text": "considering a removal from that corps as highly advisable, both on his"}
{"doc_id": "gutenberg_1342", "para_id": 7896, "text": "account and my niece’s. It is Mr. Wickham’s intention to go into the"}
{"doc_id": "gutenberg_1342", "para_id": 7897, "text": "Regulars; and, among his former friends, there are still some who are"}
{"doc_id": "gutenberg_1342", "para_id": 7898, "text": "able and willing to assist him in the army. He has the promise of an"}
{"doc_id": "gutenberg_1342", "para_id": 7899, "text": "ensigncy in General----’s regiment, now quartered in the north. It is"}
{"doc_id": "gutenberg_1342", "para_id": 7900, "text": "an advantage to have it so far from this part of the kingdom. He"}
{"doc_id": "gutenberg_1342", "para_id": 7901, "text": "promises fairly; and I hope among different people, where they may each"}
{"doc_id": "gutenberg_1342", "para_id": 7902, "text": "have a character to preserve, they will both be more prudent. I have"}
{"doc_id": "gutenberg_1342", "para_id": 7903, "text": "written to Colonel Forster, to inform him of our present arrangements,"}
{"doc_id": "gutenberg_1342", "para_id": 7904, "text": "and to request that he will satisfy the various creditors of Mr. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 7905, "text": "in and near Brighton with assurances of speedy payment, for which I have"}
{"doc_id": "gutenberg_1342", "para_id": 7906, "text": "pledged myself. And will you give yourself the trouble of carrying"}
{"doc_id": "gutenberg_1342", "para_id": 7907, "text": "similar assurances to his creditors in Meryton, of whom I shall subjoin"}
{"doc_id": "gutenberg_1342", "para_id": 7908, "text": "a list, according to his information? He has given in all his debts; I"}
{"doc_id": "gutenberg_1342", "para_id": 7909, "text": "hope at least he has not deceived us. Haggerston has our directions, and"}
{"doc_id": "gutenberg_1342", "para_id": 7910, "text": "all will be completed in a week. They will then join his regiment,"}
{"doc_id": "gutenberg_1342", "para_id": 7911, "text": "unless they are first invited to Longbourn; and I understand from Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 7912, "text": "Gardiner that my niece is very desirous of seeing you all before she"}
{"doc_id": "gutenberg_1342", "para_id": 7913, "text": "leaves the south. She is well, and begs to be dutifully remembered to"}
{"doc_id": "gutenberg_1342", "para_id": 7914, "text": "Mr. Bennet and his daughters saw all the advantages of Wickham’s"}
{"doc_id": "gutenberg_1342", "para_id": 7915, "text": "removal from the ----shire, as clearly as Mr. Gardiner could do. But"}
{"doc_id": "gutenberg_1342", "para_id": 7916, "text": "Mrs. Bennet was not so well pleased with it. Lydia’s being settled in"}
{"doc_id": "gutenberg_1342", "para_id": 7917, "text": "the north, just when she had expected most pleasure and pride in her"}
{"doc_id": "gutenberg_1342", "para_id": 7918, "text": "company, for she had by no means given up her plan of their residing in"}
{"doc_id": "gutenberg_1342", "para_id": 7919, "text": "Hertfordshire, was a severe disappointment; and, besides, it was such a"}
{"doc_id": "gutenberg_1342", "para_id": 7920, "text": "pity that Lydia should be taken from a regiment where she was acquainted"}
{"doc_id": "gutenberg_1342", "para_id": 7921, "text": "“She is so fond of Mrs. Forster,” said she, “it will be quite shocking"}
{"doc_id": "gutenberg_1342", "para_id": 7922, "text": "to send her away! And there are several of the young men, too, that she"}
{"doc_id": "gutenberg_1342", "para_id": 7923, "text": "likes very much. The officers may not be so pleasant in General----’s"}
{"doc_id": "gutenberg_1342", "para_id": 7924, "text": "His daughter’s request, for such it might be considered, of being"}
{"doc_id": "gutenberg_1342", "para_id": 7925, "text": "admitted into her family again, before she set off for the north,"}
{"doc_id": "gutenberg_1342", "para_id": 7926, "text": "received at first an absolute negative. But Jane and Elizabeth, who"}
{"doc_id": "gutenberg_1342", "para_id": 7927, "text": "agreed in wishing, for the sake of their sister’s feelings and"}
{"doc_id": "gutenberg_1342", "para_id": 7928, "text": "consequence, that she should be noticed on her marriage by her parents,"}
{"doc_id": "gutenberg_1342", "para_id": 7929, "text": "urged him so earnestly, yet so rationally and so mildly, to receive her"}
{"doc_id": "gutenberg_1342", "para_id": 7930, "text": "and her husband at Longbourn, as soon as they were married, that he was"}
{"doc_id": "gutenberg_1342", "para_id": 7931, "text": "prevailed on to think as they thought, and act as they wished. And their"}
{"doc_id": "gutenberg_1342", "para_id": 7932, "text": "mother had the satisfaction of knowing, that she should be able to show"}
{"doc_id": "gutenberg_1342", "para_id": 7933, "text": "her married daughter in the neighbourhood, before she was banished to"}
{"doc_id": "gutenberg_1342", "para_id": 7934, "text": "the north. When Mr. Bennet wrote again to his brother, therefore, he"}
{"doc_id": "gutenberg_1342", "para_id": 7935, "text": "sent his permission for them to come; and it was settled, that, as soon"}
{"doc_id": "gutenberg_1342", "para_id": 7936, "text": "as the ceremony was over, they should proceed to Longbourn. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 7937, "text": "was surprised, however, that Wickham should consent to such a scheme;"}
{"doc_id": "gutenberg_1342", "para_id": 7938, "text": "and, had she consulted only her own inclination, any meeting with him"}
{"doc_id": "gutenberg_1342", "para_id": 7939, "text": "Their sister’s wedding-day arrived; and Jane and Elizabeth felt for her"}
{"doc_id": "gutenberg_1342", "para_id": 7940, "text": "probably more than she felt for herself. The carriage was sent to meet"}
{"doc_id": "gutenberg_1342", "para_id": 7941, "text": "them at----, and they were to return in it by dinnertime. Their arrival"}
{"doc_id": "gutenberg_1342", "para_id": 7942, "text": "was dreaded by the elder Miss Bennets--and Jane more especially, who"}
{"doc_id": "gutenberg_1342", "para_id": 7943, "text": "gave Lydia the feelings which would have attended herself, had _she_"}
{"doc_id": "gutenberg_1342", "para_id": 7944, "text": "been the culprit, and was wretched in the thought of what her sister"}
{"doc_id": "gutenberg_1342", "para_id": 7945, "text": "They came. The family were assembled in the breakfast-room to receive"}
{"doc_id": "gutenberg_1342", "para_id": 7946, "text": "them. Smiles decked the face of Mrs. Bennet, as the carriage drove up to"}
{"doc_id": "gutenberg_1342", "para_id": 7947, "text": "the door; her husband looked impenetrably grave; her daughters, alarmed,"}
{"doc_id": "gutenberg_1342", "para_id": 7948, "text": "Lydia’s voice was heard in the vestibule; the door was thrown open, and"}
{"doc_id": "gutenberg_1342", "para_id": 7949, "text": "she ran into the room. Her mother stepped forwards, embraced her, and"}
{"doc_id": "gutenberg_1342", "para_id": 7950, "text": "welcomed her with rapture; gave her hand with an affectionate smile to"}
{"doc_id": "gutenberg_1342", "para_id": 7951, "text": "Wickham, who followed his lady; and wished them both joy, with an"}
{"doc_id": "gutenberg_1342", "para_id": 7952, "text": "Their reception from Mr. Bennet, to whom they then turned, was not quite"}
{"doc_id": "gutenberg_1342", "para_id": 7953, "text": "so cordial. His countenance rather gained in austerity; and he scarcely"}
{"doc_id": "gutenberg_1342", "para_id": 7954, "text": "opened his lips. The easy assurance of the young couple, indeed, was"}
{"doc_id": "gutenberg_1342", "para_id": 7955, "text": "Elizabeth was disgusted, and even Miss Bennet was shocked. Lydia was"}
{"doc_id": "gutenberg_1342", "para_id": 7956, "text": "Lydia still; untamed, unabashed, wild, noisy, and fearless. She turned"}
{"doc_id": "gutenberg_1342", "para_id": 7957, "text": "from sister to sister, demanding their congratulations; and when at"}
{"doc_id": "gutenberg_1342", "para_id": 7958, "text": "length they all sat down, looked eagerly round the room, took notice of"}
{"doc_id": "gutenberg_1342", "para_id": 7959, "text": "some little alteration in it, and observed, with a laugh, that it was a"}
{"doc_id": "gutenberg_1342", "para_id": 7960, "text": "Wickham was not at all more distressed than herself; but his manners"}
{"doc_id": "gutenberg_1342", "para_id": 7961, "text": "were always so pleasing, that, had his character and his marriage been"}
{"doc_id": "gutenberg_1342", "para_id": 7962, "text": "exactly what they ought, his smiles and his easy address, while he"}
{"doc_id": "gutenberg_1342", "para_id": 7963, "text": "claimed their relationship, would have delighted them all. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 7964, "text": "had not before believed him quite equal to such assurance; but she sat"}
{"doc_id": "gutenberg_1342", "para_id": 7965, "text": "down, resolving within herself to draw no limits in future to the"}
{"doc_id": "gutenberg_1342", "para_id": 7966, "text": "impudence of an impudent man. _She_ blushed, and Jane blushed; but the"}
{"doc_id": "gutenberg_1342", "para_id": 7967, "text": "cheeks of the two who caused their confusion suffered no variation of"}
{"doc_id": "gutenberg_1342", "para_id": 7968, "text": "There was no want of discourse. The bride and her mother could neither"}
{"doc_id": "gutenberg_1342", "para_id": 7969, "text": "of them talk fast enough; and Wickham, who happened to sit near"}
{"doc_id": "gutenberg_1342", "para_id": 7970, "text": "Elizabeth, began inquiring after his acquaintance in that neighbourhood,"}
{"doc_id": "gutenberg_1342", "para_id": 7971, "text": "with a good-humoured ease, which she felt very unable to equal in her"}
{"doc_id": "gutenberg_1342", "para_id": 7972, "text": "replies. They seemed each of them to have the happiest memories in the"}
{"doc_id": "gutenberg_1342", "para_id": 7973, "text": "world. Nothing of the past was recollected with pain; and Lydia led"}
{"doc_id": "gutenberg_1342", "para_id": 7974, "text": "voluntarily to subjects which her sisters would not have alluded to for"}
{"doc_id": "gutenberg_1342", "para_id": 7975, "text": "“Only think of its being three months,” she cried, “since I went away:"}
{"doc_id": "gutenberg_1342", "para_id": 7976, "text": "it seems but a fortnight, I declare; and yet there have been things"}
{"doc_id": "gutenberg_1342", "para_id": 7977, "text": "enough happened in the time. Good gracious! when I went away, I am sure"}
{"doc_id": "gutenberg_1342", "para_id": 7978, "text": "I had no more idea of being married till I came back again! though I"}
{"doc_id": "gutenberg_1342", "para_id": 7979, "text": "Her father lifted up his eyes, Jane was distressed, Elizabeth looked"}
{"doc_id": "gutenberg_1342", "para_id": 7980, "text": "expressively at Lydia; but she, who never heard nor saw anything of"}
{"doc_id": "gutenberg_1342", "para_id": 7981, "text": "which she chose to be insensible, gaily continued,--"}
{"doc_id": "gutenberg_1342", "para_id": 7982, "text": "“Oh, mamma, do the people hereabouts know I am married to-day? I was"}
{"doc_id": "gutenberg_1342", "para_id": 7983, "text": "afraid they might not; and we overtook William Goulding in his curricle,"}
{"doc_id": "gutenberg_1342", "para_id": 7984, "text": "so I was determined he should know it, and so I let down the side glass"}
{"doc_id": "gutenberg_1342", "para_id": 7985, "text": "next to him, and took off my glove and let my hand just rest upon the"}
{"doc_id": "gutenberg_1342", "para_id": 7986, "text": "window frame, so that he might see the ring, and then I bowed and"}
{"doc_id": "gutenberg_1342", "para_id": 7987, "text": "Elizabeth could bear it no longer. She got up and ran out of the room;"}
{"doc_id": "gutenberg_1342", "para_id": 7988, "text": "and returned no more, till she heard them passing through the hall to"}
{"doc_id": "gutenberg_1342", "para_id": 7989, "text": "the dining-parlour. She then joined them soon enough to see Lydia, with"}
{"doc_id": "gutenberg_1342", "para_id": 7990, "text": "anxious parade, walk up to her mother’s right hand, and hear her say to"}
{"doc_id": "gutenberg_1342", "para_id": 7991, "text": "“Ah, Jane, I take your place now, and you must go lower, because I am a"}
{"doc_id": "gutenberg_1342", "para_id": 7992, "text": "It was not to be supposed that time would give Lydia that embarrassment"}
{"doc_id": "gutenberg_1342", "para_id": 7993, "text": "from which she had been so wholly free at first. Her ease and good"}
{"doc_id": "gutenberg_1342", "para_id": 7994, "text": "spirits increased. She longed to see Mrs. Philips, the Lucases, and all"}
{"doc_id": "gutenberg_1342", "para_id": 7995, "text": "their other neighbours, and to hear herself called “Mrs. Wickham” by"}
{"doc_id": "gutenberg_1342", "para_id": 7996, "text": "each of them; and in the meantime she went after dinner to show her ring"}
{"doc_id": "gutenberg_1342", "para_id": 7997, "text": "and boast of being married to Mrs. Hill and the two housemaids."}
{"doc_id": "gutenberg_1342", "para_id": 7998, "text": "“Well, mamma,” said she, when they were all returned to the"}
{"doc_id": "gutenberg_1342", "para_id": 7999, "text": "breakfast-room, “and what do you think of my husband? Is not he a"}
{"doc_id": "gutenberg_1342", "para_id": 8000, "text": "charming man? I am sure my sisters must all envy me. I only hope they"}
{"doc_id": "gutenberg_1342", "para_id": 8001, "text": "may have half my good luck. They must all go to Brighton. That is the"}
{"doc_id": "gutenberg_1342", "para_id": 8002, "text": "place to get husbands. What a pity it is, mamma, we did not all go!”"}
{"doc_id": "gutenberg_1342", "para_id": 8003, "text": "“Very true; and if I had my will we should. But, my dear Lydia, I don’t"}
{"doc_id": "gutenberg_1342", "para_id": 8004, "text": "at all like your going such a way off. Must it be so?”"}
{"doc_id": "gutenberg_1342", "para_id": 8005, "text": "“Oh, Lord! yes; there is nothing in that. I shall like it of all things."}
{"doc_id": "gutenberg_1342", "para_id": 8006, "text": "You and papa, and my sisters, must come down and see us. We shall be at"}
{"doc_id": "gutenberg_1342", "para_id": 8007, "text": "Newcastle all the winter, and I dare say there will be some balls, and I"}
{"doc_id": "gutenberg_1342", "para_id": 8008, "text": "“I should like it beyond anything!” said her mother."}
{"doc_id": "gutenberg_1342", "para_id": 8009, "text": "“And then when you go away, you may leave one or two of my sisters"}
{"doc_id": "gutenberg_1342", "para_id": 8010, "text": "behind you; and I dare say I shall get husbands for them before the"}
{"doc_id": "gutenberg_1342", "para_id": 8011, "text": "“I thank you for my share of the favour,” said Elizabeth; “but I do not"}
{"doc_id": "gutenberg_1342", "para_id": 8012, "text": "Their visitors were not to remain above ten days with them. Mr. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 8013, "text": "had received his commission before he left London, and he was to join"}
{"doc_id": "gutenberg_1342", "para_id": 8014, "text": "No one but Mrs. Bennet regretted that their stay would be so short; and"}
{"doc_id": "gutenberg_1342", "para_id": 8015, "text": "she made the most of the time by visiting about with her daughter, and"}
{"doc_id": "gutenberg_1342", "para_id": 8016, "text": "having very frequent parties at home. These parties were acceptable to"}
{"doc_id": "gutenberg_1342", "para_id": 8017, "text": "all; to avoid a family circle was even more desirable to such as did"}
{"doc_id": "gutenberg_1342", "para_id": 8018, "text": "Wickham’s affection for Lydia was just what Elizabeth had expected to"}
{"doc_id": "gutenberg_1342", "para_id": 8019, "text": "find it; not equal to Lydia’s for him. She had scarcely needed her"}
{"doc_id": "gutenberg_1342", "para_id": 8020, "text": "present observation to be satisfied, from the reason of things, that"}
{"doc_id": "gutenberg_1342", "para_id": 8021, "text": "their elopement had been brought on by the strength of her love rather"}
{"doc_id": "gutenberg_1342", "para_id": 8022, "text": "than by his; and she would have wondered why, without violently caring"}
{"doc_id": "gutenberg_1342", "para_id": 8023, "text": "for her, he chose to elope with her at all, had she not felt certain"}
{"doc_id": "gutenberg_1342", "para_id": 8024, "text": "that his flight was rendered necessary by distress of circumstances; and"}
{"doc_id": "gutenberg_1342", "para_id": 8025, "text": "if that were the case, he was not the young man to resist an opportunity"}
{"doc_id": "gutenberg_1342", "para_id": 8026, "text": "Lydia was exceedingly fond of him. He was her dear Wickham on every"}
{"doc_id": "gutenberg_1342", "para_id": 8027, "text": "occasion; no one was to be put in competition with him. He did"}
{"doc_id": "gutenberg_1342", "para_id": 8028, "text": "everything best in the world; and she was sure he would kill more birds"}
{"doc_id": "gutenberg_1342", "para_id": 8029, "text": "on the first of September than anybody else in the country."}
{"doc_id": "gutenberg_1342", "para_id": 8030, "text": "One morning, soon after their arrival, as she was sitting with her two"}
{"doc_id": "gutenberg_1342", "para_id": 8031, "text": "“Lizzy, I never gave _you_ an account of my wedding, I believe. You were"}
{"doc_id": "gutenberg_1342", "para_id": 8032, "text": "not by, when I told mamma, and the others, all about it. Are not you"}
{"doc_id": "gutenberg_1342", "para_id": 8033, "text": "“No, really,” replied Elizabeth; “I think there cannot be too little"}
{"doc_id": "gutenberg_1342", "para_id": 8034, "text": "“La! You are so strange! But I must tell you how it went off. We were"}
{"doc_id": "gutenberg_1342", "para_id": 8035, "text": "married, you know, at St. Clement’s, because Wickham’s lodgings were in"}
{"doc_id": "gutenberg_1342", "para_id": 8036, "text": "that parish. And it was settled that we should all be there by eleven"}
{"doc_id": "gutenberg_1342", "para_id": 8037, "text": "o’clock. My uncle and aunt and I were to go together; and the others"}
{"doc_id": "gutenberg_1342", "para_id": 8038, "text": "“Well, Monday morning came, and I was in such a fuss! I was so afraid,"}
{"doc_id": "gutenberg_1342", "para_id": 8039, "text": "you know, that something would happen to put it off, and then I should"}
{"doc_id": "gutenberg_1342", "para_id": 8040, "text": "have gone quite distracted. And there was my aunt, all the time I was"}
{"doc_id": "gutenberg_1342", "para_id": 8041, "text": "dressing, preaching and talking away just as if she was reading a"}
{"doc_id": "gutenberg_1342", "para_id": 8042, "text": "sermon. However, I did not hear above one word in ten, for I was"}
{"doc_id": "gutenberg_1342", "para_id": 8043, "text": "thinking, you may suppose, of my dear Wickham. I longed to know whether"}
{"doc_id": "gutenberg_1342", "para_id": 8044, "text": "“Well, and so we breakfasted at ten as usual: I thought it would never"}
{"doc_id": "gutenberg_1342", "para_id": 8045, "text": "be over; for, by the bye, you are to understand that my uncle and aunt"}
{"doc_id": "gutenberg_1342", "para_id": 8046, "text": "were horrid unpleasant all the time I was with them. If you’ll believe"}
{"doc_id": "gutenberg_1342", "para_id": 8047, "text": "me, I did not once put my foot out of doors, though I was there a"}
{"doc_id": "gutenberg_1342", "para_id": 8048, "text": "fortnight. Not one party, or scheme, or anything! To be sure, London was"}
{"doc_id": "gutenberg_1342", "para_id": 8049, "text": "rather thin, but, however, the Little Theatre was open."}
{"doc_id": "gutenberg_1342", "para_id": 8050, "text": "“Well, and so, just as the carriage came to the door, my uncle was"}
{"doc_id": "gutenberg_1342", "para_id": 8051, "text": "called away upon business to that horrid man Mr. Stone. And then, you"}
{"doc_id": "gutenberg_1342", "para_id": 8052, "text": "know, when once they get together, there is no end of it. Well, I was so"}
{"doc_id": "gutenberg_1342", "para_id": 8053, "text": "frightened I did not know what to do, for my uncle was to give me away;"}
{"doc_id": "gutenberg_1342", "para_id": 8054, "text": "and if we were beyond the hour we could not be married all day. But,"}
{"doc_id": "gutenberg_1342", "para_id": 8055, "text": "luckily, he came back again in ten minutes’ time, and then we all set"}
{"doc_id": "gutenberg_1342", "para_id": 8056, "text": "out. However, I recollected afterwards, that if he _had_ been prevented"}
{"doc_id": "gutenberg_1342", "para_id": 8057, "text": "going, the wedding need not be put off, for Mr. Darcy might have done as"}
{"doc_id": "gutenberg_1342", "para_id": 8058, "text": "“Mr. Darcy!” repeated Elizabeth, in utter amazement."}
{"doc_id": "gutenberg_1342", "para_id": 8059, "text": "“Oh, yes! he was to come there with Wickham, you know. But, gracious me!"}
{"doc_id": "gutenberg_1342", "para_id": 8060, "text": "I quite forgot! I ought not to have said a word about it. I promised"}
{"doc_id": "gutenberg_1342", "para_id": 8061, "text": "them so faithfully! What will Wickham say? It was to be such a secret!”"}
{"doc_id": "gutenberg_1342", "para_id": 8062, "text": "“If it was to be a secret,” said Jane, “say not another word on the"}
{"doc_id": "gutenberg_1342", "para_id": 8063, "text": "subject. You may depend upon my seeking no further.”"}
{"doc_id": "gutenberg_1342", "para_id": 8064, "text": "“Oh, certainly,” said Elizabeth, though burning with curiosity; “we will"}
{"doc_id": "gutenberg_1342", "para_id": 8065, "text": "“Thank you,” said Lydia; “for if you did, I should certainly tell you"}
{"doc_id": "gutenberg_1342", "para_id": 8066, "text": "On such encouragement to ask, Elizabeth was forced to put it out of her"}
{"doc_id": "gutenberg_1342", "para_id": 8067, "text": "But to live in ignorance on such a point was impossible; or at least it"}
{"doc_id": "gutenberg_1342", "para_id": 8068, "text": "was impossible not to try for information. Mr. Darcy had been at her"}
{"doc_id": "gutenberg_1342", "para_id": 8069, "text": "sister’s wedding. It was exactly a scene, and exactly among people,"}
{"doc_id": "gutenberg_1342", "para_id": 8070, "text": "where he had apparently least to do, and least temptation to go."}
{"doc_id": "gutenberg_1342", "para_id": 8071, "text": "Conjectures as to the meaning of it, rapid and wild, hurried into her"}
{"doc_id": "gutenberg_1342", "para_id": 8072, "text": "brain; but she was satisfied with none. Those that best pleased her, as"}
{"doc_id": "gutenberg_1342", "para_id": 8073, "text": "placing his conduct in the noblest light, seemed most improbable. She"}
{"doc_id": "gutenberg_1342", "para_id": 8074, "text": "could not bear such suspense; and hastily seizing a sheet of paper,"}
{"doc_id": "gutenberg_1342", "para_id": 8075, "text": "wrote a short letter to her aunt, to request an explanation of what"}
{"doc_id": "gutenberg_1342", "para_id": 8076, "text": "Lydia had dropped, if it were compatible with the secrecy which had been"}
{"doc_id": "gutenberg_1342", "para_id": 8077, "text": "“You may readily comprehend,” she added, “what my curiosity must be to"}
{"doc_id": "gutenberg_1342", "para_id": 8078, "text": "know how a person unconnected with any of us, and, comparatively"}
{"doc_id": "gutenberg_1342", "para_id": 8079, "text": "speaking, a stranger to our family, should have been amongst you at such"}
{"doc_id": "gutenberg_1342", "para_id": 8080, "text": "a time. Pray write instantly, and let me understand it--unless it is,"}
{"doc_id": "gutenberg_1342", "para_id": 8081, "text": "for very cogent reasons, to remain in the secrecy which Lydia seems to"}
{"doc_id": "gutenberg_1342", "para_id": 8082, "text": "think necessary; and then I must endeavour to be satisfied with"}
{"doc_id": "gutenberg_1342", "para_id": 8083, "text": "“Not that I _shall_, though,” she added to herself, and she finished the"}
{"doc_id": "gutenberg_1342", "para_id": 8084, "text": "letter; “and, my dear aunt, if you do not tell me in an honourable"}
{"doc_id": "gutenberg_1342", "para_id": 8085, "text": "manner, I shall certainly be reduced to tricks and stratagems to find it"}
{"doc_id": "gutenberg_1342", "para_id": 8086, "text": "Jane’s delicate sense of honour would not allow her to speak to"}
{"doc_id": "gutenberg_1342", "para_id": 8087, "text": "Elizabeth privately of what Lydia had let fall; Elizabeth was glad of"}
{"doc_id": "gutenberg_1342", "para_id": 8088, "text": "it:--till it appeared whether her inquiries would receive any"}
{"doc_id": "gutenberg_1342", "para_id": 8089, "text": "satisfaction, she had rather be without a confidante."}
{"doc_id": "gutenberg_1342", "para_id": 8090, "text": "Elizabeth had the satisfaction of receiving an answer to her letter as"}
{"doc_id": "gutenberg_1342", "para_id": 8091, "text": "soon as she possibly could. She was no sooner in possession of it, than"}
{"doc_id": "gutenberg_1342", "para_id": 8092, "text": "hurrying into the little copse, where she was least likely to be"}
{"doc_id": "gutenberg_1342", "para_id": 8093, "text": "interrupted, she sat down on one of the benches, and prepared to be"}
{"doc_id": "gutenberg_1342", "para_id": 8094, "text": "happy; for the length of the letter convinced her that it did not"}
{"doc_id": "gutenberg_1342", "para_id": 8095, "text": "“I have just received your letter, and shall devote this whole"}
{"doc_id": "gutenberg_1342", "para_id": 8096, "text": "morning to answering it, as I foresee that a _little_ writing will"}
{"doc_id": "gutenberg_1342", "para_id": 8097, "text": "not comprise what I have to tell you. I must confess myself"}
{"doc_id": "gutenberg_1342", "para_id": 8098, "text": "surprised by your application; I did not expect it from _you_."}
{"doc_id": "gutenberg_1342", "para_id": 8099, "text": "Don’t think me angry, however, for I only mean to let you know,"}
{"doc_id": "gutenberg_1342", "para_id": 8100, "text": "that I had not imagined such inquiries to be necessary on _your_"}
{"doc_id": "gutenberg_1342", "para_id": 8101, "text": "side. If you do not choose to understand me, forgive my"}
{"doc_id": "gutenberg_1342", "para_id": 8102, "text": "impertinence. Your uncle is as much surprised as I am; and nothing"}
{"doc_id": "gutenberg_1342", "para_id": 8103, "text": "but the belief of your being a party concerned would have allowed"}
{"doc_id": "gutenberg_1342", "para_id": 8104, "text": "him to act as he has done. But if you are really innocent and"}
{"doc_id": "gutenberg_1342", "para_id": 8105, "text": "ignorant, I must be more explicit. On the very day of my coming"}
{"doc_id": "gutenberg_1342", "para_id": 8106, "text": "home from Longbourn, your uncle had a most unexpected visitor. Mr."}
{"doc_id": "gutenberg_1342", "para_id": 8107, "text": "Darcy called, and was shut up with him several hours. It was all"}
{"doc_id": "gutenberg_1342", "para_id": 8108, "text": "over before I arrived; so my curiosity was not so dreadfully racked"}
{"doc_id": "gutenberg_1342", "para_id": 8109, "text": "as _yours_ seems to have been. He came to tell Mr. Gardiner that he"}
{"doc_id": "gutenberg_1342", "para_id": 8110, "text": "had found out where your sister and Mr. Wickham were, and that he"}
{"doc_id": "gutenberg_1342", "para_id": 8111, "text": "had seen and talked with them both--Wickham repeatedly, Lydia once."}
{"doc_id": "gutenberg_1342", "para_id": 8112, "text": "From what I can collect, he left Derbyshire only one day after"}
{"doc_id": "gutenberg_1342", "para_id": 8113, "text": "ourselves, and came to town with the resolution of hunting for"}
{"doc_id": "gutenberg_1342", "para_id": 8114, "text": "them. The motive professed was his conviction of its being owing to"}
{"doc_id": "gutenberg_1342", "para_id": 8115, "text": "himself that Wickham’s worthlessness had not been so well known as"}
{"doc_id": "gutenberg_1342", "para_id": 8116, "text": "to make it impossible for any young woman of character to love or"}
{"doc_id": "gutenberg_1342", "para_id": 8117, "text": "confide in him. He generously imputed the whole to his mistaken"}
{"doc_id": "gutenberg_1342", "para_id": 8118, "text": "pride, and confessed that he had before thought it beneath him to"}
{"doc_id": "gutenberg_1342", "para_id": 8119, "text": "lay his private actions open to the world. His character was to"}
{"doc_id": "gutenberg_1342", "para_id": 8120, "text": "speak for itself. He called it, therefore, his duty to step"}
{"doc_id": "gutenberg_1342", "para_id": 8121, "text": "forward, and endeavour to remedy an evil which had been brought on"}
{"doc_id": "gutenberg_1342", "para_id": 8122, "text": "by himself. If he _had another_ motive, I am sure it would never"}
{"doc_id": "gutenberg_1342", "para_id": 8123, "text": "disgrace him. He had been some days in town before he was able to"}
{"doc_id": "gutenberg_1342", "para_id": 8124, "text": "discover them; but he had something to direct his search, which was"}
{"doc_id": "gutenberg_1342", "para_id": 8125, "text": "more than _we_ had; and the consciousness of this was another"}
{"doc_id": "gutenberg_1342", "para_id": 8126, "text": "reason for his resolving to follow us. There is a lady, it seems, a"}
{"doc_id": "gutenberg_1342", "para_id": 8127, "text": "Mrs. Younge, who was some time ago governess to Miss Darcy, and was"}
{"doc_id": "gutenberg_1342", "para_id": 8128, "text": "dismissed from her charge on some cause of disapprobation, though"}
{"doc_id": "gutenberg_1342", "para_id": 8129, "text": "he did not say what. She then took a large house in Edward Street,"}
{"doc_id": "gutenberg_1342", "para_id": 8130, "text": "and has since maintained herself by letting lodgings. This Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 8131, "text": "Younge was, he knew, intimately acquainted with Wickham; and he"}
{"doc_id": "gutenberg_1342", "para_id": 8132, "text": "went to her for intelligence of him, as soon as he got to town. But"}
{"doc_id": "gutenberg_1342", "para_id": 8133, "text": "it was two or three days before he could get from her what he"}
{"doc_id": "gutenberg_1342", "para_id": 8134, "text": "wanted. She would not betray her trust, I suppose, without bribery"}
{"doc_id": "gutenberg_1342", "para_id": 8135, "text": "and corruption, for she really did know where her friend was to be"}
{"doc_id": "gutenberg_1342", "para_id": 8136, "text": "found. Wickham, indeed, had gone to her on their first arrival in"}
{"doc_id": "gutenberg_1342", "para_id": 8137, "text": "London; and had she been able to receive them into her house, they"}
{"doc_id": "gutenberg_1342", "para_id": 8138, "text": "would have taken up their abode with her. At length, however, our"}
{"doc_id": "gutenberg_1342", "para_id": 8139, "text": "kind friend procured the wished-for direction. They were in ----"}
{"doc_id": "gutenberg_1342", "para_id": 8140, "text": "Street. He saw Wickham, and afterwards insisted on seeing Lydia."}
{"doc_id": "gutenberg_1342", "para_id": 8141, "text": "His first object with her, he acknowledged, had been to persuade"}
{"doc_id": "gutenberg_1342", "para_id": 8142, "text": "her to quit her present disgraceful situation, and return to her"}
{"doc_id": "gutenberg_1342", "para_id": 8143, "text": "friends as soon as they could be prevailed on to receive her,"}
{"doc_id": "gutenberg_1342", "para_id": 8144, "text": "offering his assistance as far as it would go. But he found Lydia"}
{"doc_id": "gutenberg_1342", "para_id": 8145, "text": "absolutely resolved on remaining where she was. She cared for none"}
{"doc_id": "gutenberg_1342", "para_id": 8146, "text": "of her friends; she wanted no help of his; she would not hear of"}
{"doc_id": "gutenberg_1342", "para_id": 8147, "text": "leaving Wickham. She was sure they should be married some time or"}
{"doc_id": "gutenberg_1342", "para_id": 8148, "text": "other, and it did not much signify when. Since such were her"}
{"doc_id": "gutenberg_1342", "para_id": 8149, "text": "feelings, it only remained, he thought, to secure and expedite a"}
{"doc_id": "gutenberg_1342", "para_id": 8150, "text": "marriage, which, in his very first conversation with Wickham, he"}
{"doc_id": "gutenberg_1342", "para_id": 8151, "text": "easily learnt had never been _his_ design. He confessed himself"}
{"doc_id": "gutenberg_1342", "para_id": 8152, "text": "obliged to leave the regiment on account of some debts of honour"}
{"doc_id": "gutenberg_1342", "para_id": 8153, "text": "which were very pressing; and scrupled not to lay all the ill"}
{"doc_id": "gutenberg_1342", "para_id": 8154, "text": "consequences of Lydia’s flight on her own folly alone. He meant to"}
{"doc_id": "gutenberg_1342", "para_id": 8155, "text": "resign his commission immediately; and as to his future situation,"}
{"doc_id": "gutenberg_1342", "para_id": 8156, "text": "he could conjecture very little about it. He must go somewhere, but"}
{"doc_id": "gutenberg_1342", "para_id": 8157, "text": "he did not know where, and he knew he should have nothing to live"}
{"doc_id": "gutenberg_1342", "para_id": 8158, "text": "on. Mr. Darcy asked why he did not marry your sister at once."}
{"doc_id": "gutenberg_1342", "para_id": 8159, "text": "Though Mr. Bennet was not imagined to be very rich, he would have"}
{"doc_id": "gutenberg_1342", "para_id": 8160, "text": "been able to do something for him, and his situation must have been"}
{"doc_id": "gutenberg_1342", "para_id": 8161, "text": "benefited by marriage. But he found, in reply to this question,"}
{"doc_id": "gutenberg_1342", "para_id": 8162, "text": "that Wickham still cherished the hope of more effectually making"}
{"doc_id": "gutenberg_1342", "para_id": 8163, "text": "his fortune by marriage, in some other country. Under such"}
{"doc_id": "gutenberg_1342", "para_id": 8164, "text": "circumstances, however, he was not likely to be proof against the"}
{"doc_id": "gutenberg_1342", "para_id": 8165, "text": "temptation of immediate relief. They met several times, for there"}
{"doc_id": "gutenberg_1342", "para_id": 8166, "text": "was much to be discussed. Wickham, of course, wanted more than he"}
{"doc_id": "gutenberg_1342", "para_id": 8167, "text": "could get; but at length was reduced to be reasonable. Everything"}
{"doc_id": "gutenberg_1342", "para_id": 8168, "text": "being settled between _them_, Mr. Darcy’s next step was to make"}
{"doc_id": "gutenberg_1342", "para_id": 8169, "text": "your uncle acquainted with it, and he first called in Gracechurch"}
{"doc_id": "gutenberg_1342", "para_id": 8170, "text": "Street the evening before I came home. But Mr. Gardiner could not"}
{"doc_id": "gutenberg_1342", "para_id": 8171, "text": "be seen; and Mr. Darcy found, on further inquiry, that your father"}
{"doc_id": "gutenberg_1342", "para_id": 8172, "text": "was still with him, but would quit town the next morning. He did"}
{"doc_id": "gutenberg_1342", "para_id": 8173, "text": "not judge your father to be a person whom he could so properly"}
{"doc_id": "gutenberg_1342", "para_id": 8174, "text": "consult as your uncle, and therefore readily postponed seeing him"}
{"doc_id": "gutenberg_1342", "para_id": 8175, "text": "till after the departure of the former. He did not leave his name,"}
{"doc_id": "gutenberg_1342", "para_id": 8176, "text": "and till the next day it was only known that a gentleman had called"}
{"doc_id": "gutenberg_1342", "para_id": 8177, "text": "on business. On Saturday he came again. Your father was gone, your"}
{"doc_id": "gutenberg_1342", "para_id": 8178, "text": "uncle at home, and, as I said before, they had a great deal of talk"}
{"doc_id": "gutenberg_1342", "para_id": 8179, "text": "together. They met again on Sunday, and then _I_ saw him too. It"}
{"doc_id": "gutenberg_1342", "para_id": 8180, "text": "was not all settled before Monday: as soon as it was, the express"}
{"doc_id": "gutenberg_1342", "para_id": 8181, "text": "was sent off to Longbourn. But our visitor was very obstinate. I"}
{"doc_id": "gutenberg_1342", "para_id": 8182, "text": "fancy, Lizzy, that obstinacy is the real defect of his character,"}
{"doc_id": "gutenberg_1342", "para_id": 8183, "text": "after all. He has been accused of many faults at different times;"}
{"doc_id": "gutenberg_1342", "para_id": 8184, "text": "but _this_ is the true one. Nothing was to be done that he did not"}
{"doc_id": "gutenberg_1342", "para_id": 8185, "text": "do himself; though I am sure (and I do not speak it to be thanked,"}
{"doc_id": "gutenberg_1342", "para_id": 8186, "text": "therefore say nothing about it) your uncle would most readily have"}
{"doc_id": "gutenberg_1342", "para_id": 8187, "text": "settled the whole. They battled it together for a long time, which"}
{"doc_id": "gutenberg_1342", "para_id": 8188, "text": "was more than either the gentleman or lady concerned in it"}
{"doc_id": "gutenberg_1342", "para_id": 8189, "text": "deserved. But at last your uncle was forced to yield, and instead"}
{"doc_id": "gutenberg_1342", "para_id": 8190, "text": "of being allowed to be of use to his niece, was forced to put up"}
{"doc_id": "gutenberg_1342", "para_id": 8191, "text": "with only having the probable credit of it, which went sorely"}
{"doc_id": "gutenberg_1342", "para_id": 8192, "text": "against the grain; and I really believe your letter this morning"}
{"doc_id": "gutenberg_1342", "para_id": 8193, "text": "gave him great pleasure, because it required an explanation that"}
{"doc_id": "gutenberg_1342", "para_id": 8194, "text": "would rob him of his borrowed feathers, and give the praise where"}
{"doc_id": "gutenberg_1342", "para_id": 8195, "text": "it was due. But, Lizzy, this must go no further than yourself, or"}
{"doc_id": "gutenberg_1342", "para_id": 8196, "text": "Jane at most. You know pretty well, I suppose, what has been done"}
{"doc_id": "gutenberg_1342", "para_id": 8197, "text": "for the young people. His debts are to be paid, amounting, I"}
{"doc_id": "gutenberg_1342", "para_id": 8198, "text": "believe, to considerably more than a thousand pounds, another"}
{"doc_id": "gutenberg_1342", "para_id": 8199, "text": "thousand in addition to her own settled upon _her_, and his"}
{"doc_id": "gutenberg_1342", "para_id": 8200, "text": "commission purchased. The reason why all this was to be done by him"}
{"doc_id": "gutenberg_1342", "para_id": 8201, "text": "alone, was such as I have given above. It was owing to him, to his"}
{"doc_id": "gutenberg_1342", "para_id": 8202, "text": "reserve and want of proper consideration, that Wickham’s character"}
{"doc_id": "gutenberg_1342", "para_id": 8203, "text": "had been so misunderstood, and consequently that he had been"}
{"doc_id": "gutenberg_1342", "para_id": 8204, "text": "received and noticed as he was. Perhaps there was some truth in"}
{"doc_id": "gutenberg_1342", "para_id": 8205, "text": "_this_; though I doubt whether _his_ reserve, or _anybody’s_"}
{"doc_id": "gutenberg_1342", "para_id": 8206, "text": "reserve can be answerable for the event. But in spite of all this"}
{"doc_id": "gutenberg_1342", "para_id": 8207, "text": "fine talking, my dear Lizzy, you may rest perfectly assured that"}
{"doc_id": "gutenberg_1342", "para_id": 8208, "text": "your uncle would never have yielded, if we had not given him credit"}
{"doc_id": "gutenberg_1342", "para_id": 8209, "text": "for _another interest_ in the affair. When all this was resolved"}
{"doc_id": "gutenberg_1342", "para_id": 8210, "text": "on, he returned again to his friends, who were still staying at"}
{"doc_id": "gutenberg_1342", "para_id": 8211, "text": "Pemberley; but it was agreed that he should be in London once more"}
{"doc_id": "gutenberg_1342", "para_id": 8212, "text": "when the wedding took place, and all money matters were then to"}
{"doc_id": "gutenberg_1342", "para_id": 8213, "text": "receive the last finish. I believe I have now told you everything."}
{"doc_id": "gutenberg_1342", "para_id": 8214, "text": "It is a relation which you tell me is to give you great surprise; I"}
{"doc_id": "gutenberg_1342", "para_id": 8215, "text": "hope at least it will not afford you any displeasure. Lydia came to"}
{"doc_id": "gutenberg_1342", "para_id": 8216, "text": "us, and Wickham had constant admission to the house. _He_ was"}
{"doc_id": "gutenberg_1342", "para_id": 8217, "text": "exactly what he had been when I knew him in Hertfordshire; but I"}
{"doc_id": "gutenberg_1342", "para_id": 8218, "text": "would not tell you how little I was satisfied with _her_ behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 8219, "text": "while she stayed with us, if I had not perceived, by Jane’s letter"}
{"doc_id": "gutenberg_1342", "para_id": 8220, "text": "last Wednesday, that her conduct on coming home was exactly of a"}
{"doc_id": "gutenberg_1342", "para_id": 8221, "text": "piece with it, and therefore what I now tell you can give you no"}
{"doc_id": "gutenberg_1342", "para_id": 8222, "text": "fresh pain. I talked to her repeatedly in the most serious manner,"}
{"doc_id": "gutenberg_1342", "para_id": 8223, "text": "representing to her the wickedness of what she had done, and all"}
{"doc_id": "gutenberg_1342", "para_id": 8224, "text": "the unhappiness she had brought on her family. If she heard me, it"}
{"doc_id": "gutenberg_1342", "para_id": 8225, "text": "was by good luck, for I am sure she did not listen. I was sometimes"}
{"doc_id": "gutenberg_1342", "para_id": 8226, "text": "quite provoked; but then I recollected my dear Elizabeth and Jane,"}
{"doc_id": "gutenberg_1342", "para_id": 8227, "text": "and for their sakes had patience with her. Mr. Darcy was punctual"}
{"doc_id": "gutenberg_1342", "para_id": 8228, "text": "in his return, and, as Lydia informed you, attended the wedding. He"}
{"doc_id": "gutenberg_1342", "para_id": 8229, "text": "dined with us the next day, and was to leave town again on"}
{"doc_id": "gutenberg_1342", "para_id": 8230, "text": "Wednesday or Thursday. Will you be very angry with me, my dear"}
{"doc_id": "gutenberg_1342", "para_id": 8231, "text": "Lizzy, if I take this opportunity of saying (what I was never bold"}
{"doc_id": "gutenberg_1342", "para_id": 8232, "text": "enough to say before) how much I like him? His behaviour to us has,"}
{"doc_id": "gutenberg_1342", "para_id": 8233, "text": "in every respect, been as pleasing as when we were in Derbyshire."}
{"doc_id": "gutenberg_1342", "para_id": 8234, "text": "His understanding and opinions all please me; he wants nothing but"}
{"doc_id": "gutenberg_1342", "para_id": 8235, "text": "a little more liveliness, and _that_, if he marry _prudently_, his"}
{"doc_id": "gutenberg_1342", "para_id": 8236, "text": "wife may teach him. I thought him very sly; he hardly ever"}
{"doc_id": "gutenberg_1342", "para_id": 8237, "text": "mentioned your name. But slyness seems the fashion. Pray forgive"}
{"doc_id": "gutenberg_1342", "para_id": 8238, "text": "me, if I have been very presuming, or at least do not punish me so"}
{"doc_id": "gutenberg_1342", "para_id": 8239, "text": "far as to exclude me from P. I shall never be quite happy till I"}
{"doc_id": "gutenberg_1342", "para_id": 8240, "text": "have been all round the park. A low phaeton with a nice little pair"}
{"doc_id": "gutenberg_1342", "para_id": 8241, "text": "of ponies would be the very thing. But I must write no more. The"}
{"doc_id": "gutenberg_1342", "para_id": 8242, "text": "The contents of this letter threw Elizabeth into a flutter of spirits,"}
{"doc_id": "gutenberg_1342", "para_id": 8243, "text": "in which it was difficult to determine whether pleasure or pain bore the"}
{"doc_id": "gutenberg_1342", "para_id": 8244, "text": "greatest share. The vague and unsettled suspicions which uncertainty had"}
{"doc_id": "gutenberg_1342", "para_id": 8245, "text": "produced, of what Mr. Darcy might have been doing to forward her"}
{"doc_id": "gutenberg_1342", "para_id": 8246, "text": "sister’s match--which she had feared to encourage, as an exertion of"}
{"doc_id": "gutenberg_1342", "para_id": 8247, "text": "goodness too great to be probable, and at the same time dreaded to be"}
{"doc_id": "gutenberg_1342", "para_id": 8248, "text": "just, from the pain of obligation--were proved beyond their greatest"}
{"doc_id": "gutenberg_1342", "para_id": 8249, "text": "extent to be true! He had followed them purposely to town, he had taken"}
{"doc_id": "gutenberg_1342", "para_id": 8250, "text": "on himself all the trouble and mortification attendant on such a"}
{"doc_id": "gutenberg_1342", "para_id": 8251, "text": "research; in which supplication had been necessary to a woman whom he"}
{"doc_id": "gutenberg_1342", "para_id": 8252, "text": "must abominate and despise, and where he was reduced to meet, frequently"}
{"doc_id": "gutenberg_1342", "para_id": 8253, "text": "meet, reason with, persuade, and finally bribe the man whom he always"}
{"doc_id": "gutenberg_1342", "para_id": 8254, "text": "most wished to avoid, and whose very name it was punishment to him to"}
{"doc_id": "gutenberg_1342", "para_id": 8255, "text": "pronounce. He had done all this for a girl whom he could neither regard"}
{"doc_id": "gutenberg_1342", "para_id": 8256, "text": "nor esteem. Her heart did whisper that he had done it for her. But it"}
{"doc_id": "gutenberg_1342", "para_id": 8257, "text": "was a hope shortly checked by other considerations; and she soon felt"}
{"doc_id": "gutenberg_1342", "para_id": 8258, "text": "that even her vanity was insufficient, when required to depend on his"}
{"doc_id": "gutenberg_1342", "para_id": 8259, "text": "affection for her, for a woman who had already refused him, as able to"}
{"doc_id": "gutenberg_1342", "para_id": 8260, "text": "overcome a sentiment so natural as abhorrence against relationship with"}
{"doc_id": "gutenberg_1342", "para_id": 8261, "text": "Wickham. Brother-in-law of Wickham! Every kind of pride must revolt from"}
{"doc_id": "gutenberg_1342", "para_id": 8262, "text": "the connection. He had, to be sure, done much. She was ashamed to think"}
{"doc_id": "gutenberg_1342", "para_id": 8263, "text": "how much. But he had given a reason for his interference, which asked no"}
{"doc_id": "gutenberg_1342", "para_id": 8264, "text": "extraordinary stretch of belief. It was reasonable that he should feel"}
{"doc_id": "gutenberg_1342", "para_id": 8265, "text": "he had been wrong; he had liberality, and he had the means of exercising"}
{"doc_id": "gutenberg_1342", "para_id": 8266, "text": "it; and though she would not place herself as his principal inducement,"}
{"doc_id": "gutenberg_1342", "para_id": 8267, "text": "she could perhaps believe, that remaining partiality for her might"}
{"doc_id": "gutenberg_1342", "para_id": 8268, "text": "assist his endeavours in a cause where her peace of mind must be"}
{"doc_id": "gutenberg_1342", "para_id": 8269, "text": "materially concerned. It was painful, exceedingly painful, to know that"}
{"doc_id": "gutenberg_1342", "para_id": 8270, "text": "they were under obligations to a person who could never receive a"}
{"doc_id": "gutenberg_1342", "para_id": 8271, "text": "return. They owed the restoration of Lydia, her character, everything to"}
{"doc_id": "gutenberg_1342", "para_id": 8272, "text": "him. Oh, how heartily did she grieve over every ungracious sensation she"}
{"doc_id": "gutenberg_1342", "para_id": 8273, "text": "had ever encouraged, every saucy speech she had ever directed towards"}
{"doc_id": "gutenberg_1342", "para_id": 8274, "text": "him! For herself she was humbled; but she was proud of him,--proud that"}
{"doc_id": "gutenberg_1342", "para_id": 8275, "text": "in a cause of compassion and honour he had been able to get the better"}
{"doc_id": "gutenberg_1342", "para_id": 8276, "text": "of himself. She read over her aunt’s commendation of him again and"}
{"doc_id": "gutenberg_1342", "para_id": 8277, "text": "again. It was hardly enough; but it pleased her. She was even sensible"}
{"doc_id": "gutenberg_1342", "para_id": 8278, "text": "of some pleasure, though mixed with regret, on finding how steadfastly"}
{"doc_id": "gutenberg_1342", "para_id": 8279, "text": "both she and her uncle had been persuaded that affection and confidence"}
{"doc_id": "gutenberg_1342", "para_id": 8280, "text": "She was roused from her seat and her reflections, by someone’s approach;"}
{"doc_id": "gutenberg_1342", "para_id": 8281, "text": "and, before she could strike into another path, she was overtaken by"}
{"doc_id": "gutenberg_1342", "para_id": 8282, "text": "“I am afraid I interrupt your solitary ramble, my dear sister?” said he,"}
{"doc_id": "gutenberg_1342", "para_id": 8283, "text": "“You certainly do,” she replied with a smile; “but it does not follow"}
{"doc_id": "gutenberg_1342", "para_id": 8284, "text": "“I should be sorry, indeed, if it were. _We_ were always good friends,"}
{"doc_id": "gutenberg_1342", "para_id": 8285, "text": "“I do not know. Mrs. Bennet and Lydia are going in the carriage to"}
{"doc_id": "gutenberg_1342", "para_id": 8286, "text": "Meryton. And so, my dear sister, I find, from our uncle and aunt, that"}
{"doc_id": "gutenberg_1342", "para_id": 8287, "text": "“I almost envy you the pleasure, and yet I believe it would be too much"}
{"doc_id": "gutenberg_1342", "para_id": 8288, "text": "for me, or else I could take it in my way to Newcastle. And you saw the"}
{"doc_id": "gutenberg_1342", "para_id": 8289, "text": "old housekeeper, I suppose? Poor Reynolds, she was always very fond of"}
{"doc_id": "gutenberg_1342", "para_id": 8290, "text": "me. But of course she did not mention my name to you.”"}
{"doc_id": "gutenberg_1342", "para_id": 8291, "text": "“That you were gone into the army, and she was afraid had--not turned"}
{"doc_id": "gutenberg_1342", "para_id": 8292, "text": "out well. At such a distance as _that_, you know, things are strangely"}
{"doc_id": "gutenberg_1342", "para_id": 8293, "text": "“Certainly,” he replied, biting his lips. Elizabeth hoped she had"}
{"doc_id": "gutenberg_1342", "para_id": 8294, "text": "“I was surprised to see Darcy in town last month. We passed each other"}
{"doc_id": "gutenberg_1342", "para_id": 8295, "text": "several times. I wonder what he can be doing there.”"}
{"doc_id": "gutenberg_1342", "para_id": 8296, "text": "“Perhaps preparing for his marriage with Miss de Bourgh,” said"}
{"doc_id": "gutenberg_1342", "para_id": 8297, "text": "Elizabeth. “It must be something particular to take him there at this"}
{"doc_id": "gutenberg_1342", "para_id": 8298, "text": "“Undoubtedly. Did you see him while you were at Lambton? I thought I"}
{"doc_id": "gutenberg_1342", "para_id": 8299, "text": "“I have heard, indeed, that she is uncommonly improved within this year"}
{"doc_id": "gutenberg_1342", "para_id": 8300, "text": "or two. When I last saw her, she was not very promising. I am very glad"}
{"doc_id": "gutenberg_1342", "para_id": 8301, "text": "“I dare say she will; she has got over the most trying age.”"}
{"doc_id": "gutenberg_1342", "para_id": 8302, "text": "“I mention it because it is the living which I ought to have had. A most"}
{"doc_id": "gutenberg_1342", "para_id": 8303, "text": "delightful place! Excellent parsonage-house! It would have suited me in"}
{"doc_id": "gutenberg_1342", "para_id": 8304, "text": "“Exceedingly well. I should have considered it as part of my duty, and"}
{"doc_id": "gutenberg_1342", "para_id": 8305, "text": "the exertion would soon have been nothing. One ought not to repine; but,"}
{"doc_id": "gutenberg_1342", "para_id": 8306, "text": "to be sure, it would have been such a thing for me! The quiet, the"}
{"doc_id": "gutenberg_1342", "para_id": 8307, "text": "retirement of such a life, would have answered all my ideas of"}
{"doc_id": "gutenberg_1342", "para_id": 8308, "text": "happiness! But it was not to be. Did you ever hear Darcy mention the"}
{"doc_id": "gutenberg_1342", "para_id": 8309, "text": "“I _have_ heard from authority, which I thought _as good_, that it was"}
{"doc_id": "gutenberg_1342", "para_id": 8310, "text": "left you conditionally only, and at the will of the present patron.”"}
{"doc_id": "gutenberg_1342", "para_id": 8311, "text": "“You have! Yes, there was something in _that_; I told you so from the"}
{"doc_id": "gutenberg_1342", "para_id": 8312, "text": "“I _did_ hear, too, that there was a time when sermon-making was not so"}
{"doc_id": "gutenberg_1342", "para_id": 8313, "text": "palatable to you as it seems to be at present; that you actually"}
{"doc_id": "gutenberg_1342", "para_id": 8314, "text": "declared your resolution of never taking orders, and that the business"}
{"doc_id": "gutenberg_1342", "para_id": 8315, "text": "“You did! and it was not wholly without foundation. You may remember"}
{"doc_id": "gutenberg_1342", "para_id": 8316, "text": "what I told you on that point, when first we talked of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 8317, "text": "They were now almost at the door of the house, for she had walked fast"}
{"doc_id": "gutenberg_1342", "para_id": 8318, "text": "to get rid of him; and unwilling, for her sister’s sake, to provoke him,"}
{"doc_id": "gutenberg_1342", "para_id": 8319, "text": "she only said in reply, with a good-humoured smile,--"}
{"doc_id": "gutenberg_1342", "para_id": 8320, "text": "“Come, Mr. Wickham, we are brother and sister, you know. Do not let us"}
{"doc_id": "gutenberg_1342", "para_id": 8321, "text": "quarrel about the past. In future, I hope we shall be always of one"}
{"doc_id": "gutenberg_1342", "para_id": 8322, "text": "She held out her hand: he kissed it with affectionate gallantry, though"}
{"doc_id": "gutenberg_1342", "para_id": 8323, "text": "he hardly knew how to look, and they entered the house."}
{"doc_id": "gutenberg_1342", "para_id": 8324, "text": "Mr. Wickham was so perfectly satisfied with this conversation, that he"}
{"doc_id": "gutenberg_1342", "para_id": 8325, "text": "never again distressed himself, or provoked his dear sister Elizabeth,"}
{"doc_id": "gutenberg_1342", "para_id": 8326, "text": "by introducing the subject of it; and she was pleased to find that she"}
{"doc_id": "gutenberg_1342", "para_id": 8327, "text": "The day of his and Lydia’s departure soon came; and Mrs. Bennet was"}
{"doc_id": "gutenberg_1342", "para_id": 8328, "text": "forced to submit to a separation, which, as her husband by no means"}
{"doc_id": "gutenberg_1342", "para_id": 8329, "text": "entered into her scheme of their all going to Newcastle, was likely to"}
{"doc_id": "gutenberg_1342", "para_id": 8330, "text": "“Oh, my dear Lydia,” she cried, “when shall we meet again?”"}
{"doc_id": "gutenberg_1342", "para_id": 8331, "text": "“Oh, Lord! I don’t know. Not these two or three years, perhaps.”"}
{"doc_id": "gutenberg_1342", "para_id": 8332, "text": "“As often as I can. But you know married women have never much time for"}
{"doc_id": "gutenberg_1342", "para_id": 8333, "text": "writing. My sisters may write to _me_. They will have nothing else to"}
{"doc_id": "gutenberg_1342", "para_id": 8334, "text": "Mr. Wickham’s adieus were much more affectionate than his wife’s. He"}
{"doc_id": "gutenberg_1342", "para_id": 8335, "text": "smiled, looked handsome, and said many pretty things."}
{"doc_id": "gutenberg_1342", "para_id": 8336, "text": "“He is as fine a fellow,” said Mr. Bennet, as soon as they were out of"}
{"doc_id": "gutenberg_1342", "para_id": 8337, "text": "the house, “as ever I saw. He simpers, and smirks, and makes love to us"}
{"doc_id": "gutenberg_1342", "para_id": 8338, "text": "all. I am prodigiously proud of him. I defy even Sir William Lucas"}
{"doc_id": "gutenberg_1342", "para_id": 8339, "text": "The loss of her daughter made Mrs. Bennet very dull for several days."}
{"doc_id": "gutenberg_1342", "para_id": 8340, "text": "“I often think,” said she, “that there is nothing so bad as parting with"}
{"doc_id": "gutenberg_1342", "para_id": 8341, "text": "“This is the consequence, you see, madam, of marrying a daughter,” said"}
{"doc_id": "gutenberg_1342", "para_id": 8342, "text": "Elizabeth. “It must make you better satisfied that your other four are"}
{"doc_id": "gutenberg_1342", "para_id": 8343, "text": "“It is no such thing. Lydia does not leave me because she is married;"}
{"doc_id": "gutenberg_1342", "para_id": 8344, "text": "but only because her husband’s regiment happens to be so far off. If"}
{"doc_id": "gutenberg_1342", "para_id": 8345, "text": "that had been nearer, she would not have gone so soon.”"}
{"doc_id": "gutenberg_1342", "para_id": 8346, "text": "But the spiritless condition which this event threw her into was shortly"}
{"doc_id": "gutenberg_1342", "para_id": 8347, "text": "relieved, and her mind opened again to the agitation of hope, by an"}
{"doc_id": "gutenberg_1342", "para_id": 8348, "text": "article of news which then began to be in circulation. The housekeeper"}
{"doc_id": "gutenberg_1342", "para_id": 8349, "text": "at Netherfield had received orders to prepare for the arrival of her"}
{"doc_id": "gutenberg_1342", "para_id": 8350, "text": "master, who was coming down in a day or two, to shoot there for several"}
{"doc_id": "gutenberg_1342", "para_id": 8351, "text": "weeks. Mrs. Bennet was quite in the fidgets. She looked at Jane, and"}
{"doc_id": "gutenberg_1342", "para_id": 8352, "text": "“Well, well, and so Mr. Bingley is coming down, sister,” (for Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 8353, "text": "Philips first brought her the news). “Well, so much the better. Not that"}
{"doc_id": "gutenberg_1342", "para_id": 8354, "text": "I care about it, though. He is nothing to us, you know, and I am sure I"}
{"doc_id": "gutenberg_1342", "para_id": 8355, "text": "never want to see him again. But, however, he is very welcome to come to"}
{"doc_id": "gutenberg_1342", "para_id": 8356, "text": "Netherfield, if he likes it. And who knows what _may_ happen? But that"}
{"doc_id": "gutenberg_1342", "para_id": 8357, "text": "is nothing to us. You know, sister, we agreed long ago never to mention"}
{"doc_id": "gutenberg_1342", "para_id": 8358, "text": "a word about it. And so, it is quite certain he is coming?”"}
{"doc_id": "gutenberg_1342", "para_id": 8359, "text": "“You may depend on it,” replied the other, “for Mrs. Nichols was in"}
{"doc_id": "gutenberg_1342", "para_id": 8360, "text": "Meryton last night: I saw her passing by, and went out myself on purpose"}
{"doc_id": "gutenberg_1342", "para_id": 8361, "text": "to know the truth of it; and she told me that it was certainly true. He"}
{"doc_id": "gutenberg_1342", "para_id": 8362, "text": "comes down on Thursday, at the latest, very likely on Wednesday. She was"}
{"doc_id": "gutenberg_1342", "para_id": 8363, "text": "going to the butcher’s, she told me, on purpose to order in some meat on"}
{"doc_id": "gutenberg_1342", "para_id": 8364, "text": "Wednesday, and she has got three couple of ducks just fit to be killed.”"}
{"doc_id": "gutenberg_1342", "para_id": 8365, "text": "Miss Bennet had not been able to hear of his coming without changing"}
{"doc_id": "gutenberg_1342", "para_id": 8366, "text": "colour. It was many months since she had mentioned his name to"}
{"doc_id": "gutenberg_1342", "para_id": 8367, "text": "Elizabeth; but now, as soon as they were alone together, she said,--"}
{"doc_id": "gutenberg_1342", "para_id": 8368, "text": "“I saw you look at me to-day, Lizzy, when my aunt told us of the present"}
{"doc_id": "gutenberg_1342", "para_id": 8369, "text": "report; and I know I appeared distressed; but don’t imagine it was from"}
{"doc_id": "gutenberg_1342", "para_id": 8370, "text": "any silly cause. I was only confused for the moment, because I felt that"}
{"doc_id": "gutenberg_1342", "para_id": 8371, "text": "I _should_ be looked at. I do assure you that the news does not affect"}
{"doc_id": "gutenberg_1342", "para_id": 8372, "text": "me either with pleasure or pain. I am glad of one thing, that he comes"}
{"doc_id": "gutenberg_1342", "para_id": 8373, "text": "alone; because we shall see the less of him. Not that I am afraid of"}
{"doc_id": "gutenberg_1342", "para_id": 8374, "text": "Elizabeth did not know what to make of it. Had she not seen him in"}
{"doc_id": "gutenberg_1342", "para_id": 8375, "text": "Derbyshire, she might have supposed him capable of coming there with no"}
{"doc_id": "gutenberg_1342", "para_id": 8376, "text": "other view than what was acknowledged; but she still thought him partial"}
{"doc_id": "gutenberg_1342", "para_id": 8377, "text": "to Jane, and she wavered as to the greater probability of his coming"}
{"doc_id": "gutenberg_1342", "para_id": 8378, "text": "there _with_ his friend’s permission, or being bold enough to come"}
{"doc_id": "gutenberg_1342", "para_id": 8379, "text": "“Yet it is hard,” she sometimes thought, “that this poor man cannot come"}
{"doc_id": "gutenberg_1342", "para_id": 8380, "text": "to a house, which he has legally hired, without raising all this"}
{"doc_id": "gutenberg_1342", "para_id": 8381, "text": "In spite of what her sister declared, and really believed to be her"}
{"doc_id": "gutenberg_1342", "para_id": 8382, "text": "feelings, in the expectation of his arrival, Elizabeth could easily"}
{"doc_id": "gutenberg_1342", "para_id": 8383, "text": "perceive that her spirits were affected by it. They were more disturbed,"}
{"doc_id": "gutenberg_1342", "para_id": 8384, "text": "The subject which had been so warmly canvassed between their parents,"}
{"doc_id": "gutenberg_1342", "para_id": 8385, "text": "about a twelvemonth ago, was now brought forward again."}
{"doc_id": "gutenberg_1342", "para_id": 8386, "text": "“As soon as ever Mr. Bingley comes, my dear,” said Mrs. Bennet, “you"}
{"doc_id": "gutenberg_1342", "para_id": 8387, "text": "“No, no. You forced me into visiting him last year, and promised, if I"}
{"doc_id": "gutenberg_1342", "para_id": 8388, "text": "went to see him, he should marry one of my daughters. But it ended in"}
{"doc_id": "gutenberg_1342", "para_id": 8389, "text": "nothing, and I will not be sent on a fool’s errand again.”"}
{"doc_id": "gutenberg_1342", "para_id": 8390, "text": "His wife represented to him how absolutely necessary such an attention"}
{"doc_id": "gutenberg_1342", "para_id": 8391, "text": "would be from all the neighbouring gentlemen, on his returning to"}
{"doc_id": "gutenberg_1342", "para_id": 8392, "text": "“’Tis an _etiquette_ I despise,” said he. “If he wants our society, let"}
{"doc_id": "gutenberg_1342", "para_id": 8393, "text": "him seek it. He knows where we live. I will not spend _my_ hours in"}
{"doc_id": "gutenberg_1342", "para_id": 8394, "text": "running after my neighbours every time they go away and come back"}
{"doc_id": "gutenberg_1342", "para_id": 8395, "text": "“Well, all I know is, that it will be abominably rude if you do not wait"}
{"doc_id": "gutenberg_1342", "para_id": 8396, "text": "on him. But, however, that shan’t prevent my asking him to dine here, I"}
{"doc_id": "gutenberg_1342", "para_id": 8397, "text": "am determined. We must have Mrs. Long and the Gouldings soon. That will"}
{"doc_id": "gutenberg_1342", "para_id": 8398, "text": "make thirteen with ourselves, so there will be just room at table for"}
{"doc_id": "gutenberg_1342", "para_id": 8399, "text": "Consoled by this resolution, she was the better able to bear her"}
{"doc_id": "gutenberg_1342", "para_id": 8400, "text": "husband’s incivility; though it was very mortifying to know that her"}
{"doc_id": "gutenberg_1342", "para_id": 8401, "text": "neighbours might all see Mr. Bingley, in consequence of it, before"}
{"doc_id": "gutenberg_1342", "para_id": 8402, "text": "“I begin to be sorry that he comes at all,” said Jane to her sister. “It"}
{"doc_id": "gutenberg_1342", "para_id": 8403, "text": "would be nothing; I could see him with perfect indifference; but I can"}
{"doc_id": "gutenberg_1342", "para_id": 8404, "text": "hardly bear to hear it thus perpetually talked of. My mother means well;"}
{"doc_id": "gutenberg_1342", "para_id": 8405, "text": "but she does not know, no one can know, how much I suffer from what she"}
{"doc_id": "gutenberg_1342", "para_id": 8406, "text": "says. Happy shall I be when his stay at Netherfield is over!”"}
{"doc_id": "gutenberg_1342", "para_id": 8407, "text": "“I wish I could say anything to comfort you,” replied Elizabeth; “but it"}
{"doc_id": "gutenberg_1342", "para_id": 8408, "text": "is wholly out of my power. You must feel it; and the usual satisfaction"}
{"doc_id": "gutenberg_1342", "para_id": 8409, "text": "of preaching patience to a sufferer is denied me, because you have"}
{"doc_id": "gutenberg_1342", "para_id": 8410, "text": "Mr. Bingley arrived. Mrs. Bennet, through the assistance of servants,"}
{"doc_id": "gutenberg_1342", "para_id": 8411, "text": "contrived to have the earliest tidings of it, that the period of anxiety"}
{"doc_id": "gutenberg_1342", "para_id": 8412, "text": "and fretfulness on her side be as long as it could. She counted the days"}
{"doc_id": "gutenberg_1342", "para_id": 8413, "text": "that must intervene before their invitation could be sent--hopeless of"}
{"doc_id": "gutenberg_1342", "para_id": 8414, "text": "seeing him before. But on the third morning after his arrival in"}
{"doc_id": "gutenberg_1342", "para_id": 8415, "text": "Hertfordshire, she saw him from her dressing-room window enter the"}
{"doc_id": "gutenberg_1342", "para_id": 8416, "text": "Her daughters were eagerly called to partake of her joy. Jane resolutely"}
{"doc_id": "gutenberg_1342", "para_id": 8417, "text": "kept her place at the table; but Elizabeth, to satisfy her mother, went"}
{"doc_id": "gutenberg_1342", "para_id": 8418, "text": "to the window--she looked--she saw Mr. Darcy with him, and sat down"}
{"doc_id": "gutenberg_1342", "para_id": 8419, "text": "“There is a gentleman with him, mamma,” said Kitty; “who can it be?”"}
{"doc_id": "gutenberg_1342", "para_id": 8420, "text": "“Some acquaintance or other, my dear, I suppose; I am sure I do not"}
{"doc_id": "gutenberg_1342", "para_id": 8421, "text": "“La!” replied Kitty, “it looks just like that man that used to be with"}
{"doc_id": "gutenberg_1342", "para_id": 8422, "text": "him before. Mr. what’s his name--that tall, proud man.”"}
{"doc_id": "gutenberg_1342", "para_id": 8423, "text": "“Good gracious! Mr. Darcy!--and so it does, I vow. Well, any friend of"}
{"doc_id": "gutenberg_1342", "para_id": 8424, "text": "Mr. Bingley’s will always be welcome here, to be sure; but else I must"}
{"doc_id": "gutenberg_1342", "para_id": 8425, "text": "Jane looked at Elizabeth with surprise and concern. She knew but little"}
{"doc_id": "gutenberg_1342", "para_id": 8426, "text": "of their meeting in Derbyshire, and therefore felt for the awkwardness"}
{"doc_id": "gutenberg_1342", "para_id": 8427, "text": "which must attend her sister, in seeing him almost for the first time"}
{"doc_id": "gutenberg_1342", "para_id": 8428, "text": "after receiving his explanatory letter. Both sisters were uncomfortable"}
{"doc_id": "gutenberg_1342", "para_id": 8429, "text": "enough. Each felt for the other, and of course for themselves; and their"}
{"doc_id": "gutenberg_1342", "para_id": 8430, "text": "mother talked on of her dislike of Mr. Darcy, and her resolution to be"}
{"doc_id": "gutenberg_1342", "para_id": 8431, "text": "civil to him only as Mr. Bingley’s friend, without being heard by either"}
{"doc_id": "gutenberg_1342", "para_id": 8432, "text": "of them. But Elizabeth had sources of uneasiness which could not yet be"}
{"doc_id": "gutenberg_1342", "para_id": 8433, "text": "suspected by Jane, to whom she had never yet had courage to show Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 8434, "text": "Gardiner’s letter, or to relate her own change of sentiment towards"}
{"doc_id": "gutenberg_1342", "para_id": 8435, "text": "him. To Jane, he could be only a man whose proposals she had refused,"}
{"doc_id": "gutenberg_1342", "para_id": 8436, "text": "and whose merits she had undervalued; but to her own more extensive"}
{"doc_id": "gutenberg_1342", "para_id": 8437, "text": "information, he was the person to whom the whole family were indebted"}
{"doc_id": "gutenberg_1342", "para_id": 8438, "text": "for the first of benefits, and whom she regarded herself with an"}
{"doc_id": "gutenberg_1342", "para_id": 8439, "text": "interest, if not quite so tender, at least as reasonable and just, as"}
{"doc_id": "gutenberg_1342", "para_id": 8440, "text": "what Jane felt for Bingley. Her astonishment at his coming--at his"}
{"doc_id": "gutenberg_1342", "para_id": 8441, "text": "coming to Netherfield, to Longbourn, and voluntarily seeking her again,"}
{"doc_id": "gutenberg_1342", "para_id": 8442, "text": "was almost equal to what she had known on first witnessing his altered"}
{"doc_id": "gutenberg_1342", "para_id": 8443, "text": "The colour which had been driven from her face returned for half a"}
{"doc_id": "gutenberg_1342", "para_id": 8444, "text": "minute with an additional glow, and a smile of delight added lustre to"}
{"doc_id": "gutenberg_1342", "para_id": 8445, "text": "her eyes, as she thought for that space of time that his affection and"}
{"doc_id": "gutenberg_1342", "para_id": 8446, "text": "wishes must still be unshaken; but she would not be secure."}
{"doc_id": "gutenberg_1342", "para_id": 8447, "text": "“Let me first see how he behaves,” said she; “it will then be early"}
{"doc_id": "gutenberg_1342", "para_id": 8448, "text": "She sat intently at work, striving to be composed, and without daring to"}
{"doc_id": "gutenberg_1342", "para_id": 8449, "text": "lift up her eyes, till anxious curiosity carried them to the face of her"}
{"doc_id": "gutenberg_1342", "para_id": 8450, "text": "sister as the servant was approaching the door. Jane looked a little"}
{"doc_id": "gutenberg_1342", "para_id": 8451, "text": "paler than usual, but more sedate than Elizabeth had expected. On the"}
{"doc_id": "gutenberg_1342", "para_id": 8452, "text": "gentlemen’s appearing, her colour increased; yet she received them with"}
{"doc_id": "gutenberg_1342", "para_id": 8453, "text": "tolerable ease, and with a propriety of behaviour equally free from any"}
{"doc_id": "gutenberg_1342", "para_id": 8454, "text": "symptom of resentment, or any unnecessary complaisance."}
{"doc_id": "gutenberg_1342", "para_id": 8455, "text": "Elizabeth said as little to either as civility would allow, and sat down"}
{"doc_id": "gutenberg_1342", "para_id": 8456, "text": "again to her work, with an eagerness which it did not often command. She"}
{"doc_id": "gutenberg_1342", "para_id": 8457, "text": "had ventured only one glance at Darcy. He looked serious as usual; and,"}
{"doc_id": "gutenberg_1342", "para_id": 8458, "text": "she thought, more as he had been used to look in Hertfordshire, than as"}
{"doc_id": "gutenberg_1342", "para_id": 8459, "text": "she had seen him at Pemberley. But, perhaps, he could not in her"}
{"doc_id": "gutenberg_1342", "para_id": 8460, "text": "mother’s presence be what he was before her uncle and aunt. It was a"}
{"doc_id": "gutenberg_1342", "para_id": 8461, "text": "Bingley she had likewise seen for an instant, and in that short period"}
{"doc_id": "gutenberg_1342", "para_id": 8462, "text": "saw him looking both pleased and embarrassed. He was received by Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 8463, "text": "Bennet with a degree of civility which made her two daughters ashamed,"}
{"doc_id": "gutenberg_1342", "para_id": 8464, "text": "especially when contrasted with the cold and ceremonious politeness of"}
{"doc_id": "gutenberg_1342", "para_id": 8465, "text": "Elizabeth particularly, who knew that her mother owed to the latter the"}
{"doc_id": "gutenberg_1342", "para_id": 8466, "text": "preservation of her favourite daughter from irremediable infamy, was"}
{"doc_id": "gutenberg_1342", "para_id": 8467, "text": "hurt and distressed to a most painful degree by a distinction so ill"}
{"doc_id": "gutenberg_1342", "para_id": 8468, "text": "Darcy, after inquiring of her how Mr. and Mrs. Gardiner did--a question"}
{"doc_id": "gutenberg_1342", "para_id": 8469, "text": "which she could not answer without confusion--said scarcely anything. He"}
{"doc_id": "gutenberg_1342", "para_id": 8470, "text": "was not seated by her: perhaps that was the reason of his silence; but"}
{"doc_id": "gutenberg_1342", "para_id": 8471, "text": "it had not been so in Derbyshire. There he had talked to her friends"}
{"doc_id": "gutenberg_1342", "para_id": 8472, "text": "when he could not to herself. But now several minutes elapsed, without"}
{"doc_id": "gutenberg_1342", "para_id": 8473, "text": "bringing the sound of his voice; and when occasionally, unable to resist"}
{"doc_id": "gutenberg_1342", "para_id": 8474, "text": "the impulse of curiosity, she raised her eyes to his face, she as often"}
{"doc_id": "gutenberg_1342", "para_id": 8475, "text": "found him looking at Jane as at herself, and frequently on no object but"}
{"doc_id": "gutenberg_1342", "para_id": 8476, "text": "the ground. More thoughtfulness and less anxiety to please, than when"}
{"doc_id": "gutenberg_1342", "para_id": 8477, "text": "they last met, were plainly expressed. She was disappointed, and angry"}
{"doc_id": "gutenberg_1342", "para_id": 8478, "text": "“Could I expect it to be otherwise?” said she. “Yet why did he come?”"}
{"doc_id": "gutenberg_1342", "para_id": 8479, "text": "She was in no humour for conversation with anyone but himself; and to"}
{"doc_id": "gutenberg_1342", "para_id": 8480, "text": "She inquired after his sister, but could do no more."}
{"doc_id": "gutenberg_1342", "para_id": 8481, "text": "“It is a long time, Mr. Bingley, since you went away,” said Mrs. Bennet."}
{"doc_id": "gutenberg_1342", "para_id": 8482, "text": "“I began to be afraid you would never come back again. People _did_ say,"}
{"doc_id": "gutenberg_1342", "para_id": 8483, "text": "you meant to quit the place entirely at Michaelmas; but, however, I hope"}
{"doc_id": "gutenberg_1342", "para_id": 8484, "text": "it is not true. A great many changes have happened in the neighbourhood"}
{"doc_id": "gutenberg_1342", "para_id": 8485, "text": "since you went away. Miss Lucas is married and settled: and one of my"}
{"doc_id": "gutenberg_1342", "para_id": 8486, "text": "own daughters. I suppose you have heard of it; indeed, you must have"}
{"doc_id": "gutenberg_1342", "para_id": 8487, "text": "seen it in the papers. It was in the ‘Times’ and the ‘Courier,’ I know;"}
{"doc_id": "gutenberg_1342", "para_id": 8488, "text": "though it was not put in as it ought to be. It was only said, ‘Lately,"}
{"doc_id": "gutenberg_1342", "para_id": 8489, "text": "George Wickham, Esq., to Miss Lydia Bennet,’ without there being a"}
{"doc_id": "gutenberg_1342", "para_id": 8490, "text": "syllable said of her father, or the place where she lived, or anything."}
{"doc_id": "gutenberg_1342", "para_id": 8491, "text": "It was my brother Gardiner’s drawing up, too, and I wonder how he came"}
{"doc_id": "gutenberg_1342", "para_id": 8492, "text": "to make such an awkward business of it. Did you see it?”"}
{"doc_id": "gutenberg_1342", "para_id": 8493, "text": "Bingley replied that he did, and made his congratulations. Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 8494, "text": "dared not lift up her eyes. How Mr. Darcy looked, therefore, she could"}
{"doc_id": "gutenberg_1342", "para_id": 8495, "text": "“It is a delightful thing, to be sure, to have a daughter well married,”"}
{"doc_id": "gutenberg_1342", "para_id": 8496, "text": "continued her mother; “but at the same time, Mr. Bingley, it is very"}
{"doc_id": "gutenberg_1342", "para_id": 8497, "text": "hard to have her taken away from me. They are gone down to Newcastle, a"}
{"doc_id": "gutenberg_1342", "para_id": 8498, "text": "place quite northward it seems, and there they are to stay, I do not"}
{"doc_id": "gutenberg_1342", "para_id": 8499, "text": "know how long. His regiment is there; for I suppose you have heard of"}
{"doc_id": "gutenberg_1342", "para_id": 8500, "text": "his leaving the ----shire, and of his being gone into the Regulars."}
{"doc_id": "gutenberg_1342", "para_id": 8501, "text": "Thank heaven! he has _some_ friends, though, perhaps, not so many as he"}
{"doc_id": "gutenberg_1342", "para_id": 8502, "text": "Elizabeth, who knew this to be levelled at Mr. Darcy, was in such misery"}
{"doc_id": "gutenberg_1342", "para_id": 8503, "text": "of shame that she could hardly keep her seat. It drew from her, however,"}
{"doc_id": "gutenberg_1342", "para_id": 8504, "text": "the exertion of speaking, which nothing else had so effectually done"}
{"doc_id": "gutenberg_1342", "para_id": 8505, "text": "before; and she asked Bingley whether he meant to make any stay in the"}
{"doc_id": "gutenberg_1342", "para_id": 8506, "text": "“When you have killed all your own birds, Mr. Bingley,” said her mother,"}
{"doc_id": "gutenberg_1342", "para_id": 8507, "text": "“I beg you will come here and shoot as many as you please on Mr."}
{"doc_id": "gutenberg_1342", "para_id": 8508, "text": "Bennet’s manor. I am sure he will be vastly happy to oblige you, and"}
{"doc_id": "gutenberg_1342", "para_id": 8509, "text": "Elizabeth’s misery increased at such unnecessary, such officious"}
{"doc_id": "gutenberg_1342", "para_id": 8510, "text": "attention! Were the same fair prospect to arise at present, as had"}
{"doc_id": "gutenberg_1342", "para_id": 8511, "text": "flattered them a year ago, everything, she was persuaded, would be"}
{"doc_id": "gutenberg_1342", "para_id": 8512, "text": "hastening to the same vexatious conclusion. At that instant she felt,"}
{"doc_id": "gutenberg_1342", "para_id": 8513, "text": "that years of happiness could not make Jane or herself amends for"}
{"doc_id": "gutenberg_1342", "para_id": 8514, "text": "“The first wish of my heart,” said she to herself, “is never more to be"}
{"doc_id": "gutenberg_1342", "para_id": 8515, "text": "in company with either of them. Their society can afford no pleasure"}
{"doc_id": "gutenberg_1342", "para_id": 8516, "text": "that will atone for such wretchedness as this! Let me never see either"}
{"doc_id": "gutenberg_1342", "para_id": 8517, "text": "Yet the misery, for which years of happiness were to offer no"}
{"doc_id": "gutenberg_1342", "para_id": 8518, "text": "compensation, received soon afterwards material relief, from observing"}
{"doc_id": "gutenberg_1342", "para_id": 8519, "text": "how much the beauty of her sister rekindled the admiration of her former"}
{"doc_id": "gutenberg_1342", "para_id": 8520, "text": "lover. When first he came in, he had spoken to her but little, but every"}
{"doc_id": "gutenberg_1342", "para_id": 8521, "text": "five minutes seemed to be giving her more of his attention. He found her"}
{"doc_id": "gutenberg_1342", "para_id": 8522, "text": "as handsome as she had been last year; as good-natured, and as"}
{"doc_id": "gutenberg_1342", "para_id": 8523, "text": "unaffected, though not quite so chatty. Jane was anxious that no"}
{"doc_id": "gutenberg_1342", "para_id": 8524, "text": "difference should be perceived in her at all, and was really persuaded"}
{"doc_id": "gutenberg_1342", "para_id": 8525, "text": "that she talked as much as ever; but her mind was so busily engaged,"}
{"doc_id": "gutenberg_1342", "para_id": 8526, "text": "When the gentlemen rose to go away, Mrs. Bennet was mindful of her"}
{"doc_id": "gutenberg_1342", "para_id": 8527, "text": "intended civility, and they were invited and engaged to dine at"}
{"doc_id": "gutenberg_1342", "para_id": 8528, "text": "“You are quite a visit in my debt, Mr. Bingley,” she added; “for when"}
{"doc_id": "gutenberg_1342", "para_id": 8529, "text": "you went to town last winter, you promised to take a family dinner with"}
{"doc_id": "gutenberg_1342", "para_id": 8530, "text": "us as soon as you returned. I have not forgot, you see; and I assure you"}
{"doc_id": "gutenberg_1342", "para_id": 8531, "text": "I was very much disappointed that you did not come back and keep your"}
{"doc_id": "gutenberg_1342", "para_id": 8532, "text": "Bingley looked a little silly at this reflection, and said something of"}
{"doc_id": "gutenberg_1342", "para_id": 8533, "text": "his concern at having been prevented by business. They then went away."}
{"doc_id": "gutenberg_1342", "para_id": 8534, "text": "Mrs. Bennet had been strongly inclined to ask them to stay and dine"}
{"doc_id": "gutenberg_1342", "para_id": 8535, "text": "there that day; but, though she always kept a very good table, she did"}
{"doc_id": "gutenberg_1342", "para_id": 8536, "text": "not think anything less than two courses could be good enough for a man"}
{"doc_id": "gutenberg_1342", "para_id": 8537, "text": "on whom she had such anxious designs, or satisfy the appetite and pride"}
{"doc_id": "gutenberg_1342", "para_id": 8538, "text": "As soon as they were gone, Elizabeth walked out to recover her spirits;"}
{"doc_id": "gutenberg_1342", "para_id": 8539, "text": "or, in other words, to dwell without interruption on those subjects"}
{"doc_id": "gutenberg_1342", "para_id": 8540, "text": "which must deaden them more. Mr. Darcy’s behaviour astonished and vexed"}
{"doc_id": "gutenberg_1342", "para_id": 8541, "text": "“Why, if he came only to be silent, grave, and indifferent,” said she,"}
{"doc_id": "gutenberg_1342", "para_id": 8542, "text": "She could settle it in no way that gave her pleasure."}
{"doc_id": "gutenberg_1342", "para_id": 8543, "text": "“He could be still amiable, still pleasing to my uncle and aunt, when he"}
{"doc_id": "gutenberg_1342", "para_id": 8544, "text": "was in town; and why not to me? If he fears me, why come hither? If he"}
{"doc_id": "gutenberg_1342", "para_id": 8545, "text": "no longer cares for me, why silent? Teasing, teasing man! I will think"}
{"doc_id": "gutenberg_1342", "para_id": 8546, "text": "Her resolution was for a short time involuntarily kept by the approach"}
{"doc_id": "gutenberg_1342", "para_id": 8547, "text": "of her sister, who joined her with a cheerful look which showed her"}
{"doc_id": "gutenberg_1342", "para_id": 8548, "text": "better satisfied with their visitors than Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 8549, "text": "“Now,” said she, “that this first meeting is over, I feel perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 8550, "text": "easy. I know my own strength, and I shall never be embarrassed again by"}
{"doc_id": "gutenberg_1342", "para_id": 8551, "text": "his coming. I am glad he dines here on Tuesday. It will then be publicly"}
{"doc_id": "gutenberg_1342", "para_id": 8552, "text": "seen, that on both sides we meet only as common and indifferent"}
{"doc_id": "gutenberg_1342", "para_id": 8553, "text": "“Yes, very indifferent, indeed,” said Elizabeth, laughingly. “Oh, Jane!"}
{"doc_id": "gutenberg_1342", "para_id": 8554, "text": "“My dear Lizzy, you cannot think me so weak as to be in danger now.”"}
{"doc_id": "gutenberg_1342", "para_id": 8555, "text": "“I think you are in very great danger of making him as much in love with"}
{"doc_id": "gutenberg_1342", "para_id": 8556, "text": "They did not see the gentlemen again till Tuesday; and Mrs. Bennet, in"}
{"doc_id": "gutenberg_1342", "para_id": 8557, "text": "the meanwhile, was giving way to all the happy schemes which the"}
{"doc_id": "gutenberg_1342", "para_id": 8558, "text": "good-humour and common politeness of Bingley, in half an hour’s visit,"}
{"doc_id": "gutenberg_1342", "para_id": 8559, "text": "On Tuesday there was a large party assembled at Longbourn; and the two"}
{"doc_id": "gutenberg_1342", "para_id": 8560, "text": "who were most anxiously expected, to the credit of their punctuality as"}
{"doc_id": "gutenberg_1342", "para_id": 8561, "text": "sportsmen, were in very good time. When they repaired to the"}
{"doc_id": "gutenberg_1342", "para_id": 8562, "text": "dining-room, Elizabeth eagerly watched to see whether Bingley would take"}
{"doc_id": "gutenberg_1342", "para_id": 8563, "text": "the place which, in all their former parties, had belonged to him, by"}
{"doc_id": "gutenberg_1342", "para_id": 8564, "text": "her sister. Her prudent mother, occupied by the same ideas, forbore to"}
{"doc_id": "gutenberg_1342", "para_id": 8565, "text": "invite him to sit by herself. On entering the room, he seemed to"}
{"doc_id": "gutenberg_1342", "para_id": 8566, "text": "hesitate; but Jane happened to look round, and happened to smile: it was"}
{"doc_id": "gutenberg_1342", "para_id": 8567, "text": "Elizabeth, with a triumphant sensation, looked towards his friend. He"}
{"doc_id": "gutenberg_1342", "para_id": 8568, "text": "bore it with noble indifference; and she would have imagined that"}
{"doc_id": "gutenberg_1342", "para_id": 8569, "text": "Bingley had received his sanction to be happy, had she not seen his eyes"}
{"doc_id": "gutenberg_1342", "para_id": 8570, "text": "likewise turned towards Mr. Darcy, with an expression of half-laughing"}
{"doc_id": "gutenberg_1342", "para_id": 8571, "text": "His behaviour to her sister was such during dinnertime as showed an"}
{"doc_id": "gutenberg_1342", "para_id": 8572, "text": "admiration of her, which, though more guarded than formerly, persuaded"}
{"doc_id": "gutenberg_1342", "para_id": 8573, "text": "Elizabeth, that, if left wholly to himself, Jane’s happiness, and his"}
{"doc_id": "gutenberg_1342", "para_id": 8574, "text": "own, would be speedily secured. Though she dared not depend upon the"}
{"doc_id": "gutenberg_1342", "para_id": 8575, "text": "consequence, she yet received pleasure from observing his behaviour. It"}
{"doc_id": "gutenberg_1342", "para_id": 8576, "text": "gave her all the animation that her spirits could boast; for she was in"}
{"doc_id": "gutenberg_1342", "para_id": 8577, "text": "no cheerful humour. Mr. Darcy was almost as far from her as the table"}
{"doc_id": "gutenberg_1342", "para_id": 8578, "text": "could divide them. He was on one side of her mother. She knew how little"}
{"doc_id": "gutenberg_1342", "para_id": 8579, "text": "such a situation would give pleasure to either, or make either appear to"}
{"doc_id": "gutenberg_1342", "para_id": 8580, "text": "advantage. She was not near enough to hear any of their discourse; but"}
{"doc_id": "gutenberg_1342", "para_id": 8581, "text": "she could see how seldom they spoke to each other, and how formal and"}
{"doc_id": "gutenberg_1342", "para_id": 8582, "text": "cold was their manner whenever they did. Her mother’s ungraciousness"}
{"doc_id": "gutenberg_1342", "para_id": 8583, "text": "made the sense of what they owed him more painful to Elizabeth’s mind;"}
{"doc_id": "gutenberg_1342", "para_id": 8584, "text": "and she would, at times, have given anything to be privileged to tell"}
{"doc_id": "gutenberg_1342", "para_id": 8585, "text": "him, that his kindness was neither unknown nor unfelt by the whole of"}
{"doc_id": "gutenberg_1342", "para_id": 8586, "text": "She was in hopes that the evening would afford some opportunity of"}
{"doc_id": "gutenberg_1342", "para_id": 8587, "text": "bringing them together; that the whole of the visit would not pass away"}
{"doc_id": "gutenberg_1342", "para_id": 8588, "text": "without enabling them to enter into something more of conversation,"}
{"doc_id": "gutenberg_1342", "para_id": 8589, "text": "than the mere ceremonious salutation attending his entrance. Anxious and"}
{"doc_id": "gutenberg_1342", "para_id": 8590, "text": "uneasy, the period which passed in the drawing-room before the gentlemen"}
{"doc_id": "gutenberg_1342", "para_id": 8591, "text": "came, was wearisome and dull to a degree that almost made her uncivil."}
{"doc_id": "gutenberg_1342", "para_id": 8592, "text": "She looked forward to their entrance as the point on which all her"}
{"doc_id": "gutenberg_1342", "para_id": 8593, "text": "“If he does not come to me, _then_,” said she, “I shall give him up for"}
{"doc_id": "gutenberg_1342", "para_id": 8594, "text": "The gentlemen came; and she thought he looked as if he would have"}
{"doc_id": "gutenberg_1342", "para_id": 8595, "text": "answered her hopes; but, alas! the ladies had crowded round the table,"}
{"doc_id": "gutenberg_1342", "para_id": 8596, "text": "where Miss Bennet was making tea, and Elizabeth pouring out the coffee,"}
{"doc_id": "gutenberg_1342", "para_id": 8597, "text": "in so close a confederacy, that there was not a single vacancy near her"}
{"doc_id": "gutenberg_1342", "para_id": 8598, "text": "which would admit of a chair. And on the gentlemen’s approaching, one of"}
{"doc_id": "gutenberg_1342", "para_id": 8599, "text": "the girls moved closer to her than ever, and said, in a whisper,--"}
{"doc_id": "gutenberg_1342", "para_id": 8600, "text": "“The men shan’t come and part us, I am determined. We want none of them;"}
{"doc_id": "gutenberg_1342", "para_id": 8601, "text": "Darcy had walked away to another part of the room. She followed him with"}
{"doc_id": "gutenberg_1342", "para_id": 8602, "text": "her eyes, envied everyone to whom he spoke, had scarcely patience enough"}
{"doc_id": "gutenberg_1342", "para_id": 8603, "text": "to help anybody to coffee, and then was enraged against herself for"}
{"doc_id": "gutenberg_1342", "para_id": 8604, "text": "“A man who has once been refused! How could I ever be foolish enough to"}
{"doc_id": "gutenberg_1342", "para_id": 8605, "text": "expect a renewal of his love? Is there one among the sex who would not"}
{"doc_id": "gutenberg_1342", "para_id": 8606, "text": "protest against such a weakness as a second proposal to the same woman?"}
{"doc_id": "gutenberg_1342", "para_id": 8607, "text": "There is no indignity so abhorrent to their feelings.”"}
{"doc_id": "gutenberg_1342", "para_id": 8608, "text": "She was a little revived, however, by his bringing back his coffee-cup"}
{"doc_id": "gutenberg_1342", "para_id": 8609, "text": "himself; and she seized the opportunity of saying,--"}
{"doc_id": "gutenberg_1342", "para_id": 8610, "text": "“Mrs. Annesley is with her. The others have been gone on to Scarborough"}
{"doc_id": "gutenberg_1342", "para_id": 8611, "text": "She could think of nothing more to say; but if he wished to converse"}
{"doc_id": "gutenberg_1342", "para_id": 8612, "text": "with her, he might have better success. He stood by her, however, for"}
{"doc_id": "gutenberg_1342", "para_id": 8613, "text": "some minutes, in silence; and, at last, on the young lady’s whispering"}
{"doc_id": "gutenberg_1342", "para_id": 8614, "text": "When the tea things were removed, and the card tables placed, the ladies"}
{"doc_id": "gutenberg_1342", "para_id": 8615, "text": "all rose; and Elizabeth was then hoping to be soon joined by him, when"}
{"doc_id": "gutenberg_1342", "para_id": 8616, "text": "all her views were overthrown, by seeing him fall a victim to her"}
{"doc_id": "gutenberg_1342", "para_id": 8617, "text": "mother’s rapacity for whist players, and in a few moments after seated"}
{"doc_id": "gutenberg_1342", "para_id": 8618, "text": "with the rest of the party. She now lost every expectation of pleasure."}
{"doc_id": "gutenberg_1342", "para_id": 8619, "text": "They were confined for the evening at different tables; and she had"}
{"doc_id": "gutenberg_1342", "para_id": 8620, "text": "nothing to hope, but that his eyes were so often turned towards her side"}
{"doc_id": "gutenberg_1342", "para_id": 8621, "text": "of the room, as to make him play as unsuccessfully as herself."}
{"doc_id": "gutenberg_1342", "para_id": 8622, "text": "Mrs. Bennet had designed to keep the two Netherfield gentlemen to"}
{"doc_id": "gutenberg_1342", "para_id": 8623, "text": "supper; but their carriage was, unluckily, ordered before any of the"}
{"doc_id": "gutenberg_1342", "para_id": 8624, "text": "others, and she had no opportunity of detaining them."}
{"doc_id": "gutenberg_1342", "para_id": 8625, "text": "“Well, girls,” said she, as soon as they were left to themselves, “what"}
{"doc_id": "gutenberg_1342", "para_id": 8626, "text": "say you to the day? I think everything has passed off uncommonly well, I"}
{"doc_id": "gutenberg_1342", "para_id": 8627, "text": "assure you. The dinner was as well dressed as any I ever saw. The"}
{"doc_id": "gutenberg_1342", "para_id": 8628, "text": "venison was roasted to a turn--and everybody said, they never saw so fat"}
{"doc_id": "gutenberg_1342", "para_id": 8629, "text": "a haunch. The soup was fifty times better than what we had at the"}
{"doc_id": "gutenberg_1342", "para_id": 8630, "text": "Lucases’ last week; and even Mr. Darcy acknowledged that the partridges"}
{"doc_id": "gutenberg_1342", "para_id": 8631, "text": "were remarkably well done; and I suppose he has two or three French"}
{"doc_id": "gutenberg_1342", "para_id": 8632, "text": "cooks at least. And, my dear Jane, I never saw you look in greater"}
{"doc_id": "gutenberg_1342", "para_id": 8633, "text": "beauty. Mrs. Long said so too, for I asked her whether you did not. And"}
{"doc_id": "gutenberg_1342", "para_id": 8634, "text": "what do you think she said besides? ‘Ah! Mrs. Bennet, we shall have her"}
{"doc_id": "gutenberg_1342", "para_id": 8635, "text": "at Netherfield at last!’ She did, indeed. I do think Mrs. Long is as"}
{"doc_id": "gutenberg_1342", "para_id": 8636, "text": "good a creature as ever lived--and her nieces are very pretty behaved"}
{"doc_id": "gutenberg_1342", "para_id": 8637, "text": "girls, and not at all handsome: I like them prodigiously.”"}
{"doc_id": "gutenberg_1342", "para_id": 8638, "text": "Mrs. Bennet, in short, was in very great spirits: she had seen enough of"}
{"doc_id": "gutenberg_1342", "para_id": 8639, "text": "Bingley’s behaviour to Jane to be convinced that she would get him at"}
{"doc_id": "gutenberg_1342", "para_id": 8640, "text": "last; and her expectations of advantage to her family, when in a happy"}
{"doc_id": "gutenberg_1342", "para_id": 8641, "text": "humour, were so far beyond reason, that she was quite disappointed at"}
{"doc_id": "gutenberg_1342", "para_id": 8642, "text": "not seeing him there again the next day, to make his proposals."}
{"doc_id": "gutenberg_1342", "para_id": 8643, "text": "“It has been a very agreeable day,” said Miss Bennet to Elizabeth. “The"}
{"doc_id": "gutenberg_1342", "para_id": 8644, "text": "party seemed so well selected, so suitable one with the other. I hope we"}
{"doc_id": "gutenberg_1342", "para_id": 8645, "text": "“Lizzy, you must not do so. You must not suspect me. It mortifies me. I"}
{"doc_id": "gutenberg_1342", "para_id": 8646, "text": "assure you that I have now learnt to enjoy his conversation as an"}
{"doc_id": "gutenberg_1342", "para_id": 8647, "text": "agreeable and sensible young man without having a wish beyond it. I am"}
{"doc_id": "gutenberg_1342", "para_id": 8648, "text": "perfectly satisfied, from what his manners now are, that he never had"}
{"doc_id": "gutenberg_1342", "para_id": 8649, "text": "any design of engaging my affection. It is only that he is blessed with"}
{"doc_id": "gutenberg_1342", "para_id": 8650, "text": "greater sweetness of address, and a stronger desire of generally"}
{"doc_id": "gutenberg_1342", "para_id": 8651, "text": "“You are very cruel,” said her sister, “you will not let me smile, and"}
{"doc_id": "gutenberg_1342", "para_id": 8652, "text": "“How hard it is in some cases to be believed! And how impossible in"}
{"doc_id": "gutenberg_1342", "para_id": 8653, "text": "others! But why should you wish to persuade me that I feel more than I"}
{"doc_id": "gutenberg_1342", "para_id": 8654, "text": "“That is a question which I hardly know how to answer. We all love to"}
{"doc_id": "gutenberg_1342", "para_id": 8655, "text": "instruct, though we can teach only what is not worth knowing. Forgive"}
{"doc_id": "gutenberg_1342", "para_id": 8656, "text": "me; and if you persist in indifference, do not make _me_ your"}
{"doc_id": "gutenberg_1342", "para_id": 8657, "text": "A few days after this visit, Mr. Bingley called again, and alone. His"}
{"doc_id": "gutenberg_1342", "para_id": 8658, "text": "friend had left him that morning for London, but was to return home in"}
{"doc_id": "gutenberg_1342", "para_id": 8659, "text": "ten days’ time. He sat with them above an hour, and was in remarkably"}
{"doc_id": "gutenberg_1342", "para_id": 8660, "text": "good spirits. Mrs. Bennet invited him to dine with them; but, with many"}
{"doc_id": "gutenberg_1342", "para_id": 8661, "text": "expressions of concern, he confessed himself engaged elsewhere."}
{"doc_id": "gutenberg_1342", "para_id": 8662, "text": "“Next time you call,” said she, “I hope we shall be more lucky.”"}
{"doc_id": "gutenberg_1342", "para_id": 8663, "text": "He should be particularly happy at any time, etc., etc.; and if she"}
{"doc_id": "gutenberg_1342", "para_id": 8664, "text": "would give him leave, would take an early opportunity of waiting on"}
{"doc_id": "gutenberg_1342", "para_id": 8665, "text": "Yes, he had no engagement at all for to-morrow; and her invitation was"}
{"doc_id": "gutenberg_1342", "para_id": 8666, "text": "He came, and in such very good time, that the ladies were none of them"}
{"doc_id": "gutenberg_1342", "para_id": 8667, "text": "dressed. In ran Mrs. Bennet to her daughters’ room, in her"}
{"doc_id": "gutenberg_1342", "para_id": 8668, "text": "dressing-gown, and with her hair half finished, crying out,--"}
{"doc_id": "gutenberg_1342", "para_id": 8669, "text": "“My dear Jane, make haste and hurry down. He is come--Mr. Bingley is"}
{"doc_id": "gutenberg_1342", "para_id": 8670, "text": "come. He is, indeed. Make haste, make haste. Here, Sarah, come to Miss"}
{"doc_id": "gutenberg_1342", "para_id": 8671, "text": "Bennet this moment, and help her on with her gown. Never mind Miss"}
{"doc_id": "gutenberg_1342", "para_id": 8672, "text": "“We will be down as soon as we can,” said Jane; “but I dare say Kitty is"}
{"doc_id": "gutenberg_1342", "para_id": 8673, "text": "forwarder than either of us, for she went upstairs half an hour ago.”"}
{"doc_id": "gutenberg_1342", "para_id": 8674, "text": "“Oh! hang Kitty! what has she to do with it? Come, be quick, be quick!"}
{"doc_id": "gutenberg_1342", "para_id": 8675, "text": "But when her mother was gone, Jane would not be prevailed on to go down"}
{"doc_id": "gutenberg_1342", "para_id": 8676, "text": "The same anxiety to get them by themselves was visible again in the"}
{"doc_id": "gutenberg_1342", "para_id": 8677, "text": "evening. After tea, Mr. Bennet retired to the library, as was his"}
{"doc_id": "gutenberg_1342", "para_id": 8678, "text": "custom, and Mary went upstairs to her instrument. Two obstacles of the"}
{"doc_id": "gutenberg_1342", "para_id": 8679, "text": "five being thus removed, Mrs. Bennet sat looking and winking at"}
{"doc_id": "gutenberg_1342", "para_id": 8680, "text": "Elizabeth and Catherine for a considerable time, without making any"}
{"doc_id": "gutenberg_1342", "para_id": 8681, "text": "impression on them. Elizabeth would not observe her; and when at last"}
{"doc_id": "gutenberg_1342", "para_id": 8682, "text": "Kitty did, she very innocently said, “What is the matter, mamma? What do"}
{"doc_id": "gutenberg_1342", "para_id": 8683, "text": "“Nothing, child, nothing. I did not wink at you.” She then sat still"}
{"doc_id": "gutenberg_1342", "para_id": 8684, "text": "five minutes longer; but unable to waste such a precious occasion, she"}
{"doc_id": "gutenberg_1342", "para_id": 8685, "text": "“Come here, my love, I want to speak to you,” took her out of the room."}
{"doc_id": "gutenberg_1342", "para_id": 8686, "text": "Jane instantly gave a look at Elizabeth which spoke her distress at such"}
{"doc_id": "gutenberg_1342", "para_id": 8687, "text": "premeditation, and her entreaty that _she_ would not give in to it. In a"}
{"doc_id": "gutenberg_1342", "para_id": 8688, "text": "few minutes, Mrs. Bennet half opened the door and called out,--"}
{"doc_id": "gutenberg_1342", "para_id": 8689, "text": "“We may as well leave them by themselves, you know,” said her mother as"}
{"doc_id": "gutenberg_1342", "para_id": 8690, "text": "soon as she was in the hall. “Kitty and I are going upstairs to sit in"}
{"doc_id": "gutenberg_1342", "para_id": 8691, "text": "Elizabeth made no attempt to reason with her mother, but remained"}
{"doc_id": "gutenberg_1342", "para_id": 8692, "text": "quietly in the hall till she and Kitty were out of sight, then returned"}
{"doc_id": "gutenberg_1342", "para_id": 8693, "text": "Mrs. Bennet’s schemes for this day were ineffectual. Bingley was"}
{"doc_id": "gutenberg_1342", "para_id": 8694, "text": "everything that was charming, except the professed lover of her"}
{"doc_id": "gutenberg_1342", "para_id": 8695, "text": "daughter. His ease and cheerfulness rendered him a most agreeable"}
{"doc_id": "gutenberg_1342", "para_id": 8696, "text": "addition to their evening party; and he bore with the ill-judged"}
{"doc_id": "gutenberg_1342", "para_id": 8697, "text": "officiousness of the mother, and heard all her silly remarks with a"}
{"doc_id": "gutenberg_1342", "para_id": 8698, "text": "forbearance and command of countenance particularly grateful to the"}
{"doc_id": "gutenberg_1342", "para_id": 8699, "text": "He scarcely needed an invitation to stay supper; and before he went away"}
{"doc_id": "gutenberg_1342", "para_id": 8700, "text": "an engagement was formed, chiefly through his own and Mrs. Bennet’s"}
{"doc_id": "gutenberg_1342", "para_id": 8701, "text": "means, for his coming next morning to shoot with her husband."}
{"doc_id": "gutenberg_1342", "para_id": 8702, "text": "After this day, Jane said no more of her indifference. Not a word passed"}
{"doc_id": "gutenberg_1342", "para_id": 8703, "text": "between the sisters concerning Bingley; but Elizabeth went to bed in the"}
{"doc_id": "gutenberg_1342", "para_id": 8704, "text": "happy belief that all must speedily be concluded, unless Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 8705, "text": "returned within the stated time. Seriously, however, she felt tolerably"}
{"doc_id": "gutenberg_1342", "para_id": 8706, "text": "persuaded that all this must have taken place with that gentleman’s"}
{"doc_id": "gutenberg_1342", "para_id": 8707, "text": "Bingley was punctual to his appointment; and he and Mr. Bennet spent the"}
{"doc_id": "gutenberg_1342", "para_id": 8708, "text": "morning together, as had been agreed on. The latter was much more"}
{"doc_id": "gutenberg_1342", "para_id": 8709, "text": "agreeable than his companion expected. There was nothing of presumption"}
{"doc_id": "gutenberg_1342", "para_id": 8710, "text": "or folly in Bingley that could provoke his ridicule, or disgust him into"}
{"doc_id": "gutenberg_1342", "para_id": 8711, "text": "silence; and he was more communicative, and less eccentric, than the"}
{"doc_id": "gutenberg_1342", "para_id": 8712, "text": "other had ever seen him. Bingley of course returned with him to dinner;"}
{"doc_id": "gutenberg_1342", "para_id": 8713, "text": "and in the evening Mrs. Bennet’s invention was again at work to get"}
{"doc_id": "gutenberg_1342", "para_id": 8714, "text": "everybody away from him and her daughter. Elizabeth, who had a letter to"}
{"doc_id": "gutenberg_1342", "para_id": 8715, "text": "write, went into the breakfast-room for that purpose soon after tea; for"}
{"doc_id": "gutenberg_1342", "para_id": 8716, "text": "as the others were all going to sit down to cards, she could not be"}
{"doc_id": "gutenberg_1342", "para_id": 8717, "text": "But on her returning to the drawing-room, when her letter was finished,"}
{"doc_id": "gutenberg_1342", "para_id": 8718, "text": "she saw, to her infinite surprise, there was reason to fear that her"}
{"doc_id": "gutenberg_1342", "para_id": 8719, "text": "mother had been too ingenious for her. On opening the door, she"}
{"doc_id": "gutenberg_1342", "para_id": 8720, "text": "perceived her sister and Bingley standing together over the hearth, as"}
{"doc_id": "gutenberg_1342", "para_id": 8721, "text": "if engaged in earnest conversation; and had this led to no suspicion,"}
{"doc_id": "gutenberg_1342", "para_id": 8722, "text": "the faces of both, as they hastily turned round and moved away from each"}
{"doc_id": "gutenberg_1342", "para_id": 8723, "text": "other, would have told it all. _Their_ situation was awkward enough; but"}
{"doc_id": "gutenberg_1342", "para_id": 8724, "text": "_hers_ she thought was still worse. Not a syllable was uttered by"}
{"doc_id": "gutenberg_1342", "para_id": 8725, "text": "either; and Elizabeth was on the point of going away again, when"}
{"doc_id": "gutenberg_1342", "para_id": 8726, "text": "Bingley, who as well as the other had sat down, suddenly rose, and,"}
{"doc_id": "gutenberg_1342", "para_id": 8727, "text": "whispering a few words to her sister, ran out of the room."}
{"doc_id": "gutenberg_1342", "para_id": 8728, "text": "Jane could have no reserves from Elizabeth, where confidence would give"}
{"doc_id": "gutenberg_1342", "para_id": 8729, "text": "pleasure; and, instantly embracing her, acknowledged, with the liveliest"}
{"doc_id": "gutenberg_1342", "para_id": 8730, "text": "emotion, that she was the happiest creature in the world."}
{"doc_id": "gutenberg_1342", "para_id": 8731, "text": "“’Tis too much!” she added, “by far too much. I do not deserve it. Oh,"}
{"doc_id": "gutenberg_1342", "para_id": 8732, "text": "Elizabeth’s congratulations were given with a sincerity, a warmth, a"}
{"doc_id": "gutenberg_1342", "para_id": 8733, "text": "delight, which words could but poorly express. Every sentence of"}
{"doc_id": "gutenberg_1342", "para_id": 8734, "text": "kindness was a fresh source of happiness to Jane. But she would not"}
{"doc_id": "gutenberg_1342", "para_id": 8735, "text": "allow herself to stay with her sister, or say half that remained to be"}
{"doc_id": "gutenberg_1342", "para_id": 8736, "text": "“I must go instantly to my mother,” she cried. “I would not on any"}
{"doc_id": "gutenberg_1342", "para_id": 8737, "text": "account trifle with her affectionate solicitude, or allow her to hear it"}
{"doc_id": "gutenberg_1342", "para_id": 8738, "text": "from anyone but myself. He is gone to my father already. Oh, Lizzy, to"}
{"doc_id": "gutenberg_1342", "para_id": 8739, "text": "know that what I have to relate will give such pleasure to all my dear"}
{"doc_id": "gutenberg_1342", "para_id": 8740, "text": "She then hastened away to her mother, who had purposely broken up the"}
{"doc_id": "gutenberg_1342", "para_id": 8741, "text": "Elizabeth, who was left by herself, now smiled at the rapidity and ease"}
{"doc_id": "gutenberg_1342", "para_id": 8742, "text": "with which an affair was finally settled, that had given them so many"}
{"doc_id": "gutenberg_1342", "para_id": 8743, "text": "“And this,” said she, “is the end of all his friend’s anxious"}
{"doc_id": "gutenberg_1342", "para_id": 8744, "text": "circumspection! of all his sister’s falsehood and contrivance! the"}
{"doc_id": "gutenberg_1342", "para_id": 8745, "text": "In a few minutes she was joined by Bingley, whose conference with her"}
{"doc_id": "gutenberg_1342", "para_id": 8746, "text": "“Where is your sister?” said he hastily, as he opened the door."}
{"doc_id": "gutenberg_1342", "para_id": 8747, "text": "“With my mother upstairs. She will be down in a moment, I dare say.”"}
{"doc_id": "gutenberg_1342", "para_id": 8748, "text": "He then shut the door, and, coming up to her, claimed the good wishes"}
{"doc_id": "gutenberg_1342", "para_id": 8749, "text": "and affection of a sister. Elizabeth honestly and heartily expressed her"}
{"doc_id": "gutenberg_1342", "para_id": 8750, "text": "delight in the prospect of their relationship. They shook hands with"}
{"doc_id": "gutenberg_1342", "para_id": 8751, "text": "great cordiality; and then, till her sister came down, she had to listen"}
{"doc_id": "gutenberg_1342", "para_id": 8752, "text": "to all he had to say of his own happiness, and of Jane’s perfections;"}
{"doc_id": "gutenberg_1342", "para_id": 8753, "text": "and in spite of his being a lover, Elizabeth really believed all his"}
{"doc_id": "gutenberg_1342", "para_id": 8754, "text": "expectations of felicity to be rationally founded, because they had for"}
{"doc_id": "gutenberg_1342", "para_id": 8755, "text": "basis the excellent understanding and super-excellent disposition of"}
{"doc_id": "gutenberg_1342", "para_id": 8756, "text": "Jane, and a general similarity of feeling and taste between her and"}
{"doc_id": "gutenberg_1342", "para_id": 8757, "text": "It was an evening of no common delight to them all; the satisfaction of"}
{"doc_id": "gutenberg_1342", "para_id": 8758, "text": "Miss Bennet’s mind gave such a glow of sweet animation to her face, as"}
{"doc_id": "gutenberg_1342", "para_id": 8759, "text": "made her look handsomer than ever. Kitty simpered and smiled, and hoped"}
{"doc_id": "gutenberg_1342", "para_id": 8760, "text": "her turn was coming soon. Mrs. Bennet could not give her consent, or"}
{"doc_id": "gutenberg_1342", "para_id": 8761, "text": "speak her approbation in terms warm enough to satisfy her feelings,"}
{"doc_id": "gutenberg_1342", "para_id": 8762, "text": "though she talked to Bingley of nothing else, for half an hour; and when"}
{"doc_id": "gutenberg_1342", "para_id": 8763, "text": "Mr. Bennet joined them at supper, his voice and manner plainly showed"}
{"doc_id": "gutenberg_1342", "para_id": 8764, "text": "Not a word, however, passed his lips in allusion to it, till their"}
{"doc_id": "gutenberg_1342", "para_id": 8765, "text": "visitor took his leave for the night; but as soon as he was gone, he"}
{"doc_id": "gutenberg_1342", "para_id": 8766, "text": "“Jane, I congratulate you. You will be a very happy woman.”"}
{"doc_id": "gutenberg_1342", "para_id": 8767, "text": "Jane went to him instantly, kissed him, and thanked him for his"}
{"doc_id": "gutenberg_1342", "para_id": 8768, "text": "“You are a good girl,” he replied, “and I have great pleasure in"}
{"doc_id": "gutenberg_1342", "para_id": 8769, "text": "thinking you will be so happily settled. I have not a doubt of your"}
{"doc_id": "gutenberg_1342", "para_id": 8770, "text": "doing very well together. Your tempers are by no means unlike. You are"}
{"doc_id": "gutenberg_1342", "para_id": 8771, "text": "each of you so complying, that nothing will ever be resolved on; so"}
{"doc_id": "gutenberg_1342", "para_id": 8772, "text": "easy, that every servant will cheat you; and so generous, that you will"}
{"doc_id": "gutenberg_1342", "para_id": 8773, "text": "“I hope not so. Imprudence or thoughtlessness in money matters would be"}
{"doc_id": "gutenberg_1342", "para_id": 8774, "text": "“Exceed their income! My dear Mr. Bennet,” cried his wife, “what are you"}
{"doc_id": "gutenberg_1342", "para_id": 8775, "text": "talking of? Why, he has four or five thousand a year, and very likely"}
{"doc_id": "gutenberg_1342", "para_id": 8776, "text": "more.” Then addressing her daughter, “Oh, my dear, dear Jane, I am so"}
{"doc_id": "gutenberg_1342", "para_id": 8777, "text": "happy! I am sure I shan’t get a wink of sleep all night. I knew how it"}
{"doc_id": "gutenberg_1342", "para_id": 8778, "text": "would be. I always said it must be so, at last. I was sure you could not"}
{"doc_id": "gutenberg_1342", "para_id": 8779, "text": "be so beautiful for nothing! I remember, as soon as ever I saw him, when"}
{"doc_id": "gutenberg_1342", "para_id": 8780, "text": "he first came into Hertfordshire last year, I thought how likely it was"}
{"doc_id": "gutenberg_1342", "para_id": 8781, "text": "that you should come together. Oh, he is the handsomest young man that"}
{"doc_id": "gutenberg_1342", "para_id": 8782, "text": "Wickham, Lydia, were all forgotten. Jane was beyond competition her"}
{"doc_id": "gutenberg_1342", "para_id": 8783, "text": "favourite child. At that moment she cared for no other. Her younger"}
{"doc_id": "gutenberg_1342", "para_id": 8784, "text": "sisters soon began to make interest with her for objects of happiness"}
{"doc_id": "gutenberg_1342", "para_id": 8785, "text": "Mary petitioned for the use of the library at Netherfield; and Kitty"}
{"doc_id": "gutenberg_1342", "para_id": 8786, "text": "begged very hard for a few balls there every winter."}
{"doc_id": "gutenberg_1342", "para_id": 8787, "text": "Bingley, from this time, was of course a daily visitor at Longbourn;"}
{"doc_id": "gutenberg_1342", "para_id": 8788, "text": "coming frequently before breakfast, and always remaining till after"}
{"doc_id": "gutenberg_1342", "para_id": 8789, "text": "supper; unless when some barbarous neighbour, who could not be enough"}
{"doc_id": "gutenberg_1342", "para_id": 8790, "text": "detested, had given him an invitation to dinner, which he thought"}
{"doc_id": "gutenberg_1342", "para_id": 8791, "text": "Elizabeth had now but little time for conversation with her sister; for"}
{"doc_id": "gutenberg_1342", "para_id": 8792, "text": "while he was present Jane had no attention to bestow on anyone else: but"}
{"doc_id": "gutenberg_1342", "para_id": 8793, "text": "she found herself considerably useful to both of them, in those hours of"}
{"doc_id": "gutenberg_1342", "para_id": 8794, "text": "separation that must sometimes occur. In the absence of Jane, he always"}
{"doc_id": "gutenberg_1342", "para_id": 8795, "text": "attached himself to Elizabeth for the pleasure of talking of her; and"}
{"doc_id": "gutenberg_1342", "para_id": 8796, "text": "when Bingley was gone, Jane constantly sought the same means of relief."}
{"doc_id": "gutenberg_1342", "para_id": 8797, "text": "“He has made me so happy,” said she, one evening, “by telling me that he"}
{"doc_id": "gutenberg_1342", "para_id": 8798, "text": "was totally ignorant of my being in town last spring! I had not believed"}
{"doc_id": "gutenberg_1342", "para_id": 8799, "text": "“I suspected as much,” replied Elizabeth. “But how did he account for"}
{"doc_id": "gutenberg_1342", "para_id": 8800, "text": "“It must have been his sisters’ doing. They were certainly no friends to"}
{"doc_id": "gutenberg_1342", "para_id": 8801, "text": "his acquaintance with me, which I cannot wonder at, since he might have"}
{"doc_id": "gutenberg_1342", "para_id": 8802, "text": "chosen so much more advantageously in many respects. But when they see,"}
{"doc_id": "gutenberg_1342", "para_id": 8803, "text": "as I trust they will, that their brother is happy with me, they will"}
{"doc_id": "gutenberg_1342", "para_id": 8804, "text": "learn to be contented, and we shall be on good terms again: though we"}
{"doc_id": "gutenberg_1342", "para_id": 8805, "text": "“That is the most unforgiving speech,” said Elizabeth, “that I ever"}
{"doc_id": "gutenberg_1342", "para_id": 8806, "text": "heard you utter. Good girl! It would vex me, indeed, to see you again"}
{"doc_id": "gutenberg_1342", "para_id": 8807, "text": "“Would you believe it, Lizzy, that when he went to town last November he"}
{"doc_id": "gutenberg_1342", "para_id": 8808, "text": "really loved me, and nothing but a persuasion of _my_ being indifferent"}
{"doc_id": "gutenberg_1342", "para_id": 8809, "text": "“He made a little mistake, to be sure; but it is to the credit of his"}
{"doc_id": "gutenberg_1342", "para_id": 8810, "text": "This naturally introduced a panegyric from Jane on his diffidence, and"}
{"doc_id": "gutenberg_1342", "para_id": 8811, "text": "Elizabeth was pleased to find that he had not betrayed the interference"}
{"doc_id": "gutenberg_1342", "para_id": 8812, "text": "of his friend; for, though Jane had the most generous and forgiving"}
{"doc_id": "gutenberg_1342", "para_id": 8813, "text": "heart in the world, she knew it was a circumstance which must prejudice"}
{"doc_id": "gutenberg_1342", "para_id": 8814, "text": "“I am certainly the most fortunate creature that ever existed!” cried"}
{"doc_id": "gutenberg_1342", "para_id": 8815, "text": "Jane. “Oh, Lizzy, why am I thus singled from my family, and blessed"}
{"doc_id": "gutenberg_1342", "para_id": 8816, "text": "above them all? If I could but see you as happy! If there were but such"}
{"doc_id": "gutenberg_1342", "para_id": 8817, "text": "“If you were to give me forty such men I never could be so happy as you."}
{"doc_id": "gutenberg_1342", "para_id": 8818, "text": "Till I have your disposition, your goodness, I never can have your"}
{"doc_id": "gutenberg_1342", "para_id": 8819, "text": "happiness. No, no, let me shift for myself; and, perhaps, if I have very"}
{"doc_id": "gutenberg_1342", "para_id": 8820, "text": "good luck, I may meet with another Mr. Collins in time.”"}
{"doc_id": "gutenberg_1342", "para_id": 8821, "text": "The situation of affairs in the Longbourn family could not be long a"}
{"doc_id": "gutenberg_1342", "para_id": 8822, "text": "secret. Mrs. Bennet was privileged to whisper it to Mrs. Philips, and"}
{"doc_id": "gutenberg_1342", "para_id": 8823, "text": "she ventured, without any permission, to do the same by all her"}
{"doc_id": "gutenberg_1342", "para_id": 8824, "text": "The Bennets were speedily pronounced to be the luckiest family in the"}
{"doc_id": "gutenberg_1342", "para_id": 8825, "text": "world; though only a few weeks before, when Lydia had first run away,"}
{"doc_id": "gutenberg_1342", "para_id": 8826, "text": "they had been generally proved to be marked out for misfortune."}
{"doc_id": "gutenberg_1342", "para_id": 8827, "text": "One morning, about a week after Bingley’s engagement with Jane had been"}
{"doc_id": "gutenberg_1342", "para_id": 8828, "text": "formed, as he and the females of the family were sitting together in the"}
{"doc_id": "gutenberg_1342", "para_id": 8829, "text": "dining-room, their attention was suddenly drawn to the window by the"}
{"doc_id": "gutenberg_1342", "para_id": 8830, "text": "sound of a carriage; and they perceived a chaise and four driving up the"}
{"doc_id": "gutenberg_1342", "para_id": 8831, "text": "lawn. It was too early in the morning for visitors; and besides, the"}
{"doc_id": "gutenberg_1342", "para_id": 8832, "text": "equipage did not answer to that of any of their neighbours. The horses"}
{"doc_id": "gutenberg_1342", "para_id": 8833, "text": "were post; and neither the carriage, nor the livery of the servant who"}
{"doc_id": "gutenberg_1342", "para_id": 8834, "text": "preceded it, were familiar to them. As it was certain, however, that"}
{"doc_id": "gutenberg_1342", "para_id": 8835, "text": "somebody was coming, Bingley instantly prevailed on Miss Bennet to avoid"}
{"doc_id": "gutenberg_1342", "para_id": 8836, "text": "the confinement of such an intrusion, and walk away with him into the"}
{"doc_id": "gutenberg_1342", "para_id": 8837, "text": "shrubbery. They both set off; and the conjectures of the remaining three"}
{"doc_id": "gutenberg_1342", "para_id": 8838, "text": "continued, though with little satisfaction, till the door was thrown"}
{"doc_id": "gutenberg_1342", "para_id": 8839, "text": "open, and their visitor entered. It was Lady Catherine de Bourgh."}
{"doc_id": "gutenberg_1342", "para_id": 8840, "text": "They were of course all intending to be surprised: but their"}
{"doc_id": "gutenberg_1342", "para_id": 8841, "text": "astonishment was beyond their expectation; and on the part of Mrs."}
{"doc_id": "gutenberg_1342", "para_id": 8842, "text": "Bennet and Kitty, though she was perfectly unknown to them, even"}
{"doc_id": "gutenberg_1342", "para_id": 8843, "text": "She entered the room with an air more than usually ungracious, made no"}
{"doc_id": "gutenberg_1342", "para_id": 8844, "text": "other reply to Elizabeth’s salutation than a slight inclination of the"}
{"doc_id": "gutenberg_1342", "para_id": 8845, "text": "head, and sat down without saying a word. Elizabeth had mentioned her"}
{"doc_id": "gutenberg_1342", "para_id": 8846, "text": "name to her mother on her Ladyship’s entrance, though no request of"}
{"doc_id": "gutenberg_1342", "para_id": 8847, "text": "Mrs. Bennet, all amazement, though flattered by having a guest of such"}
{"doc_id": "gutenberg_1342", "para_id": 8848, "text": "high importance, received her with the utmost politeness. After sitting"}
{"doc_id": "gutenberg_1342", "para_id": 8849, "text": "for a moment in silence, she said, very stiffly, to Elizabeth,--"}
{"doc_id": "gutenberg_1342", "para_id": 8850, "text": "“I hope you are well, Miss Bennet. That lady, I suppose, is your"}
{"doc_id": "gutenberg_1342", "para_id": 8851, "text": "“Yes, madam,” said Mrs. Bennet, delighted to speak to a Lady Catherine."}
{"doc_id": "gutenberg_1342", "para_id": 8852, "text": "“She is my youngest girl but one. My youngest of all is lately married,"}
{"doc_id": "gutenberg_1342", "para_id": 8853, "text": "and my eldest is somewhere about the ground, walking with a young man,"}
{"doc_id": "gutenberg_1342", "para_id": 8854, "text": "who, I believe, will soon become a part of the family.”"}
{"doc_id": "gutenberg_1342", "para_id": 8855, "text": "“You have a very small park here,” returned Lady Catherine, after a"}
{"doc_id": "gutenberg_1342", "para_id": 8856, "text": "“It is nothing in comparison of Rosings, my Lady, I dare say; but, I"}
{"doc_id": "gutenberg_1342", "para_id": 8857, "text": "assure you, it is much larger than Sir William Lucas’s.”"}
{"doc_id": "gutenberg_1342", "para_id": 8858, "text": "“This must be a most inconvenient sitting-room for the evening in"}
{"doc_id": "gutenberg_1342", "para_id": 8859, "text": "Mrs. Bennet assured her that they never sat there after dinner; and then"}
{"doc_id": "gutenberg_1342", "para_id": 8860, "text": "“May I take the liberty of asking your Ladyship whether you left Mr. and"}
{"doc_id": "gutenberg_1342", "para_id": 8861, "text": "“Yes, very well. I saw them the night before last.”"}
{"doc_id": "gutenberg_1342", "para_id": 8862, "text": "Elizabeth now expected that she would produce a letter for her from"}
{"doc_id": "gutenberg_1342", "para_id": 8863, "text": "Charlotte, as it seemed the only probable motive for her calling. But no"}
{"doc_id": "gutenberg_1342", "para_id": 8864, "text": "Mrs. Bennet, with great civility, begged her Ladyship to take some"}
{"doc_id": "gutenberg_1342", "para_id": 8865, "text": "refreshment: but Lady Catherine very resolutely, and not very politely,"}
{"doc_id": "gutenberg_1342", "para_id": 8866, "text": "declined eating anything; and then, rising up, said to Elizabeth,--"}
{"doc_id": "gutenberg_1342", "para_id": 8867, "text": "“Miss Bennet, there seemed to be a prettyish kind of a little wilderness"}
{"doc_id": "gutenberg_1342", "para_id": 8868, "text": "on one side of your lawn. I should be glad to take a turn in it, if you"}
{"doc_id": "gutenberg_1342", "para_id": 8869, "text": "“Go, my dear,” cried her mother, “and show her Ladyship about the"}
{"doc_id": "gutenberg_1342", "para_id": 8870, "text": "different walks. I think she will be pleased with the hermitage.”"}
{"doc_id": "gutenberg_1342", "para_id": 8871, "text": "Elizabeth obeyed; and, running into her own room for her parasol,"}
{"doc_id": "gutenberg_1342", "para_id": 8872, "text": "attended her noble guest downstairs. As they passed through the hall,"}
{"doc_id": "gutenberg_1342", "para_id": 8873, "text": "Lady Catherine opened the doors into the dining-parlour and"}
{"doc_id": "gutenberg_1342", "para_id": 8874, "text": "drawing-room, and pronouncing them, after a short survey, to be"}
{"doc_id": "gutenberg_1342", "para_id": 8875, "text": "Her carriage remained at the door, and Elizabeth saw that her"}
{"doc_id": "gutenberg_1342", "para_id": 8876, "text": "waiting-woman was in it. They proceeded in silence along the gravel walk"}
{"doc_id": "gutenberg_1342", "para_id": 8877, "text": "that led to the copse; Elizabeth was determined to make no effort for"}
{"doc_id": "gutenberg_1342", "para_id": 8878, "text": "conversation with a woman who was now more than usually insolent and"}
{"doc_id": "gutenberg_1342", "para_id": 8879, "text": "“How could I ever think her like her nephew?” said she, as she looked in"}
{"doc_id": "gutenberg_1342", "para_id": 8880, "text": "As soon as they entered the copse, Lady Catherine began in the following"}
{"doc_id": "gutenberg_1342", "para_id": 8881, "text": "“You can be at no loss, Miss Bennet, to understand the reason of my"}
{"doc_id": "gutenberg_1342", "para_id": 8882, "text": "journey hither. Your own heart, your own conscience, must tell you why I"}
{"doc_id": "gutenberg_1342", "para_id": 8883, "text": "“Indeed, you are mistaken, madam; I have not been at all able to account"}
{"doc_id": "gutenberg_1342", "para_id": 8884, "text": "“Miss Bennet,” replied her Ladyship, in an angry tone, “you ought to"}
{"doc_id": "gutenberg_1342", "para_id": 8885, "text": "know that I am not to be trifled with. But however insincere _you_ may"}
{"doc_id": "gutenberg_1342", "para_id": 8886, "text": "choose to be, you shall not find _me_ so. My character has ever been"}
{"doc_id": "gutenberg_1342", "para_id": 8887, "text": "celebrated for its sincerity and frankness; and in a cause of such"}
{"doc_id": "gutenberg_1342", "para_id": 8888, "text": "moment as this, I shall certainly not depart from it. A report of a most"}
{"doc_id": "gutenberg_1342", "para_id": 8889, "text": "alarming nature reached me two days ago. I was told, that not only your"}
{"doc_id": "gutenberg_1342", "para_id": 8890, "text": "sister was on the point of being most advantageously married, but that"}
{"doc_id": "gutenberg_1342", "para_id": 8891, "text": "_you_--that Miss Elizabeth Bennet would, in all likelihood, be soon"}
{"doc_id": "gutenberg_1342", "para_id": 8892, "text": "afterwards united to my nephew--my own nephew, Mr. Darcy. Though I"}
{"doc_id": "gutenberg_1342", "para_id": 8893, "text": "_know_ it must be a scandalous falsehood, though I would not injure him"}
{"doc_id": "gutenberg_1342", "para_id": 8894, "text": "so much as to suppose the truth of it possible, I instantly resolved on"}
{"doc_id": "gutenberg_1342", "para_id": 8895, "text": "setting off for this place, that I might make my sentiments known to"}
{"doc_id": "gutenberg_1342", "para_id": 8896, "text": "“If you believed it impossible to be true,” said Elizabeth, colouring"}
{"doc_id": "gutenberg_1342", "para_id": 8897, "text": "with astonishment and disdain, “I wonder you took the trouble of coming"}
{"doc_id": "gutenberg_1342", "para_id": 8898, "text": "“At once to insist upon having such a report universally contradicted.”"}
{"doc_id": "gutenberg_1342", "para_id": 8899, "text": "“Your coming to Longbourn, to see me and my family,” said Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 8900, "text": "coolly, “will be rather a confirmation of it--if, indeed, such a report"}
{"doc_id": "gutenberg_1342", "para_id": 8901, "text": "“If! do you then pretend to be ignorant of it? Has it not been"}
{"doc_id": "gutenberg_1342", "para_id": 8902, "text": "industriously circulated by yourselves? Do you not know that such a"}
{"doc_id": "gutenberg_1342", "para_id": 8903, "text": "“And can you likewise declare, that there is no _foundation_ for it?”"}
{"doc_id": "gutenberg_1342", "para_id": 8904, "text": "“I do not pretend to possess equal frankness with your Ladyship. _You_"}
{"doc_id": "gutenberg_1342", "para_id": 8905, "text": "may ask questions which _I_ shall not choose to answer.”"}
{"doc_id": "gutenberg_1342", "para_id": 8906, "text": "“This is not to be borne. Miss Bennet, I insist on being satisfied. Has"}
{"doc_id": "gutenberg_1342", "para_id": 8907, "text": "“It ought to be so; it must be so, while he retains the use of his"}
{"doc_id": "gutenberg_1342", "para_id": 8908, "text": "reason. But _your_ arts and allurements may, in a moment of infatuation,"}
{"doc_id": "gutenberg_1342", "para_id": 8909, "text": "have made him forget what he owes to himself and to all his family. You"}
{"doc_id": "gutenberg_1342", "para_id": 8910, "text": "“If I have, I shall be the last person to confess it.”"}
{"doc_id": "gutenberg_1342", "para_id": 8911, "text": "“Miss Bennet, do you know who I am? I have not been accustomed to such"}
{"doc_id": "gutenberg_1342", "para_id": 8912, "text": "language as this. I am almost the nearest relation he has in the world,"}
{"doc_id": "gutenberg_1342", "para_id": 8913, "text": "“But you are not entitled to know _mine_; nor will such behaviour as"}
{"doc_id": "gutenberg_1342", "para_id": 8914, "text": "“Let me be rightly understood. This match, to which you have the"}
{"doc_id": "gutenberg_1342", "para_id": 8915, "text": "presumption to aspire, can never take place. No, never. Mr. Darcy is"}
{"doc_id": "gutenberg_1342", "para_id": 8916, "text": "engaged to _my daughter_. Now, what have you to say?”"}
{"doc_id": "gutenberg_1342", "para_id": 8917, "text": "“Only this,--that if he is so, you can have no reason to suppose he will"}
{"doc_id": "gutenberg_1342", "para_id": 8918, "text": "Lady Catherine hesitated for a moment, and then replied,--"}
{"doc_id": "gutenberg_1342", "para_id": 8919, "text": "“The engagement between them is of a peculiar kind. From their infancy,"}
{"doc_id": "gutenberg_1342", "para_id": 8920, "text": "they have been intended for each other. It was the favourite wish of"}
{"doc_id": "gutenberg_1342", "para_id": 8921, "text": "_his_ mother, as well as of hers. While in their cradles we planned the"}
{"doc_id": "gutenberg_1342", "para_id": 8922, "text": "union; and now, at the moment when the wishes of both sisters would be"}
{"doc_id": "gutenberg_1342", "para_id": 8923, "text": "accomplished, is their marriage to be prevented by a young woman of"}
{"doc_id": "gutenberg_1342", "para_id": 8924, "text": "inferior birth, of no importance in the world, and wholly unallied to"}
{"doc_id": "gutenberg_1342", "para_id": 8925, "text": "the family? Do you pay no regard to the wishes of his friends--to his"}
{"doc_id": "gutenberg_1342", "para_id": 8926, "text": "tacit engagement with Miss de Bourgh? Are you lost to every feeling of"}
{"doc_id": "gutenberg_1342", "para_id": 8927, "text": "propriety and delicacy? Have you not heard me say, that from his"}
{"doc_id": "gutenberg_1342", "para_id": 8928, "text": "“Yes; and I had heard it before. But what is that to me? If there is no"}
{"doc_id": "gutenberg_1342", "para_id": 8929, "text": "other objection to my marrying your nephew, I shall certainly not be"}
{"doc_id": "gutenberg_1342", "para_id": 8930, "text": "kept from it by knowing that his mother and aunt wished him to marry"}
{"doc_id": "gutenberg_1342", "para_id": 8931, "text": "Miss de Bourgh. You both did as much as you could in planning the"}
{"doc_id": "gutenberg_1342", "para_id": 8932, "text": "marriage. Its completion depended on others. If Mr. Darcy is neither by"}
{"doc_id": "gutenberg_1342", "para_id": 8933, "text": "honour nor inclination confined to his cousin, why is not he to make"}
{"doc_id": "gutenberg_1342", "para_id": 8934, "text": "another choice? And if I am that choice, why may not I accept him?”"}
{"doc_id": "gutenberg_1342", "para_id": 8935, "text": "“Because honour, decorum, prudence--nay, interest--forbid it. Yes, Miss"}
{"doc_id": "gutenberg_1342", "para_id": 8936, "text": "Bennet, interest; for do not expect to be noticed by his family or"}
{"doc_id": "gutenberg_1342", "para_id": 8937, "text": "friends, if you wilfully act against the inclinations of all. You will"}
{"doc_id": "gutenberg_1342", "para_id": 8938, "text": "be censured, slighted, and despised, by everyone connected with him."}
{"doc_id": "gutenberg_1342", "para_id": 8939, "text": "Your alliance will be a disgrace; your name will never even be mentioned"}
{"doc_id": "gutenberg_1342", "para_id": 8940, "text": "“These are heavy misfortunes,” replied Elizabeth. “But the wife of Mr."}
{"doc_id": "gutenberg_1342", "para_id": 8941, "text": "Darcy must have such extraordinary sources of happiness necessarily"}
{"doc_id": "gutenberg_1342", "para_id": 8942, "text": "attached to her situation, that she could, upon the whole, have no cause"}
{"doc_id": "gutenberg_1342", "para_id": 8943, "text": "“Obstinate, headstrong girl! I am ashamed of you! Is this your gratitude"}
{"doc_id": "gutenberg_1342", "para_id": 8944, "text": "for my attentions to you last spring? Is nothing due to me on that"}
{"doc_id": "gutenberg_1342", "para_id": 8945, "text": "score? Let us sit down. You are to understand, Miss Bennet, that I came"}
{"doc_id": "gutenberg_1342", "para_id": 8946, "text": "here with the determined resolution of carrying my purpose; nor will I"}
{"doc_id": "gutenberg_1342", "para_id": 8947, "text": "be dissuaded from it. I have not been used to submit to any person’s"}
{"doc_id": "gutenberg_1342", "para_id": 8948, "text": "whims. I have not been in the habit of brooking disappointment.”"}
{"doc_id": "gutenberg_1342", "para_id": 8949, "text": "“_That_ will make your Ladyship’s situation at present more pitiable;"}
{"doc_id": "gutenberg_1342", "para_id": 8950, "text": "“I will not be interrupted! Hear me in silence. My daughter and my"}
{"doc_id": "gutenberg_1342", "para_id": 8951, "text": "nephew are formed for each other. They are descended, on the maternal"}
{"doc_id": "gutenberg_1342", "para_id": 8952, "text": "side, from the same noble line; and, on the father’s, from respectable,"}
{"doc_id": "gutenberg_1342", "para_id": 8953, "text": "honourable, and ancient, though untitled, families. Their fortune on"}
{"doc_id": "gutenberg_1342", "para_id": 8954, "text": "both sides is splendid. They are destined for each other by the voice of"}
{"doc_id": "gutenberg_1342", "para_id": 8955, "text": "every member of their respective houses; and what is to divide"}
{"doc_id": "gutenberg_1342", "para_id": 8956, "text": "them?--the upstart pretensions of a young woman without family,"}
{"doc_id": "gutenberg_1342", "para_id": 8957, "text": "connections, or fortune! Is this to be endured? But it must not, shall"}
{"doc_id": "gutenberg_1342", "para_id": 8958, "text": "not be! If you were sensible of your own good, you would not wish to"}
{"doc_id": "gutenberg_1342", "para_id": 8959, "text": "quit the sphere in which you have been brought up.”"}
{"doc_id": "gutenberg_1342", "para_id": 8960, "text": "“In marrying your nephew, I should not consider myself as quitting that"}
{"doc_id": "gutenberg_1342", "para_id": 8961, "text": "sphere. He is a gentleman; I am a gentleman’s daughter; so far we are"}
{"doc_id": "gutenberg_1342", "para_id": 8962, "text": "“True. You _are_ a gentleman’s daughter. But what was your mother? Who"}
{"doc_id": "gutenberg_1342", "para_id": 8963, "text": "are your uncles and aunts? Do not imagine me ignorant of their"}
{"doc_id": "gutenberg_1342", "para_id": 8964, "text": "“Whatever my connections may be,” said Elizabeth, “if your nephew does"}
{"doc_id": "gutenberg_1342", "para_id": 8965, "text": "Though Elizabeth would not, for the mere purpose of obliging Lady"}
{"doc_id": "gutenberg_1342", "para_id": 8966, "text": "Catherine, have answered this question, she could not but say, after a"}
{"doc_id": "gutenberg_1342", "para_id": 8967, "text": "“And will you promise me never to enter into such an engagement?”"}
{"doc_id": "gutenberg_1342", "para_id": 8968, "text": "“Miss Bennet, I am shocked and astonished. I expected to find a more"}
{"doc_id": "gutenberg_1342", "para_id": 8969, "text": "reasonable young woman. But do not deceive yourself into a belief that I"}
{"doc_id": "gutenberg_1342", "para_id": 8970, "text": "will ever recede. I shall not go away till you have given me the"}
{"doc_id": "gutenberg_1342", "para_id": 8971, "text": "“And I certainly _never_ shall give it. I am not to be intimidated into"}
{"doc_id": "gutenberg_1342", "para_id": 8972, "text": "anything so wholly unreasonable. Your Ladyship wants Mr. Darcy to marry"}
{"doc_id": "gutenberg_1342", "para_id": 8973, "text": "your daughter; but would my giving you the wished-for promise make"}
{"doc_id": "gutenberg_1342", "para_id": 8974, "text": "_their_ marriage at all more probable? Supposing him to be attached to"}
{"doc_id": "gutenberg_1342", "para_id": 8975, "text": "me, would _my_ refusing to accept his hand make him wish to bestow it on"}
{"doc_id": "gutenberg_1342", "para_id": 8976, "text": "his cousin? Allow me to say, Lady Catherine, that the arguments with"}
{"doc_id": "gutenberg_1342", "para_id": 8977, "text": "which you have supported this extraordinary application have been as"}
{"doc_id": "gutenberg_1342", "para_id": 8978, "text": "frivolous as the application was ill-judged. You have widely mistaken my"}
{"doc_id": "gutenberg_1342", "para_id": 8979, "text": "character, if you think I can be worked on by such persuasions as these."}
{"doc_id": "gutenberg_1342", "para_id": 8980, "text": "How far your nephew might approve of your interference in _his_ affairs,"}
{"doc_id": "gutenberg_1342", "para_id": 8981, "text": "I cannot tell; but you have certainly no right to concern yourself in"}
{"doc_id": "gutenberg_1342", "para_id": 8982, "text": "mine. I must beg, therefore, to be importuned no further on the"}
{"doc_id": "gutenberg_1342", "para_id": 8983, "text": "“Not so hasty, if you please. I have by no means done. To all the"}
{"doc_id": "gutenberg_1342", "para_id": 8984, "text": "objections I have already urged I have still another to add. I am no"}
{"doc_id": "gutenberg_1342", "para_id": 8985, "text": "stranger to the particulars of your youngest sister’s infamous"}
{"doc_id": "gutenberg_1342", "para_id": 8986, "text": "elopement. I know it all; that the young man’s marrying her was a"}
{"doc_id": "gutenberg_1342", "para_id": 8987, "text": "patched-up business, at the expense of your father and uncle. And is"}
{"doc_id": "gutenberg_1342", "para_id": 8988, "text": "_such_ a girl to be my nephew’s sister? Is _her_ husband, who is the son"}
{"doc_id": "gutenberg_1342", "para_id": 8989, "text": "of his late father’s steward, to be his brother? Heaven and earth!--of"}
{"doc_id": "gutenberg_1342", "para_id": 8990, "text": "what are you thinking? Are the shades of Pemberley to be thus polluted?”"}
{"doc_id": "gutenberg_1342", "para_id": 8991, "text": "“You can _now_ have nothing further to say,” she resentfully answered."}
{"doc_id": "gutenberg_1342", "para_id": 8992, "text": "“You have insulted me, in every possible method. I must beg to return to"}
{"doc_id": "gutenberg_1342", "para_id": 8993, "text": "And she rose as she spoke. Lady Catherine rose also, and they turned"}
{"doc_id": "gutenberg_1342", "para_id": 8994, "text": "“You have no regard, then, for the honour and credit of my nephew!"}
{"doc_id": "gutenberg_1342", "para_id": 8995, "text": "Unfeeling, selfish girl! Do you not consider that a connection with you"}
{"doc_id": "gutenberg_1342", "para_id": 8996, "text": "“Lady Catherine, I have nothing further to say. You know my sentiments.”"}
{"doc_id": "gutenberg_1342", "para_id": 8997, "text": "“I have said no such thing. I am only resolved to act in that manner,"}
{"doc_id": "gutenberg_1342", "para_id": 8998, "text": "which will, in my own opinion, constitute my happiness, without"}
{"doc_id": "gutenberg_1342", "para_id": 8999, "text": "reference to _you_, or to any person so wholly unconnected with me.”"}
{"doc_id": "gutenberg_1342", "para_id": 9000, "text": "“It is well. You refuse, then, to oblige me. You refuse to obey the"}
{"doc_id": "gutenberg_1342", "para_id": 9001, "text": "claims of duty, honour, and gratitude. You are determined to ruin him in"}
{"doc_id": "gutenberg_1342", "para_id": 9002, "text": "the opinion of all his friends, and make him the contempt of the world.”"}
{"doc_id": "gutenberg_1342", "para_id": 9003, "text": "“Neither duty, nor honour, nor gratitude,” replied Elizabeth, “has any"}
{"doc_id": "gutenberg_1342", "para_id": 9004, "text": "possible claim on me, in the present instance. No principle of either"}
{"doc_id": "gutenberg_1342", "para_id": 9005, "text": "would be violated by my marriage with Mr. Darcy. And with regard to the"}
{"doc_id": "gutenberg_1342", "para_id": 9006, "text": "resentment of his family, or the indignation of the world, if the former"}
{"doc_id": "gutenberg_1342", "para_id": 9007, "text": "_were_ excited by his marrying me, it would not give me one moment’s"}
{"doc_id": "gutenberg_1342", "para_id": 9008, "text": "concern--and the world in general would have too much sense to join in"}
{"doc_id": "gutenberg_1342", "para_id": 9009, "text": "“And this is your real opinion! This is your final resolve! Very well. I"}
{"doc_id": "gutenberg_1342", "para_id": 9010, "text": "shall now know how to act. Do not imagine, Miss Bennet, that your"}
{"doc_id": "gutenberg_1342", "para_id": 9011, "text": "ambition will ever be gratified. I came to try you. I hoped to find you"}
{"doc_id": "gutenberg_1342", "para_id": 9012, "text": "reasonable; but depend upon it I will carry my point.”"}
{"doc_id": "gutenberg_1342", "para_id": 9013, "text": "In this manner Lady Catherine talked on till they were at the door of"}
{"doc_id": "gutenberg_1342", "para_id": 9014, "text": "the carriage, when, turning hastily round, she added,--"}
{"doc_id": "gutenberg_1342", "para_id": 9015, "text": "“I take no leave of you, Miss Bennet. I send no compliments to your"}
{"doc_id": "gutenberg_1342", "para_id": 9016, "text": "mother. You deserve no such attention. I am most seriously displeased.”"}
{"doc_id": "gutenberg_1342", "para_id": 9017, "text": "Elizabeth made no answer; and without attempting to persuade her"}
{"doc_id": "gutenberg_1342", "para_id": 9018, "text": "Ladyship to return into the house, walked quietly into it herself. She"}
{"doc_id": "gutenberg_1342", "para_id": 9019, "text": "heard the carriage drive away as she proceeded upstairs. Her mother"}
{"doc_id": "gutenberg_1342", "para_id": 9020, "text": "impatiently met her at the door of her dressing-room, to ask why Lady"}
{"doc_id": "gutenberg_1342", "para_id": 9021, "text": "Catherine would not come in again and rest herself."}
{"doc_id": "gutenberg_1342", "para_id": 9022, "text": "“She did not choose it,” said her daughter; “she would go.”"}
{"doc_id": "gutenberg_1342", "para_id": 9023, "text": "“She is a very fine-looking woman! and her calling here was prodigiously"}
{"doc_id": "gutenberg_1342", "para_id": 9024, "text": "civil! for she only came, I suppose, to tell us the Collinses were well."}
{"doc_id": "gutenberg_1342", "para_id": 9025, "text": "She is on her road somewhere, I dare say; and so, passing through"}
{"doc_id": "gutenberg_1342", "para_id": 9026, "text": "Meryton, thought she might as well call on you. I suppose she had"}
{"doc_id": "gutenberg_1342", "para_id": 9027, "text": "Elizabeth was forced to give in to a little falsehood here; for to"}
{"doc_id": "gutenberg_1342", "para_id": 9028, "text": "acknowledge the substance of their conversation was impossible."}
{"doc_id": "gutenberg_1342", "para_id": 9029, "text": "The discomposure of spirits which this extraordinary visit threw"}
{"doc_id": "gutenberg_1342", "para_id": 9030, "text": "Elizabeth into could not be easily overcome; nor could she for many"}
{"doc_id": "gutenberg_1342", "para_id": 9031, "text": "hours learn to think of it less than incessantly. Lady Catherine, it"}
{"doc_id": "gutenberg_1342", "para_id": 9032, "text": "appeared, had actually taken the trouble of this journey from Rosings"}
{"doc_id": "gutenberg_1342", "para_id": 9033, "text": "for the sole purpose of breaking off her supposed engagement with Mr."}
{"doc_id": "gutenberg_1342", "para_id": 9034, "text": "Darcy. It was a rational scheme, to be sure! but from what the report of"}
{"doc_id": "gutenberg_1342", "para_id": 9035, "text": "their engagement could originate, Elizabeth was at a loss to imagine;"}
{"doc_id": "gutenberg_1342", "para_id": 9036, "text": "till she recollected that _his_ being the intimate friend of Bingley,"}
{"doc_id": "gutenberg_1342", "para_id": 9037, "text": "and _her_ being the sister of Jane, was enough, at a time when the"}
{"doc_id": "gutenberg_1342", "para_id": 9038, "text": "expectation of one wedding made everybody eager for another, to supply"}
{"doc_id": "gutenberg_1342", "para_id": 9039, "text": "the idea. She had not herself forgotten to feel that the marriage of her"}
{"doc_id": "gutenberg_1342", "para_id": 9040, "text": "sister must bring them more frequently together. And her neighbours at"}
{"doc_id": "gutenberg_1342", "para_id": 9041, "text": "Lucas Lodge, therefore, (for through their communication with the"}
{"doc_id": "gutenberg_1342", "para_id": 9042, "text": "Collinses, the report, she concluded, had reached Lady Catherine,) had"}
{"doc_id": "gutenberg_1342", "para_id": 9043, "text": "only set _that_ down as almost certain and immediate which _she_ had"}
{"doc_id": "gutenberg_1342", "para_id": 9044, "text": "In revolving Lady Catherine’s expressions, however, she could not help"}
{"doc_id": "gutenberg_1342", "para_id": 9045, "text": "feeling some uneasiness as to the possible consequence of her persisting"}
{"doc_id": "gutenberg_1342", "para_id": 9046, "text": "in this interference. From what she had said of her resolution to"}
{"doc_id": "gutenberg_1342", "para_id": 9047, "text": "prevent the marriage, it occurred to Elizabeth that she must meditate an"}
{"doc_id": "gutenberg_1342", "para_id": 9048, "text": "application to her nephew; and how he might take a similar"}
{"doc_id": "gutenberg_1342", "para_id": 9049, "text": "representation of the evils attached to a connection with her she dared"}
{"doc_id": "gutenberg_1342", "para_id": 9050, "text": "not pronounce. She knew not the exact degree of his affection for his"}
{"doc_id": "gutenberg_1342", "para_id": 9051, "text": "aunt, or his dependence on her judgment, but it was natural to suppose"}
{"doc_id": "gutenberg_1342", "para_id": 9052, "text": "that he thought much higher of her Ladyship than _she_ could do; and it"}
{"doc_id": "gutenberg_1342", "para_id": 9053, "text": "was certain, that in enumerating the miseries of a marriage with _one_"}
{"doc_id": "gutenberg_1342", "para_id": 9054, "text": "whose immediate connections were so unequal to his own, his aunt would"}
{"doc_id": "gutenberg_1342", "para_id": 9055, "text": "address him on his weakest side. With his notions of dignity, he would"}
{"doc_id": "gutenberg_1342", "para_id": 9056, "text": "probably feel that the arguments, which to Elizabeth had appeared weak"}
{"doc_id": "gutenberg_1342", "para_id": 9057, "text": "and ridiculous, contained much good sense and solid reasoning."}
{"doc_id": "gutenberg_1342", "para_id": 9058, "text": "If he had been wavering before, as to what he should do, which had often"}
{"doc_id": "gutenberg_1342", "para_id": 9059, "text": "seemed likely, the advice and entreaty of so near a relation might"}
{"doc_id": "gutenberg_1342", "para_id": 9060, "text": "settle every doubt, and determine him at once to be as happy as dignity"}
{"doc_id": "gutenberg_1342", "para_id": 9061, "text": "unblemished could make him. In that case he would return no more. Lady"}
{"doc_id": "gutenberg_1342", "para_id": 9062, "text": "Catherine might see him in her way through town; and his engagement to"}
{"doc_id": "gutenberg_1342", "para_id": 9063, "text": "Bingley of coming again to Netherfield must give way."}
{"doc_id": "gutenberg_1342", "para_id": 9064, "text": "“If, therefore, an excuse for not keeping his promise should come to his"}
{"doc_id": "gutenberg_1342", "para_id": 9065, "text": "friend within a few days,” she added, “I shall know how to understand"}
{"doc_id": "gutenberg_1342", "para_id": 9066, "text": "it. I shall then give over every expectation, every wish of his"}
{"doc_id": "gutenberg_1342", "para_id": 9067, "text": "constancy. If he is satisfied with only regretting me, when he might"}
{"doc_id": "gutenberg_1342", "para_id": 9068, "text": "have obtained my affections and hand, I shall soon cease to regret him"}
{"doc_id": "gutenberg_1342", "para_id": 9069, "text": "The surprise of the rest of the family, on hearing who their visitor had"}
{"doc_id": "gutenberg_1342", "para_id": 9070, "text": "been, was very great: but they obligingly satisfied it with the same"}
{"doc_id": "gutenberg_1342", "para_id": 9071, "text": "kind of supposition which had appeased Mrs. Bennet’s curiosity; and"}
{"doc_id": "gutenberg_1342", "para_id": 9072, "text": "Elizabeth was spared from much teasing on the subject."}
{"doc_id": "gutenberg_1342", "para_id": 9073, "text": "The next morning, as she was going down stairs, she was met by her"}
{"doc_id": "gutenberg_1342", "para_id": 9074, "text": "father, who came out of his library with a letter in his hand."}
{"doc_id": "gutenberg_1342", "para_id": 9075, "text": "“Lizzy,” said he, “I was going to look for you: come into my room.”"}
{"doc_id": "gutenberg_1342", "para_id": 9076, "text": "She followed him thither; and her curiosity to know what he had to tell"}
{"doc_id": "gutenberg_1342", "para_id": 9077, "text": "her was heightened by the supposition of its being in some manner"}
{"doc_id": "gutenberg_1342", "para_id": 9078, "text": "connected with the letter he held. It suddenly struck her that it might"}
{"doc_id": "gutenberg_1342", "para_id": 9079, "text": "be from Lady Catherine, and she anticipated with dismay all the"}
{"doc_id": "gutenberg_1342", "para_id": 9080, "text": "She followed her father to the fireplace, and they both sat down. He"}
{"doc_id": "gutenberg_1342", "para_id": 9081, "text": "“I have received a letter this morning that has astonished me"}
{"doc_id": "gutenberg_1342", "para_id": 9082, "text": "exceedingly. As it principally concerns yourself, you ought to know its"}
{"doc_id": "gutenberg_1342", "para_id": 9083, "text": "contents. I did not know before that I had _two_ daughters on the brink"}
{"doc_id": "gutenberg_1342", "para_id": 9084, "text": "of matrimony. Let me congratulate you on a very important conquest.”"}
{"doc_id": "gutenberg_1342", "para_id": 9085, "text": "The colour now rushed into Elizabeth’s cheeks in the instantaneous"}
{"doc_id": "gutenberg_1342", "para_id": 9086, "text": "conviction of its being a letter from the nephew, instead of the aunt;"}
{"doc_id": "gutenberg_1342", "para_id": 9087, "text": "and she was undetermined whether most to be pleased that he explained"}
{"doc_id": "gutenberg_1342", "para_id": 9088, "text": "himself at all, or offended that his letter was not rather addressed to"}
{"doc_id": "gutenberg_1342", "para_id": 9089, "text": "“You look conscious. Young ladies have great penetration in such matters"}
{"doc_id": "gutenberg_1342", "para_id": 9090, "text": "as these; but I think I may defy even _your_ sagacity to discover the"}
{"doc_id": "gutenberg_1342", "para_id": 9091, "text": "name of your admirer. This letter is from Mr. Collins.”"}
{"doc_id": "gutenberg_1342", "para_id": 9092, "text": "“Something very much to the purpose, of course. He begins with"}
{"doc_id": "gutenberg_1342", "para_id": 9093, "text": "congratulations on the approaching nuptials of my eldest daughter, of"}
{"doc_id": "gutenberg_1342", "para_id": 9094, "text": "which, it seems, he has been told by some of the good-natured, gossiping"}
{"doc_id": "gutenberg_1342", "para_id": 9095, "text": "Lucases. I shall not sport with your impatience by reading what he says"}
{"doc_id": "gutenberg_1342", "para_id": 9096, "text": "on that point. What relates to yourself is as follows:--‘Having thus"}
{"doc_id": "gutenberg_1342", "para_id": 9097, "text": "offered you the sincere congratulations of Mrs. Collins and myself on"}
{"doc_id": "gutenberg_1342", "para_id": 9098, "text": "this happy event, let me now add a short hint on the subject of another,"}
{"doc_id": "gutenberg_1342", "para_id": 9099, "text": "of which we have been advertised by the same authority. Your daughter"}
{"doc_id": "gutenberg_1342", "para_id": 9100, "text": "Elizabeth, it is presumed, will not long bear the name of Bennet, after"}
{"doc_id": "gutenberg_1342", "para_id": 9101, "text": "her eldest sister has resigned it; and the chosen partner of her fate"}
{"doc_id": "gutenberg_1342", "para_id": 9102, "text": "may be reasonably looked up to as one of the most illustrious personages"}
{"doc_id": "gutenberg_1342", "para_id": 9103, "text": "in this land.’ Can you possibly guess, Lizzy, who is meant by this?"}
{"doc_id": "gutenberg_1342", "para_id": 9104, "text": "‘This young gentleman is blessed, in a peculiar way, with everything the"}
{"doc_id": "gutenberg_1342", "para_id": 9105, "text": "heart of mortal can most desire,--splendid property, noble kindred, and"}
{"doc_id": "gutenberg_1342", "para_id": 9106, "text": "extensive patronage. Yet, in spite of all these temptations, let me warn"}
{"doc_id": "gutenberg_1342", "para_id": 9107, "text": "my cousin Elizabeth, and yourself, of what evils you may incur by a"}
{"doc_id": "gutenberg_1342", "para_id": 9108, "text": "precipitate closure with this gentleman’s proposals, which, of course,"}
{"doc_id": "gutenberg_1342", "para_id": 9109, "text": "you will be inclined to take immediate advantage of.’ Have you any idea,"}
{"doc_id": "gutenberg_1342", "para_id": 9110, "text": "Lizzy, who this gentleman is? But now it comes out. ‘My motive for"}
{"doc_id": "gutenberg_1342", "para_id": 9111, "text": "cautioning you is as follows:--We have reason to imagine that his aunt,"}
{"doc_id": "gutenberg_1342", "para_id": 9112, "text": "Lady Catherine de Bourgh, does not look on the match with a friendly"}
{"doc_id": "gutenberg_1342", "para_id": 9113, "text": "eye.’ _Mr. Darcy_, you see, is the man! Now, Lizzy, I think I _have_"}
{"doc_id": "gutenberg_1342", "para_id": 9114, "text": "surprised you. Could he, or the Lucases, have pitched on any man, within"}
{"doc_id": "gutenberg_1342", "para_id": 9115, "text": "the circle of our acquaintance, whose name would have given the lie more"}
{"doc_id": "gutenberg_1342", "para_id": 9116, "text": "effectually to what they related? Mr. Darcy, who never looks at any"}
{"doc_id": "gutenberg_1342", "para_id": 9117, "text": "woman but to see a blemish, and who probably never looked at _you_ in"}
{"doc_id": "gutenberg_1342", "para_id": 9118, "text": "Elizabeth tried to join in her father’s pleasantry, but could only force"}
{"doc_id": "gutenberg_1342", "para_id": 9119, "text": "one most reluctant smile. Never had his wit been directed in a manner so"}
{"doc_id": "gutenberg_1342", "para_id": 9120, "text": "“‘After mentioning the likelihood of this marriage to her Ladyship last"}
{"doc_id": "gutenberg_1342", "para_id": 9121, "text": "night, she immediately, with her usual condescension, expressed what she"}
{"doc_id": "gutenberg_1342", "para_id": 9122, "text": "felt on the occasion; when it became apparent, that, on the score of"}
{"doc_id": "gutenberg_1342", "para_id": 9123, "text": "some family objections on the part of my cousin, she would never give"}
{"doc_id": "gutenberg_1342", "para_id": 9124, "text": "her consent to what she termed so disgraceful a match. I thought it my"}
{"doc_id": "gutenberg_1342", "para_id": 9125, "text": "duty to give the speediest intelligence of this to my cousin, that she"}
{"doc_id": "gutenberg_1342", "para_id": 9126, "text": "and her noble admirer may be aware of what they are about, and not run"}
{"doc_id": "gutenberg_1342", "para_id": 9127, "text": "hastily into a marriage which has not been properly sanctioned.’ Mr."}
{"doc_id": "gutenberg_1342", "para_id": 9128, "text": "Collins, moreover, adds, ‘I am truly rejoiced that my cousin Lydia’s sad"}
{"doc_id": "gutenberg_1342", "para_id": 9129, "text": "business has been so well hushed up, and am only concerned that their"}
{"doc_id": "gutenberg_1342", "para_id": 9130, "text": "living together before the marriage took place should be so generally"}
{"doc_id": "gutenberg_1342", "para_id": 9131, "text": "known. I must not, however, neglect the duties of my station, or refrain"}
{"doc_id": "gutenberg_1342", "para_id": 9132, "text": "from declaring my amazement, at hearing that you received the young"}
{"doc_id": "gutenberg_1342", "para_id": 9133, "text": "couple into your house as soon as they were married. It was an"}
{"doc_id": "gutenberg_1342", "para_id": 9134, "text": "encouragement of vice; and had I been the rector of Longbourn, I should"}
{"doc_id": "gutenberg_1342", "para_id": 9135, "text": "very strenuously have opposed it. You ought certainly to forgive them as"}
{"doc_id": "gutenberg_1342", "para_id": 9136, "text": "a Christian, but never to admit them in your sight, or allow their"}
{"doc_id": "gutenberg_1342", "para_id": 9137, "text": "names to be mentioned in your hearing.’ _That_ is his notion of"}
{"doc_id": "gutenberg_1342", "para_id": 9138, "text": "Christian forgiveness! The rest of his letter is only about his dear"}
{"doc_id": "gutenberg_1342", "para_id": 9139, "text": "Charlotte’s situation, and his expectation of a young olive-branch. But,"}
{"doc_id": "gutenberg_1342", "para_id": 9140, "text": "Lizzy, you look as if you did not enjoy it. You are not going to be"}
{"doc_id": "gutenberg_1342", "para_id": 9141, "text": "_missish_, I hope, and pretend to be affronted at an idle report. For"}
{"doc_id": "gutenberg_1342", "para_id": 9142, "text": "what do we live, but to make sport for our neighbours, and laugh at them"}
{"doc_id": "gutenberg_1342", "para_id": 9143, "text": "“Oh,” cried Elizabeth, “I am exceedingly diverted. But it is so"}
{"doc_id": "gutenberg_1342", "para_id": 9144, "text": "“Yes, _that_ is what makes it amusing. Had they fixed on any other man"}
{"doc_id": "gutenberg_1342", "para_id": 9145, "text": "it would have been nothing; but _his_ perfect indifference and _your_"}
{"doc_id": "gutenberg_1342", "para_id": 9146, "text": "pointed dislike make it so delightfully absurd! Much as I abominate"}
{"doc_id": "gutenberg_1342", "para_id": 9147, "text": "writing, I would not give up Mr. Collins’s correspondence for any"}
{"doc_id": "gutenberg_1342", "para_id": 9148, "text": "consideration. Nay, when I read a letter of his, I cannot help giving"}
{"doc_id": "gutenberg_1342", "para_id": 9149, "text": "him the preference even over Wickham, much as I value the impudence and"}
{"doc_id": "gutenberg_1342", "para_id": 9150, "text": "hypocrisy of my son-in-law. And pray, Lizzy, what said Lady Catherine"}
{"doc_id": "gutenberg_1342", "para_id": 9151, "text": "about this report? Did she call to refuse her consent?”"}
{"doc_id": "gutenberg_1342", "para_id": 9152, "text": "To this question his daughter replied only with a laugh; and as it had"}
{"doc_id": "gutenberg_1342", "para_id": 9153, "text": "been asked without the least suspicion, she was not distressed by his"}
{"doc_id": "gutenberg_1342", "para_id": 9154, "text": "repeating it. Elizabeth had never been more at a loss to make her"}
{"doc_id": "gutenberg_1342", "para_id": 9155, "text": "feelings appear what they were not. It was necessary to laugh when she"}
{"doc_id": "gutenberg_1342", "para_id": 9156, "text": "would rather have cried. Her father had most cruelly mortified her by"}
{"doc_id": "gutenberg_1342", "para_id": 9157, "text": "what he said of Mr. Darcy’s indifference; and she could do nothing but"}
{"doc_id": "gutenberg_1342", "para_id": 9158, "text": "wonder at such a want of penetration, or fear that, perhaps, instead of"}
{"doc_id": "gutenberg_1342", "para_id": 9159, "text": "his seeing too _little_, she might have fancied too _much_."}
{"doc_id": "gutenberg_1342", "para_id": 9160, "text": "Instead of receiving any such letter of excuse from his friend, as"}
{"doc_id": "gutenberg_1342", "para_id": 9161, "text": "Elizabeth half expected Mr. Bingley to do, he was able to bring Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 9162, "text": "with him to Longbourn before many days had passed after Lady Catherine’s"}
{"doc_id": "gutenberg_1342", "para_id": 9163, "text": "visit. The gentlemen arrived early; and, before Mrs. Bennet had time to"}
{"doc_id": "gutenberg_1342", "para_id": 9164, "text": "tell him of their having seen his aunt, of which her daughter sat in"}
{"doc_id": "gutenberg_1342", "para_id": 9165, "text": "momentary dread, Bingley, who wanted to be alone with Jane, proposed"}
{"doc_id": "gutenberg_1342", "para_id": 9166, "text": "their all walking out. It was agreed to. Mrs. Bennet was not in the"}
{"doc_id": "gutenberg_1342", "para_id": 9167, "text": "habit of walking, Mary could never spare time, but the remaining five"}
{"doc_id": "gutenberg_1342", "para_id": 9168, "text": "set off together. Bingley and Jane, however, soon allowed the others to"}
{"doc_id": "gutenberg_1342", "para_id": 9169, "text": "outstrip them. They lagged behind, while Elizabeth, Kitty, and Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 9170, "text": "were to entertain each other. Very little was said by either; Kitty was"}
{"doc_id": "gutenberg_1342", "para_id": 9171, "text": "too much afraid of him to talk; Elizabeth was secretly forming a"}
{"doc_id": "gutenberg_1342", "para_id": 9172, "text": "desperate resolution; and, perhaps, he might be doing the same."}
{"doc_id": "gutenberg_1342", "para_id": 9173, "text": "They walked towards the Lucases’, because Kitty wished to call upon"}
{"doc_id": "gutenberg_1342", "para_id": 9174, "text": "Maria; and as Elizabeth saw no occasion for making it a general concern,"}
{"doc_id": "gutenberg_1342", "para_id": 9175, "text": "when Kitty left them she went boldly on with him alone. Now was the"}
{"doc_id": "gutenberg_1342", "para_id": 9176, "text": "moment for her resolution to be executed; and while her courage was"}
{"doc_id": "gutenberg_1342", "para_id": 9177, "text": "“Mr. Darcy, I am a very selfish creature, and for the sake of giving"}
{"doc_id": "gutenberg_1342", "para_id": 9178, "text": "relief to my own feelings care not how much I may be wounding yours. I"}
{"doc_id": "gutenberg_1342", "para_id": 9179, "text": "can no longer help thanking you for your unexampled kindness to my poor"}
{"doc_id": "gutenberg_1342", "para_id": 9180, "text": "sister. Ever since I have known it I have been most anxious to"}
{"doc_id": "gutenberg_1342", "para_id": 9181, "text": "acknowledge to you how gratefully I feel it. Were it known to the rest"}
{"doc_id": "gutenberg_1342", "para_id": 9182, "text": "of my family I should not have merely my own gratitude to express.”"}
{"doc_id": "gutenberg_1342", "para_id": 9183, "text": "“I am sorry, exceedingly sorry,” replied Darcy, in a tone of surprise"}
{"doc_id": "gutenberg_1342", "para_id": 9184, "text": "and emotion, “that you have ever been informed of what may, in a"}
{"doc_id": "gutenberg_1342", "para_id": 9185, "text": "mistaken light, have given you uneasiness. I did not think Mrs. Gardiner"}
{"doc_id": "gutenberg_1342", "para_id": 9186, "text": "“You must not blame my aunt. Lydia’s thoughtlessness first betrayed to"}
{"doc_id": "gutenberg_1342", "para_id": 9187, "text": "me that you had been concerned in the matter; and, of course, I could"}
{"doc_id": "gutenberg_1342", "para_id": 9188, "text": "not rest till I knew the particulars. Let me thank you again and again,"}
{"doc_id": "gutenberg_1342", "para_id": 9189, "text": "in the name of all my family, for that generous compassion which induced"}
{"doc_id": "gutenberg_1342", "para_id": 9190, "text": "you to take so much trouble, and bear so many mortifications, for the"}
{"doc_id": "gutenberg_1342", "para_id": 9191, "text": "“If you _will_ thank me,” he replied, “let it be for yourself alone."}
{"doc_id": "gutenberg_1342", "para_id": 9192, "text": "That the wish of giving happiness to you might add force to the other"}
{"doc_id": "gutenberg_1342", "para_id": 9193, "text": "inducements which led me on, I shall not attempt to deny. But your"}
{"doc_id": "gutenberg_1342", "para_id": 9194, "text": "_family_ owe me nothing. Much as I respect them, I believe I thought"}
{"doc_id": "gutenberg_1342", "para_id": 9195, "text": "Elizabeth was too much embarrassed to say a word. After a short pause,"}
{"doc_id": "gutenberg_1342", "para_id": 9196, "text": "her companion added, “You are too generous to trifle with me. If your"}
{"doc_id": "gutenberg_1342", "para_id": 9197, "text": "feelings are still what they were last April, tell me so at once. _My_"}
{"doc_id": "gutenberg_1342", "para_id": 9198, "text": "affections and wishes are unchanged; but one word from you will silence"}
{"doc_id": "gutenberg_1342", "para_id": 9199, "text": "Elizabeth, feeling all the more than common awkwardness and anxiety of"}
{"doc_id": "gutenberg_1342", "para_id": 9200, "text": "his situation, now forced herself to speak; and immediately, though not"}
{"doc_id": "gutenberg_1342", "para_id": 9201, "text": "very fluently, gave him to understand that her sentiments had undergone"}
{"doc_id": "gutenberg_1342", "para_id": 9202, "text": "so material a change since the period to which he alluded, as to make"}
{"doc_id": "gutenberg_1342", "para_id": 9203, "text": "her receive with gratitude and pleasure his present assurances. The"}
{"doc_id": "gutenberg_1342", "para_id": 9204, "text": "happiness which this reply produced was such as he had probably never"}
{"doc_id": "gutenberg_1342", "para_id": 9205, "text": "felt before; and he expressed himself on the occasion as sensibly and as"}
{"doc_id": "gutenberg_1342", "para_id": 9206, "text": "warmly as a man violently in love can be supposed to do. Had Elizabeth"}
{"doc_id": "gutenberg_1342", "para_id": 9207, "text": "been able to encounter his eyes, she might have seen how well the"}
{"doc_id": "gutenberg_1342", "para_id": 9208, "text": "expression of heartfelt delight diffused over his face became him: but"}
{"doc_id": "gutenberg_1342", "para_id": 9209, "text": "though she could not look she could listen; and he told her of feelings"}
{"doc_id": "gutenberg_1342", "para_id": 9210, "text": "which, in proving of what importance she was to him, made his affection"}
{"doc_id": "gutenberg_1342", "para_id": 9211, "text": "They walked on without knowing in what direction. There was too much to"}
{"doc_id": "gutenberg_1342", "para_id": 9212, "text": "be thought, and felt, and said, for attention to any other objects. She"}
{"doc_id": "gutenberg_1342", "para_id": 9213, "text": "soon learnt that they were indebted for their present good understanding"}
{"doc_id": "gutenberg_1342", "para_id": 9214, "text": "to the efforts of his aunt, who _did_ call on him in her return through"}
{"doc_id": "gutenberg_1342", "para_id": 9215, "text": "London, and there relate her journey to Longbourn, its motive, and the"}
{"doc_id": "gutenberg_1342", "para_id": 9216, "text": "substance of her conversation with Elizabeth; dwelling emphatically on"}
{"doc_id": "gutenberg_1342", "para_id": 9217, "text": "every expression of the latter, which, in her Ladyship’s apprehension,"}
{"doc_id": "gutenberg_1342", "para_id": 9218, "text": "peculiarly denoted her perverseness and assurance, in the belief that"}
{"doc_id": "gutenberg_1342", "para_id": 9219, "text": "such a relation must assist her endeavours to obtain that promise from"}
{"doc_id": "gutenberg_1342", "para_id": 9220, "text": "her nephew which _she_ had refused to give. But, unluckily for her"}
{"doc_id": "gutenberg_1342", "para_id": 9221, "text": "Ladyship, its effect had been exactly contrariwise."}
{"doc_id": "gutenberg_1342", "para_id": 9222, "text": "“It taught me to hope,” said he, “as I had scarcely ever allowed myself"}
{"doc_id": "gutenberg_1342", "para_id": 9223, "text": "to hope before. I knew enough of your disposition to be certain, that"}
{"doc_id": "gutenberg_1342", "para_id": 9224, "text": "had you been absolutely, irrevocably decided against me, you would have"}
{"doc_id": "gutenberg_1342", "para_id": 9225, "text": "acknowledged it to Lady Catherine frankly and openly.”"}
{"doc_id": "gutenberg_1342", "para_id": 9226, "text": "Elizabeth coloured and laughed as she replied, “Yes, you know enough of"}
{"doc_id": "gutenberg_1342", "para_id": 9227, "text": "my _frankness_ to believe me capable of _that_. After abusing you so"}
{"doc_id": "gutenberg_1342", "para_id": 9228, "text": "abominably to your face, I could have no scruple in abusing you to all"}
{"doc_id": "gutenberg_1342", "para_id": 9229, "text": "“What did you say of me that I did not deserve? For though your"}
{"doc_id": "gutenberg_1342", "para_id": 9230, "text": "accusations were ill-founded, formed on mistaken premises, my behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 9231, "text": "to you at the time had merited the severest reproof. It was"}
{"doc_id": "gutenberg_1342", "para_id": 9232, "text": "unpardonable. I cannot think of it without abhorrence.”"}
{"doc_id": "gutenberg_1342", "para_id": 9233, "text": "“We will not quarrel for the greater share of blame annexed to that"}
{"doc_id": "gutenberg_1342", "para_id": 9234, "text": "evening,” said Elizabeth. “The conduct of neither, if strictly"}
{"doc_id": "gutenberg_1342", "para_id": 9235, "text": "examined, will be irreproachable; but since then we have both, I hope,"}
{"doc_id": "gutenberg_1342", "para_id": 9236, "text": "“I cannot be so easily reconciled to myself. The recollection of what I"}
{"doc_id": "gutenberg_1342", "para_id": 9237, "text": "then said, of my conduct, my manners, my expressions during the whole of"}
{"doc_id": "gutenberg_1342", "para_id": 9238, "text": "it, is now, and has been many months, inexpressibly painful to me. Your"}
{"doc_id": "gutenberg_1342", "para_id": 9239, "text": "reproof, so well applied, I shall never forget: ‘Had you behaved in a"}
{"doc_id": "gutenberg_1342", "para_id": 9240, "text": "more gentlemanlike manner.’ Those were your words. You know not, you can"}
{"doc_id": "gutenberg_1342", "para_id": 9241, "text": "scarcely conceive, how they have tortured me; though it was some time, I"}
{"doc_id": "gutenberg_1342", "para_id": 9242, "text": "confess, before I was reasonable enough to allow their justice.”"}
{"doc_id": "gutenberg_1342", "para_id": 9243, "text": "“I was certainly very far from expecting them to make so strong an"}
{"doc_id": "gutenberg_1342", "para_id": 9244, "text": "impression. I had not the smallest idea of their being ever felt in such"}
{"doc_id": "gutenberg_1342", "para_id": 9245, "text": "“I can easily believe it. You thought me then devoid of every proper"}
{"doc_id": "gutenberg_1342", "para_id": 9246, "text": "feeling, I am sure you did. The turn of your countenance I shall never"}
{"doc_id": "gutenberg_1342", "para_id": 9247, "text": "forget, as you said that I could not have addressed you in any possible"}
{"doc_id": "gutenberg_1342", "para_id": 9248, "text": "“Oh, do not repeat what I then said. These recollections will not do at"}
{"doc_id": "gutenberg_1342", "para_id": 9249, "text": "all. I assure you that I have long been most heartily ashamed of it.”"}
{"doc_id": "gutenberg_1342", "para_id": 9250, "text": "Darcy mentioned his letter. “Did it,” said he,--“did it _soon_ make you"}
{"doc_id": "gutenberg_1342", "para_id": 9251, "text": "think better of me? Did you, on reading it, give any credit to its"}
{"doc_id": "gutenberg_1342", "para_id": 9252, "text": "She explained what its effects on her had been, and how gradually all"}
{"doc_id": "gutenberg_1342", "para_id": 9253, "text": "“I knew,” said he, “that what I wrote must give you pain, but it was"}
{"doc_id": "gutenberg_1342", "para_id": 9254, "text": "necessary. I hope you have destroyed the letter. There was one part,"}
{"doc_id": "gutenberg_1342", "para_id": 9255, "text": "especially the opening of it, which I should dread your having the power"}
{"doc_id": "gutenberg_1342", "para_id": 9256, "text": "of reading again. I can remember some expressions which might justly"}
{"doc_id": "gutenberg_1342", "para_id": 9257, "text": "“The letter shall certainly be burnt, if you believe it essential to the"}
{"doc_id": "gutenberg_1342", "para_id": 9258, "text": "preservation of my regard; but, though we have both reason to think my"}
{"doc_id": "gutenberg_1342", "para_id": 9259, "text": "opinions not entirely unalterable, they are not, I hope, quite so easily"}
{"doc_id": "gutenberg_1342", "para_id": 9260, "text": "“When I wrote that letter,” replied Darcy, “I believed myself perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 9261, "text": "calm and cool; but I am since convinced that it was written in a"}
{"doc_id": "gutenberg_1342", "para_id": 9262, "text": "“The letter, perhaps, began in bitterness, but it did not end so. The"}
{"doc_id": "gutenberg_1342", "para_id": 9263, "text": "adieu is charity itself. But think no more of the letter. The feelings"}
{"doc_id": "gutenberg_1342", "para_id": 9264, "text": "of the person who wrote and the person who received it are now so widely"}
{"doc_id": "gutenberg_1342", "para_id": 9265, "text": "different from what they were then, that every unpleasant circumstance"}
{"doc_id": "gutenberg_1342", "para_id": 9266, "text": "attending it ought to be forgotten. You must learn some of my"}
{"doc_id": "gutenberg_1342", "para_id": 9267, "text": "philosophy. Think only of the past as its remembrance gives you"}
{"doc_id": "gutenberg_1342", "para_id": 9268, "text": "“I cannot give you credit for any philosophy of the kind. _Your_"}
{"doc_id": "gutenberg_1342", "para_id": 9269, "text": "retrospections must be so totally void of reproach, that the contentment"}
{"doc_id": "gutenberg_1342", "para_id": 9270, "text": "arising from them is not of philosophy, but, what is much better, of"}
{"doc_id": "gutenberg_1342", "para_id": 9271, "text": "ignorance. But with _me_, it is not so. Painful recollections will"}
{"doc_id": "gutenberg_1342", "para_id": 9272, "text": "intrude, which cannot, which ought not to be repelled. I have been a"}
{"doc_id": "gutenberg_1342", "para_id": 9273, "text": "selfish being all my life, in practice, though not in principle. As a"}
{"doc_id": "gutenberg_1342", "para_id": 9274, "text": "child I was taught what was _right_, but I was not taught to correct my"}
{"doc_id": "gutenberg_1342", "para_id": 9275, "text": "temper. I was given good principles, but left to follow them in pride"}
{"doc_id": "gutenberg_1342", "para_id": 9276, "text": "and conceit. Unfortunately an only son (for many years an only _child_),"}
{"doc_id": "gutenberg_1342", "para_id": 9277, "text": "I was spoiled by my parents, who, though good themselves, (my father"}
{"doc_id": "gutenberg_1342", "para_id": 9278, "text": "particularly, all that was benevolent and amiable,) allowed, encouraged,"}
{"doc_id": "gutenberg_1342", "para_id": 9279, "text": "almost taught me to be selfish and overbearing, to care for none beyond"}
{"doc_id": "gutenberg_1342", "para_id": 9280, "text": "my own family circle, to think meanly of all the rest of the world, to"}
{"doc_id": "gutenberg_1342", "para_id": 9281, "text": "_wish_ at least to think meanly of their sense and worth compared with"}
{"doc_id": "gutenberg_1342", "para_id": 9282, "text": "my own. Such I was, from eight to eight-and-twenty; and such I might"}
{"doc_id": "gutenberg_1342", "para_id": 9283, "text": "still have been but for you, dearest, loveliest Elizabeth! What do I not"}
{"doc_id": "gutenberg_1342", "para_id": 9284, "text": "owe you! You taught me a lesson, hard indeed at first, but most"}
{"doc_id": "gutenberg_1342", "para_id": 9285, "text": "advantageous. By you, I was properly humbled. I came to you without a"}
{"doc_id": "gutenberg_1342", "para_id": 9286, "text": "doubt of my reception. You showed me how insufficient were all my"}
{"doc_id": "gutenberg_1342", "para_id": 9287, "text": "pretensions to please a woman worthy of being pleased.”"}
{"doc_id": "gutenberg_1342", "para_id": 9288, "text": "“Indeed I had. What will you think of my vanity? I believed you to be"}
{"doc_id": "gutenberg_1342", "para_id": 9289, "text": "“My manners must have been in fault, but not intentionally, I assure"}
{"doc_id": "gutenberg_1342", "para_id": 9290, "text": "you. I never meant to deceive you, but my spirits might often lead me"}
{"doc_id": "gutenberg_1342", "para_id": 9291, "text": "wrong. How you must have hated me after _that_ evening!”"}
{"doc_id": "gutenberg_1342", "para_id": 9292, "text": "“Hate you! I was angry, perhaps, at first, but my anger soon began to"}
{"doc_id": "gutenberg_1342", "para_id": 9293, "text": "“I am almost afraid of asking what you thought of me when we met at"}
{"doc_id": "gutenberg_1342", "para_id": 9294, "text": "“Your surprise could not be greater than _mine_ in being noticed by you."}
{"doc_id": "gutenberg_1342", "para_id": 9295, "text": "My conscience told me that I deserved no extraordinary politeness, and I"}
{"doc_id": "gutenberg_1342", "para_id": 9296, "text": "confess that I did not expect to receive _more_ than my due.”"}
{"doc_id": "gutenberg_1342", "para_id": 9297, "text": "“My object _then_,” replied Darcy, “was to show you, by every civility"}
{"doc_id": "gutenberg_1342", "para_id": 9298, "text": "in my power, that I was not so mean as to resent the past; and I hoped"}
{"doc_id": "gutenberg_1342", "para_id": 9299, "text": "to obtain your forgiveness, to lessen your ill opinion, by letting you"}
{"doc_id": "gutenberg_1342", "para_id": 9300, "text": "see that your reproofs had been attended to. How soon any other wishes"}
{"doc_id": "gutenberg_1342", "para_id": 9301, "text": "introduced themselves, I can hardly tell, but I believe in about half"}
{"doc_id": "gutenberg_1342", "para_id": 9302, "text": "He then told her of Georgiana’s delight in her acquaintance, and of her"}
{"doc_id": "gutenberg_1342", "para_id": 9303, "text": "disappointment at its sudden interruption; which naturally leading to"}
{"doc_id": "gutenberg_1342", "para_id": 9304, "text": "the cause of that interruption, she soon learnt that his resolution of"}
{"doc_id": "gutenberg_1342", "para_id": 9305, "text": "following her from Derbyshire in quest of her sister had been formed"}
{"doc_id": "gutenberg_1342", "para_id": 9306, "text": "before he quitted the inn, and that his gravity and thoughtfulness there"}
{"doc_id": "gutenberg_1342", "para_id": 9307, "text": "had arisen from no other struggles than what such a purpose must"}
{"doc_id": "gutenberg_1342", "para_id": 9308, "text": "She expressed her gratitude again, but it was too painful a subject to"}
{"doc_id": "gutenberg_1342", "para_id": 9309, "text": "After walking several miles in a leisurely manner, and too busy to know"}
{"doc_id": "gutenberg_1342", "para_id": 9310, "text": "anything about it, they found at last, on examining their watches, that"}
{"doc_id": "gutenberg_1342", "para_id": 9311, "text": "“What could have become of Mr. Bingley and Jane?” was a wonder which"}
{"doc_id": "gutenberg_1342", "para_id": 9312, "text": "introduced the discussion of _their_ affairs. Darcy was delighted with"}
{"doc_id": "gutenberg_1342", "para_id": 9313, "text": "their engagement; his friend had given him the earliest information of"}
{"doc_id": "gutenberg_1342", "para_id": 9314, "text": "“I must ask whether you were surprised?” said Elizabeth."}
{"doc_id": "gutenberg_1342", "para_id": 9315, "text": "“Not at all. When I went away, I felt that it would soon happen.”"}
{"doc_id": "gutenberg_1342", "para_id": 9316, "text": "“That is to say, you had given your permission. I guessed as much.” And"}
{"doc_id": "gutenberg_1342", "para_id": 9317, "text": "though he exclaimed at the term, she found that it had been pretty much"}
{"doc_id": "gutenberg_1342", "para_id": 9318, "text": "“On the evening before my going to London,” said he, “I made a"}
{"doc_id": "gutenberg_1342", "para_id": 9319, "text": "confession to him, which I believe I ought to have made long ago. I told"}
{"doc_id": "gutenberg_1342", "para_id": 9320, "text": "him of all that had occurred to make my former interference in his"}
{"doc_id": "gutenberg_1342", "para_id": 9321, "text": "affairs absurd and impertinent. His surprise was great. He had never had"}
{"doc_id": "gutenberg_1342", "para_id": 9322, "text": "the slightest suspicion. I told him, moreover, that I believed myself"}
{"doc_id": "gutenberg_1342", "para_id": 9323, "text": "mistaken in supposing, as I had done, that your sister was indifferent"}
{"doc_id": "gutenberg_1342", "para_id": 9324, "text": "to him; and as I could easily perceive that his attachment to her was"}
{"doc_id": "gutenberg_1342", "para_id": 9325, "text": "unabated, I felt no doubt of their happiness together.”"}
{"doc_id": "gutenberg_1342", "para_id": 9326, "text": "Elizabeth could not help smiling at his easy manner of directing his"}
{"doc_id": "gutenberg_1342", "para_id": 9327, "text": "“Did you speak from your own observation,” said she, “when you told him"}
{"doc_id": "gutenberg_1342", "para_id": 9328, "text": "that my sister loved him, or merely from my information last spring?”"}
{"doc_id": "gutenberg_1342", "para_id": 9329, "text": "“From the former. I had narrowly observed her, during the two visits"}
{"doc_id": "gutenberg_1342", "para_id": 9330, "text": "which I had lately made her here; and I was convinced of her affection.”"}
{"doc_id": "gutenberg_1342", "para_id": 9331, "text": "“And your assurance of it, I suppose, carried immediate conviction to"}
{"doc_id": "gutenberg_1342", "para_id": 9332, "text": "“It did. Bingley is most unaffectedly modest. His diffidence had"}
{"doc_id": "gutenberg_1342", "para_id": 9333, "text": "prevented his depending on his own judgment in so anxious a case, but"}
{"doc_id": "gutenberg_1342", "para_id": 9334, "text": "his reliance on mine made everything easy. I was obliged to confess one"}
{"doc_id": "gutenberg_1342", "para_id": 9335, "text": "thing, which for a time, and not unjustly, offended him. I could not"}
{"doc_id": "gutenberg_1342", "para_id": 9336, "text": "allow myself to conceal that your sister had been in town three months"}
{"doc_id": "gutenberg_1342", "para_id": 9337, "text": "last winter, that I had known it, and purposely kept it from him. He was"}
{"doc_id": "gutenberg_1342", "para_id": 9338, "text": "angry. But his anger, I am persuaded, lasted no longer than he remained"}
{"doc_id": "gutenberg_1342", "para_id": 9339, "text": "in any doubt of your sister’s sentiments. He has heartily forgiven me"}
{"doc_id": "gutenberg_1342", "para_id": 9340, "text": "Elizabeth longed to observe that Mr. Bingley had been a most delightful"}
{"doc_id": "gutenberg_1342", "para_id": 9341, "text": "friend; so easily guided that his worth was invaluable; but she checked"}
{"doc_id": "gutenberg_1342", "para_id": 9342, "text": "herself. She remembered that he had yet to learn to be laughed at, and"}
{"doc_id": "gutenberg_1342", "para_id": 9343, "text": "it was rather too early to begin. In anticipating the happiness of"}
{"doc_id": "gutenberg_1342", "para_id": 9344, "text": "Bingley, which of course was to be inferior only to his own, he"}
{"doc_id": "gutenberg_1342", "para_id": 9345, "text": "continued the conversation till they reached the house. In the hall they"}
{"doc_id": "gutenberg_1342", "para_id": 9346, "text": "“My dear Lizzy, where can you have been walking to?” was a question"}
{"doc_id": "gutenberg_1342", "para_id": 9347, "text": "which Elizabeth received from Jane as soon as she entered the room, and"}
{"doc_id": "gutenberg_1342", "para_id": 9348, "text": "from all the others when they sat down to table. She had only to say in"}
{"doc_id": "gutenberg_1342", "para_id": 9349, "text": "reply, that they had wandered about till she was beyond her own"}
{"doc_id": "gutenberg_1342", "para_id": 9350, "text": "knowledge. She coloured as she spoke; but neither that, nor anything"}
{"doc_id": "gutenberg_1342", "para_id": 9351, "text": "The evening passed quietly, unmarked by anything extraordinary. The"}
{"doc_id": "gutenberg_1342", "para_id": 9352, "text": "acknowledged lovers talked and laughed; the unacknowledged were silent."}
{"doc_id": "gutenberg_1342", "para_id": 9353, "text": "Darcy was not of a disposition in which happiness overflows in mirth;"}
{"doc_id": "gutenberg_1342", "para_id": 9354, "text": "and Elizabeth, agitated and confused, rather _knew_ that she was happy"}
{"doc_id": "gutenberg_1342", "para_id": 9355, "text": "than _felt_ herself to be so; for, besides the immediate embarrassment,"}
{"doc_id": "gutenberg_1342", "para_id": 9356, "text": "there were other evils before her. She anticipated what would be felt in"}
{"doc_id": "gutenberg_1342", "para_id": 9357, "text": "the family when her situation became known: she was aware that no one"}
{"doc_id": "gutenberg_1342", "para_id": 9358, "text": "liked him but Jane; and even feared that with the others it was a"}
{"doc_id": "gutenberg_1342", "para_id": 9359, "text": "_dislike_ which not all his fortune and consequence might do away."}
{"doc_id": "gutenberg_1342", "para_id": 9360, "text": "At night she opened her heart to Jane. Though suspicion was very far"}
{"doc_id": "gutenberg_1342", "para_id": 9361, "text": "from Miss Bennet’s general habits, she was absolutely incredulous here."}
{"doc_id": "gutenberg_1342", "para_id": 9362, "text": "“You are joking, Lizzy. This cannot be! Engaged to Mr. Darcy! No, no,"}
{"doc_id": "gutenberg_1342", "para_id": 9363, "text": "you shall not deceive me: I know it to be impossible.”"}
{"doc_id": "gutenberg_1342", "para_id": 9364, "text": "“This is a wretched beginning, indeed! My sole dependence was on you;"}
{"doc_id": "gutenberg_1342", "para_id": 9365, "text": "and I am sure nobody else will believe me, if you do not. Yet, indeed, I"}
{"doc_id": "gutenberg_1342", "para_id": 9366, "text": "am in earnest. I speak nothing but the truth. He still loves me, and we"}
{"doc_id": "gutenberg_1342", "para_id": 9367, "text": "Jane looked at her doubtingly. “Oh, Lizzy! it cannot be. I know how much"}
{"doc_id": "gutenberg_1342", "para_id": 9368, "text": "“You know nothing of the matter. _That_ is all to be forgot. Perhaps I"}
{"doc_id": "gutenberg_1342", "para_id": 9369, "text": "did not always love him so well as I do now; but in such cases as these"}
{"doc_id": "gutenberg_1342", "para_id": 9370, "text": "a good memory is unpardonable. This is the last time I shall ever"}
{"doc_id": "gutenberg_1342", "para_id": 9371, "text": "Miss Bennet still looked all amazement. Elizabeth again, and more"}
{"doc_id": "gutenberg_1342", "para_id": 9372, "text": "“Good heaven! can it be really so? Yet now I must believe you,” cried"}
{"doc_id": "gutenberg_1342", "para_id": 9373, "text": "Jane. “My dear, dear Lizzy, I would, I do congratulate you; but are you"}
{"doc_id": "gutenberg_1342", "para_id": 9374, "text": "certain--forgive the question--are you quite certain that you can be"}
{"doc_id": "gutenberg_1342", "para_id": 9375, "text": "“There can be no doubt of that. It is settled between us already that we"}
{"doc_id": "gutenberg_1342", "para_id": 9376, "text": "are to be the happiest couple in the world. But are you pleased, Jane?"}
{"doc_id": "gutenberg_1342", "para_id": 9377, "text": "“Very, very much. Nothing could give either Bingley or myself more"}
{"doc_id": "gutenberg_1342", "para_id": 9378, "text": "delight. But we considered it, we talked of it as impossible. And do you"}
{"doc_id": "gutenberg_1342", "para_id": 9379, "text": "really love him quite well enough? Oh, Lizzy! do anything rather than"}
{"doc_id": "gutenberg_1342", "para_id": 9380, "text": "marry without affection. Are you quite sure that you feel what you ought"}
{"doc_id": "gutenberg_1342", "para_id": 9381, "text": "“Oh, yes! You will only think I feel _more_ than I ought to do when I"}
{"doc_id": "gutenberg_1342", "para_id": 9382, "text": "“Why, I must confess that I love him better than I do Bingley. I am"}
{"doc_id": "gutenberg_1342", "para_id": 9383, "text": "“My dearest sister, now be, _be_ serious. I want to talk very seriously."}
{"doc_id": "gutenberg_1342", "para_id": 9384, "text": "Let me know everything that I am to know without delay. Will you tell me"}
{"doc_id": "gutenberg_1342", "para_id": 9385, "text": "“It has been coming on so gradually, that I hardly know when it began;"}
{"doc_id": "gutenberg_1342", "para_id": 9386, "text": "but I believe I must date it from my first seeing his beautiful grounds"}
{"doc_id": "gutenberg_1342", "para_id": 9387, "text": "Another entreaty that she would be serious, however, produced the"}
{"doc_id": "gutenberg_1342", "para_id": 9388, "text": "desired effect; and she soon satisfied Jane by her solemn assurances of"}
{"doc_id": "gutenberg_1342", "para_id": 9389, "text": "attachment. When convinced on that article, Miss Bennet had nothing"}
{"doc_id": "gutenberg_1342", "para_id": 9390, "text": "“Now I am quite happy,” said she, “for you will be as happy as myself. I"}
{"doc_id": "gutenberg_1342", "para_id": 9391, "text": "always had a value for him. Were it for nothing but his love of you, I"}
{"doc_id": "gutenberg_1342", "para_id": 9392, "text": "must always have esteemed him; but now, as Bingley’s friend and your"}
{"doc_id": "gutenberg_1342", "para_id": 9393, "text": "husband, there can be only Bingley and yourself more dear to me. But,"}
{"doc_id": "gutenberg_1342", "para_id": 9394, "text": "Lizzy, you have been very sly, very reserved with me. How little did you"}
{"doc_id": "gutenberg_1342", "para_id": 9395, "text": "tell me of what passed at Pemberley and Lambton! I owe all that I know"}
{"doc_id": "gutenberg_1342", "para_id": 9396, "text": "Elizabeth told her the motives of her secrecy. She had been unwilling to"}
{"doc_id": "gutenberg_1342", "para_id": 9397, "text": "mention Bingley; and the unsettled state of her own feelings had made"}
{"doc_id": "gutenberg_1342", "para_id": 9398, "text": "her equally avoid the name of his friend: but now she would no longer"}
{"doc_id": "gutenberg_1342", "para_id": 9399, "text": "conceal from her his share in Lydia’s marriage. All was acknowledged,"}
{"doc_id": "gutenberg_1342", "para_id": 9400, "text": "“Good gracious!” cried Mrs. Bennet, as she stood at a window the next"}
{"doc_id": "gutenberg_1342", "para_id": 9401, "text": "morning, “if that disagreeable Mr. Darcy is not coming here again with"}
{"doc_id": "gutenberg_1342", "para_id": 9402, "text": "our dear Bingley! What can he mean by being so tiresome as to be always"}
{"doc_id": "gutenberg_1342", "para_id": 9403, "text": "coming here? I had no notion but he would go a-shooting, or something or"}
{"doc_id": "gutenberg_1342", "para_id": 9404, "text": "other, and not disturb us with his company. What shall we do with him?"}
{"doc_id": "gutenberg_1342", "para_id": 9405, "text": "Lizzy, you must walk out with him again, that he may not be in Bingley’s"}
{"doc_id": "gutenberg_1342", "para_id": 9406, "text": "Elizabeth could hardly help laughing at so convenient a proposal; yet"}
{"doc_id": "gutenberg_1342", "para_id": 9407, "text": "was really vexed that her mother should be always giving him such an"}
{"doc_id": "gutenberg_1342", "para_id": 9408, "text": "As soon as they entered, Bingley looked at her so expressively, and"}
{"doc_id": "gutenberg_1342", "para_id": 9409, "text": "shook hands with such warmth, as left no doubt of his good information;"}
{"doc_id": "gutenberg_1342", "para_id": 9410, "text": "and he soon afterwards said aloud, “Mrs. Bennet, have you no more lanes"}
{"doc_id": "gutenberg_1342", "para_id": 9411, "text": "hereabouts in which Lizzy may lose her way again to-day?”"}
{"doc_id": "gutenberg_1342", "para_id": 9412, "text": "“I advise Mr. Darcy, and Lizzy, and Kitty,” said Mrs. Bennet, “to walk"}
{"doc_id": "gutenberg_1342", "para_id": 9413, "text": "to Oakham Mount this morning. It is a nice long walk, and Mr. Darcy has"}
{"doc_id": "gutenberg_1342", "para_id": 9414, "text": "“It may do very well for the others,” replied Mr. Bingley; “but I am"}
{"doc_id": "gutenberg_1342", "para_id": 9415, "text": "sure it will be too much for Kitty. Won’t it, Kitty?”"}
{"doc_id": "gutenberg_1342", "para_id": 9416, "text": "Kitty owned that she had rather stay at home. Darcy professed a great"}
{"doc_id": "gutenberg_1342", "para_id": 9417, "text": "curiosity to see the view from the Mount, and Elizabeth silently"}
{"doc_id": "gutenberg_1342", "para_id": 9418, "text": "consented. As she went upstairs to get ready, Mrs. Bennet followed her,"}
{"doc_id": "gutenberg_1342", "para_id": 9419, "text": "“I am quite sorry, Lizzy, that you should be forced to have that"}
{"doc_id": "gutenberg_1342", "para_id": 9420, "text": "disagreeable man all to yourself; but I hope you will not mind it. It is"}
{"doc_id": "gutenberg_1342", "para_id": 9421, "text": "all for Jane’s sake, you know; and there is no occasion for talking to"}
{"doc_id": "gutenberg_1342", "para_id": 9422, "text": "him except just now and then; so do not put yourself to inconvenience.”"}
{"doc_id": "gutenberg_1342", "para_id": 9423, "text": "During their walk, it was resolved that Mr. Bennet’s consent should be"}
{"doc_id": "gutenberg_1342", "para_id": 9424, "text": "asked in the course of the evening: Elizabeth reserved to herself the"}
{"doc_id": "gutenberg_1342", "para_id": 9425, "text": "application for her mother’s. She could not determine how her mother"}
{"doc_id": "gutenberg_1342", "para_id": 9426, "text": "would take it; sometimes doubting whether all his wealth and grandeur"}
{"doc_id": "gutenberg_1342", "para_id": 9427, "text": "would be enough to overcome her abhorrence of the man; but whether she"}
{"doc_id": "gutenberg_1342", "para_id": 9428, "text": "were violently set against the match, or violently delighted with it, it"}
{"doc_id": "gutenberg_1342", "para_id": 9429, "text": "was certain that her manner would be equally ill adapted to do credit to"}
{"doc_id": "gutenberg_1342", "para_id": 9430, "text": "her sense; and she could no more bear that Mr. Darcy should hear the"}
{"doc_id": "gutenberg_1342", "para_id": 9431, "text": "first raptures of her joy, than the first vehemence of her"}
{"doc_id": "gutenberg_1342", "para_id": 9432, "text": "In the evening, soon after Mr. Bennet withdrew to the library, she saw"}
{"doc_id": "gutenberg_1342", "para_id": 9433, "text": "Mr. Darcy rise also and follow him, and her agitation on seeing it was"}
{"doc_id": "gutenberg_1342", "para_id": 9434, "text": "extreme. She did not fear her father’s opposition, but he was going to"}
{"doc_id": "gutenberg_1342", "para_id": 9435, "text": "be made unhappy, and that it should be through her means; that _she_,"}
{"doc_id": "gutenberg_1342", "para_id": 9436, "text": "his favourite child, should be distressing him by her choice, should be"}
{"doc_id": "gutenberg_1342", "para_id": 9437, "text": "filling him with fears and regrets in disposing of her, was a wretched"}
{"doc_id": "gutenberg_1342", "para_id": 9438, "text": "reflection, and she sat in misery till Mr. Darcy appeared again, when,"}
{"doc_id": "gutenberg_1342", "para_id": 9439, "text": "looking at him, she was a little relieved by his smile. In a few minutes"}
{"doc_id": "gutenberg_1342", "para_id": 9440, "text": "he approached the table where she was sitting with Kitty; and, while"}
{"doc_id": "gutenberg_1342", "para_id": 9441, "text": "pretending to admire her work, said in a whisper, “Go to your father; he"}
{"doc_id": "gutenberg_1342", "para_id": 9442, "text": "Her father was walking about the room, looking grave and anxious."}
{"doc_id": "gutenberg_1342", "para_id": 9443, "text": "“Lizzy,” said he, “what are you doing? Are you out of your senses to be"}
{"doc_id": "gutenberg_1342", "para_id": 9444, "text": "accepting this man? Have not you always hated him?”"}
{"doc_id": "gutenberg_1342", "para_id": 9445, "text": "How earnestly did she then wish that her former opinions had been more"}
{"doc_id": "gutenberg_1342", "para_id": 9446, "text": "reasonable, her expressions more moderate! It would have spared her from"}
{"doc_id": "gutenberg_1342", "para_id": 9447, "text": "explanations and professions which it was exceedingly awkward to give;"}
{"doc_id": "gutenberg_1342", "para_id": 9448, "text": "but they were now necessary, and she assured him, with some confusion,"}
{"doc_id": "gutenberg_1342", "para_id": 9449, "text": "“Or, in other words, you are determined to have him. He is rich, to be"}
{"doc_id": "gutenberg_1342", "para_id": 9450, "text": "sure, and you may have more fine clothes and fine carriages than Jane."}
{"doc_id": "gutenberg_1342", "para_id": 9451, "text": "“Have you any other objection,” said Elizabeth, “than your belief of my"}
{"doc_id": "gutenberg_1342", "para_id": 9452, "text": "“None at all. We all know him to be a proud, unpleasant sort of man; but"}
{"doc_id": "gutenberg_1342", "para_id": 9453, "text": "“I do, I do like him,” she replied, with tears in her eyes; “I love him."}
{"doc_id": "gutenberg_1342", "para_id": 9454, "text": "Indeed he has no improper pride. He is perfectly amiable. You do not"}
{"doc_id": "gutenberg_1342", "para_id": 9455, "text": "know what he really is; then pray do not pain me by speaking of him in"}
{"doc_id": "gutenberg_1342", "para_id": 9456, "text": "“Lizzy,” said her father, “I have given him my consent. He is the kind"}
{"doc_id": "gutenberg_1342", "para_id": 9457, "text": "of man, indeed, to whom I should never dare refuse anything, which he"}
{"doc_id": "gutenberg_1342", "para_id": 9458, "text": "condescended to ask. I now give it to _you_, if you are resolved on"}
{"doc_id": "gutenberg_1342", "para_id": 9459, "text": "having him. But let me advise you to think better of it. I know your"}
{"doc_id": "gutenberg_1342", "para_id": 9460, "text": "disposition, Lizzy. I know that you could be neither happy nor"}
{"doc_id": "gutenberg_1342", "para_id": 9461, "text": "respectable, unless you truly esteemed your husband, unless you looked"}
{"doc_id": "gutenberg_1342", "para_id": 9462, "text": "up to him as a superior. Your lively talents would place you in the"}
{"doc_id": "gutenberg_1342", "para_id": 9463, "text": "greatest danger in an unequal marriage. You could scarcely escape"}
{"doc_id": "gutenberg_1342", "para_id": 9464, "text": "discredit and misery. My child, let me not have the grief of seeing"}
{"doc_id": "gutenberg_1342", "para_id": 9465, "text": "_you_ unable to respect your partner in life. You know not what you are"}
{"doc_id": "gutenberg_1342", "para_id": 9466, "text": "Elizabeth, still more affected, was earnest and solemn in her reply;"}
{"doc_id": "gutenberg_1342", "para_id": 9467, "text": "and, at length, by repeated assurances that Mr. Darcy was really the"}
{"doc_id": "gutenberg_1342", "para_id": 9468, "text": "object of her choice, by explaining the gradual change which her"}
{"doc_id": "gutenberg_1342", "para_id": 9469, "text": "estimation of him had undergone, relating her absolute certainty that"}
{"doc_id": "gutenberg_1342", "para_id": 9470, "text": "his affection was not the work of a day, but had stood the test of many"}
{"doc_id": "gutenberg_1342", "para_id": 9471, "text": "months’ suspense, and enumerating with energy all his good qualities,"}
{"doc_id": "gutenberg_1342", "para_id": 9472, "text": "she did conquer her father’s incredulity, and reconcile him to the"}
{"doc_id": "gutenberg_1342", "para_id": 9473, "text": "“Well, my dear,” said he, when she ceased speaking, “I have no more to"}
{"doc_id": "gutenberg_1342", "para_id": 9474, "text": "say. If this be the case, he deserves you. I could not have parted with"}
{"doc_id": "gutenberg_1342", "para_id": 9475, "text": "To complete the favourable impression, she then told him what Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 9476, "text": "had voluntarily done for Lydia. He heard her with astonishment."}
{"doc_id": "gutenberg_1342", "para_id": 9477, "text": "“This is an evening of wonders, indeed! And so, Darcy did everything;"}
{"doc_id": "gutenberg_1342", "para_id": 9478, "text": "made up the match, gave the money, paid the fellow’s debts, and got him"}
{"doc_id": "gutenberg_1342", "para_id": 9479, "text": "his commission! So much the better. It will save me a world of trouble"}
{"doc_id": "gutenberg_1342", "para_id": 9480, "text": "and economy. Had it been your uncle’s doing, I must and _would_ have"}
{"doc_id": "gutenberg_1342", "para_id": 9481, "text": "paid him; but these violent young lovers carry everything their own"}
{"doc_id": "gutenberg_1342", "para_id": 9482, "text": "way. I shall offer to pay him to-morrow, he will rant and storm about"}
{"doc_id": "gutenberg_1342", "para_id": 9483, "text": "his love for you, and there will be an end of the matter.”"}
{"doc_id": "gutenberg_1342", "para_id": 9484, "text": "He then recollected her embarrassment a few days before on his reading"}
{"doc_id": "gutenberg_1342", "para_id": 9485, "text": "Mr. Collins’s letter; and after laughing at her some time, allowed her"}
{"doc_id": "gutenberg_1342", "para_id": 9486, "text": "at last to go, saying, as she quitted the room, “If any young men come"}
{"doc_id": "gutenberg_1342", "para_id": 9487, "text": "for Mary or Kitty, send them in, for I am quite at leisure.”"}
{"doc_id": "gutenberg_1342", "para_id": 9488, "text": "Elizabeth’s mind was now relieved from a very heavy weight; and, after"}
{"doc_id": "gutenberg_1342", "para_id": 9489, "text": "half an hour’s quiet reflection in her own room, she was able to join"}
{"doc_id": "gutenberg_1342", "para_id": 9490, "text": "the others with tolerable composure. Everything was too recent for"}
{"doc_id": "gutenberg_1342", "para_id": 9491, "text": "gaiety, but the evening passed tranquilly away; there was no longer"}
{"doc_id": "gutenberg_1342", "para_id": 9492, "text": "anything material to be dreaded, and the comfort of ease and familiarity"}
{"doc_id": "gutenberg_1342", "para_id": 9493, "text": "When her mother went up to her dressing-room at night, she followed her,"}
{"doc_id": "gutenberg_1342", "para_id": 9494, "text": "and made the important communication. Its effect was most extraordinary;"}
{"doc_id": "gutenberg_1342", "para_id": 9495, "text": "for, on first hearing it, Mrs. Bennet sat quite still, and unable to"}
{"doc_id": "gutenberg_1342", "para_id": 9496, "text": "utter a syllable. Nor was it under many, many minutes, that she could"}
{"doc_id": "gutenberg_1342", "para_id": 9497, "text": "comprehend what she heard, though not in general backward to credit what"}
{"doc_id": "gutenberg_1342", "para_id": 9498, "text": "was for the advantage of her family, or that came in the shape of a"}
{"doc_id": "gutenberg_1342", "para_id": 9499, "text": "lover to any of them. She began at length to recover, to fidget about in"}
{"doc_id": "gutenberg_1342", "para_id": 9500, "text": "her chair, get up, sit down again, wonder, and bless herself."}
{"doc_id": "gutenberg_1342", "para_id": 9501, "text": "“Good gracious! Lord bless me! only think! dear me! Mr. Darcy! Who would"}
{"doc_id": "gutenberg_1342", "para_id": 9502, "text": "have thought it? And is it really true? Oh, my sweetest Lizzy! how rich"}
{"doc_id": "gutenberg_1342", "para_id": 9503, "text": "and how great you will be! What pin-money, what jewels, what carriages"}
{"doc_id": "gutenberg_1342", "para_id": 9504, "text": "you will have! Jane’s is nothing to it--nothing at all. I am so"}
{"doc_id": "gutenberg_1342", "para_id": 9505, "text": "pleased--so happy. Such a charming man! so handsome! so tall! Oh, my"}
{"doc_id": "gutenberg_1342", "para_id": 9506, "text": "dear Lizzy! pray apologize for my having disliked him so much before. I"}
{"doc_id": "gutenberg_1342", "para_id": 9507, "text": "hope he will overlook it. Dear, dear Lizzy. A house in town! Everything"}
{"doc_id": "gutenberg_1342", "para_id": 9508, "text": "that is charming! Three daughters married! Ten thousand a year! Oh,"}
{"doc_id": "gutenberg_1342", "para_id": 9509, "text": "Lord! what will become of me? I shall go distracted.”"}
{"doc_id": "gutenberg_1342", "para_id": 9510, "text": "This was enough to prove that her approbation need not be doubted; and"}
{"doc_id": "gutenberg_1342", "para_id": 9511, "text": "Elizabeth, rejoicing that such an effusion was heard only by herself,"}
{"doc_id": "gutenberg_1342", "para_id": 9512, "text": "soon went away. But before she had been three minutes in her own room,"}
{"doc_id": "gutenberg_1342", "para_id": 9513, "text": "“My dearest child,” she cried, “I can think of nothing else. Ten"}
{"doc_id": "gutenberg_1342", "para_id": 9514, "text": "thousand a year, and very likely more! ’Tis as good as a lord! And a"}
{"doc_id": "gutenberg_1342", "para_id": 9515, "text": "special licence--you must and shall be married by a special licence."}
{"doc_id": "gutenberg_1342", "para_id": 9516, "text": "But, my dearest love, tell me what dish Mr. Darcy is particularly fond"}
{"doc_id": "gutenberg_1342", "para_id": 9517, "text": "This was a sad omen of what her mother’s behaviour to the gentleman"}
{"doc_id": "gutenberg_1342", "para_id": 9518, "text": "himself might be; and Elizabeth found that, though in the certain"}
{"doc_id": "gutenberg_1342", "para_id": 9519, "text": "possession of his warmest affection, and secure of her relations’"}
{"doc_id": "gutenberg_1342", "para_id": 9520, "text": "consent, there was still something to be wished for. But the morrow"}
{"doc_id": "gutenberg_1342", "para_id": 9521, "text": "passed off much better than she expected; for Mrs. Bennet luckily stood"}
{"doc_id": "gutenberg_1342", "para_id": 9522, "text": "in such awe of her intended son-in-law, that she ventured not to speak"}
{"doc_id": "gutenberg_1342", "para_id": 9523, "text": "to him, unless it was in her power to offer him any attention, or mark"}
{"doc_id": "gutenberg_1342", "para_id": 9524, "text": "Elizabeth had the satisfaction of seeing her father taking pains to get"}
{"doc_id": "gutenberg_1342", "para_id": 9525, "text": "acquainted with him; and Mr. Bennet soon assured her that he was rising"}
{"doc_id": "gutenberg_1342", "para_id": 9526, "text": "“I admire all my three sons-in-law highly,” said he. “Wickham, perhaps,"}
{"doc_id": "gutenberg_1342", "para_id": 9527, "text": "is my favourite; but I think I shall like _your_ husband quite as well"}
{"doc_id": "gutenberg_1342", "para_id": 9528, "text": "Elizabeth’s spirits soon rising to playfulness again, she wanted Mr."}
{"doc_id": "gutenberg_1342", "para_id": 9529, "text": "Darcy to account for his having ever fallen in love with her. “How could"}
{"doc_id": "gutenberg_1342", "para_id": 9530, "text": "you begin?” said she. “I can comprehend your going on charmingly, when"}
{"doc_id": "gutenberg_1342", "para_id": 9531, "text": "you had once made a beginning; but what could set you off in the first"}
{"doc_id": "gutenberg_1342", "para_id": 9532, "text": "“I cannot fix on the hour, or the spot, or the look, or the words, which"}
{"doc_id": "gutenberg_1342", "para_id": 9533, "text": "laid the foundation. It is too long ago. I was in the middle before I"}
{"doc_id": "gutenberg_1342", "para_id": 9534, "text": "“My beauty you had early withstood, and as for my manners--my behaviour"}
{"doc_id": "gutenberg_1342", "para_id": 9535, "text": "to _you_ was at least always bordering on the uncivil, and I never spoke"}
{"doc_id": "gutenberg_1342", "para_id": 9536, "text": "to you without rather wishing to give you pain than not. Now, be"}
{"doc_id": "gutenberg_1342", "para_id": 9537, "text": "“You may as well call it impertinence at once. It was very little less."}
{"doc_id": "gutenberg_1342", "para_id": 9538, "text": "The fact is, that you were sick of civility, of deference, of officious"}
{"doc_id": "gutenberg_1342", "para_id": 9539, "text": "attention. You were disgusted with the women who were always speaking,"}
{"doc_id": "gutenberg_1342", "para_id": 9540, "text": "and looking, and thinking for _your_ approbation alone. I roused and"}
{"doc_id": "gutenberg_1342", "para_id": 9541, "text": "interested you, because I was so unlike _them_. Had you not been really"}
{"doc_id": "gutenberg_1342", "para_id": 9542, "text": "amiable you would have hated me for it: but in spite of the pains you"}
{"doc_id": "gutenberg_1342", "para_id": 9543, "text": "took to disguise yourself, your feelings were always noble and just; and"}
{"doc_id": "gutenberg_1342", "para_id": 9544, "text": "in your heart you thoroughly despised the persons who so assiduously"}
{"doc_id": "gutenberg_1342", "para_id": 9545, "text": "courted you. There--I have saved you the trouble of accounting for it;"}
{"doc_id": "gutenberg_1342", "para_id": 9546, "text": "and really, all things considered, I begin to think it perfectly"}
{"doc_id": "gutenberg_1342", "para_id": 9547, "text": "reasonable. To be sure you know no actual good of me--but nobody thinks"}
{"doc_id": "gutenberg_1342", "para_id": 9548, "text": "“Was there no good in your affectionate behaviour to Jane, while she was"}
{"doc_id": "gutenberg_1342", "para_id": 9549, "text": "“Dearest Jane! who could have done less for her? But make a virtue of it"}
{"doc_id": "gutenberg_1342", "para_id": 9550, "text": "by all means. My good qualities are under your protection, and you are"}
{"doc_id": "gutenberg_1342", "para_id": 9551, "text": "to exaggerate them as much as possible; and, in return, it belongs to me"}
{"doc_id": "gutenberg_1342", "para_id": 9552, "text": "to find occasions for teasing and quarrelling with you as often as may"}
{"doc_id": "gutenberg_1342", "para_id": 9553, "text": "be; and I shall begin directly, by asking you what made you so unwilling"}
{"doc_id": "gutenberg_1342", "para_id": 9554, "text": "to come to the point at last? What made you so shy of me, when you"}
{"doc_id": "gutenberg_1342", "para_id": 9555, "text": "first called, and afterwards dined here? Why, especially, when you"}
{"doc_id": "gutenberg_1342", "para_id": 9556, "text": "called, did you look as if you did not care about me?”"}
{"doc_id": "gutenberg_1342", "para_id": 9557, "text": "“Because you were grave and silent, and gave me no encouragement.”"}
{"doc_id": "gutenberg_1342", "para_id": 9558, "text": "“You might have talked to me more when you came to dinner.”"}
{"doc_id": "gutenberg_1342", "para_id": 9559, "text": "“How unlucky that you should have a reasonable answer to give, and that"}
{"doc_id": "gutenberg_1342", "para_id": 9560, "text": "I should be so reasonable as to admit it! But I wonder how long you"}
{"doc_id": "gutenberg_1342", "para_id": 9561, "text": "_would_ have gone on, if you had been left to yourself. I wonder when"}
{"doc_id": "gutenberg_1342", "para_id": 9562, "text": "you _would_ have spoken if I had not asked you! My resolution of"}
{"doc_id": "gutenberg_1342", "para_id": 9563, "text": "thanking you for your kindness to Lydia had certainly great effect. _Too"}
{"doc_id": "gutenberg_1342", "para_id": 9564, "text": "much_, I am afraid; for what becomes of the moral, if our comfort"}
{"doc_id": "gutenberg_1342", "para_id": 9565, "text": "springs from a breach of promise, for I ought not to have mentioned the"}
{"doc_id": "gutenberg_1342", "para_id": 9566, "text": "“You need not distress yourself. The moral will be perfectly fair. Lady"}
{"doc_id": "gutenberg_1342", "para_id": 9567, "text": "Catherine’s unjustifiable endeavours to separate us were the means of"}
{"doc_id": "gutenberg_1342", "para_id": 9568, "text": "removing all my doubts. I am not indebted for my present happiness to"}
{"doc_id": "gutenberg_1342", "para_id": 9569, "text": "your eager desire of expressing your gratitude. I was not in a humour to"}
{"doc_id": "gutenberg_1342", "para_id": 9570, "text": "wait for an opening of yours. My aunt’s intelligence had given me hope,"}
{"doc_id": "gutenberg_1342", "para_id": 9571, "text": "“Lady Catherine has been of infinite use, which ought to make her happy,"}
{"doc_id": "gutenberg_1342", "para_id": 9572, "text": "for she loves to be of use. But tell me, what did you come down to"}
{"doc_id": "gutenberg_1342", "para_id": 9573, "text": "Netherfield for? Was it merely to ride to Longbourn and be embarrassed?"}
{"doc_id": "gutenberg_1342", "para_id": 9574, "text": "or had you intended any more serious consequences?”"}
{"doc_id": "gutenberg_1342", "para_id": 9575, "text": "“My real purpose was to see _you_, and to judge, if I could, whether I"}
{"doc_id": "gutenberg_1342", "para_id": 9576, "text": "might ever hope to make you love me. My avowed one, or what I avowed to"}
{"doc_id": "gutenberg_1342", "para_id": 9577, "text": "myself, was to see whether your sister was still partial to Bingley, and"}
{"doc_id": "gutenberg_1342", "para_id": 9578, "text": "if she were, to make the confession to him which I have since made.”"}
{"doc_id": "gutenberg_1342", "para_id": 9579, "text": "“Shall you ever have courage to announce to Lady Catherine what is to"}
{"doc_id": "gutenberg_1342", "para_id": 9580, "text": "“I am more likely to want time than courage, Elizabeth. But it ought to"}
{"doc_id": "gutenberg_1342", "para_id": 9581, "text": "be done; and if you will give me a sheet of paper it shall be done"}
{"doc_id": "gutenberg_1342", "para_id": 9582, "text": "“And if I had not a letter to write myself, I might sit by you, and"}
{"doc_id": "gutenberg_1342", "para_id": 9583, "text": "admire the evenness of your writing, as another young lady once did. But"}
{"doc_id": "gutenberg_1342", "para_id": 9584, "text": "I have an aunt, too, who must not be longer neglected.”"}
{"doc_id": "gutenberg_1342", "para_id": 9585, "text": "From an unwillingness to confess how much her intimacy with Mr. Darcy"}
{"doc_id": "gutenberg_1342", "para_id": 9586, "text": "had been overrated, Elizabeth had never yet answered Mrs. Gardiner’s"}
{"doc_id": "gutenberg_1342", "para_id": 9587, "text": "long letter; but now, having _that_ to communicate which she knew would"}
{"doc_id": "gutenberg_1342", "para_id": 9588, "text": "be most welcome, she was almost ashamed to find that her uncle and aunt"}
{"doc_id": "gutenberg_1342", "para_id": 9589, "text": "had already lost three days of happiness, and immediately wrote as"}
{"doc_id": "gutenberg_1342", "para_id": 9590, "text": "“I would have thanked you before, my dear aunt, as I ought to have done,"}
{"doc_id": "gutenberg_1342", "para_id": 9591, "text": "for your long, kind, satisfactory detail of particulars; but, to say the"}
{"doc_id": "gutenberg_1342", "para_id": 9592, "text": "truth, I was too cross to write. You supposed more than really existed."}
{"doc_id": "gutenberg_1342", "para_id": 9593, "text": "But _now_ suppose as much as you choose; give a loose to your fancy,"}
{"doc_id": "gutenberg_1342", "para_id": 9594, "text": "indulge your imagination in every possible flight which the subject will"}
{"doc_id": "gutenberg_1342", "para_id": 9595, "text": "afford, and unless you believe me actually married, you cannot greatly"}
{"doc_id": "gutenberg_1342", "para_id": 9596, "text": "err. You must write again very soon, and praise him a great deal more"}
{"doc_id": "gutenberg_1342", "para_id": 9597, "text": "than you did in your last. I thank you again and again, for not going to"}
{"doc_id": "gutenberg_1342", "para_id": 9598, "text": "the Lakes. How could I be so silly as to wish it! Your idea of the"}
{"doc_id": "gutenberg_1342", "para_id": 9599, "text": "ponies is delightful. We will go round the park every day. I am the"}
{"doc_id": "gutenberg_1342", "para_id": 9600, "text": "happiest creature in the world. Perhaps other people have said so"}
{"doc_id": "gutenberg_1342", "para_id": 9601, "text": "before, but no one with such justice. I am happier even than Jane; she"}
{"doc_id": "gutenberg_1342", "para_id": 9602, "text": "only smiles, I laugh. Mr. Darcy sends you all the love in the world that"}
{"doc_id": "gutenberg_1342", "para_id": 9603, "text": "can be spared from me. You are all to come to Pemberley at Christmas."}
{"doc_id": "gutenberg_1342", "para_id": 9604, "text": "Mr. Darcy’s letter to Lady Catherine was in a different style, and still"}
{"doc_id": "gutenberg_1342", "para_id": 9605, "text": "different from either was what Mr. Bennet sent to Mr. Collins, in return"}
{"doc_id": "gutenberg_1342", "para_id": 9606, "text": "“I must trouble you once more for congratulations. Elizabeth will"}
{"doc_id": "gutenberg_1342", "para_id": 9607, "text": "soon be the wife of Mr. Darcy. Console Lady Catherine as well as"}
{"doc_id": "gutenberg_1342", "para_id": 9608, "text": "you can. But, if I were you, I would stand by the nephew. He has"}
{"doc_id": "gutenberg_1342", "para_id": 9609, "text": "Miss Bingley’s congratulations to her brother on his approaching"}
{"doc_id": "gutenberg_1342", "para_id": 9610, "text": "marriage were all that was affectionate and insincere. She wrote even to"}
{"doc_id": "gutenberg_1342", "para_id": 9611, "text": "Jane on the occasion, to express her delight, and repeat all her former"}
{"doc_id": "gutenberg_1342", "para_id": 9612, "text": "professions of regard. Jane was not deceived, but she was affected; and"}
{"doc_id": "gutenberg_1342", "para_id": 9613, "text": "though feeling no reliance on her, could not help writing her a much"}
{"doc_id": "gutenberg_1342", "para_id": 9614, "text": "The joy which Miss Darcy expressed on receiving similar information was"}
{"doc_id": "gutenberg_1342", "para_id": 9615, "text": "as sincere as her brother’s in sending it. Four sides of paper were"}
{"doc_id": "gutenberg_1342", "para_id": 9616, "text": "insufficient to contain all her delight, and all her earnest desire of"}
{"doc_id": "gutenberg_1342", "para_id": 9617, "text": "Before any answer could arrive from Mr. Collins, or any congratulations"}
{"doc_id": "gutenberg_1342", "para_id": 9618, "text": "to Elizabeth from his wife, the Longbourn family heard that the"}
{"doc_id": "gutenberg_1342", "para_id": 9619, "text": "Collinses were come themselves to Lucas Lodge. The reason of this"}
{"doc_id": "gutenberg_1342", "para_id": 9620, "text": "sudden removal was soon evident. Lady Catherine had been rendered so"}
{"doc_id": "gutenberg_1342", "para_id": 9621, "text": "exceedingly angry by the contents of her nephew’s letter, that"}
{"doc_id": "gutenberg_1342", "para_id": 9622, "text": "Charlotte, really rejoicing in the match, was anxious to get away till"}
{"doc_id": "gutenberg_1342", "para_id": 9623, "text": "the storm was blown over. At such a moment, the arrival of her friend"}
{"doc_id": "gutenberg_1342", "para_id": 9624, "text": "was a sincere pleasure to Elizabeth, though in the course of their"}
{"doc_id": "gutenberg_1342", "para_id": 9625, "text": "meetings she must sometimes think the pleasure dearly bought, when she"}
{"doc_id": "gutenberg_1342", "para_id": 9626, "text": "saw Mr. Darcy exposed to all the parading and obsequious civility of her"}
{"doc_id": "gutenberg_1342", "para_id": 9627, "text": "husband. He bore it, however, with admirable calmness. He could even"}
{"doc_id": "gutenberg_1342", "para_id": 9628, "text": "listen to Sir William Lucas, when he complimented him on carrying away"}
{"doc_id": "gutenberg_1342", "para_id": 9629, "text": "the brightest jewel of the country, and expressed his hopes of their all"}
{"doc_id": "gutenberg_1342", "para_id": 9630, "text": "meeting frequently at St. James’s, with very decent composure. If he did"}
{"doc_id": "gutenberg_1342", "para_id": 9631, "text": "shrug his shoulders, it was not till Sir William was out of sight."}
{"doc_id": "gutenberg_1342", "para_id": 9632, "text": "Mrs. Philips’s vulgarity was another, and, perhaps, a greater tax on his"}
{"doc_id": "gutenberg_1342", "para_id": 9633, "text": "forbearance; and though Mrs. Philips, as well as her sister, stood in"}
{"doc_id": "gutenberg_1342", "para_id": 9634, "text": "too much awe of him to speak with the familiarity which Bingley’s"}
{"doc_id": "gutenberg_1342", "para_id": 9635, "text": "good-humour encouraged; yet, whenever she _did_ speak, she must be"}
{"doc_id": "gutenberg_1342", "para_id": 9636, "text": "vulgar. Nor was her respect for him, though it made her more quiet, at"}
{"doc_id": "gutenberg_1342", "para_id": 9637, "text": "all likely to make her more elegant. Elizabeth did all she could to"}
{"doc_id": "gutenberg_1342", "para_id": 9638, "text": "shield him from the frequent notice of either, and was ever anxious to"}
{"doc_id": "gutenberg_1342", "para_id": 9639, "text": "keep him to herself, and to those of her family with whom he might"}
{"doc_id": "gutenberg_1342", "para_id": 9640, "text": "converse without mortification; and though the uncomfortable feelings"}
{"doc_id": "gutenberg_1342", "para_id": 9641, "text": "arising from all this took from the season of courtship much of its"}
{"doc_id": "gutenberg_1342", "para_id": 9642, "text": "pleasure, it added to the hope of the future; and she looked forward"}
{"doc_id": "gutenberg_1342", "para_id": 9643, "text": "with delight to the time when they should be removed from society so"}
{"doc_id": "gutenberg_1342", "para_id": 9644, "text": "little pleasing to either, to all the comfort and elegance of their"}
{"doc_id": "gutenberg_1342", "para_id": 9645, "text": "Happy for all her maternal feelings was the day on which Mrs. Bennet got"}
{"doc_id": "gutenberg_1342", "para_id": 9646, "text": "rid of her two most deserving daughters. With what delighted pride she"}
{"doc_id": "gutenberg_1342", "para_id": 9647, "text": "afterwards visited Mrs. Bingley, and talked of Mrs. Darcy, may be"}
{"doc_id": "gutenberg_1342", "para_id": 9648, "text": "guessed. I wish I could say, for the sake of her family, that the"}
{"doc_id": "gutenberg_1342", "para_id": 9649, "text": "accomplishment of her earnest desire in the establishment of so many of"}
{"doc_id": "gutenberg_1342", "para_id": 9650, "text": "her children produced so happy an effect as to make her a sensible,"}
{"doc_id": "gutenberg_1342", "para_id": 9651, "text": "amiable, well-informed woman for the rest of her life; though, perhaps,"}
{"doc_id": "gutenberg_1342", "para_id": 9652, "text": "it was lucky for her husband, who might not have relished domestic"}
{"doc_id": "gutenberg_1342", "para_id": 9653, "text": "felicity in so unusual a form, that she still was occasionally nervous"}
{"doc_id": "gutenberg_1342", "para_id": 9654, "text": "Mr. Bennet missed his second daughter exceedingly; his affection for her"}
{"doc_id": "gutenberg_1342", "para_id": 9655, "text": "drew him oftener from home than anything else could do. He delighted in"}
{"doc_id": "gutenberg_1342", "para_id": 9656, "text": "going to Pemberley, especially when he was least expected."}
{"doc_id": "gutenberg_1342", "para_id": 9657, "text": "Mr. Bingley and Jane remained at Netherfield only a twelvemonth. So near"}
{"doc_id": "gutenberg_1342", "para_id": 9658, "text": "a vicinity to her mother and Meryton relations was not desirable even to"}
{"doc_id": "gutenberg_1342", "para_id": 9659, "text": "_his_ easy temper, or _her_ affectionate heart. The darling wish of his"}
{"doc_id": "gutenberg_1342", "para_id": 9660, "text": "sisters was then gratified: he bought an estate in a neighbouring county"}
{"doc_id": "gutenberg_1342", "para_id": 9661, "text": "to Derbyshire; and Jane and Elizabeth, in addition to every other source"}
{"doc_id": "gutenberg_1342", "para_id": 9662, "text": "of happiness, were within thirty miles of each other."}
{"doc_id": "gutenberg_1342", "para_id": 9663, "text": "Kitty, to her very material advantage, spent the chief of her time with"}
{"doc_id": "gutenberg_1342", "para_id": 9664, "text": "her two elder sisters. In society so superior to what she had generally"}
{"doc_id": "gutenberg_1342", "para_id": 9665, "text": "known, her improvement was great. She was not of so ungovernable a"}
{"doc_id": "gutenberg_1342", "para_id": 9666, "text": "temper as Lydia; and, removed from the influence of Lydia’s example, she"}
{"doc_id": "gutenberg_1342", "para_id": 9667, "text": "became, by proper attention and management, less irritable, less"}
{"doc_id": "gutenberg_1342", "para_id": 9668, "text": "ignorant, and less insipid. From the further disadvantage of Lydia’s"}
{"doc_id": "gutenberg_1342", "para_id": 9669, "text": "society she was of course carefully kept; and though Mrs. Wickham"}
{"doc_id": "gutenberg_1342", "para_id": 9670, "text": "frequently invited her to come and stay with her, with the promise of"}
{"doc_id": "gutenberg_1342", "para_id": 9671, "text": "balls and young men, her father would never consent to her going."}
{"doc_id": "gutenberg_1342", "para_id": 9672, "text": "Mary was the only daughter who remained at home; and she was necessarily"}
{"doc_id": "gutenberg_1342", "para_id": 9673, "text": "drawn from the pursuit of accomplishments by Mrs. Bennet’s being quite"}
{"doc_id": "gutenberg_1342", "para_id": 9674, "text": "unable to sit alone. Mary was obliged to mix more with the world, but"}
{"doc_id": "gutenberg_1342", "para_id": 9675, "text": "she could still moralize over every morning visit; and as she was no"}
{"doc_id": "gutenberg_1342", "para_id": 9676, "text": "longer mortified by comparisons between her sisters’ beauty and her own,"}
{"doc_id": "gutenberg_1342", "para_id": 9677, "text": "it was suspected by her father that she submitted to the change without"}
{"doc_id": "gutenberg_1342", "para_id": 9678, "text": "As for Wickham and Lydia, their characters suffered no revolution from"}
{"doc_id": "gutenberg_1342", "para_id": 9679, "text": "the marriage of her sisters. He bore with philosophy the conviction that"}
{"doc_id": "gutenberg_1342", "para_id": 9680, "text": "Elizabeth must now become acquainted with whatever of his ingratitude"}
{"doc_id": "gutenberg_1342", "para_id": 9681, "text": "and falsehood had before been unknown to her; and, in spite of"}
{"doc_id": "gutenberg_1342", "para_id": 9682, "text": "everything, was not wholly without hope that Darcy might yet be"}
{"doc_id": "gutenberg_1342", "para_id": 9683, "text": "prevailed on to make his fortune. The congratulatory letter which"}
{"doc_id": "gutenberg_1342", "para_id": 9684, "text": "Elizabeth received from Lydia on her marriage explained to her that, by"}
{"doc_id": "gutenberg_1342", "para_id": 9685, "text": "his wife at least, if not by himself, such a hope was cherished. The"}
{"doc_id": "gutenberg_1342", "para_id": 9686, "text": "“I wish you joy. If you love Mr. Darcy half so well as I do my dear"}
{"doc_id": "gutenberg_1342", "para_id": 9687, "text": "Wickham, you must be very happy. It is a great comfort to have you"}
{"doc_id": "gutenberg_1342", "para_id": 9688, "text": "so rich; and when you have nothing else to do, I hope you will"}
{"doc_id": "gutenberg_1342", "para_id": 9689, "text": "think of us. I am sure Wickham would like a place at court very"}
{"doc_id": "gutenberg_1342", "para_id": 9690, "text": "much; and I do not think we shall have quite money enough to live"}
{"doc_id": "gutenberg_1342", "para_id": 9691, "text": "upon without some help. Any place would do of about three or four"}
{"doc_id": "gutenberg_1342", "para_id": 9692, "text": "hundred a year; but, however, do not speak to Mr. Darcy about it,"}
{"doc_id": "gutenberg_1342", "para_id": 9693, "text": "As it happened that Elizabeth had much rather not, she endeavoured in"}
{"doc_id": "gutenberg_1342", "para_id": 9694, "text": "her answer to put an end to every entreaty and expectation of the kind."}
{"doc_id": "gutenberg_1342", "para_id": 9695, "text": "Such relief, however, as it was in her power to afford, by the practice"}
{"doc_id": "gutenberg_1342", "para_id": 9696, "text": "of what might be called economy in her own private expenses, she"}
{"doc_id": "gutenberg_1342", "para_id": 9697, "text": "frequently sent them. It had always been evident to her that such an"}
{"doc_id": "gutenberg_1342", "para_id": 9698, "text": "income as theirs, under the direction of two persons so extravagant in"}
{"doc_id": "gutenberg_1342", "para_id": 9699, "text": "their wants, and heedless of the future, must be very insufficient to"}
{"doc_id": "gutenberg_1342", "para_id": 9700, "text": "their support; and whenever they changed their quarters, either Jane or"}
{"doc_id": "gutenberg_1342", "para_id": 9701, "text": "herself were sure of being applied to for some little assistance towards"}
{"doc_id": "gutenberg_1342", "para_id": 9702, "text": "discharging their bills. Their manner of living, even when the"}
{"doc_id": "gutenberg_1342", "para_id": 9703, "text": "restoration of peace dismissed them to a home, was unsettled in the"}
{"doc_id": "gutenberg_1342", "para_id": 9704, "text": "extreme. They were always moving from place to place in quest of a"}
{"doc_id": "gutenberg_1342", "para_id": 9705, "text": "cheap situation, and always spending more than they ought. His affection"}
{"doc_id": "gutenberg_1342", "para_id": 9706, "text": "for her soon sunk into indifference: hers lasted a little longer; and,"}
{"doc_id": "gutenberg_1342", "para_id": 9707, "text": "in spite of her youth and her manners, she retained all the claims to"}
{"doc_id": "gutenberg_1342", "para_id": 9708, "text": "reputation which her marriage had given her. Though Darcy could never"}
{"doc_id": "gutenberg_1342", "para_id": 9709, "text": "receive _him_ at Pemberley, yet, for Elizabeth’s sake, he assisted him"}
{"doc_id": "gutenberg_1342", "para_id": 9710, "text": "further in his profession. Lydia was occasionally a visitor there, when"}
{"doc_id": "gutenberg_1342", "para_id": 9711, "text": "her husband was gone to enjoy himself in London or Bath; and with the"}
{"doc_id": "gutenberg_1342", "para_id": 9712, "text": "Bingleys they both of them frequently stayed so long, that even"}
{"doc_id": "gutenberg_1342", "para_id": 9713, "text": "Bingley’s good-humour was overcome, and he proceeded so far as to _talk_"}
{"doc_id": "gutenberg_1342", "para_id": 9714, "text": "Miss Bingley was very deeply mortified by Darcy’s marriage; but as she"}
{"doc_id": "gutenberg_1342", "para_id": 9715, "text": "thought it advisable to retain the right of visiting at Pemberley, she"}
{"doc_id": "gutenberg_1342", "para_id": 9716, "text": "dropped all her resentment; was fonder than ever of Georgiana, almost as"}
{"doc_id": "gutenberg_1342", "para_id": 9717, "text": "attentive to Darcy as heretofore, and paid off every arrear of civility"}
{"doc_id": "gutenberg_1342", "para_id": 9718, "text": "Pemberley was now Georgiana’s home; and the attachment of the sisters"}
{"doc_id": "gutenberg_1342", "para_id": 9719, "text": "was exactly what Darcy had hoped to see. They were able to love each"}
{"doc_id": "gutenberg_1342", "para_id": 9720, "text": "other, even as well as they intended. Georgiana had the highest opinion"}
{"doc_id": "gutenberg_1342", "para_id": 9721, "text": "in the world of Elizabeth; though at first she often listened with an"}
{"doc_id": "gutenberg_1342", "para_id": 9722, "text": "astonishment bordering on alarm at her lively, sportive manner of"}
{"doc_id": "gutenberg_1342", "para_id": 9723, "text": "talking to her brother. He, who had always inspired in herself a respect"}
{"doc_id": "gutenberg_1342", "para_id": 9724, "text": "which almost overcame her affection, she now saw the object of open"}
{"doc_id": "gutenberg_1342", "para_id": 9725, "text": "pleasantry. Her mind received knowledge which had never before fallen in"}
{"doc_id": "gutenberg_1342", "para_id": 9726, "text": "her way. By Elizabeth’s instructions she began to comprehend that a"}
{"doc_id": "gutenberg_1342", "para_id": 9727, "text": "woman may take liberties with her husband, which a brother will not"}
{"doc_id": "gutenberg_1342", "para_id": 9728, "text": "always allow in a sister more than ten years younger than himself."}
{"doc_id": "gutenberg_1342", "para_id": 9729, "text": "Lady Catherine was extremely indignant on the marriage of her nephew;"}
{"doc_id": "gutenberg_1342", "para_id": 9730, "text": "and as she gave way to all the genuine frankness of her character, in"}
{"doc_id": "gutenberg_1342", "para_id": 9731, "text": "her reply to the letter which announced its arrangement, she sent him"}
{"doc_id": "gutenberg_1342", "para_id": 9732, "text": "language so very abusive, especially of Elizabeth, that for some time"}
{"doc_id": "gutenberg_1342", "para_id": 9733, "text": "all intercourse was at an end. But at length, by Elizabeth’s persuasion,"}
{"doc_id": "gutenberg_1342", "para_id": 9734, "text": "he was prevailed on to overlook the offence, and seek a reconciliation;"}
{"doc_id": "gutenberg_1342", "para_id": 9735, "text": "and, after a little further resistance on the part of his aunt, her"}
{"doc_id": "gutenberg_1342", "para_id": 9736, "text": "resentment gave way, either to her affection for him, or her curiosity"}
{"doc_id": "gutenberg_1342", "para_id": 9737, "text": "to see how his wife conducted herself; and she condescended to wait on"}
{"doc_id": "gutenberg_1342", "para_id": 9738, "text": "them at Pemberley, in spite of that pollution which its woods had"}
{"doc_id": "gutenberg_1342", "para_id": 9739, "text": "received, not merely from the presence of such a mistress, but the"}
{"doc_id": "gutenberg_1342", "para_id": 9740, "text": "With the Gardiners they were always on the most intimate terms. Darcy,"}
{"doc_id": "gutenberg_1342", "para_id": 9741, "text": "as well as Elizabeth, really loved them; and they were both ever"}
{"doc_id": "gutenberg_1342", "para_id": 9742, "text": "sensible of the warmest gratitude towards the persons who, by bringing"}
{"doc_id": "gutenberg_1342", "para_id": 9743, "text": "her into Derbyshire, had been the means of uniting them."}
